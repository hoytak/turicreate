/* Copyright Â© 2017 Apple Inc. All rights reserved.
 *
 * Use of this source code is governed by a BSD-3-clause license that can
 * be found in the LICENSE.txt file or at https://opensource.org/licenses/BSD-3-Clause
 */
// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: NeuralNetwork.proto

#ifndef PROTOBUF_NeuralNetwork_2eproto__INCLUDED
#define PROTOBUF_NeuralNetwork_2eproto__INCLUDED

#include <string>

#include <protobuf/stubs/common.h>

#if GOOGLE_PROTOBUF_VERSION < 3001000
#error This file was generated by a newer version of protoc which is
#error incompatible with your Protocol Buffer headers.  Please update
#error your headers.
#endif
#if 3001000 < GOOGLE_PROTOBUF_MIN_PROTOC_VERSION
#error This file was generated by an older version of protoc which is
#error incompatible with your Protocol Buffer headers.  Please
#error regenerate this file with a newer version of protoc.
#endif

#include <protobuf/arena.h>
#include <protobuf/arenastring.h>
#include <protobuf/generated_message_util.h>
#include <protobuf/message_lite.h>
#include <protobuf/repeated_field.h>
#include <protobuf/extension_set.h>
#include <protobuf/generated_enum_util.h>
#include "DataStructures.pb.h"  // IWYU pragma: export
// @@protoc_insertion_point(includes)

namespace CoreML {
namespace Specification {

// Internal implementation detail -- do not call these.
void protobuf_AddDesc_NeuralNetwork_2eproto();
void protobuf_InitDefaults_NeuralNetwork_2eproto();
void protobuf_AssignDesc_NeuralNetwork_2eproto();
void protobuf_ShutdownFile_NeuralNetwork_2eproto();

class ActivationELU;
class ActivationLeakyReLU;
class ActivationLinear;
class ActivationPReLU;
class ActivationParametricSoftplus;
class ActivationParams;
class ActivationReLU;
class ActivationScaledTanh;
class ActivationSigmoid;
class ActivationSigmoidHard;
class ActivationSoftplus;
class ActivationSoftsign;
class ActivationTanh;
class ActivationThresholdedReLU;
class AddLayerParams;
class AverageLayerParams;
class BatchnormLayerParams;
class BiDirectionalLSTMLayerParams;
class BiasLayerParams;
class BorderAmounts;
class BorderAmounts_EdgeSizes;
class ConcatLayerParams;
class ConvolutionLayerParams;
class CropLayerParams;
class DotProductLayerParams;
class EmbeddingLayerParams;
class FlattenLayerParams;
class GRULayerParams;
class InnerProductLayerParams;
class L2NormalizeLayerParams;
class LRNLayerParams;
class LSTMParams;
class LSTMWeightParams;
class LoadConstantLayerParams;
class MaxLayerParams;
class MeanVarianceNormalizeLayerParams;
class MinLayerParams;
class MultiplyLayerParams;
class NeuralNetwork;
class NeuralNetworkClassifier;
class NeuralNetworkImageScaler;
class NeuralNetworkLayer;
class NeuralNetworkMeanImage;
class NeuralNetworkPreprocessing;
class NeuralNetworkRegressor;
class PaddingLayerParams;
class PaddingLayerParams_PaddingConstant;
class PaddingLayerParams_PaddingReflection;
class PaddingLayerParams_PaddingReplication;
class PermuteLayerParams;
class PoolingLayerParams;
class PoolingLayerParams_ValidCompletePadding;
class ReduceLayerParams;
class ReshapeLayerParams;
class SamePadding;
class ScaleLayerParams;
class SequenceRepeatLayerParams;
class SimpleRecurrentLayerParams;
class SoftmaxLayerParams;
class SplitLayerParams;
class UnaryFunctionLayerParams;
class UniDirectionalLSTMLayerParams;
class UpsampleLayerParams;
class ValidPadding;
class WeightParams;

enum SamePadding_SamePaddingMode {
  SamePadding_SamePaddingMode_BOTTOM_RIGHT_HEAVY = 0,
  SamePadding_SamePaddingMode_TOP_LEFT_HEAVY = 1,
  SamePadding_SamePaddingMode_SamePadding_SamePaddingMode_INT_MIN_SENTINEL_DO_NOT_USE_ = ::google::protobuf::kint32min,
  SamePadding_SamePaddingMode_SamePadding_SamePaddingMode_INT_MAX_SENTINEL_DO_NOT_USE_ = ::google::protobuf::kint32max
};
bool SamePadding_SamePaddingMode_IsValid(int value);
const SamePadding_SamePaddingMode SamePadding_SamePaddingMode_SamePaddingMode_MIN = SamePadding_SamePaddingMode_BOTTOM_RIGHT_HEAVY;
const SamePadding_SamePaddingMode SamePadding_SamePaddingMode_SamePaddingMode_MAX = SamePadding_SamePaddingMode_TOP_LEFT_HEAVY;
const int SamePadding_SamePaddingMode_SamePaddingMode_ARRAYSIZE = SamePadding_SamePaddingMode_SamePaddingMode_MAX + 1;

enum PoolingLayerParams_PoolingType {
  PoolingLayerParams_PoolingType_MAX = 0,
  PoolingLayerParams_PoolingType_AVERAGE = 1,
  PoolingLayerParams_PoolingType_L2 = 2,
  PoolingLayerParams_PoolingType_PoolingLayerParams_PoolingType_INT_MIN_SENTINEL_DO_NOT_USE_ = ::google::protobuf::kint32min,
  PoolingLayerParams_PoolingType_PoolingLayerParams_PoolingType_INT_MAX_SENTINEL_DO_NOT_USE_ = ::google::protobuf::kint32max
};
bool PoolingLayerParams_PoolingType_IsValid(int value);
const PoolingLayerParams_PoolingType PoolingLayerParams_PoolingType_PoolingType_MIN = PoolingLayerParams_PoolingType_MAX;
const PoolingLayerParams_PoolingType PoolingLayerParams_PoolingType_PoolingType_MAX = PoolingLayerParams_PoolingType_L2;
const int PoolingLayerParams_PoolingType_PoolingType_ARRAYSIZE = PoolingLayerParams_PoolingType_PoolingType_MAX + 1;

enum UnaryFunctionLayerParams_Operation {
  UnaryFunctionLayerParams_Operation_SQRT = 0,
  UnaryFunctionLayerParams_Operation_RSQRT = 1,
  UnaryFunctionLayerParams_Operation_INVERSE = 2,
  UnaryFunctionLayerParams_Operation_POWER = 3,
  UnaryFunctionLayerParams_Operation_EXP = 4,
  UnaryFunctionLayerParams_Operation_LOG = 5,
  UnaryFunctionLayerParams_Operation_ABS = 6,
  UnaryFunctionLayerParams_Operation_THRESHOLD = 7,
  UnaryFunctionLayerParams_Operation_UnaryFunctionLayerParams_Operation_INT_MIN_SENTINEL_DO_NOT_USE_ = ::google::protobuf::kint32min,
  UnaryFunctionLayerParams_Operation_UnaryFunctionLayerParams_Operation_INT_MAX_SENTINEL_DO_NOT_USE_ = ::google::protobuf::kint32max
};
bool UnaryFunctionLayerParams_Operation_IsValid(int value);
const UnaryFunctionLayerParams_Operation UnaryFunctionLayerParams_Operation_Operation_MIN = UnaryFunctionLayerParams_Operation_SQRT;
const UnaryFunctionLayerParams_Operation UnaryFunctionLayerParams_Operation_Operation_MAX = UnaryFunctionLayerParams_Operation_THRESHOLD;
const int UnaryFunctionLayerParams_Operation_Operation_ARRAYSIZE = UnaryFunctionLayerParams_Operation_Operation_MAX + 1;

enum FlattenLayerParams_FlattenOrder {
  FlattenLayerParams_FlattenOrder_CHANNEL_FIRST = 0,
  FlattenLayerParams_FlattenOrder_CHANNEL_LAST = 1,
  FlattenLayerParams_FlattenOrder_FlattenLayerParams_FlattenOrder_INT_MIN_SENTINEL_DO_NOT_USE_ = ::google::protobuf::kint32min,
  FlattenLayerParams_FlattenOrder_FlattenLayerParams_FlattenOrder_INT_MAX_SENTINEL_DO_NOT_USE_ = ::google::protobuf::kint32max
};
bool FlattenLayerParams_FlattenOrder_IsValid(int value);
const FlattenLayerParams_FlattenOrder FlattenLayerParams_FlattenOrder_FlattenOrder_MIN = FlattenLayerParams_FlattenOrder_CHANNEL_FIRST;
const FlattenLayerParams_FlattenOrder FlattenLayerParams_FlattenOrder_FlattenOrder_MAX = FlattenLayerParams_FlattenOrder_CHANNEL_LAST;
const int FlattenLayerParams_FlattenOrder_FlattenOrder_ARRAYSIZE = FlattenLayerParams_FlattenOrder_FlattenOrder_MAX + 1;

enum ReshapeLayerParams_ReshapeOrder {
  ReshapeLayerParams_ReshapeOrder_CHANNEL_FIRST = 0,
  ReshapeLayerParams_ReshapeOrder_CHANNEL_LAST = 1,
  ReshapeLayerParams_ReshapeOrder_ReshapeLayerParams_ReshapeOrder_INT_MIN_SENTINEL_DO_NOT_USE_ = ::google::protobuf::kint32min,
  ReshapeLayerParams_ReshapeOrder_ReshapeLayerParams_ReshapeOrder_INT_MAX_SENTINEL_DO_NOT_USE_ = ::google::protobuf::kint32max
};
bool ReshapeLayerParams_ReshapeOrder_IsValid(int value);
const ReshapeLayerParams_ReshapeOrder ReshapeLayerParams_ReshapeOrder_ReshapeOrder_MIN = ReshapeLayerParams_ReshapeOrder_CHANNEL_FIRST;
const ReshapeLayerParams_ReshapeOrder ReshapeLayerParams_ReshapeOrder_ReshapeOrder_MAX = ReshapeLayerParams_ReshapeOrder_CHANNEL_LAST;
const int ReshapeLayerParams_ReshapeOrder_ReshapeOrder_ARRAYSIZE = ReshapeLayerParams_ReshapeOrder_ReshapeOrder_MAX + 1;

enum ReduceLayerParams_ReduceOperation {
  ReduceLayerParams_ReduceOperation_SUM = 0,
  ReduceLayerParams_ReduceOperation_AVG = 1,
  ReduceLayerParams_ReduceOperation_PROD = 2,
  ReduceLayerParams_ReduceOperation_LOGSUM = 3,
  ReduceLayerParams_ReduceOperation_SUMSQUARE = 4,
  ReduceLayerParams_ReduceOperation_L1 = 5,
  ReduceLayerParams_ReduceOperation_L2 = 6,
  ReduceLayerParams_ReduceOperation_ReduceLayerParams_ReduceOperation_INT_MIN_SENTINEL_DO_NOT_USE_ = ::google::protobuf::kint32min,
  ReduceLayerParams_ReduceOperation_ReduceLayerParams_ReduceOperation_INT_MAX_SENTINEL_DO_NOT_USE_ = ::google::protobuf::kint32max
};
bool ReduceLayerParams_ReduceOperation_IsValid(int value);
const ReduceLayerParams_ReduceOperation ReduceLayerParams_ReduceOperation_ReduceOperation_MIN = ReduceLayerParams_ReduceOperation_SUM;
const ReduceLayerParams_ReduceOperation ReduceLayerParams_ReduceOperation_ReduceOperation_MAX = ReduceLayerParams_ReduceOperation_L2;
const int ReduceLayerParams_ReduceOperation_ReduceOperation_ARRAYSIZE = ReduceLayerParams_ReduceOperation_ReduceOperation_MAX + 1;

// ===================================================================

class NeuralNetwork : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.NeuralNetwork) */ {
 public:
  NeuralNetwork();
  virtual ~NeuralNetwork();

  NeuralNetwork(const NeuralNetwork& from);

  inline NeuralNetwork& operator=(const NeuralNetwork& from) {
    CopyFrom(from);
    return *this;
  }

  static const NeuralNetwork& default_instance();

  static const NeuralNetwork* internal_default_instance();

  void Swap(NeuralNetwork* other);

  // implements Message ----------------------------------------------

  inline NeuralNetwork* New() const { return New(NULL); }

  NeuralNetwork* New(::google::protobuf::Arena* arena) const;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from);
  void CopyFrom(const NeuralNetwork& from);
  void MergeFrom(const NeuralNetwork& from);
  void Clear();
  bool IsInitialized() const;

  size_t ByteSizeLong() const;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input);
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const;
  void DiscardUnknownFields();
  int GetCachedSize() const { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(NeuralNetwork* other);
  void UnsafeMergeFrom(const NeuralNetwork& from);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return _arena_ptr_;
  }
  inline ::google::protobuf::Arena* MaybeArenaPtr() const {
    return _arena_ptr_;
  }
  public:

  ::std::string GetTypeName() const;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated .CoreML.Specification.NeuralNetworkLayer layers = 1;
  int layers_size() const;
  void clear_layers();
  static const int kLayersFieldNumber = 1;
  const ::CoreML::Specification::NeuralNetworkLayer& layers(int index) const;
  ::CoreML::Specification::NeuralNetworkLayer* mutable_layers(int index);
  ::CoreML::Specification::NeuralNetworkLayer* add_layers();
  ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkLayer >*
      mutable_layers();
  const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkLayer >&
      layers() const;

  // repeated .CoreML.Specification.NeuralNetworkPreprocessing preprocessing = 2;
  int preprocessing_size() const;
  void clear_preprocessing();
  static const int kPreprocessingFieldNumber = 2;
  const ::CoreML::Specification::NeuralNetworkPreprocessing& preprocessing(int index) const;
  ::CoreML::Specification::NeuralNetworkPreprocessing* mutable_preprocessing(int index);
  ::CoreML::Specification::NeuralNetworkPreprocessing* add_preprocessing();
  ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkPreprocessing >*
      mutable_preprocessing();
  const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkPreprocessing >&
      preprocessing() const;

  // @@protoc_insertion_point(class_scope:CoreML.Specification.NeuralNetwork)
 private:

  ::google::protobuf::internal::ArenaStringPtr _unknown_fields_;
  ::google::protobuf::Arena* _arena_ptr_;

  ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkLayer > layers_;
  ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkPreprocessing > preprocessing_;
  mutable int _cached_size_;
  friend void  protobuf_InitDefaults_NeuralNetwork_2eproto_impl();
  friend void  protobuf_AddDesc_NeuralNetwork_2eproto_impl();
  friend void protobuf_AssignDesc_NeuralNetwork_2eproto();
  friend void protobuf_ShutdownFile_NeuralNetwork_2eproto();

  void InitAsDefaultInstance();
};
extern ::google::protobuf::internal::ExplicitlyConstructed<NeuralNetwork> NeuralNetwork_default_instance_;

// -------------------------------------------------------------------

class NeuralNetworkImageScaler : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.NeuralNetworkImageScaler) */ {
 public:
  NeuralNetworkImageScaler();
  virtual ~NeuralNetworkImageScaler();

  NeuralNetworkImageScaler(const NeuralNetworkImageScaler& from);

  inline NeuralNetworkImageScaler& operator=(const NeuralNetworkImageScaler& from) {
    CopyFrom(from);
    return *this;
  }

  static const NeuralNetworkImageScaler& default_instance();

  static const NeuralNetworkImageScaler* internal_default_instance();

  void Swap(NeuralNetworkImageScaler* other);

  // implements Message ----------------------------------------------

  inline NeuralNetworkImageScaler* New() const { return New(NULL); }

  NeuralNetworkImageScaler* New(::google::protobuf::Arena* arena) const;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from);
  void CopyFrom(const NeuralNetworkImageScaler& from);
  void MergeFrom(const NeuralNetworkImageScaler& from);
  void Clear();
  bool IsInitialized() const;

  size_t ByteSizeLong() const;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input);
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const;
  void DiscardUnknownFields();
  int GetCachedSize() const { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(NeuralNetworkImageScaler* other);
  void UnsafeMergeFrom(const NeuralNetworkImageScaler& from);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return _arena_ptr_;
  }
  inline ::google::protobuf::Arena* MaybeArenaPtr() const {
    return _arena_ptr_;
  }
  public:

  ::std::string GetTypeName() const;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // optional float channelScale = 10;
  void clear_channelscale();
  static const int kChannelScaleFieldNumber = 10;
  float channelscale() const;
  void set_channelscale(float value);

  // optional float blueBias = 20;
  void clear_bluebias();
  static const int kBlueBiasFieldNumber = 20;
  float bluebias() const;
  void set_bluebias(float value);

  // optional float greenBias = 21;
  void clear_greenbias();
  static const int kGreenBiasFieldNumber = 21;
  float greenbias() const;
  void set_greenbias(float value);

  // optional float redBias = 22;
  void clear_redbias();
  static const int kRedBiasFieldNumber = 22;
  float redbias() const;
  void set_redbias(float value);

  // optional float grayBias = 30;
  void clear_graybias();
  static const int kGrayBiasFieldNumber = 30;
  float graybias() const;
  void set_graybias(float value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.NeuralNetworkImageScaler)
 private:

  ::google::protobuf::internal::ArenaStringPtr _unknown_fields_;
  ::google::protobuf::Arena* _arena_ptr_;

  float channelscale_;
  float bluebias_;
  float greenbias_;
  float redbias_;
  float graybias_;
  mutable int _cached_size_;
  friend void  protobuf_InitDefaults_NeuralNetwork_2eproto_impl();
  friend void  protobuf_AddDesc_NeuralNetwork_2eproto_impl();
  friend void protobuf_AssignDesc_NeuralNetwork_2eproto();
  friend void protobuf_ShutdownFile_NeuralNetwork_2eproto();

  void InitAsDefaultInstance();
};
extern ::google::protobuf::internal::ExplicitlyConstructed<NeuralNetworkImageScaler> NeuralNetworkImageScaler_default_instance_;

// -------------------------------------------------------------------

class NeuralNetworkMeanImage : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.NeuralNetworkMeanImage) */ {
 public:
  NeuralNetworkMeanImage();
  virtual ~NeuralNetworkMeanImage();

  NeuralNetworkMeanImage(const NeuralNetworkMeanImage& from);

  inline NeuralNetworkMeanImage& operator=(const NeuralNetworkMeanImage& from) {
    CopyFrom(from);
    return *this;
  }

  static const NeuralNetworkMeanImage& default_instance();

  static const NeuralNetworkMeanImage* internal_default_instance();

  void Swap(NeuralNetworkMeanImage* other);

  // implements Message ----------------------------------------------

  inline NeuralNetworkMeanImage* New() const { return New(NULL); }

  NeuralNetworkMeanImage* New(::google::protobuf::Arena* arena) const;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from);
  void CopyFrom(const NeuralNetworkMeanImage& from);
  void MergeFrom(const NeuralNetworkMeanImage& from);
  void Clear();
  bool IsInitialized() const;

  size_t ByteSizeLong() const;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input);
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const;
  void DiscardUnknownFields();
  int GetCachedSize() const { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(NeuralNetworkMeanImage* other);
  void UnsafeMergeFrom(const NeuralNetworkMeanImage& from);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return _arena_ptr_;
  }
  inline ::google::protobuf::Arena* MaybeArenaPtr() const {
    return _arena_ptr_;
  }
  public:

  ::std::string GetTypeName() const;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated float meanImage = 1;
  int meanimage_size() const;
  void clear_meanimage();
  static const int kMeanImageFieldNumber = 1;
  float meanimage(int index) const;
  void set_meanimage(int index, float value);
  void add_meanimage(float value);
  const ::google::protobuf::RepeatedField< float >&
      meanimage() const;
  ::google::protobuf::RepeatedField< float >*
      mutable_meanimage();

  // @@protoc_insertion_point(class_scope:CoreML.Specification.NeuralNetworkMeanImage)
 private:

  ::google::protobuf::internal::ArenaStringPtr _unknown_fields_;
  ::google::protobuf::Arena* _arena_ptr_;

  ::google::protobuf::RepeatedField< float > meanimage_;
  mutable int _meanimage_cached_byte_size_;
  mutable int _cached_size_;
  friend void  protobuf_InitDefaults_NeuralNetwork_2eproto_impl();
  friend void  protobuf_AddDesc_NeuralNetwork_2eproto_impl();
  friend void protobuf_AssignDesc_NeuralNetwork_2eproto();
  friend void protobuf_ShutdownFile_NeuralNetwork_2eproto();

  void InitAsDefaultInstance();
};
extern ::google::protobuf::internal::ExplicitlyConstructed<NeuralNetworkMeanImage> NeuralNetworkMeanImage_default_instance_;

// -------------------------------------------------------------------

class NeuralNetworkPreprocessing : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.NeuralNetworkPreprocessing) */ {
 public:
  NeuralNetworkPreprocessing();
  virtual ~NeuralNetworkPreprocessing();

  NeuralNetworkPreprocessing(const NeuralNetworkPreprocessing& from);

  inline NeuralNetworkPreprocessing& operator=(const NeuralNetworkPreprocessing& from) {
    CopyFrom(from);
    return *this;
  }

  static const NeuralNetworkPreprocessing& default_instance();

  enum PreprocessorCase {
    kScaler = 10,
    kMeanImage = 11,
    PREPROCESSOR_NOT_SET = 0,
  };

  static const NeuralNetworkPreprocessing* internal_default_instance();

  void Swap(NeuralNetworkPreprocessing* other);

  // implements Message ----------------------------------------------

  inline NeuralNetworkPreprocessing* New() const { return New(NULL); }

  NeuralNetworkPreprocessing* New(::google::protobuf::Arena* arena) const;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from);
  void CopyFrom(const NeuralNetworkPreprocessing& from);
  void MergeFrom(const NeuralNetworkPreprocessing& from);
  void Clear();
  bool IsInitialized() const;

  size_t ByteSizeLong() const;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input);
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const;
  void DiscardUnknownFields();
  int GetCachedSize() const { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(NeuralNetworkPreprocessing* other);
  void UnsafeMergeFrom(const NeuralNetworkPreprocessing& from);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return _arena_ptr_;
  }
  inline ::google::protobuf::Arena* MaybeArenaPtr() const {
    return _arena_ptr_;
  }
  public:

  ::std::string GetTypeName() const;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // optional string featureName = 1;
  void clear_featurename();
  static const int kFeatureNameFieldNumber = 1;
  const ::std::string& featurename() const;
  void set_featurename(const ::std::string& value);
  void set_featurename(const char* value);
  void set_featurename(const char* value, size_t size);
  ::std::string* mutable_featurename();
  ::std::string* release_featurename();
  void set_allocated_featurename(::std::string* featurename);

  // optional .CoreML.Specification.NeuralNetworkImageScaler scaler = 10;
  bool has_scaler() const;
  void clear_scaler();
  static const int kScalerFieldNumber = 10;
  const ::CoreML::Specification::NeuralNetworkImageScaler& scaler() const;
  ::CoreML::Specification::NeuralNetworkImageScaler* mutable_scaler();
  ::CoreML::Specification::NeuralNetworkImageScaler* release_scaler();
  void set_allocated_scaler(::CoreML::Specification::NeuralNetworkImageScaler* scaler);

  // optional .CoreML.Specification.NeuralNetworkMeanImage meanImage = 11;
  bool has_meanimage() const;
  void clear_meanimage();
  static const int kMeanImageFieldNumber = 11;
  const ::CoreML::Specification::NeuralNetworkMeanImage& meanimage() const;
  ::CoreML::Specification::NeuralNetworkMeanImage* mutable_meanimage();
  ::CoreML::Specification::NeuralNetworkMeanImage* release_meanimage();
  void set_allocated_meanimage(::CoreML::Specification::NeuralNetworkMeanImage* meanimage);

  PreprocessorCase preprocessor_case() const;
  // @@protoc_insertion_point(class_scope:CoreML.Specification.NeuralNetworkPreprocessing)
 private:
  inline void set_has_scaler();
  inline void set_has_meanimage();

  inline bool has_preprocessor() const;
  void clear_preprocessor();
  inline void clear_has_preprocessor();

  ::google::protobuf::internal::ArenaStringPtr _unknown_fields_;
  ::google::protobuf::Arena* _arena_ptr_;

  ::google::protobuf::internal::ArenaStringPtr featurename_;
  union PreprocessorUnion {
    PreprocessorUnion() {}
    ::CoreML::Specification::NeuralNetworkImageScaler* scaler_;
    ::CoreML::Specification::NeuralNetworkMeanImage* meanimage_;
  } preprocessor_;
  mutable int _cached_size_;
  ::google::protobuf::uint32 _oneof_case_[1];

  friend void  protobuf_InitDefaults_NeuralNetwork_2eproto_impl();
  friend void  protobuf_AddDesc_NeuralNetwork_2eproto_impl();
  friend void protobuf_AssignDesc_NeuralNetwork_2eproto();
  friend void protobuf_ShutdownFile_NeuralNetwork_2eproto();

  void InitAsDefaultInstance();
};
extern ::google::protobuf::internal::ExplicitlyConstructed<NeuralNetworkPreprocessing> NeuralNetworkPreprocessing_default_instance_;

// -------------------------------------------------------------------

class ActivationReLU : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ActivationReLU) */ {
 public:
  ActivationReLU();
  virtual ~ActivationReLU();

  ActivationReLU(const ActivationReLU& from);

  inline ActivationReLU& operator=(const ActivationReLU& from) {
    CopyFrom(from);
    return *this;
  }

  static const ActivationReLU& default_instance();

  static const ActivationReLU* internal_default_instance();

  void Swap(ActivationReLU* other);

  // implements Message ----------------------------------------------

  inline ActivationReLU* New() const { return New(NULL); }

  ActivationReLU* New(::google::protobuf::Arena* arena) const;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from);
  void CopyFrom(const ActivationReLU& from);
  void MergeFrom(const ActivationReLU& from);
  void Clear();
  bool IsInitialized() const;

  size_t ByteSizeLong() const;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input);
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const;
  void DiscardUnknownFields();
  int GetCachedSize() const { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ActivationReLU* other);
  void UnsafeMergeFrom(const ActivationReLU& from);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return _arena_ptr_;
  }
  inline ::google::protobuf::Arena* MaybeArenaPtr() const {
    return _arena_ptr_;
  }
  public:

  ::std::string GetTypeName() const;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ActivationReLU)
 private:

  ::google::protobuf::internal::ArenaStringPtr _unknown_fields_;
  ::google::protobuf::Arena* _arena_ptr_;

  mutable int _cached_size_;
  friend void  protobuf_InitDefaults_NeuralNetwork_2eproto_impl();
  friend void  protobuf_AddDesc_NeuralNetwork_2eproto_impl();
  friend void protobuf_AssignDesc_NeuralNetwork_2eproto();
  friend void protobuf_ShutdownFile_NeuralNetwork_2eproto();

  void InitAsDefaultInstance();
};
extern ::google::protobuf::internal::ExplicitlyConstructed<ActivationReLU> ActivationReLU_default_instance_;

// -------------------------------------------------------------------

class ActivationLeakyReLU : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ActivationLeakyReLU) */ {
 public:
  ActivationLeakyReLU();
  virtual ~ActivationLeakyReLU();

  ActivationLeakyReLU(const ActivationLeakyReLU& from);

  inline ActivationLeakyReLU& operator=(const ActivationLeakyReLU& from) {
    CopyFrom(from);
    return *this;
  }

  static const ActivationLeakyReLU& default_instance();

  static const ActivationLeakyReLU* internal_default_instance();

  void Swap(ActivationLeakyReLU* other);

  // implements Message ----------------------------------------------

  inline ActivationLeakyReLU* New() const { return New(NULL); }

  ActivationLeakyReLU* New(::google::protobuf::Arena* arena) const;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from);
  void CopyFrom(const ActivationLeakyReLU& from);
  void MergeFrom(const ActivationLeakyReLU& from);
  void Clear();
  bool IsInitialized() const;

  size_t ByteSizeLong() const;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input);
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const;
  void DiscardUnknownFields();
  int GetCachedSize() const { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ActivationLeakyReLU* other);
  void UnsafeMergeFrom(const ActivationLeakyReLU& from);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return _arena_ptr_;
  }
  inline ::google::protobuf::Arena* MaybeArenaPtr() const {
    return _arena_ptr_;
  }
  public:

  ::std::string GetTypeName() const;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // optional float alpha = 1;
  void clear_alpha();
  static const int kAlphaFieldNumber = 1;
  float alpha() const;
  void set_alpha(float value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ActivationLeakyReLU)
 private:

  ::google::protobuf::internal::ArenaStringPtr _unknown_fields_;
  ::google::protobuf::Arena* _arena_ptr_;

  float alpha_;
  mutable int _cached_size_;
  friend void  protobuf_InitDefaults_NeuralNetwork_2eproto_impl();
  friend void  protobuf_AddDesc_NeuralNetwork_2eproto_impl();
  friend void protobuf_AssignDesc_NeuralNetwork_2eproto();
  friend void protobuf_ShutdownFile_NeuralNetwork_2eproto();

  void InitAsDefaultInstance();
};
extern ::google::protobuf::internal::ExplicitlyConstructed<ActivationLeakyReLU> ActivationLeakyReLU_default_instance_;

// -------------------------------------------------------------------

class ActivationTanh : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ActivationTanh) */ {
 public:
  ActivationTanh();
  virtual ~ActivationTanh();

  ActivationTanh(const ActivationTanh& from);

  inline ActivationTanh& operator=(const ActivationTanh& from) {
    CopyFrom(from);
    return *this;
  }

  static const ActivationTanh& default_instance();

  static const ActivationTanh* internal_default_instance();

  void Swap(ActivationTanh* other);

  // implements Message ----------------------------------------------

  inline ActivationTanh* New() const { return New(NULL); }

  ActivationTanh* New(::google::protobuf::Arena* arena) const;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from);
  void CopyFrom(const ActivationTanh& from);
  void MergeFrom(const ActivationTanh& from);
  void Clear();
  bool IsInitialized() const;

  size_t ByteSizeLong() const;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input);
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const;
  void DiscardUnknownFields();
  int GetCachedSize() const { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ActivationTanh* other);
  void UnsafeMergeFrom(const ActivationTanh& from);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return _arena_ptr_;
  }
  inline ::google::protobuf::Arena* MaybeArenaPtr() const {
    return _arena_ptr_;
  }
  public:

  ::std::string GetTypeName() const;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ActivationTanh)
 private:

  ::google::protobuf::internal::ArenaStringPtr _unknown_fields_;
  ::google::protobuf::Arena* _arena_ptr_;

  mutable int _cached_size_;
  friend void  protobuf_InitDefaults_NeuralNetwork_2eproto_impl();
  friend void  protobuf_AddDesc_NeuralNetwork_2eproto_impl();
  friend void protobuf_AssignDesc_NeuralNetwork_2eproto();
  friend void protobuf_ShutdownFile_NeuralNetwork_2eproto();

  void InitAsDefaultInstance();
};
extern ::google::protobuf::internal::ExplicitlyConstructed<ActivationTanh> ActivationTanh_default_instance_;

// -------------------------------------------------------------------

class ActivationScaledTanh : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ActivationScaledTanh) */ {
 public:
  ActivationScaledTanh();
  virtual ~ActivationScaledTanh();

  ActivationScaledTanh(const ActivationScaledTanh& from);

  inline ActivationScaledTanh& operator=(const ActivationScaledTanh& from) {
    CopyFrom(from);
    return *this;
  }

  static const ActivationScaledTanh& default_instance();

  static const ActivationScaledTanh* internal_default_instance();

  void Swap(ActivationScaledTanh* other);

  // implements Message ----------------------------------------------

  inline ActivationScaledTanh* New() const { return New(NULL); }

  ActivationScaledTanh* New(::google::protobuf::Arena* arena) const;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from);
  void CopyFrom(const ActivationScaledTanh& from);
  void MergeFrom(const ActivationScaledTanh& from);
  void Clear();
  bool IsInitialized() const;

  size_t ByteSizeLong() const;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input);
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const;
  void DiscardUnknownFields();
  int GetCachedSize() const { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ActivationScaledTanh* other);
  void UnsafeMergeFrom(const ActivationScaledTanh& from);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return _arena_ptr_;
  }
  inline ::google::protobuf::Arena* MaybeArenaPtr() const {
    return _arena_ptr_;
  }
  public:

  ::std::string GetTypeName() const;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // optional float alpha = 1;
  void clear_alpha();
  static const int kAlphaFieldNumber = 1;
  float alpha() const;
  void set_alpha(float value);

  // optional float beta = 2;
  void clear_beta();
  static const int kBetaFieldNumber = 2;
  float beta() const;
  void set_beta(float value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ActivationScaledTanh)
 private:

  ::google::protobuf::internal::ArenaStringPtr _unknown_fields_;
  ::google::protobuf::Arena* _arena_ptr_;

  float alpha_;
  float beta_;
  mutable int _cached_size_;
  friend void  protobuf_InitDefaults_NeuralNetwork_2eproto_impl();
  friend void  protobuf_AddDesc_NeuralNetwork_2eproto_impl();
  friend void protobuf_AssignDesc_NeuralNetwork_2eproto();
  friend void protobuf_ShutdownFile_NeuralNetwork_2eproto();

  void InitAsDefaultInstance();
};
extern ::google::protobuf::internal::ExplicitlyConstructed<ActivationScaledTanh> ActivationScaledTanh_default_instance_;

// -------------------------------------------------------------------

class ActivationSigmoid : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ActivationSigmoid) */ {
 public:
  ActivationSigmoid();
  virtual ~ActivationSigmoid();

  ActivationSigmoid(const ActivationSigmoid& from);

  inline ActivationSigmoid& operator=(const ActivationSigmoid& from) {
    CopyFrom(from);
    return *this;
  }

  static const ActivationSigmoid& default_instance();

  static const ActivationSigmoid* internal_default_instance();

  void Swap(ActivationSigmoid* other);

  // implements Message ----------------------------------------------

  inline ActivationSigmoid* New() const { return New(NULL); }

  ActivationSigmoid* New(::google::protobuf::Arena* arena) const;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from);
  void CopyFrom(const ActivationSigmoid& from);
  void MergeFrom(const ActivationSigmoid& from);
  void Clear();
  bool IsInitialized() const;

  size_t ByteSizeLong() const;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input);
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const;
  void DiscardUnknownFields();
  int GetCachedSize() const { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ActivationSigmoid* other);
  void UnsafeMergeFrom(const ActivationSigmoid& from);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return _arena_ptr_;
  }
  inline ::google::protobuf::Arena* MaybeArenaPtr() const {
    return _arena_ptr_;
  }
  public:

  ::std::string GetTypeName() const;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ActivationSigmoid)
 private:

  ::google::protobuf::internal::ArenaStringPtr _unknown_fields_;
  ::google::protobuf::Arena* _arena_ptr_;

  mutable int _cached_size_;
  friend void  protobuf_InitDefaults_NeuralNetwork_2eproto_impl();
  friend void  protobuf_AddDesc_NeuralNetwork_2eproto_impl();
  friend void protobuf_AssignDesc_NeuralNetwork_2eproto();
  friend void protobuf_ShutdownFile_NeuralNetwork_2eproto();

  void InitAsDefaultInstance();
};
extern ::google::protobuf::internal::ExplicitlyConstructed<ActivationSigmoid> ActivationSigmoid_default_instance_;

// -------------------------------------------------------------------

class ActivationLinear : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ActivationLinear) */ {
 public:
  ActivationLinear();
  virtual ~ActivationLinear();

  ActivationLinear(const ActivationLinear& from);

  inline ActivationLinear& operator=(const ActivationLinear& from) {
    CopyFrom(from);
    return *this;
  }

  static const ActivationLinear& default_instance();

  static const ActivationLinear* internal_default_instance();

  void Swap(ActivationLinear* other);

  // implements Message ----------------------------------------------

  inline ActivationLinear* New() const { return New(NULL); }

  ActivationLinear* New(::google::protobuf::Arena* arena) const;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from);
  void CopyFrom(const ActivationLinear& from);
  void MergeFrom(const ActivationLinear& from);
  void Clear();
  bool IsInitialized() const;

  size_t ByteSizeLong() const;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input);
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const;
  void DiscardUnknownFields();
  int GetCachedSize() const { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ActivationLinear* other);
  void UnsafeMergeFrom(const ActivationLinear& from);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return _arena_ptr_;
  }
  inline ::google::protobuf::Arena* MaybeArenaPtr() const {
    return _arena_ptr_;
  }
  public:

  ::std::string GetTypeName() const;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // optional float alpha = 1;
  void clear_alpha();
  static const int kAlphaFieldNumber = 1;
  float alpha() const;
  void set_alpha(float value);

  // optional float beta = 2;
  void clear_beta();
  static const int kBetaFieldNumber = 2;
  float beta() const;
  void set_beta(float value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ActivationLinear)
 private:

  ::google::protobuf::internal::ArenaStringPtr _unknown_fields_;
  ::google::protobuf::Arena* _arena_ptr_;

  float alpha_;
  float beta_;
  mutable int _cached_size_;
  friend void  protobuf_InitDefaults_NeuralNetwork_2eproto_impl();
  friend void  protobuf_AddDesc_NeuralNetwork_2eproto_impl();
  friend void protobuf_AssignDesc_NeuralNetwork_2eproto();
  friend void protobuf_ShutdownFile_NeuralNetwork_2eproto();

  void InitAsDefaultInstance();
};
extern ::google::protobuf::internal::ExplicitlyConstructed<ActivationLinear> ActivationLinear_default_instance_;

// -------------------------------------------------------------------

class ActivationSigmoidHard : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ActivationSigmoidHard) */ {
 public:
  ActivationSigmoidHard();
  virtual ~ActivationSigmoidHard();

  ActivationSigmoidHard(const ActivationSigmoidHard& from);

  inline ActivationSigmoidHard& operator=(const ActivationSigmoidHard& from) {
    CopyFrom(from);
    return *this;
  }

  static const ActivationSigmoidHard& default_instance();

  static const ActivationSigmoidHard* internal_default_instance();

  void Swap(ActivationSigmoidHard* other);

  // implements Message ----------------------------------------------

  inline ActivationSigmoidHard* New() const { return New(NULL); }

  ActivationSigmoidHard* New(::google::protobuf::Arena* arena) const;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from);
  void CopyFrom(const ActivationSigmoidHard& from);
  void MergeFrom(const ActivationSigmoidHard& from);
  void Clear();
  bool IsInitialized() const;

  size_t ByteSizeLong() const;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input);
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const;
  void DiscardUnknownFields();
  int GetCachedSize() const { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ActivationSigmoidHard* other);
  void UnsafeMergeFrom(const ActivationSigmoidHard& from);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return _arena_ptr_;
  }
  inline ::google::protobuf::Arena* MaybeArenaPtr() const {
    return _arena_ptr_;
  }
  public:

  ::std::string GetTypeName() const;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // optional float alpha = 1;
  void clear_alpha();
  static const int kAlphaFieldNumber = 1;
  float alpha() const;
  void set_alpha(float value);

  // optional float beta = 2;
  void clear_beta();
  static const int kBetaFieldNumber = 2;
  float beta() const;
  void set_beta(float value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ActivationSigmoidHard)
 private:

  ::google::protobuf::internal::ArenaStringPtr _unknown_fields_;
  ::google::protobuf::Arena* _arena_ptr_;

  float alpha_;
  float beta_;
  mutable int _cached_size_;
  friend void  protobuf_InitDefaults_NeuralNetwork_2eproto_impl();
  friend void  protobuf_AddDesc_NeuralNetwork_2eproto_impl();
  friend void protobuf_AssignDesc_NeuralNetwork_2eproto();
  friend void protobuf_ShutdownFile_NeuralNetwork_2eproto();

  void InitAsDefaultInstance();
};
extern ::google::protobuf::internal::ExplicitlyConstructed<ActivationSigmoidHard> ActivationSigmoidHard_default_instance_;

// -------------------------------------------------------------------

class ActivationPReLU : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ActivationPReLU) */ {
 public:
  ActivationPReLU();
  virtual ~ActivationPReLU();

  ActivationPReLU(const ActivationPReLU& from);

  inline ActivationPReLU& operator=(const ActivationPReLU& from) {
    CopyFrom(from);
    return *this;
  }

  static const ActivationPReLU& default_instance();

  static const ActivationPReLU* internal_default_instance();

  void Swap(ActivationPReLU* other);

  // implements Message ----------------------------------------------

  inline ActivationPReLU* New() const { return New(NULL); }

  ActivationPReLU* New(::google::protobuf::Arena* arena) const;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from);
  void CopyFrom(const ActivationPReLU& from);
  void MergeFrom(const ActivationPReLU& from);
  void Clear();
  bool IsInitialized() const;

  size_t ByteSizeLong() const;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input);
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const;
  void DiscardUnknownFields();
  int GetCachedSize() const { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ActivationPReLU* other);
  void UnsafeMergeFrom(const ActivationPReLU& from);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return _arena_ptr_;
  }
  inline ::google::protobuf::Arena* MaybeArenaPtr() const {
    return _arena_ptr_;
  }
  public:

  ::std::string GetTypeName() const;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // optional .CoreML.Specification.WeightParams alpha = 1;
  bool has_alpha() const;
  void clear_alpha();
  static const int kAlphaFieldNumber = 1;
  const ::CoreML::Specification::WeightParams& alpha() const;
  ::CoreML::Specification::WeightParams* mutable_alpha();
  ::CoreML::Specification::WeightParams* release_alpha();
  void set_allocated_alpha(::CoreML::Specification::WeightParams* alpha);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ActivationPReLU)
 private:

  ::google::protobuf::internal::ArenaStringPtr _unknown_fields_;
  ::google::protobuf::Arena* _arena_ptr_;

  ::CoreML::Specification::WeightParams* alpha_;
  mutable int _cached_size_;
  friend void  protobuf_InitDefaults_NeuralNetwork_2eproto_impl();
  friend void  protobuf_AddDesc_NeuralNetwork_2eproto_impl();
  friend void protobuf_AssignDesc_NeuralNetwork_2eproto();
  friend void protobuf_ShutdownFile_NeuralNetwork_2eproto();

  void InitAsDefaultInstance();
};
extern ::google::protobuf::internal::ExplicitlyConstructed<ActivationPReLU> ActivationPReLU_default_instance_;

// -------------------------------------------------------------------

class ActivationELU : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ActivationELU) */ {
 public:
  ActivationELU();
  virtual ~ActivationELU();

  ActivationELU(const ActivationELU& from);

  inline ActivationELU& operator=(const ActivationELU& from) {
    CopyFrom(from);
    return *this;
  }

  static const ActivationELU& default_instance();

  static const ActivationELU* internal_default_instance();

  void Swap(ActivationELU* other);

  // implements Message ----------------------------------------------

  inline ActivationELU* New() const { return New(NULL); }

  ActivationELU* New(::google::protobuf::Arena* arena) const;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from);
  void CopyFrom(const ActivationELU& from);
  void MergeFrom(const ActivationELU& from);
  void Clear();
  bool IsInitialized() const;

  size_t ByteSizeLong() const;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input);
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const;
  void DiscardUnknownFields();
  int GetCachedSize() const { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ActivationELU* other);
  void UnsafeMergeFrom(const ActivationELU& from);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return _arena_ptr_;
  }
  inline ::google::protobuf::Arena* MaybeArenaPtr() const {
    return _arena_ptr_;
  }
  public:

  ::std::string GetTypeName() const;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // optional float alpha = 1;
  void clear_alpha();
  static const int kAlphaFieldNumber = 1;
  float alpha() const;
  void set_alpha(float value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ActivationELU)
 private:

  ::google::protobuf::internal::ArenaStringPtr _unknown_fields_;
  ::google::protobuf::Arena* _arena_ptr_;

  float alpha_;
  mutable int _cached_size_;
  friend void  protobuf_InitDefaults_NeuralNetwork_2eproto_impl();
  friend void  protobuf_AddDesc_NeuralNetwork_2eproto_impl();
  friend void protobuf_AssignDesc_NeuralNetwork_2eproto();
  friend void protobuf_ShutdownFile_NeuralNetwork_2eproto();

  void InitAsDefaultInstance();
};
extern ::google::protobuf::internal::ExplicitlyConstructed<ActivationELU> ActivationELU_default_instance_;

// -------------------------------------------------------------------

class ActivationThresholdedReLU : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ActivationThresholdedReLU) */ {
 public:
  ActivationThresholdedReLU();
  virtual ~ActivationThresholdedReLU();

  ActivationThresholdedReLU(const ActivationThresholdedReLU& from);

  inline ActivationThresholdedReLU& operator=(const ActivationThresholdedReLU& from) {
    CopyFrom(from);
    return *this;
  }

  static const ActivationThresholdedReLU& default_instance();

  static const ActivationThresholdedReLU* internal_default_instance();

  void Swap(ActivationThresholdedReLU* other);

  // implements Message ----------------------------------------------

  inline ActivationThresholdedReLU* New() const { return New(NULL); }

  ActivationThresholdedReLU* New(::google::protobuf::Arena* arena) const;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from);
  void CopyFrom(const ActivationThresholdedReLU& from);
  void MergeFrom(const ActivationThresholdedReLU& from);
  void Clear();
  bool IsInitialized() const;

  size_t ByteSizeLong() const;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input);
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const;
  void DiscardUnknownFields();
  int GetCachedSize() const { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ActivationThresholdedReLU* other);
  void UnsafeMergeFrom(const ActivationThresholdedReLU& from);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return _arena_ptr_;
  }
  inline ::google::protobuf::Arena* MaybeArenaPtr() const {
    return _arena_ptr_;
  }
  public:

  ::std::string GetTypeName() const;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // optional float alpha = 1;
  void clear_alpha();
  static const int kAlphaFieldNumber = 1;
  float alpha() const;
  void set_alpha(float value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ActivationThresholdedReLU)
 private:

  ::google::protobuf::internal::ArenaStringPtr _unknown_fields_;
  ::google::protobuf::Arena* _arena_ptr_;

  float alpha_;
  mutable int _cached_size_;
  friend void  protobuf_InitDefaults_NeuralNetwork_2eproto_impl();
  friend void  protobuf_AddDesc_NeuralNetwork_2eproto_impl();
  friend void protobuf_AssignDesc_NeuralNetwork_2eproto();
  friend void protobuf_ShutdownFile_NeuralNetwork_2eproto();

  void InitAsDefaultInstance();
};
extern ::google::protobuf::internal::ExplicitlyConstructed<ActivationThresholdedReLU> ActivationThresholdedReLU_default_instance_;

// -------------------------------------------------------------------

class ActivationSoftsign : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ActivationSoftsign) */ {
 public:
  ActivationSoftsign();
  virtual ~ActivationSoftsign();

  ActivationSoftsign(const ActivationSoftsign& from);

  inline ActivationSoftsign& operator=(const ActivationSoftsign& from) {
    CopyFrom(from);
    return *this;
  }

  static const ActivationSoftsign& default_instance();

  static const ActivationSoftsign* internal_default_instance();

  void Swap(ActivationSoftsign* other);

  // implements Message ----------------------------------------------

  inline ActivationSoftsign* New() const { return New(NULL); }

  ActivationSoftsign* New(::google::protobuf::Arena* arena) const;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from);
  void CopyFrom(const ActivationSoftsign& from);
  void MergeFrom(const ActivationSoftsign& from);
  void Clear();
  bool IsInitialized() const;

  size_t ByteSizeLong() const;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input);
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const;
  void DiscardUnknownFields();
  int GetCachedSize() const { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ActivationSoftsign* other);
  void UnsafeMergeFrom(const ActivationSoftsign& from);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return _arena_ptr_;
  }
  inline ::google::protobuf::Arena* MaybeArenaPtr() const {
    return _arena_ptr_;
  }
  public:

  ::std::string GetTypeName() const;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ActivationSoftsign)
 private:

  ::google::protobuf::internal::ArenaStringPtr _unknown_fields_;
  ::google::protobuf::Arena* _arena_ptr_;

  mutable int _cached_size_;
  friend void  protobuf_InitDefaults_NeuralNetwork_2eproto_impl();
  friend void  protobuf_AddDesc_NeuralNetwork_2eproto_impl();
  friend void protobuf_AssignDesc_NeuralNetwork_2eproto();
  friend void protobuf_ShutdownFile_NeuralNetwork_2eproto();

  void InitAsDefaultInstance();
};
extern ::google::protobuf::internal::ExplicitlyConstructed<ActivationSoftsign> ActivationSoftsign_default_instance_;

// -------------------------------------------------------------------

class ActivationSoftplus : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ActivationSoftplus) */ {
 public:
  ActivationSoftplus();
  virtual ~ActivationSoftplus();

  ActivationSoftplus(const ActivationSoftplus& from);

  inline ActivationSoftplus& operator=(const ActivationSoftplus& from) {
    CopyFrom(from);
    return *this;
  }

  static const ActivationSoftplus& default_instance();

  static const ActivationSoftplus* internal_default_instance();

  void Swap(ActivationSoftplus* other);

  // implements Message ----------------------------------------------

  inline ActivationSoftplus* New() const { return New(NULL); }

  ActivationSoftplus* New(::google::protobuf::Arena* arena) const;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from);
  void CopyFrom(const ActivationSoftplus& from);
  void MergeFrom(const ActivationSoftplus& from);
  void Clear();
  bool IsInitialized() const;

  size_t ByteSizeLong() const;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input);
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const;
  void DiscardUnknownFields();
  int GetCachedSize() const { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ActivationSoftplus* other);
  void UnsafeMergeFrom(const ActivationSoftplus& from);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return _arena_ptr_;
  }
  inline ::google::protobuf::Arena* MaybeArenaPtr() const {
    return _arena_ptr_;
  }
  public:

  ::std::string GetTypeName() const;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ActivationSoftplus)
 private:

  ::google::protobuf::internal::ArenaStringPtr _unknown_fields_;
  ::google::protobuf::Arena* _arena_ptr_;

  mutable int _cached_size_;
  friend void  protobuf_InitDefaults_NeuralNetwork_2eproto_impl();
  friend void  protobuf_AddDesc_NeuralNetwork_2eproto_impl();
  friend void protobuf_AssignDesc_NeuralNetwork_2eproto();
  friend void protobuf_ShutdownFile_NeuralNetwork_2eproto();

  void InitAsDefaultInstance();
};
extern ::google::protobuf::internal::ExplicitlyConstructed<ActivationSoftplus> ActivationSoftplus_default_instance_;

// -------------------------------------------------------------------

class ActivationParametricSoftplus : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ActivationParametricSoftplus) */ {
 public:
  ActivationParametricSoftplus();
  virtual ~ActivationParametricSoftplus();

  ActivationParametricSoftplus(const ActivationParametricSoftplus& from);

  inline ActivationParametricSoftplus& operator=(const ActivationParametricSoftplus& from) {
    CopyFrom(from);
    return *this;
  }

  static const ActivationParametricSoftplus& default_instance();

  static const ActivationParametricSoftplus* internal_default_instance();

  void Swap(ActivationParametricSoftplus* other);

  // implements Message ----------------------------------------------

  inline ActivationParametricSoftplus* New() const { return New(NULL); }

  ActivationParametricSoftplus* New(::google::protobuf::Arena* arena) const;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from);
  void CopyFrom(const ActivationParametricSoftplus& from);
  void MergeFrom(const ActivationParametricSoftplus& from);
  void Clear();
  bool IsInitialized() const;

  size_t ByteSizeLong() const;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input);
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const;
  void DiscardUnknownFields();
  int GetCachedSize() const { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ActivationParametricSoftplus* other);
  void UnsafeMergeFrom(const ActivationParametricSoftplus& from);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return _arena_ptr_;
  }
  inline ::google::protobuf::Arena* MaybeArenaPtr() const {
    return _arena_ptr_;
  }
  public:

  ::std::string GetTypeName() const;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // optional .CoreML.Specification.WeightParams alpha = 1;
  bool has_alpha() const;
  void clear_alpha();
  static const int kAlphaFieldNumber = 1;
  const ::CoreML::Specification::WeightParams& alpha() const;
  ::CoreML::Specification::WeightParams* mutable_alpha();
  ::CoreML::Specification::WeightParams* release_alpha();
  void set_allocated_alpha(::CoreML::Specification::WeightParams* alpha);

  // optional .CoreML.Specification.WeightParams beta = 2;
  bool has_beta() const;
  void clear_beta();
  static const int kBetaFieldNumber = 2;
  const ::CoreML::Specification::WeightParams& beta() const;
  ::CoreML::Specification::WeightParams* mutable_beta();
  ::CoreML::Specification::WeightParams* release_beta();
  void set_allocated_beta(::CoreML::Specification::WeightParams* beta);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ActivationParametricSoftplus)
 private:

  ::google::protobuf::internal::ArenaStringPtr _unknown_fields_;
  ::google::protobuf::Arena* _arena_ptr_;

  ::CoreML::Specification::WeightParams* alpha_;
  ::CoreML::Specification::WeightParams* beta_;
  mutable int _cached_size_;
  friend void  protobuf_InitDefaults_NeuralNetwork_2eproto_impl();
  friend void  protobuf_AddDesc_NeuralNetwork_2eproto_impl();
  friend void protobuf_AssignDesc_NeuralNetwork_2eproto();
  friend void protobuf_ShutdownFile_NeuralNetwork_2eproto();

  void InitAsDefaultInstance();
};
extern ::google::protobuf::internal::ExplicitlyConstructed<ActivationParametricSoftplus> ActivationParametricSoftplus_default_instance_;

// -------------------------------------------------------------------

class ActivationParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ActivationParams) */ {
 public:
  ActivationParams();
  virtual ~ActivationParams();

  ActivationParams(const ActivationParams& from);

  inline ActivationParams& operator=(const ActivationParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const ActivationParams& default_instance();

  enum NonlinearityTypeCase {
    kLinear = 5,
    kReLU = 10,
    kLeakyReLU = 15,
    kThresholdedReLU = 20,
    kPReLU = 25,
    kTanh = 30,
    kScaledTanh = 31,
    kSigmoid = 40,
    kSigmoidHard = 41,
    kELU = 50,
    kSoftsign = 60,
    kSoftplus = 70,
    kParametricSoftplus = 71,
    NONLINEARITYTYPE_NOT_SET = 0,
  };

  static const ActivationParams* internal_default_instance();

  void Swap(ActivationParams* other);

  // implements Message ----------------------------------------------

  inline ActivationParams* New() const { return New(NULL); }

  ActivationParams* New(::google::protobuf::Arena* arena) const;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from);
  void CopyFrom(const ActivationParams& from);
  void MergeFrom(const ActivationParams& from);
  void Clear();
  bool IsInitialized() const;

  size_t ByteSizeLong() const;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input);
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const;
  void DiscardUnknownFields();
  int GetCachedSize() const { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ActivationParams* other);
  void UnsafeMergeFrom(const ActivationParams& from);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return _arena_ptr_;
  }
  inline ::google::protobuf::Arena* MaybeArenaPtr() const {
    return _arena_ptr_;
  }
  public:

  ::std::string GetTypeName() const;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // optional .CoreML.Specification.ActivationLinear linear = 5;
  bool has_linear() const;
  void clear_linear();
  static const int kLinearFieldNumber = 5;
  const ::CoreML::Specification::ActivationLinear& linear() const;
  ::CoreML::Specification::ActivationLinear* mutable_linear();
  ::CoreML::Specification::ActivationLinear* release_linear();
  void set_allocated_linear(::CoreML::Specification::ActivationLinear* linear);

  // optional .CoreML.Specification.ActivationReLU ReLU = 10;
  bool has_relu() const;
  void clear_relu();
  static const int kReLUFieldNumber = 10;
  const ::CoreML::Specification::ActivationReLU& relu() const;
  ::CoreML::Specification::ActivationReLU* mutable_relu();
  ::CoreML::Specification::ActivationReLU* release_relu();
  void set_allocated_relu(::CoreML::Specification::ActivationReLU* relu);

  // optional .CoreML.Specification.ActivationLeakyReLU leakyReLU = 15;
  bool has_leakyrelu() const;
  void clear_leakyrelu();
  static const int kLeakyReLUFieldNumber = 15;
  const ::CoreML::Specification::ActivationLeakyReLU& leakyrelu() const;
  ::CoreML::Specification::ActivationLeakyReLU* mutable_leakyrelu();
  ::CoreML::Specification::ActivationLeakyReLU* release_leakyrelu();
  void set_allocated_leakyrelu(::CoreML::Specification::ActivationLeakyReLU* leakyrelu);

  // optional .CoreML.Specification.ActivationThresholdedReLU thresholdedReLU = 20;
  bool has_thresholdedrelu() const;
  void clear_thresholdedrelu();
  static const int kThresholdedReLUFieldNumber = 20;
  const ::CoreML::Specification::ActivationThresholdedReLU& thresholdedrelu() const;
  ::CoreML::Specification::ActivationThresholdedReLU* mutable_thresholdedrelu();
  ::CoreML::Specification::ActivationThresholdedReLU* release_thresholdedrelu();
  void set_allocated_thresholdedrelu(::CoreML::Specification::ActivationThresholdedReLU* thresholdedrelu);

  // optional .CoreML.Specification.ActivationPReLU PReLU = 25;
  bool has_prelu() const;
  void clear_prelu();
  static const int kPReLUFieldNumber = 25;
  const ::CoreML::Specification::ActivationPReLU& prelu() const;
  ::CoreML::Specification::ActivationPReLU* mutable_prelu();
  ::CoreML::Specification::ActivationPReLU* release_prelu();
  void set_allocated_prelu(::CoreML::Specification::ActivationPReLU* prelu);

  // optional .CoreML.Specification.ActivationTanh tanh = 30;
  bool has_tanh() const;
  void clear_tanh();
  static const int kTanhFieldNumber = 30;
  const ::CoreML::Specification::ActivationTanh& tanh() const;
  ::CoreML::Specification::ActivationTanh* mutable_tanh();
  ::CoreML::Specification::ActivationTanh* release_tanh();
  void set_allocated_tanh(::CoreML::Specification::ActivationTanh* tanh);

  // optional .CoreML.Specification.ActivationScaledTanh scaledTanh = 31;
  bool has_scaledtanh() const;
  void clear_scaledtanh();
  static const int kScaledTanhFieldNumber = 31;
  const ::CoreML::Specification::ActivationScaledTanh& scaledtanh() const;
  ::CoreML::Specification::ActivationScaledTanh* mutable_scaledtanh();
  ::CoreML::Specification::ActivationScaledTanh* release_scaledtanh();
  void set_allocated_scaledtanh(::CoreML::Specification::ActivationScaledTanh* scaledtanh);

  // optional .CoreML.Specification.ActivationSigmoid sigmoid = 40;
  bool has_sigmoid() const;
  void clear_sigmoid();
  static const int kSigmoidFieldNumber = 40;
  const ::CoreML::Specification::ActivationSigmoid& sigmoid() const;
  ::CoreML::Specification::ActivationSigmoid* mutable_sigmoid();
  ::CoreML::Specification::ActivationSigmoid* release_sigmoid();
  void set_allocated_sigmoid(::CoreML::Specification::ActivationSigmoid* sigmoid);

  // optional .CoreML.Specification.ActivationSigmoidHard sigmoidHard = 41;
  bool has_sigmoidhard() const;
  void clear_sigmoidhard();
  static const int kSigmoidHardFieldNumber = 41;
  const ::CoreML::Specification::ActivationSigmoidHard& sigmoidhard() const;
  ::CoreML::Specification::ActivationSigmoidHard* mutable_sigmoidhard();
  ::CoreML::Specification::ActivationSigmoidHard* release_sigmoidhard();
  void set_allocated_sigmoidhard(::CoreML::Specification::ActivationSigmoidHard* sigmoidhard);

  // optional .CoreML.Specification.ActivationELU ELU = 50;
  bool has_elu() const;
  void clear_elu();
  static const int kELUFieldNumber = 50;
  const ::CoreML::Specification::ActivationELU& elu() const;
  ::CoreML::Specification::ActivationELU* mutable_elu();
  ::CoreML::Specification::ActivationELU* release_elu();
  void set_allocated_elu(::CoreML::Specification::ActivationELU* elu);

  // optional .CoreML.Specification.ActivationSoftsign softsign = 60;
  bool has_softsign() const;
  void clear_softsign();
  static const int kSoftsignFieldNumber = 60;
  const ::CoreML::Specification::ActivationSoftsign& softsign() const;
  ::CoreML::Specification::ActivationSoftsign* mutable_softsign();
  ::CoreML::Specification::ActivationSoftsign* release_softsign();
  void set_allocated_softsign(::CoreML::Specification::ActivationSoftsign* softsign);

  // optional .CoreML.Specification.ActivationSoftplus softplus = 70;
  bool has_softplus() const;
  void clear_softplus();
  static const int kSoftplusFieldNumber = 70;
  const ::CoreML::Specification::ActivationSoftplus& softplus() const;
  ::CoreML::Specification::ActivationSoftplus* mutable_softplus();
  ::CoreML::Specification::ActivationSoftplus* release_softplus();
  void set_allocated_softplus(::CoreML::Specification::ActivationSoftplus* softplus);

  // optional .CoreML.Specification.ActivationParametricSoftplus parametricSoftplus = 71;
  bool has_parametricsoftplus() const;
  void clear_parametricsoftplus();
  static const int kParametricSoftplusFieldNumber = 71;
  const ::CoreML::Specification::ActivationParametricSoftplus& parametricsoftplus() const;
  ::CoreML::Specification::ActivationParametricSoftplus* mutable_parametricsoftplus();
  ::CoreML::Specification::ActivationParametricSoftplus* release_parametricsoftplus();
  void set_allocated_parametricsoftplus(::CoreML::Specification::ActivationParametricSoftplus* parametricsoftplus);

  NonlinearityTypeCase NonlinearityType_case() const;
  // @@protoc_insertion_point(class_scope:CoreML.Specification.ActivationParams)
 private:
  inline void set_has_linear();
  inline void set_has_relu();
  inline void set_has_leakyrelu();
  inline void set_has_thresholdedrelu();
  inline void set_has_prelu();
  inline void set_has_tanh();
  inline void set_has_scaledtanh();
  inline void set_has_sigmoid();
  inline void set_has_sigmoidhard();
  inline void set_has_elu();
  inline void set_has_softsign();
  inline void set_has_softplus();
  inline void set_has_parametricsoftplus();

  inline bool has_NonlinearityType() const;
  void clear_NonlinearityType();
  inline void clear_has_NonlinearityType();

  ::google::protobuf::internal::ArenaStringPtr _unknown_fields_;
  ::google::protobuf::Arena* _arena_ptr_;

  union NonlinearityTypeUnion {
    NonlinearityTypeUnion() {}
    ::CoreML::Specification::ActivationLinear* linear_;
    ::CoreML::Specification::ActivationReLU* relu_;
    ::CoreML::Specification::ActivationLeakyReLU* leakyrelu_;
    ::CoreML::Specification::ActivationThresholdedReLU* thresholdedrelu_;
    ::CoreML::Specification::ActivationPReLU* prelu_;
    ::CoreML::Specification::ActivationTanh* tanh_;
    ::CoreML::Specification::ActivationScaledTanh* scaledtanh_;
    ::CoreML::Specification::ActivationSigmoid* sigmoid_;
    ::CoreML::Specification::ActivationSigmoidHard* sigmoidhard_;
    ::CoreML::Specification::ActivationELU* elu_;
    ::CoreML::Specification::ActivationSoftsign* softsign_;
    ::CoreML::Specification::ActivationSoftplus* softplus_;
    ::CoreML::Specification::ActivationParametricSoftplus* parametricsoftplus_;
  } NonlinearityType_;
  mutable int _cached_size_;
  ::google::protobuf::uint32 _oneof_case_[1];

  friend void  protobuf_InitDefaults_NeuralNetwork_2eproto_impl();
  friend void  protobuf_AddDesc_NeuralNetwork_2eproto_impl();
  friend void protobuf_AssignDesc_NeuralNetwork_2eproto();
  friend void protobuf_ShutdownFile_NeuralNetwork_2eproto();

  void InitAsDefaultInstance();
};
extern ::google::protobuf::internal::ExplicitlyConstructed<ActivationParams> ActivationParams_default_instance_;

// -------------------------------------------------------------------

class NeuralNetworkLayer : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.NeuralNetworkLayer) */ {
 public:
  NeuralNetworkLayer();
  virtual ~NeuralNetworkLayer();

  NeuralNetworkLayer(const NeuralNetworkLayer& from);

  inline NeuralNetworkLayer& operator=(const NeuralNetworkLayer& from) {
    CopyFrom(from);
    return *this;
  }

  static const NeuralNetworkLayer& default_instance();

  enum LayerCase {
    kConvolution = 100,
    kPooling = 120,
    kActivation = 130,
    kInnerProduct = 140,
    kEmbedding = 150,
    kBatchnorm = 160,
    kMvn = 165,
    kL2Normalize = 170,
    kSoftmax = 175,
    kLrn = 180,
    kCrop = 190,
    kPadding = 200,
    kUpsample = 210,
    kUnary = 220,
    kAdd = 230,
    kMultiply = 231,
    kAverage = 240,
    kScale = 245,
    kBias = 250,
    kMax = 260,
    kMin = 261,
    kDot = 270,
    kReduce = 280,
    kLoadConstant = 290,
    kReshape = 300,
    kFlatten = 301,
    kPermute = 310,
    kConcat = 320,
    kSplit = 330,
    kSequenceRepeat = 340,
    kSimpleRecurrent = 400,
    kGru = 410,
    kUniDirectionalLSTM = 420,
    kBiDirectionalLSTM = 430,
    LAYER_NOT_SET = 0,
  };

  static const NeuralNetworkLayer* internal_default_instance();

  void Swap(NeuralNetworkLayer* other);

  // implements Message ----------------------------------------------

  inline NeuralNetworkLayer* New() const { return New(NULL); }

  NeuralNetworkLayer* New(::google::protobuf::Arena* arena) const;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from);
  void CopyFrom(const NeuralNetworkLayer& from);
  void MergeFrom(const NeuralNetworkLayer& from);
  void Clear();
  bool IsInitialized() const;

  size_t ByteSizeLong() const;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input);
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const;
  void DiscardUnknownFields();
  int GetCachedSize() const { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(NeuralNetworkLayer* other);
  void UnsafeMergeFrom(const NeuralNetworkLayer& from);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return _arena_ptr_;
  }
  inline ::google::protobuf::Arena* MaybeArenaPtr() const {
    return _arena_ptr_;
  }
  public:

  ::std::string GetTypeName() const;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // optional string name = 1;
  void clear_name();
  static const int kNameFieldNumber = 1;
  const ::std::string& name() const;
  void set_name(const ::std::string& value);
  void set_name(const char* value);
  void set_name(const char* value, size_t size);
  ::std::string* mutable_name();
  ::std::string* release_name();
  void set_allocated_name(::std::string* name);

  // repeated string input = 2;
  int input_size() const;
  void clear_input();
  static const int kInputFieldNumber = 2;
  const ::std::string& input(int index) const;
  ::std::string* mutable_input(int index);
  void set_input(int index, const ::std::string& value);
  void set_input(int index, const char* value);
  void set_input(int index, const char* value, size_t size);
  ::std::string* add_input();
  void add_input(const ::std::string& value);
  void add_input(const char* value);
  void add_input(const char* value, size_t size);
  const ::google::protobuf::RepeatedPtrField< ::std::string>& input() const;
  ::google::protobuf::RepeatedPtrField< ::std::string>* mutable_input();

  // repeated string output = 3;
  int output_size() const;
  void clear_output();
  static const int kOutputFieldNumber = 3;
  const ::std::string& output(int index) const;
  ::std::string* mutable_output(int index);
  void set_output(int index, const ::std::string& value);
  void set_output(int index, const char* value);
  void set_output(int index, const char* value, size_t size);
  ::std::string* add_output();
  void add_output(const ::std::string& value);
  void add_output(const char* value);
  void add_output(const char* value, size_t size);
  const ::google::protobuf::RepeatedPtrField< ::std::string>& output() const;
  ::google::protobuf::RepeatedPtrField< ::std::string>* mutable_output();

  // optional .CoreML.Specification.ConvolutionLayerParams convolution = 100;
  bool has_convolution() const;
  void clear_convolution();
  static const int kConvolutionFieldNumber = 100;
  const ::CoreML::Specification::ConvolutionLayerParams& convolution() const;
  ::CoreML::Specification::ConvolutionLayerParams* mutable_convolution();
  ::CoreML::Specification::ConvolutionLayerParams* release_convolution();
  void set_allocated_convolution(::CoreML::Specification::ConvolutionLayerParams* convolution);

  // optional .CoreML.Specification.PoolingLayerParams pooling = 120;
  bool has_pooling() const;
  void clear_pooling();
  static const int kPoolingFieldNumber = 120;
  const ::CoreML::Specification::PoolingLayerParams& pooling() const;
  ::CoreML::Specification::PoolingLayerParams* mutable_pooling();
  ::CoreML::Specification::PoolingLayerParams* release_pooling();
  void set_allocated_pooling(::CoreML::Specification::PoolingLayerParams* pooling);

  // optional .CoreML.Specification.ActivationParams activation = 130;
  bool has_activation() const;
  void clear_activation();
  static const int kActivationFieldNumber = 130;
  const ::CoreML::Specification::ActivationParams& activation() const;
  ::CoreML::Specification::ActivationParams* mutable_activation();
  ::CoreML::Specification::ActivationParams* release_activation();
  void set_allocated_activation(::CoreML::Specification::ActivationParams* activation);

  // optional .CoreML.Specification.InnerProductLayerParams innerProduct = 140;
  bool has_innerproduct() const;
  void clear_innerproduct();
  static const int kInnerProductFieldNumber = 140;
  const ::CoreML::Specification::InnerProductLayerParams& innerproduct() const;
  ::CoreML::Specification::InnerProductLayerParams* mutable_innerproduct();
  ::CoreML::Specification::InnerProductLayerParams* release_innerproduct();
  void set_allocated_innerproduct(::CoreML::Specification::InnerProductLayerParams* innerproduct);

  // optional .CoreML.Specification.EmbeddingLayerParams embedding = 150;
  bool has_embedding() const;
  void clear_embedding();
  static const int kEmbeddingFieldNumber = 150;
  const ::CoreML::Specification::EmbeddingLayerParams& embedding() const;
  ::CoreML::Specification::EmbeddingLayerParams* mutable_embedding();
  ::CoreML::Specification::EmbeddingLayerParams* release_embedding();
  void set_allocated_embedding(::CoreML::Specification::EmbeddingLayerParams* embedding);

  // optional .CoreML.Specification.BatchnormLayerParams batchnorm = 160;
  bool has_batchnorm() const;
  void clear_batchnorm();
  static const int kBatchnormFieldNumber = 160;
  const ::CoreML::Specification::BatchnormLayerParams& batchnorm() const;
  ::CoreML::Specification::BatchnormLayerParams* mutable_batchnorm();
  ::CoreML::Specification::BatchnormLayerParams* release_batchnorm();
  void set_allocated_batchnorm(::CoreML::Specification::BatchnormLayerParams* batchnorm);

  // optional .CoreML.Specification.MeanVarianceNormalizeLayerParams mvn = 165;
  bool has_mvn() const;
  void clear_mvn();
  static const int kMvnFieldNumber = 165;
  const ::CoreML::Specification::MeanVarianceNormalizeLayerParams& mvn() const;
  ::CoreML::Specification::MeanVarianceNormalizeLayerParams* mutable_mvn();
  ::CoreML::Specification::MeanVarianceNormalizeLayerParams* release_mvn();
  void set_allocated_mvn(::CoreML::Specification::MeanVarianceNormalizeLayerParams* mvn);

  // optional .CoreML.Specification.L2NormalizeLayerParams l2normalize = 170;
  bool has_l2normalize() const;
  void clear_l2normalize();
  static const int kL2NormalizeFieldNumber = 170;
  const ::CoreML::Specification::L2NormalizeLayerParams& l2normalize() const;
  ::CoreML::Specification::L2NormalizeLayerParams* mutable_l2normalize();
  ::CoreML::Specification::L2NormalizeLayerParams* release_l2normalize();
  void set_allocated_l2normalize(::CoreML::Specification::L2NormalizeLayerParams* l2normalize);

  // optional .CoreML.Specification.SoftmaxLayerParams softmax = 175;
  bool has_softmax() const;
  void clear_softmax();
  static const int kSoftmaxFieldNumber = 175;
  const ::CoreML::Specification::SoftmaxLayerParams& softmax() const;
  ::CoreML::Specification::SoftmaxLayerParams* mutable_softmax();
  ::CoreML::Specification::SoftmaxLayerParams* release_softmax();
  void set_allocated_softmax(::CoreML::Specification::SoftmaxLayerParams* softmax);

  // optional .CoreML.Specification.LRNLayerParams lrn = 180;
  bool has_lrn() const;
  void clear_lrn();
  static const int kLrnFieldNumber = 180;
  const ::CoreML::Specification::LRNLayerParams& lrn() const;
  ::CoreML::Specification::LRNLayerParams* mutable_lrn();
  ::CoreML::Specification::LRNLayerParams* release_lrn();
  void set_allocated_lrn(::CoreML::Specification::LRNLayerParams* lrn);

  // optional .CoreML.Specification.CropLayerParams crop = 190;
  bool has_crop() const;
  void clear_crop();
  static const int kCropFieldNumber = 190;
  const ::CoreML::Specification::CropLayerParams& crop() const;
  ::CoreML::Specification::CropLayerParams* mutable_crop();
  ::CoreML::Specification::CropLayerParams* release_crop();
  void set_allocated_crop(::CoreML::Specification::CropLayerParams* crop);

  // optional .CoreML.Specification.PaddingLayerParams padding = 200;
  bool has_padding() const;
  void clear_padding();
  static const int kPaddingFieldNumber = 200;
  const ::CoreML::Specification::PaddingLayerParams& padding() const;
  ::CoreML::Specification::PaddingLayerParams* mutable_padding();
  ::CoreML::Specification::PaddingLayerParams* release_padding();
  void set_allocated_padding(::CoreML::Specification::PaddingLayerParams* padding);

  // optional .CoreML.Specification.UpsampleLayerParams upsample = 210;
  bool has_upsample() const;
  void clear_upsample();
  static const int kUpsampleFieldNumber = 210;
  const ::CoreML::Specification::UpsampleLayerParams& upsample() const;
  ::CoreML::Specification::UpsampleLayerParams* mutable_upsample();
  ::CoreML::Specification::UpsampleLayerParams* release_upsample();
  void set_allocated_upsample(::CoreML::Specification::UpsampleLayerParams* upsample);

  // optional .CoreML.Specification.UnaryFunctionLayerParams unary = 220;
  bool has_unary() const;
  void clear_unary();
  static const int kUnaryFieldNumber = 220;
  const ::CoreML::Specification::UnaryFunctionLayerParams& unary() const;
  ::CoreML::Specification::UnaryFunctionLayerParams* mutable_unary();
  ::CoreML::Specification::UnaryFunctionLayerParams* release_unary();
  void set_allocated_unary(::CoreML::Specification::UnaryFunctionLayerParams* unary);

  // optional .CoreML.Specification.AddLayerParams add = 230;
  bool has_add() const;
  void clear_add();
  static const int kAddFieldNumber = 230;
  const ::CoreML::Specification::AddLayerParams& add() const;
  ::CoreML::Specification::AddLayerParams* mutable_add();
  ::CoreML::Specification::AddLayerParams* release_add();
  void set_allocated_add(::CoreML::Specification::AddLayerParams* add);

  // optional .CoreML.Specification.MultiplyLayerParams multiply = 231;
  bool has_multiply() const;
  void clear_multiply();
  static const int kMultiplyFieldNumber = 231;
  const ::CoreML::Specification::MultiplyLayerParams& multiply() const;
  ::CoreML::Specification::MultiplyLayerParams* mutable_multiply();
  ::CoreML::Specification::MultiplyLayerParams* release_multiply();
  void set_allocated_multiply(::CoreML::Specification::MultiplyLayerParams* multiply);

  // optional .CoreML.Specification.AverageLayerParams average = 240;
  bool has_average() const;
  void clear_average();
  static const int kAverageFieldNumber = 240;
  const ::CoreML::Specification::AverageLayerParams& average() const;
  ::CoreML::Specification::AverageLayerParams* mutable_average();
  ::CoreML::Specification::AverageLayerParams* release_average();
  void set_allocated_average(::CoreML::Specification::AverageLayerParams* average);

  // optional .CoreML.Specification.ScaleLayerParams scale = 245;
  bool has_scale() const;
  void clear_scale();
  static const int kScaleFieldNumber = 245;
  const ::CoreML::Specification::ScaleLayerParams& scale() const;
  ::CoreML::Specification::ScaleLayerParams* mutable_scale();
  ::CoreML::Specification::ScaleLayerParams* release_scale();
  void set_allocated_scale(::CoreML::Specification::ScaleLayerParams* scale);

  // optional .CoreML.Specification.BiasLayerParams bias = 250;
  bool has_bias() const;
  void clear_bias();
  static const int kBiasFieldNumber = 250;
  const ::CoreML::Specification::BiasLayerParams& bias() const;
  ::CoreML::Specification::BiasLayerParams* mutable_bias();
  ::CoreML::Specification::BiasLayerParams* release_bias();
  void set_allocated_bias(::CoreML::Specification::BiasLayerParams* bias);

  // optional .CoreML.Specification.MaxLayerParams max = 260;
  bool has_max() const;
  void clear_max();
  static const int kMaxFieldNumber = 260;
  const ::CoreML::Specification::MaxLayerParams& max() const;
  ::CoreML::Specification::MaxLayerParams* mutable_max();
  ::CoreML::Specification::MaxLayerParams* release_max();
  void set_allocated_max(::CoreML::Specification::MaxLayerParams* max);

  // optional .CoreML.Specification.MinLayerParams min = 261;
  bool has_min() const;
  void clear_min();
  static const int kMinFieldNumber = 261;
  const ::CoreML::Specification::MinLayerParams& min() const;
  ::CoreML::Specification::MinLayerParams* mutable_min();
  ::CoreML::Specification::MinLayerParams* release_min();
  void set_allocated_min(::CoreML::Specification::MinLayerParams* min);

  // optional .CoreML.Specification.DotProductLayerParams dot = 270;
  bool has_dot() const;
  void clear_dot();
  static const int kDotFieldNumber = 270;
  const ::CoreML::Specification::DotProductLayerParams& dot() const;
  ::CoreML::Specification::DotProductLayerParams* mutable_dot();
  ::CoreML::Specification::DotProductLayerParams* release_dot();
  void set_allocated_dot(::CoreML::Specification::DotProductLayerParams* dot);

  // optional .CoreML.Specification.ReduceLayerParams reduce = 280;
  bool has_reduce() const;
  void clear_reduce();
  static const int kReduceFieldNumber = 280;
  const ::CoreML::Specification::ReduceLayerParams& reduce() const;
  ::CoreML::Specification::ReduceLayerParams* mutable_reduce();
  ::CoreML::Specification::ReduceLayerParams* release_reduce();
  void set_allocated_reduce(::CoreML::Specification::ReduceLayerParams* reduce);

  // optional .CoreML.Specification.LoadConstantLayerParams loadConstant = 290;
  bool has_loadconstant() const;
  void clear_loadconstant();
  static const int kLoadConstantFieldNumber = 290;
  const ::CoreML::Specification::LoadConstantLayerParams& loadconstant() const;
  ::CoreML::Specification::LoadConstantLayerParams* mutable_loadconstant();
  ::CoreML::Specification::LoadConstantLayerParams* release_loadconstant();
  void set_allocated_loadconstant(::CoreML::Specification::LoadConstantLayerParams* loadconstant);

  // optional .CoreML.Specification.ReshapeLayerParams reshape = 300;
  bool has_reshape() const;
  void clear_reshape();
  static const int kReshapeFieldNumber = 300;
  const ::CoreML::Specification::ReshapeLayerParams& reshape() const;
  ::CoreML::Specification::ReshapeLayerParams* mutable_reshape();
  ::CoreML::Specification::ReshapeLayerParams* release_reshape();
  void set_allocated_reshape(::CoreML::Specification::ReshapeLayerParams* reshape);

  // optional .CoreML.Specification.FlattenLayerParams flatten = 301;
  bool has_flatten() const;
  void clear_flatten();
  static const int kFlattenFieldNumber = 301;
  const ::CoreML::Specification::FlattenLayerParams& flatten() const;
  ::CoreML::Specification::FlattenLayerParams* mutable_flatten();
  ::CoreML::Specification::FlattenLayerParams* release_flatten();
  void set_allocated_flatten(::CoreML::Specification::FlattenLayerParams* flatten);

  // optional .CoreML.Specification.PermuteLayerParams permute = 310;
  bool has_permute() const;
  void clear_permute();
  static const int kPermuteFieldNumber = 310;
  const ::CoreML::Specification::PermuteLayerParams& permute() const;
  ::CoreML::Specification::PermuteLayerParams* mutable_permute();
  ::CoreML::Specification::PermuteLayerParams* release_permute();
  void set_allocated_permute(::CoreML::Specification::PermuteLayerParams* permute);

  // optional .CoreML.Specification.ConcatLayerParams concat = 320;
  bool has_concat() const;
  void clear_concat();
  static const int kConcatFieldNumber = 320;
  const ::CoreML::Specification::ConcatLayerParams& concat() const;
  ::CoreML::Specification::ConcatLayerParams* mutable_concat();
  ::CoreML::Specification::ConcatLayerParams* release_concat();
  void set_allocated_concat(::CoreML::Specification::ConcatLayerParams* concat);

  // optional .CoreML.Specification.SplitLayerParams split = 330;
  bool has_split() const;
  void clear_split();
  static const int kSplitFieldNumber = 330;
  const ::CoreML::Specification::SplitLayerParams& split() const;
  ::CoreML::Specification::SplitLayerParams* mutable_split();
  ::CoreML::Specification::SplitLayerParams* release_split();
  void set_allocated_split(::CoreML::Specification::SplitLayerParams* split);

  // optional .CoreML.Specification.SequenceRepeatLayerParams sequenceRepeat = 340;
  bool has_sequencerepeat() const;
  void clear_sequencerepeat();
  static const int kSequenceRepeatFieldNumber = 340;
  const ::CoreML::Specification::SequenceRepeatLayerParams& sequencerepeat() const;
  ::CoreML::Specification::SequenceRepeatLayerParams* mutable_sequencerepeat();
  ::CoreML::Specification::SequenceRepeatLayerParams* release_sequencerepeat();
  void set_allocated_sequencerepeat(::CoreML::Specification::SequenceRepeatLayerParams* sequencerepeat);

  // optional .CoreML.Specification.SimpleRecurrentLayerParams simpleRecurrent = 400;
  bool has_simplerecurrent() const;
  void clear_simplerecurrent();
  static const int kSimpleRecurrentFieldNumber = 400;
  const ::CoreML::Specification::SimpleRecurrentLayerParams& simplerecurrent() const;
  ::CoreML::Specification::SimpleRecurrentLayerParams* mutable_simplerecurrent();
  ::CoreML::Specification::SimpleRecurrentLayerParams* release_simplerecurrent();
  void set_allocated_simplerecurrent(::CoreML::Specification::SimpleRecurrentLayerParams* simplerecurrent);

  // optional .CoreML.Specification.GRULayerParams gru = 410;
  bool has_gru() const;
  void clear_gru();
  static const int kGruFieldNumber = 410;
  const ::CoreML::Specification::GRULayerParams& gru() const;
  ::CoreML::Specification::GRULayerParams* mutable_gru();
  ::CoreML::Specification::GRULayerParams* release_gru();
  void set_allocated_gru(::CoreML::Specification::GRULayerParams* gru);

  // optional .CoreML.Specification.UniDirectionalLSTMLayerParams uniDirectionalLSTM = 420;
  bool has_unidirectionallstm() const;
  void clear_unidirectionallstm();
  static const int kUniDirectionalLSTMFieldNumber = 420;
  const ::CoreML::Specification::UniDirectionalLSTMLayerParams& unidirectionallstm() const;
  ::CoreML::Specification::UniDirectionalLSTMLayerParams* mutable_unidirectionallstm();
  ::CoreML::Specification::UniDirectionalLSTMLayerParams* release_unidirectionallstm();
  void set_allocated_unidirectionallstm(::CoreML::Specification::UniDirectionalLSTMLayerParams* unidirectionallstm);

  // optional .CoreML.Specification.BiDirectionalLSTMLayerParams biDirectionalLSTM = 430;
  bool has_bidirectionallstm() const;
  void clear_bidirectionallstm();
  static const int kBiDirectionalLSTMFieldNumber = 430;
  const ::CoreML::Specification::BiDirectionalLSTMLayerParams& bidirectionallstm() const;
  ::CoreML::Specification::BiDirectionalLSTMLayerParams* mutable_bidirectionallstm();
  ::CoreML::Specification::BiDirectionalLSTMLayerParams* release_bidirectionallstm();
  void set_allocated_bidirectionallstm(::CoreML::Specification::BiDirectionalLSTMLayerParams* bidirectionallstm);

  LayerCase layer_case() const;
  // @@protoc_insertion_point(class_scope:CoreML.Specification.NeuralNetworkLayer)
 private:
  inline void set_has_convolution();
  inline void set_has_pooling();
  inline void set_has_activation();
  inline void set_has_innerproduct();
  inline void set_has_embedding();
  inline void set_has_batchnorm();
  inline void set_has_mvn();
  inline void set_has_l2normalize();
  inline void set_has_softmax();
  inline void set_has_lrn();
  inline void set_has_crop();
  inline void set_has_padding();
  inline void set_has_upsample();
  inline void set_has_unary();
  inline void set_has_add();
  inline void set_has_multiply();
  inline void set_has_average();
  inline void set_has_scale();
  inline void set_has_bias();
  inline void set_has_max();
  inline void set_has_min();
  inline void set_has_dot();
  inline void set_has_reduce();
  inline void set_has_loadconstant();
  inline void set_has_reshape();
  inline void set_has_flatten();
  inline void set_has_permute();
  inline void set_has_concat();
  inline void set_has_split();
  inline void set_has_sequencerepeat();
  inline void set_has_simplerecurrent();
  inline void set_has_gru();
  inline void set_has_unidirectionallstm();
  inline void set_has_bidirectionallstm();

  inline bool has_layer() const;
  void clear_layer();
  inline void clear_has_layer();

  ::google::protobuf::internal::ArenaStringPtr _unknown_fields_;
  ::google::protobuf::Arena* _arena_ptr_;

  ::google::protobuf::RepeatedPtrField< ::std::string> input_;
  ::google::protobuf::RepeatedPtrField< ::std::string> output_;
  ::google::protobuf::internal::ArenaStringPtr name_;
  union LayerUnion {
    LayerUnion() {}
    ::CoreML::Specification::ConvolutionLayerParams* convolution_;
    ::CoreML::Specification::PoolingLayerParams* pooling_;
    ::CoreML::Specification::ActivationParams* activation_;
    ::CoreML::Specification::InnerProductLayerParams* innerproduct_;
    ::CoreML::Specification::EmbeddingLayerParams* embedding_;
    ::CoreML::Specification::BatchnormLayerParams* batchnorm_;
    ::CoreML::Specification::MeanVarianceNormalizeLayerParams* mvn_;
    ::CoreML::Specification::L2NormalizeLayerParams* l2normalize_;
    ::CoreML::Specification::SoftmaxLayerParams* softmax_;
    ::CoreML::Specification::LRNLayerParams* lrn_;
    ::CoreML::Specification::CropLayerParams* crop_;
    ::CoreML::Specification::PaddingLayerParams* padding_;
    ::CoreML::Specification::UpsampleLayerParams* upsample_;
    ::CoreML::Specification::UnaryFunctionLayerParams* unary_;
    ::CoreML::Specification::AddLayerParams* add_;
    ::CoreML::Specification::MultiplyLayerParams* multiply_;
    ::CoreML::Specification::AverageLayerParams* average_;
    ::CoreML::Specification::ScaleLayerParams* scale_;
    ::CoreML::Specification::BiasLayerParams* bias_;
    ::CoreML::Specification::MaxLayerParams* max_;
    ::CoreML::Specification::MinLayerParams* min_;
    ::CoreML::Specification::DotProductLayerParams* dot_;
    ::CoreML::Specification::ReduceLayerParams* reduce_;
    ::CoreML::Specification::LoadConstantLayerParams* loadconstant_;
    ::CoreML::Specification::ReshapeLayerParams* reshape_;
    ::CoreML::Specification::FlattenLayerParams* flatten_;
    ::CoreML::Specification::PermuteLayerParams* permute_;
    ::CoreML::Specification::ConcatLayerParams* concat_;
    ::CoreML::Specification::SplitLayerParams* split_;
    ::CoreML::Specification::SequenceRepeatLayerParams* sequencerepeat_;
    ::CoreML::Specification::SimpleRecurrentLayerParams* simplerecurrent_;
    ::CoreML::Specification::GRULayerParams* gru_;
    ::CoreML::Specification::UniDirectionalLSTMLayerParams* unidirectionallstm_;
    ::CoreML::Specification::BiDirectionalLSTMLayerParams* bidirectionallstm_;
  } layer_;
  mutable int _cached_size_;
  ::google::protobuf::uint32 _oneof_case_[1];

  friend void  protobuf_InitDefaults_NeuralNetwork_2eproto_impl();
  friend void  protobuf_AddDesc_NeuralNetwork_2eproto_impl();
  friend void protobuf_AssignDesc_NeuralNetwork_2eproto();
  friend void protobuf_ShutdownFile_NeuralNetwork_2eproto();

  void InitAsDefaultInstance();
};
extern ::google::protobuf::internal::ExplicitlyConstructed<NeuralNetworkLayer> NeuralNetworkLayer_default_instance_;

// -------------------------------------------------------------------

class BorderAmounts_EdgeSizes : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.BorderAmounts.EdgeSizes) */ {
 public:
  BorderAmounts_EdgeSizes();
  virtual ~BorderAmounts_EdgeSizes();

  BorderAmounts_EdgeSizes(const BorderAmounts_EdgeSizes& from);

  inline BorderAmounts_EdgeSizes& operator=(const BorderAmounts_EdgeSizes& from) {
    CopyFrom(from);
    return *this;
  }

  static const BorderAmounts_EdgeSizes& default_instance();

  static const BorderAmounts_EdgeSizes* internal_default_instance();

  void Swap(BorderAmounts_EdgeSizes* other);

  // implements Message ----------------------------------------------

  inline BorderAmounts_EdgeSizes* New() const { return New(NULL); }

  BorderAmounts_EdgeSizes* New(::google::protobuf::Arena* arena) const;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from);
  void CopyFrom(const BorderAmounts_EdgeSizes& from);
  void MergeFrom(const BorderAmounts_EdgeSizes& from);
  void Clear();
  bool IsInitialized() const;

  size_t ByteSizeLong() const;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input);
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const;
  void DiscardUnknownFields();
  int GetCachedSize() const { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(BorderAmounts_EdgeSizes* other);
  void UnsafeMergeFrom(const BorderAmounts_EdgeSizes& from);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return _arena_ptr_;
  }
  inline ::google::protobuf::Arena* MaybeArenaPtr() const {
    return _arena_ptr_;
  }
  public:

  ::std::string GetTypeName() const;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // optional uint64 startEdgeSize = 1;
  void clear_startedgesize();
  static const int kStartEdgeSizeFieldNumber = 1;
  ::google::protobuf::uint64 startedgesize() const;
  void set_startedgesize(::google::protobuf::uint64 value);

  // optional uint64 endEdgeSize = 2;
  void clear_endedgesize();
  static const int kEndEdgeSizeFieldNumber = 2;
  ::google::protobuf::uint64 endedgesize() const;
  void set_endedgesize(::google::protobuf::uint64 value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.BorderAmounts.EdgeSizes)
 private:

  ::google::protobuf::internal::ArenaStringPtr _unknown_fields_;
  ::google::protobuf::Arena* _arena_ptr_;

  ::google::protobuf::uint64 startedgesize_;
  ::google::protobuf::uint64 endedgesize_;
  mutable int _cached_size_;
  friend void  protobuf_InitDefaults_NeuralNetwork_2eproto_impl();
  friend void  protobuf_AddDesc_NeuralNetwork_2eproto_impl();
  friend void protobuf_AssignDesc_NeuralNetwork_2eproto();
  friend void protobuf_ShutdownFile_NeuralNetwork_2eproto();

  void InitAsDefaultInstance();
};
extern ::google::protobuf::internal::ExplicitlyConstructed<BorderAmounts_EdgeSizes> BorderAmounts_EdgeSizes_default_instance_;

// -------------------------------------------------------------------

class BorderAmounts : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.BorderAmounts) */ {
 public:
  BorderAmounts();
  virtual ~BorderAmounts();

  BorderAmounts(const BorderAmounts& from);

  inline BorderAmounts& operator=(const BorderAmounts& from) {
    CopyFrom(from);
    return *this;
  }

  static const BorderAmounts& default_instance();

  static const BorderAmounts* internal_default_instance();

  void Swap(BorderAmounts* other);

  // implements Message ----------------------------------------------

  inline BorderAmounts* New() const { return New(NULL); }

  BorderAmounts* New(::google::protobuf::Arena* arena) const;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from);
  void CopyFrom(const BorderAmounts& from);
  void MergeFrom(const BorderAmounts& from);
  void Clear();
  bool IsInitialized() const;

  size_t ByteSizeLong() const;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input);
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const;
  void DiscardUnknownFields();
  int GetCachedSize() const { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(BorderAmounts* other);
  void UnsafeMergeFrom(const BorderAmounts& from);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return _arena_ptr_;
  }
  inline ::google::protobuf::Arena* MaybeArenaPtr() const {
    return _arena_ptr_;
  }
  public:

  ::std::string GetTypeName() const;

  // nested types ----------------------------------------------------

  typedef BorderAmounts_EdgeSizes EdgeSizes;

  // accessors -------------------------------------------------------

  // repeated .CoreML.Specification.BorderAmounts.EdgeSizes borderAmounts = 10;
  int borderamounts_size() const;
  void clear_borderamounts();
  static const int kBorderAmountsFieldNumber = 10;
  const ::CoreML::Specification::BorderAmounts_EdgeSizes& borderamounts(int index) const;
  ::CoreML::Specification::BorderAmounts_EdgeSizes* mutable_borderamounts(int index);
  ::CoreML::Specification::BorderAmounts_EdgeSizes* add_borderamounts();
  ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::BorderAmounts_EdgeSizes >*
      mutable_borderamounts();
  const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::BorderAmounts_EdgeSizes >&
      borderamounts() const;

  // @@protoc_insertion_point(class_scope:CoreML.Specification.BorderAmounts)
 private:

  ::google::protobuf::internal::ArenaStringPtr _unknown_fields_;
  ::google::protobuf::Arena* _arena_ptr_;

  ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::BorderAmounts_EdgeSizes > borderamounts_;
  mutable int _cached_size_;
  friend void  protobuf_InitDefaults_NeuralNetwork_2eproto_impl();
  friend void  protobuf_AddDesc_NeuralNetwork_2eproto_impl();
  friend void protobuf_AssignDesc_NeuralNetwork_2eproto();
  friend void protobuf_ShutdownFile_NeuralNetwork_2eproto();

  void InitAsDefaultInstance();
};
extern ::google::protobuf::internal::ExplicitlyConstructed<BorderAmounts> BorderAmounts_default_instance_;

// -------------------------------------------------------------------

class ValidPadding : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ValidPadding) */ {
 public:
  ValidPadding();
  virtual ~ValidPadding();

  ValidPadding(const ValidPadding& from);

  inline ValidPadding& operator=(const ValidPadding& from) {
    CopyFrom(from);
    return *this;
  }

  static const ValidPadding& default_instance();

  static const ValidPadding* internal_default_instance();

  void Swap(ValidPadding* other);

  // implements Message ----------------------------------------------

  inline ValidPadding* New() const { return New(NULL); }

  ValidPadding* New(::google::protobuf::Arena* arena) const;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from);
  void CopyFrom(const ValidPadding& from);
  void MergeFrom(const ValidPadding& from);
  void Clear();
  bool IsInitialized() const;

  size_t ByteSizeLong() const;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input);
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const;
  void DiscardUnknownFields();
  int GetCachedSize() const { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ValidPadding* other);
  void UnsafeMergeFrom(const ValidPadding& from);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return _arena_ptr_;
  }
  inline ::google::protobuf::Arena* MaybeArenaPtr() const {
    return _arena_ptr_;
  }
  public:

  ::std::string GetTypeName() const;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // optional .CoreML.Specification.BorderAmounts paddingAmounts = 1;
  bool has_paddingamounts() const;
  void clear_paddingamounts();
  static const int kPaddingAmountsFieldNumber = 1;
  const ::CoreML::Specification::BorderAmounts& paddingamounts() const;
  ::CoreML::Specification::BorderAmounts* mutable_paddingamounts();
  ::CoreML::Specification::BorderAmounts* release_paddingamounts();
  void set_allocated_paddingamounts(::CoreML::Specification::BorderAmounts* paddingamounts);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ValidPadding)
 private:

  ::google::protobuf::internal::ArenaStringPtr _unknown_fields_;
  ::google::protobuf::Arena* _arena_ptr_;

  ::CoreML::Specification::BorderAmounts* paddingamounts_;
  mutable int _cached_size_;
  friend void  protobuf_InitDefaults_NeuralNetwork_2eproto_impl();
  friend void  protobuf_AddDesc_NeuralNetwork_2eproto_impl();
  friend void protobuf_AssignDesc_NeuralNetwork_2eproto();
  friend void protobuf_ShutdownFile_NeuralNetwork_2eproto();

  void InitAsDefaultInstance();
};
extern ::google::protobuf::internal::ExplicitlyConstructed<ValidPadding> ValidPadding_default_instance_;

// -------------------------------------------------------------------

class SamePadding : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.SamePadding) */ {
 public:
  SamePadding();
  virtual ~SamePadding();

  SamePadding(const SamePadding& from);

  inline SamePadding& operator=(const SamePadding& from) {
    CopyFrom(from);
    return *this;
  }

  static const SamePadding& default_instance();

  static const SamePadding* internal_default_instance();

  void Swap(SamePadding* other);

  // implements Message ----------------------------------------------

  inline SamePadding* New() const { return New(NULL); }

  SamePadding* New(::google::protobuf::Arena* arena) const;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from);
  void CopyFrom(const SamePadding& from);
  void MergeFrom(const SamePadding& from);
  void Clear();
  bool IsInitialized() const;

  size_t ByteSizeLong() const;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input);
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const;
  void DiscardUnknownFields();
  int GetCachedSize() const { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(SamePadding* other);
  void UnsafeMergeFrom(const SamePadding& from);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return _arena_ptr_;
  }
  inline ::google::protobuf::Arena* MaybeArenaPtr() const {
    return _arena_ptr_;
  }
  public:

  ::std::string GetTypeName() const;

  // nested types ----------------------------------------------------

  typedef SamePadding_SamePaddingMode SamePaddingMode;
  static const SamePaddingMode BOTTOM_RIGHT_HEAVY =
    SamePadding_SamePaddingMode_BOTTOM_RIGHT_HEAVY;
  static const SamePaddingMode TOP_LEFT_HEAVY =
    SamePadding_SamePaddingMode_TOP_LEFT_HEAVY;
  static inline bool SamePaddingMode_IsValid(int value) {
    return SamePadding_SamePaddingMode_IsValid(value);
  }
  static const SamePaddingMode SamePaddingMode_MIN =
    SamePadding_SamePaddingMode_SamePaddingMode_MIN;
  static const SamePaddingMode SamePaddingMode_MAX =
    SamePadding_SamePaddingMode_SamePaddingMode_MAX;
  static const int SamePaddingMode_ARRAYSIZE =
    SamePadding_SamePaddingMode_SamePaddingMode_ARRAYSIZE;

  // accessors -------------------------------------------------------

  // optional .CoreML.Specification.SamePadding.SamePaddingMode asymmetryMode = 1;
  void clear_asymmetrymode();
  static const int kAsymmetryModeFieldNumber = 1;
  ::CoreML::Specification::SamePadding_SamePaddingMode asymmetrymode() const;
  void set_asymmetrymode(::CoreML::Specification::SamePadding_SamePaddingMode value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.SamePadding)
 private:

  ::google::protobuf::internal::ArenaStringPtr _unknown_fields_;
  ::google::protobuf::Arena* _arena_ptr_;

  int asymmetrymode_;
  mutable int _cached_size_;
  friend void  protobuf_InitDefaults_NeuralNetwork_2eproto_impl();
  friend void  protobuf_AddDesc_NeuralNetwork_2eproto_impl();
  friend void protobuf_AssignDesc_NeuralNetwork_2eproto();
  friend void protobuf_ShutdownFile_NeuralNetwork_2eproto();

  void InitAsDefaultInstance();
};
extern ::google::protobuf::internal::ExplicitlyConstructed<SamePadding> SamePadding_default_instance_;

// -------------------------------------------------------------------

class WeightParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.WeightParams) */ {
 public:
  WeightParams();
  virtual ~WeightParams();

  WeightParams(const WeightParams& from);

  inline WeightParams& operator=(const WeightParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const WeightParams& default_instance();

  static const WeightParams* internal_default_instance();

  void Swap(WeightParams* other);

  // implements Message ----------------------------------------------

  inline WeightParams* New() const { return New(NULL); }

  WeightParams* New(::google::protobuf::Arena* arena) const;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from);
  void CopyFrom(const WeightParams& from);
  void MergeFrom(const WeightParams& from);
  void Clear();
  bool IsInitialized() const;

  size_t ByteSizeLong() const;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input);
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const;
  void DiscardUnknownFields();
  int GetCachedSize() const { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(WeightParams* other);
  void UnsafeMergeFrom(const WeightParams& from);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return _arena_ptr_;
  }
  inline ::google::protobuf::Arena* MaybeArenaPtr() const {
    return _arena_ptr_;
  }
  public:

  ::std::string GetTypeName() const;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated float floatValue = 1;
  int floatvalue_size() const;
  void clear_floatvalue();
  static const int kFloatValueFieldNumber = 1;
  float floatvalue(int index) const;
  void set_floatvalue(int index, float value);
  void add_floatvalue(float value);
  const ::google::protobuf::RepeatedField< float >&
      floatvalue() const;
  ::google::protobuf::RepeatedField< float >*
      mutable_floatvalue();

  // @@protoc_insertion_point(class_scope:CoreML.Specification.WeightParams)
 private:

  ::google::protobuf::internal::ArenaStringPtr _unknown_fields_;
  ::google::protobuf::Arena* _arena_ptr_;

  ::google::protobuf::RepeatedField< float > floatvalue_;
  mutable int _floatvalue_cached_byte_size_;
  mutable int _cached_size_;
  friend void  protobuf_InitDefaults_NeuralNetwork_2eproto_impl();
  friend void  protobuf_AddDesc_NeuralNetwork_2eproto_impl();
  friend void protobuf_AssignDesc_NeuralNetwork_2eproto();
  friend void protobuf_ShutdownFile_NeuralNetwork_2eproto();

  void InitAsDefaultInstance();
};
extern ::google::protobuf::internal::ExplicitlyConstructed<WeightParams> WeightParams_default_instance_;

// -------------------------------------------------------------------

class ConvolutionLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ConvolutionLayerParams) */ {
 public:
  ConvolutionLayerParams();
  virtual ~ConvolutionLayerParams();

  ConvolutionLayerParams(const ConvolutionLayerParams& from);

  inline ConvolutionLayerParams& operator=(const ConvolutionLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const ConvolutionLayerParams& default_instance();

  enum ConvolutionPaddingTypeCase {
    kValid = 50,
    kSame = 51,
    CONVOLUTIONPADDINGTYPE_NOT_SET = 0,
  };

  static const ConvolutionLayerParams* internal_default_instance();

  void Swap(ConvolutionLayerParams* other);

  // implements Message ----------------------------------------------

  inline ConvolutionLayerParams* New() const { return New(NULL); }

  ConvolutionLayerParams* New(::google::protobuf::Arena* arena) const;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from);
  void CopyFrom(const ConvolutionLayerParams& from);
  void MergeFrom(const ConvolutionLayerParams& from);
  void Clear();
  bool IsInitialized() const;

  size_t ByteSizeLong() const;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input);
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const;
  void DiscardUnknownFields();
  int GetCachedSize() const { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ConvolutionLayerParams* other);
  void UnsafeMergeFrom(const ConvolutionLayerParams& from);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return _arena_ptr_;
  }
  inline ::google::protobuf::Arena* MaybeArenaPtr() const {
    return _arena_ptr_;
  }
  public:

  ::std::string GetTypeName() const;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // optional uint64 outputChannels = 1;
  void clear_outputchannels();
  static const int kOutputChannelsFieldNumber = 1;
  ::google::protobuf::uint64 outputchannels() const;
  void set_outputchannels(::google::protobuf::uint64 value);

  // optional uint64 kernelChannels = 2;
  void clear_kernelchannels();
  static const int kKernelChannelsFieldNumber = 2;
  ::google::protobuf::uint64 kernelchannels() const;
  void set_kernelchannels(::google::protobuf::uint64 value);

  // optional uint64 nGroups = 10;
  void clear_ngroups();
  static const int kNGroupsFieldNumber = 10;
  ::google::protobuf::uint64 ngroups() const;
  void set_ngroups(::google::protobuf::uint64 value);

  // repeated uint64 kernelSize = 20;
  int kernelsize_size() const;
  void clear_kernelsize();
  static const int kKernelSizeFieldNumber = 20;
  ::google::protobuf::uint64 kernelsize(int index) const;
  void set_kernelsize(int index, ::google::protobuf::uint64 value);
  void add_kernelsize(::google::protobuf::uint64 value);
  const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
      kernelsize() const;
  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
      mutable_kernelsize();

  // repeated uint64 stride = 30;
  int stride_size() const;
  void clear_stride();
  static const int kStrideFieldNumber = 30;
  ::google::protobuf::uint64 stride(int index) const;
  void set_stride(int index, ::google::protobuf::uint64 value);
  void add_stride(::google::protobuf::uint64 value);
  const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
      stride() const;
  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
      mutable_stride();

  // repeated uint64 dilationFactor = 40;
  int dilationfactor_size() const;
  void clear_dilationfactor();
  static const int kDilationFactorFieldNumber = 40;
  ::google::protobuf::uint64 dilationfactor(int index) const;
  void set_dilationfactor(int index, ::google::protobuf::uint64 value);
  void add_dilationfactor(::google::protobuf::uint64 value);
  const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
      dilationfactor() const;
  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
      mutable_dilationfactor();

  // optional .CoreML.Specification.ValidPadding valid = 50;
  bool has_valid() const;
  void clear_valid();
  static const int kValidFieldNumber = 50;
  const ::CoreML::Specification::ValidPadding& valid() const;
  ::CoreML::Specification::ValidPadding* mutable_valid();
  ::CoreML::Specification::ValidPadding* release_valid();
  void set_allocated_valid(::CoreML::Specification::ValidPadding* valid);

  // optional .CoreML.Specification.SamePadding same = 51;
  bool has_same() const;
  void clear_same();
  static const int kSameFieldNumber = 51;
  const ::CoreML::Specification::SamePadding& same() const;
  ::CoreML::Specification::SamePadding* mutable_same();
  ::CoreML::Specification::SamePadding* release_same();
  void set_allocated_same(::CoreML::Specification::SamePadding* same);

  // optional bool isDeconvolution = 60;
  void clear_isdeconvolution();
  static const int kIsDeconvolutionFieldNumber = 60;
  bool isdeconvolution() const;
  void set_isdeconvolution(bool value);

  // optional bool hasBias = 70;
  void clear_hasbias();
  static const int kHasBiasFieldNumber = 70;
  bool hasbias() const;
  void set_hasbias(bool value);

  // optional .CoreML.Specification.WeightParams weights = 90;
  bool has_weights() const;
  void clear_weights();
  static const int kWeightsFieldNumber = 90;
  const ::CoreML::Specification::WeightParams& weights() const;
  ::CoreML::Specification::WeightParams* mutable_weights();
  ::CoreML::Specification::WeightParams* release_weights();
  void set_allocated_weights(::CoreML::Specification::WeightParams* weights);

  // optional .CoreML.Specification.WeightParams bias = 91;
  bool has_bias() const;
  void clear_bias();
  static const int kBiasFieldNumber = 91;
  const ::CoreML::Specification::WeightParams& bias() const;
  ::CoreML::Specification::WeightParams* mutable_bias();
  ::CoreML::Specification::WeightParams* release_bias();
  void set_allocated_bias(::CoreML::Specification::WeightParams* bias);

  // repeated uint64 outputShape = 100;
  int outputshape_size() const;
  void clear_outputshape();
  static const int kOutputShapeFieldNumber = 100;
  ::google::protobuf::uint64 outputshape(int index) const;
  void set_outputshape(int index, ::google::protobuf::uint64 value);
  void add_outputshape(::google::protobuf::uint64 value);
  const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
      outputshape() const;
  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
      mutable_outputshape();

  ConvolutionPaddingTypeCase ConvolutionPaddingType_case() const;
  // @@protoc_insertion_point(class_scope:CoreML.Specification.ConvolutionLayerParams)
 private:
  inline void set_has_valid();
  inline void set_has_same();

  inline bool has_ConvolutionPaddingType() const;
  void clear_ConvolutionPaddingType();
  inline void clear_has_ConvolutionPaddingType();

  ::google::protobuf::internal::ArenaStringPtr _unknown_fields_;
  ::google::protobuf::Arena* _arena_ptr_;

  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 > kernelsize_;
  mutable int _kernelsize_cached_byte_size_;
  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 > stride_;
  mutable int _stride_cached_byte_size_;
  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 > dilationfactor_;
  mutable int _dilationfactor_cached_byte_size_;
  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 > outputshape_;
  mutable int _outputshape_cached_byte_size_;
  ::CoreML::Specification::WeightParams* weights_;
  ::CoreML::Specification::WeightParams* bias_;
  ::google::protobuf::uint64 outputchannels_;
  ::google::protobuf::uint64 kernelchannels_;
  ::google::protobuf::uint64 ngroups_;
  bool isdeconvolution_;
  bool hasbias_;
  union ConvolutionPaddingTypeUnion {
    ConvolutionPaddingTypeUnion() {}
    ::CoreML::Specification::ValidPadding* valid_;
    ::CoreML::Specification::SamePadding* same_;
  } ConvolutionPaddingType_;
  mutable int _cached_size_;
  ::google::protobuf::uint32 _oneof_case_[1];

  friend void  protobuf_InitDefaults_NeuralNetwork_2eproto_impl();
  friend void  protobuf_AddDesc_NeuralNetwork_2eproto_impl();
  friend void protobuf_AssignDesc_NeuralNetwork_2eproto();
  friend void protobuf_ShutdownFile_NeuralNetwork_2eproto();

  void InitAsDefaultInstance();
};
extern ::google::protobuf::internal::ExplicitlyConstructed<ConvolutionLayerParams> ConvolutionLayerParams_default_instance_;

// -------------------------------------------------------------------

class InnerProductLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.InnerProductLayerParams) */ {
 public:
  InnerProductLayerParams();
  virtual ~InnerProductLayerParams();

  InnerProductLayerParams(const InnerProductLayerParams& from);

  inline InnerProductLayerParams& operator=(const InnerProductLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const InnerProductLayerParams& default_instance();

  static const InnerProductLayerParams* internal_default_instance();

  void Swap(InnerProductLayerParams* other);

  // implements Message ----------------------------------------------

  inline InnerProductLayerParams* New() const { return New(NULL); }

  InnerProductLayerParams* New(::google::protobuf::Arena* arena) const;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from);
  void CopyFrom(const InnerProductLayerParams& from);
  void MergeFrom(const InnerProductLayerParams& from);
  void Clear();
  bool IsInitialized() const;

  size_t ByteSizeLong() const;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input);
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const;
  void DiscardUnknownFields();
  int GetCachedSize() const { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(InnerProductLayerParams* other);
  void UnsafeMergeFrom(const InnerProductLayerParams& from);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return _arena_ptr_;
  }
  inline ::google::protobuf::Arena* MaybeArenaPtr() const {
    return _arena_ptr_;
  }
  public:

  ::std::string GetTypeName() const;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // optional uint64 inputChannels = 1;
  void clear_inputchannels();
  static const int kInputChannelsFieldNumber = 1;
  ::google::protobuf::uint64 inputchannels() const;
  void set_inputchannels(::google::protobuf::uint64 value);

  // optional uint64 outputChannels = 2;
  void clear_outputchannels();
  static const int kOutputChannelsFieldNumber = 2;
  ::google::protobuf::uint64 outputchannels() const;
  void set_outputchannels(::google::protobuf::uint64 value);

  // optional bool hasBias = 10;
  void clear_hasbias();
  static const int kHasBiasFieldNumber = 10;
  bool hasbias() const;
  void set_hasbias(bool value);

  // optional .CoreML.Specification.WeightParams weights = 20;
  bool has_weights() const;
  void clear_weights();
  static const int kWeightsFieldNumber = 20;
  const ::CoreML::Specification::WeightParams& weights() const;
  ::CoreML::Specification::WeightParams* mutable_weights();
  ::CoreML::Specification::WeightParams* release_weights();
  void set_allocated_weights(::CoreML::Specification::WeightParams* weights);

  // optional .CoreML.Specification.WeightParams bias = 21;
  bool has_bias() const;
  void clear_bias();
  static const int kBiasFieldNumber = 21;
  const ::CoreML::Specification::WeightParams& bias() const;
  ::CoreML::Specification::WeightParams* mutable_bias();
  ::CoreML::Specification::WeightParams* release_bias();
  void set_allocated_bias(::CoreML::Specification::WeightParams* bias);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.InnerProductLayerParams)
 private:

  ::google::protobuf::internal::ArenaStringPtr _unknown_fields_;
  ::google::protobuf::Arena* _arena_ptr_;

  ::CoreML::Specification::WeightParams* weights_;
  ::CoreML::Specification::WeightParams* bias_;
  ::google::protobuf::uint64 inputchannels_;
  ::google::protobuf::uint64 outputchannels_;
  bool hasbias_;
  mutable int _cached_size_;
  friend void  protobuf_InitDefaults_NeuralNetwork_2eproto_impl();
  friend void  protobuf_AddDesc_NeuralNetwork_2eproto_impl();
  friend void protobuf_AssignDesc_NeuralNetwork_2eproto();
  friend void protobuf_ShutdownFile_NeuralNetwork_2eproto();

  void InitAsDefaultInstance();
};
extern ::google::protobuf::internal::ExplicitlyConstructed<InnerProductLayerParams> InnerProductLayerParams_default_instance_;

// -------------------------------------------------------------------

class EmbeddingLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.EmbeddingLayerParams) */ {
 public:
  EmbeddingLayerParams();
  virtual ~EmbeddingLayerParams();

  EmbeddingLayerParams(const EmbeddingLayerParams& from);

  inline EmbeddingLayerParams& operator=(const EmbeddingLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const EmbeddingLayerParams& default_instance();

  static const EmbeddingLayerParams* internal_default_instance();

  void Swap(EmbeddingLayerParams* other);

  // implements Message ----------------------------------------------

  inline EmbeddingLayerParams* New() const { return New(NULL); }

  EmbeddingLayerParams* New(::google::protobuf::Arena* arena) const;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from);
  void CopyFrom(const EmbeddingLayerParams& from);
  void MergeFrom(const EmbeddingLayerParams& from);
  void Clear();
  bool IsInitialized() const;

  size_t ByteSizeLong() const;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input);
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const;
  void DiscardUnknownFields();
  int GetCachedSize() const { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(EmbeddingLayerParams* other);
  void UnsafeMergeFrom(const EmbeddingLayerParams& from);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return _arena_ptr_;
  }
  inline ::google::protobuf::Arena* MaybeArenaPtr() const {
    return _arena_ptr_;
  }
  public:

  ::std::string GetTypeName() const;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // optional uint64 inputDim = 1;
  void clear_inputdim();
  static const int kInputDimFieldNumber = 1;
  ::google::protobuf::uint64 inputdim() const;
  void set_inputdim(::google::protobuf::uint64 value);

  // optional uint64 outputChannels = 2;
  void clear_outputchannels();
  static const int kOutputChannelsFieldNumber = 2;
  ::google::protobuf::uint64 outputchannels() const;
  void set_outputchannels(::google::protobuf::uint64 value);

  // optional bool hasBias = 10;
  void clear_hasbias();
  static const int kHasBiasFieldNumber = 10;
  bool hasbias() const;
  void set_hasbias(bool value);

  // optional .CoreML.Specification.WeightParams weights = 20;
  bool has_weights() const;
  void clear_weights();
  static const int kWeightsFieldNumber = 20;
  const ::CoreML::Specification::WeightParams& weights() const;
  ::CoreML::Specification::WeightParams* mutable_weights();
  ::CoreML::Specification::WeightParams* release_weights();
  void set_allocated_weights(::CoreML::Specification::WeightParams* weights);

  // optional .CoreML.Specification.WeightParams bias = 21;
  bool has_bias() const;
  void clear_bias();
  static const int kBiasFieldNumber = 21;
  const ::CoreML::Specification::WeightParams& bias() const;
  ::CoreML::Specification::WeightParams* mutable_bias();
  ::CoreML::Specification::WeightParams* release_bias();
  void set_allocated_bias(::CoreML::Specification::WeightParams* bias);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.EmbeddingLayerParams)
 private:

  ::google::protobuf::internal::ArenaStringPtr _unknown_fields_;
  ::google::protobuf::Arena* _arena_ptr_;

  ::CoreML::Specification::WeightParams* weights_;
  ::CoreML::Specification::WeightParams* bias_;
  ::google::protobuf::uint64 inputdim_;
  ::google::protobuf::uint64 outputchannels_;
  bool hasbias_;
  mutable int _cached_size_;
  friend void  protobuf_InitDefaults_NeuralNetwork_2eproto_impl();
  friend void  protobuf_AddDesc_NeuralNetwork_2eproto_impl();
  friend void protobuf_AssignDesc_NeuralNetwork_2eproto();
  friend void protobuf_ShutdownFile_NeuralNetwork_2eproto();

  void InitAsDefaultInstance();
};
extern ::google::protobuf::internal::ExplicitlyConstructed<EmbeddingLayerParams> EmbeddingLayerParams_default_instance_;

// -------------------------------------------------------------------

class BatchnormLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.BatchnormLayerParams) */ {
 public:
  BatchnormLayerParams();
  virtual ~BatchnormLayerParams();

  BatchnormLayerParams(const BatchnormLayerParams& from);

  inline BatchnormLayerParams& operator=(const BatchnormLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const BatchnormLayerParams& default_instance();

  static const BatchnormLayerParams* internal_default_instance();

  void Swap(BatchnormLayerParams* other);

  // implements Message ----------------------------------------------

  inline BatchnormLayerParams* New() const { return New(NULL); }

  BatchnormLayerParams* New(::google::protobuf::Arena* arena) const;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from);
  void CopyFrom(const BatchnormLayerParams& from);
  void MergeFrom(const BatchnormLayerParams& from);
  void Clear();
  bool IsInitialized() const;

  size_t ByteSizeLong() const;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input);
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const;
  void DiscardUnknownFields();
  int GetCachedSize() const { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(BatchnormLayerParams* other);
  void UnsafeMergeFrom(const BatchnormLayerParams& from);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return _arena_ptr_;
  }
  inline ::google::protobuf::Arena* MaybeArenaPtr() const {
    return _arena_ptr_;
  }
  public:

  ::std::string GetTypeName() const;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // optional uint64 channels = 1;
  void clear_channels();
  static const int kChannelsFieldNumber = 1;
  ::google::protobuf::uint64 channels() const;
  void set_channels(::google::protobuf::uint64 value);

  // optional bool computeMeanVar = 5;
  void clear_computemeanvar();
  static const int kComputeMeanVarFieldNumber = 5;
  bool computemeanvar() const;
  void set_computemeanvar(bool value);

  // optional bool instanceNormalization = 6;
  void clear_instancenormalization();
  static const int kInstanceNormalizationFieldNumber = 6;
  bool instancenormalization() const;
  void set_instancenormalization(bool value);

  // optional float epsilon = 10;
  void clear_epsilon();
  static const int kEpsilonFieldNumber = 10;
  float epsilon() const;
  void set_epsilon(float value);

  // optional .CoreML.Specification.WeightParams gamma = 15;
  bool has_gamma() const;
  void clear_gamma();
  static const int kGammaFieldNumber = 15;
  const ::CoreML::Specification::WeightParams& gamma() const;
  ::CoreML::Specification::WeightParams* mutable_gamma();
  ::CoreML::Specification::WeightParams* release_gamma();
  void set_allocated_gamma(::CoreML::Specification::WeightParams* gamma);

  // optional .CoreML.Specification.WeightParams beta = 16;
  bool has_beta() const;
  void clear_beta();
  static const int kBetaFieldNumber = 16;
  const ::CoreML::Specification::WeightParams& beta() const;
  ::CoreML::Specification::WeightParams* mutable_beta();
  ::CoreML::Specification::WeightParams* release_beta();
  void set_allocated_beta(::CoreML::Specification::WeightParams* beta);

  // optional .CoreML.Specification.WeightParams mean = 17;
  bool has_mean() const;
  void clear_mean();
  static const int kMeanFieldNumber = 17;
  const ::CoreML::Specification::WeightParams& mean() const;
  ::CoreML::Specification::WeightParams* mutable_mean();
  ::CoreML::Specification::WeightParams* release_mean();
  void set_allocated_mean(::CoreML::Specification::WeightParams* mean);

  // optional .CoreML.Specification.WeightParams variance = 18;
  bool has_variance() const;
  void clear_variance();
  static const int kVarianceFieldNumber = 18;
  const ::CoreML::Specification::WeightParams& variance() const;
  ::CoreML::Specification::WeightParams* mutable_variance();
  ::CoreML::Specification::WeightParams* release_variance();
  void set_allocated_variance(::CoreML::Specification::WeightParams* variance);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.BatchnormLayerParams)
 private:

  ::google::protobuf::internal::ArenaStringPtr _unknown_fields_;
  ::google::protobuf::Arena* _arena_ptr_;

  ::CoreML::Specification::WeightParams* gamma_;
  ::CoreML::Specification::WeightParams* beta_;
  ::CoreML::Specification::WeightParams* mean_;
  ::CoreML::Specification::WeightParams* variance_;
  ::google::protobuf::uint64 channels_;
  bool computemeanvar_;
  bool instancenormalization_;
  float epsilon_;
  mutable int _cached_size_;
  friend void  protobuf_InitDefaults_NeuralNetwork_2eproto_impl();
  friend void  protobuf_AddDesc_NeuralNetwork_2eproto_impl();
  friend void protobuf_AssignDesc_NeuralNetwork_2eproto();
  friend void protobuf_ShutdownFile_NeuralNetwork_2eproto();

  void InitAsDefaultInstance();
};
extern ::google::protobuf::internal::ExplicitlyConstructed<BatchnormLayerParams> BatchnormLayerParams_default_instance_;

// -------------------------------------------------------------------

class PoolingLayerParams_ValidCompletePadding : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.PoolingLayerParams.ValidCompletePadding) */ {
 public:
  PoolingLayerParams_ValidCompletePadding();
  virtual ~PoolingLayerParams_ValidCompletePadding();

  PoolingLayerParams_ValidCompletePadding(const PoolingLayerParams_ValidCompletePadding& from);

  inline PoolingLayerParams_ValidCompletePadding& operator=(const PoolingLayerParams_ValidCompletePadding& from) {
    CopyFrom(from);
    return *this;
  }

  static const PoolingLayerParams_ValidCompletePadding& default_instance();

  static const PoolingLayerParams_ValidCompletePadding* internal_default_instance();

  void Swap(PoolingLayerParams_ValidCompletePadding* other);

  // implements Message ----------------------------------------------

  inline PoolingLayerParams_ValidCompletePadding* New() const { return New(NULL); }

  PoolingLayerParams_ValidCompletePadding* New(::google::protobuf::Arena* arena) const;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from);
  void CopyFrom(const PoolingLayerParams_ValidCompletePadding& from);
  void MergeFrom(const PoolingLayerParams_ValidCompletePadding& from);
  void Clear();
  bool IsInitialized() const;

  size_t ByteSizeLong() const;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input);
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const;
  void DiscardUnknownFields();
  int GetCachedSize() const { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(PoolingLayerParams_ValidCompletePadding* other);
  void UnsafeMergeFrom(const PoolingLayerParams_ValidCompletePadding& from);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return _arena_ptr_;
  }
  inline ::google::protobuf::Arena* MaybeArenaPtr() const {
    return _arena_ptr_;
  }
  public:

  ::std::string GetTypeName() const;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated uint64 paddingAmounts = 10;
  int paddingamounts_size() const;
  void clear_paddingamounts();
  static const int kPaddingAmountsFieldNumber = 10;
  ::google::protobuf::uint64 paddingamounts(int index) const;
  void set_paddingamounts(int index, ::google::protobuf::uint64 value);
  void add_paddingamounts(::google::protobuf::uint64 value);
  const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
      paddingamounts() const;
  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
      mutable_paddingamounts();

  // @@protoc_insertion_point(class_scope:CoreML.Specification.PoolingLayerParams.ValidCompletePadding)
 private:

  ::google::protobuf::internal::ArenaStringPtr _unknown_fields_;
  ::google::protobuf::Arena* _arena_ptr_;

  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 > paddingamounts_;
  mutable int _paddingamounts_cached_byte_size_;
  mutable int _cached_size_;
  friend void  protobuf_InitDefaults_NeuralNetwork_2eproto_impl();
  friend void  protobuf_AddDesc_NeuralNetwork_2eproto_impl();
  friend void protobuf_AssignDesc_NeuralNetwork_2eproto();
  friend void protobuf_ShutdownFile_NeuralNetwork_2eproto();

  void InitAsDefaultInstance();
};
extern ::google::protobuf::internal::ExplicitlyConstructed<PoolingLayerParams_ValidCompletePadding> PoolingLayerParams_ValidCompletePadding_default_instance_;

// -------------------------------------------------------------------

class PoolingLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.PoolingLayerParams) */ {
 public:
  PoolingLayerParams();
  virtual ~PoolingLayerParams();

  PoolingLayerParams(const PoolingLayerParams& from);

  inline PoolingLayerParams& operator=(const PoolingLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const PoolingLayerParams& default_instance();

  enum PoolingPaddingTypeCase {
    kValid = 30,
    kSame = 31,
    kIncludeLastPixel = 32,
    POOLINGPADDINGTYPE_NOT_SET = 0,
  };

  static const PoolingLayerParams* internal_default_instance();

  void Swap(PoolingLayerParams* other);

  // implements Message ----------------------------------------------

  inline PoolingLayerParams* New() const { return New(NULL); }

  PoolingLayerParams* New(::google::protobuf::Arena* arena) const;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from);
  void CopyFrom(const PoolingLayerParams& from);
  void MergeFrom(const PoolingLayerParams& from);
  void Clear();
  bool IsInitialized() const;

  size_t ByteSizeLong() const;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input);
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const;
  void DiscardUnknownFields();
  int GetCachedSize() const { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(PoolingLayerParams* other);
  void UnsafeMergeFrom(const PoolingLayerParams& from);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return _arena_ptr_;
  }
  inline ::google::protobuf::Arena* MaybeArenaPtr() const {
    return _arena_ptr_;
  }
  public:

  ::std::string GetTypeName() const;

  // nested types ----------------------------------------------------

  typedef PoolingLayerParams_ValidCompletePadding ValidCompletePadding;

  typedef PoolingLayerParams_PoolingType PoolingType;
  static const PoolingType MAX =
    PoolingLayerParams_PoolingType_MAX;
  static const PoolingType AVERAGE =
    PoolingLayerParams_PoolingType_AVERAGE;
  static const PoolingType L2 =
    PoolingLayerParams_PoolingType_L2;
  static inline bool PoolingType_IsValid(int value) {
    return PoolingLayerParams_PoolingType_IsValid(value);
  }
  static const PoolingType PoolingType_MIN =
    PoolingLayerParams_PoolingType_PoolingType_MIN;
  static const PoolingType PoolingType_MAX =
    PoolingLayerParams_PoolingType_PoolingType_MAX;
  static const int PoolingType_ARRAYSIZE =
    PoolingLayerParams_PoolingType_PoolingType_ARRAYSIZE;

  // accessors -------------------------------------------------------

  // optional .CoreML.Specification.PoolingLayerParams.PoolingType type = 1;
  void clear_type();
  static const int kTypeFieldNumber = 1;
  ::CoreML::Specification::PoolingLayerParams_PoolingType type() const;
  void set_type(::CoreML::Specification::PoolingLayerParams_PoolingType value);

  // repeated uint64 kernelSize = 10;
  int kernelsize_size() const;
  void clear_kernelsize();
  static const int kKernelSizeFieldNumber = 10;
  ::google::protobuf::uint64 kernelsize(int index) const;
  void set_kernelsize(int index, ::google::protobuf::uint64 value);
  void add_kernelsize(::google::protobuf::uint64 value);
  const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
      kernelsize() const;
  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
      mutable_kernelsize();

  // repeated uint64 stride = 20;
  int stride_size() const;
  void clear_stride();
  static const int kStrideFieldNumber = 20;
  ::google::protobuf::uint64 stride(int index) const;
  void set_stride(int index, ::google::protobuf::uint64 value);
  void add_stride(::google::protobuf::uint64 value);
  const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
      stride() const;
  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
      mutable_stride();

  // optional .CoreML.Specification.ValidPadding valid = 30;
  bool has_valid() const;
  void clear_valid();
  static const int kValidFieldNumber = 30;
  const ::CoreML::Specification::ValidPadding& valid() const;
  ::CoreML::Specification::ValidPadding* mutable_valid();
  ::CoreML::Specification::ValidPadding* release_valid();
  void set_allocated_valid(::CoreML::Specification::ValidPadding* valid);

  // optional .CoreML.Specification.SamePadding same = 31;
  bool has_same() const;
  void clear_same();
  static const int kSameFieldNumber = 31;
  const ::CoreML::Specification::SamePadding& same() const;
  ::CoreML::Specification::SamePadding* mutable_same();
  ::CoreML::Specification::SamePadding* release_same();
  void set_allocated_same(::CoreML::Specification::SamePadding* same);

  // optional .CoreML.Specification.PoolingLayerParams.ValidCompletePadding includeLastPixel = 32;
  bool has_includelastpixel() const;
  void clear_includelastpixel();
  static const int kIncludeLastPixelFieldNumber = 32;
  const ::CoreML::Specification::PoolingLayerParams_ValidCompletePadding& includelastpixel() const;
  ::CoreML::Specification::PoolingLayerParams_ValidCompletePadding* mutable_includelastpixel();
  ::CoreML::Specification::PoolingLayerParams_ValidCompletePadding* release_includelastpixel();
  void set_allocated_includelastpixel(::CoreML::Specification::PoolingLayerParams_ValidCompletePadding* includelastpixel);

  // optional bool avgPoolExcludePadding = 50;
  void clear_avgpoolexcludepadding();
  static const int kAvgPoolExcludePaddingFieldNumber = 50;
  bool avgpoolexcludepadding() const;
  void set_avgpoolexcludepadding(bool value);

  // optional bool globalPooling = 60;
  void clear_globalpooling();
  static const int kGlobalPoolingFieldNumber = 60;
  bool globalpooling() const;
  void set_globalpooling(bool value);

  PoolingPaddingTypeCase PoolingPaddingType_case() const;
  // @@protoc_insertion_point(class_scope:CoreML.Specification.PoolingLayerParams)
 private:
  inline void set_has_valid();
  inline void set_has_same();
  inline void set_has_includelastpixel();

  inline bool has_PoolingPaddingType() const;
  void clear_PoolingPaddingType();
  inline void clear_has_PoolingPaddingType();

  ::google::protobuf::internal::ArenaStringPtr _unknown_fields_;
  ::google::protobuf::Arena* _arena_ptr_;

  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 > kernelsize_;
  mutable int _kernelsize_cached_byte_size_;
  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 > stride_;
  mutable int _stride_cached_byte_size_;
  int type_;
  bool avgpoolexcludepadding_;
  bool globalpooling_;
  union PoolingPaddingTypeUnion {
    PoolingPaddingTypeUnion() {}
    ::CoreML::Specification::ValidPadding* valid_;
    ::CoreML::Specification::SamePadding* same_;
    ::CoreML::Specification::PoolingLayerParams_ValidCompletePadding* includelastpixel_;
  } PoolingPaddingType_;
  mutable int _cached_size_;
  ::google::protobuf::uint32 _oneof_case_[1];

  friend void  protobuf_InitDefaults_NeuralNetwork_2eproto_impl();
  friend void  protobuf_AddDesc_NeuralNetwork_2eproto_impl();
  friend void protobuf_AssignDesc_NeuralNetwork_2eproto();
  friend void protobuf_ShutdownFile_NeuralNetwork_2eproto();

  void InitAsDefaultInstance();
};
extern ::google::protobuf::internal::ExplicitlyConstructed<PoolingLayerParams> PoolingLayerParams_default_instance_;

// -------------------------------------------------------------------

class PaddingLayerParams_PaddingConstant : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.PaddingLayerParams.PaddingConstant) */ {
 public:
  PaddingLayerParams_PaddingConstant();
  virtual ~PaddingLayerParams_PaddingConstant();

  PaddingLayerParams_PaddingConstant(const PaddingLayerParams_PaddingConstant& from);

  inline PaddingLayerParams_PaddingConstant& operator=(const PaddingLayerParams_PaddingConstant& from) {
    CopyFrom(from);
    return *this;
  }

  static const PaddingLayerParams_PaddingConstant& default_instance();

  static const PaddingLayerParams_PaddingConstant* internal_default_instance();

  void Swap(PaddingLayerParams_PaddingConstant* other);

  // implements Message ----------------------------------------------

  inline PaddingLayerParams_PaddingConstant* New() const { return New(NULL); }

  PaddingLayerParams_PaddingConstant* New(::google::protobuf::Arena* arena) const;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from);
  void CopyFrom(const PaddingLayerParams_PaddingConstant& from);
  void MergeFrom(const PaddingLayerParams_PaddingConstant& from);
  void Clear();
  bool IsInitialized() const;

  size_t ByteSizeLong() const;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input);
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const;
  void DiscardUnknownFields();
  int GetCachedSize() const { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(PaddingLayerParams_PaddingConstant* other);
  void UnsafeMergeFrom(const PaddingLayerParams_PaddingConstant& from);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return _arena_ptr_;
  }
  inline ::google::protobuf::Arena* MaybeArenaPtr() const {
    return _arena_ptr_;
  }
  public:

  ::std::string GetTypeName() const;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // optional float value = 1;
  void clear_value();
  static const int kValueFieldNumber = 1;
  float value() const;
  void set_value(float value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.PaddingLayerParams.PaddingConstant)
 private:

  ::google::protobuf::internal::ArenaStringPtr _unknown_fields_;
  ::google::protobuf::Arena* _arena_ptr_;

  float value_;
  mutable int _cached_size_;
  friend void  protobuf_InitDefaults_NeuralNetwork_2eproto_impl();
  friend void  protobuf_AddDesc_NeuralNetwork_2eproto_impl();
  friend void protobuf_AssignDesc_NeuralNetwork_2eproto();
  friend void protobuf_ShutdownFile_NeuralNetwork_2eproto();

  void InitAsDefaultInstance();
};
extern ::google::protobuf::internal::ExplicitlyConstructed<PaddingLayerParams_PaddingConstant> PaddingLayerParams_PaddingConstant_default_instance_;

// -------------------------------------------------------------------

class PaddingLayerParams_PaddingReflection : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.PaddingLayerParams.PaddingReflection) */ {
 public:
  PaddingLayerParams_PaddingReflection();
  virtual ~PaddingLayerParams_PaddingReflection();

  PaddingLayerParams_PaddingReflection(const PaddingLayerParams_PaddingReflection& from);

  inline PaddingLayerParams_PaddingReflection& operator=(const PaddingLayerParams_PaddingReflection& from) {
    CopyFrom(from);
    return *this;
  }

  static const PaddingLayerParams_PaddingReflection& default_instance();

  static const PaddingLayerParams_PaddingReflection* internal_default_instance();

  void Swap(PaddingLayerParams_PaddingReflection* other);

  // implements Message ----------------------------------------------

  inline PaddingLayerParams_PaddingReflection* New() const { return New(NULL); }

  PaddingLayerParams_PaddingReflection* New(::google::protobuf::Arena* arena) const;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from);
  void CopyFrom(const PaddingLayerParams_PaddingReflection& from);
  void MergeFrom(const PaddingLayerParams_PaddingReflection& from);
  void Clear();
  bool IsInitialized() const;

  size_t ByteSizeLong() const;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input);
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const;
  void DiscardUnknownFields();
  int GetCachedSize() const { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(PaddingLayerParams_PaddingReflection* other);
  void UnsafeMergeFrom(const PaddingLayerParams_PaddingReflection& from);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return _arena_ptr_;
  }
  inline ::google::protobuf::Arena* MaybeArenaPtr() const {
    return _arena_ptr_;
  }
  public:

  ::std::string GetTypeName() const;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.PaddingLayerParams.PaddingReflection)
 private:

  ::google::protobuf::internal::ArenaStringPtr _unknown_fields_;
  ::google::protobuf::Arena* _arena_ptr_;

  mutable int _cached_size_;
  friend void  protobuf_InitDefaults_NeuralNetwork_2eproto_impl();
  friend void  protobuf_AddDesc_NeuralNetwork_2eproto_impl();
  friend void protobuf_AssignDesc_NeuralNetwork_2eproto();
  friend void protobuf_ShutdownFile_NeuralNetwork_2eproto();

  void InitAsDefaultInstance();
};
extern ::google::protobuf::internal::ExplicitlyConstructed<PaddingLayerParams_PaddingReflection> PaddingLayerParams_PaddingReflection_default_instance_;

// -------------------------------------------------------------------

class PaddingLayerParams_PaddingReplication : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.PaddingLayerParams.PaddingReplication) */ {
 public:
  PaddingLayerParams_PaddingReplication();
  virtual ~PaddingLayerParams_PaddingReplication();

  PaddingLayerParams_PaddingReplication(const PaddingLayerParams_PaddingReplication& from);

  inline PaddingLayerParams_PaddingReplication& operator=(const PaddingLayerParams_PaddingReplication& from) {
    CopyFrom(from);
    return *this;
  }

  static const PaddingLayerParams_PaddingReplication& default_instance();

  static const PaddingLayerParams_PaddingReplication* internal_default_instance();

  void Swap(PaddingLayerParams_PaddingReplication* other);

  // implements Message ----------------------------------------------

  inline PaddingLayerParams_PaddingReplication* New() const { return New(NULL); }

  PaddingLayerParams_PaddingReplication* New(::google::protobuf::Arena* arena) const;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from);
  void CopyFrom(const PaddingLayerParams_PaddingReplication& from);
  void MergeFrom(const PaddingLayerParams_PaddingReplication& from);
  void Clear();
  bool IsInitialized() const;

  size_t ByteSizeLong() const;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input);
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const;
  void DiscardUnknownFields();
  int GetCachedSize() const { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(PaddingLayerParams_PaddingReplication* other);
  void UnsafeMergeFrom(const PaddingLayerParams_PaddingReplication& from);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return _arena_ptr_;
  }
  inline ::google::protobuf::Arena* MaybeArenaPtr() const {
    return _arena_ptr_;
  }
  public:

  ::std::string GetTypeName() const;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.PaddingLayerParams.PaddingReplication)
 private:

  ::google::protobuf::internal::ArenaStringPtr _unknown_fields_;
  ::google::protobuf::Arena* _arena_ptr_;

  mutable int _cached_size_;
  friend void  protobuf_InitDefaults_NeuralNetwork_2eproto_impl();
  friend void  protobuf_AddDesc_NeuralNetwork_2eproto_impl();
  friend void protobuf_AssignDesc_NeuralNetwork_2eproto();
  friend void protobuf_ShutdownFile_NeuralNetwork_2eproto();

  void InitAsDefaultInstance();
};
extern ::google::protobuf::internal::ExplicitlyConstructed<PaddingLayerParams_PaddingReplication> PaddingLayerParams_PaddingReplication_default_instance_;

// -------------------------------------------------------------------

class PaddingLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.PaddingLayerParams) */ {
 public:
  PaddingLayerParams();
  virtual ~PaddingLayerParams();

  PaddingLayerParams(const PaddingLayerParams& from);

  inline PaddingLayerParams& operator=(const PaddingLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const PaddingLayerParams& default_instance();

  enum PaddingTypeCase {
    kConstant = 1,
    kReflection = 2,
    kReplication = 3,
    PADDINGTYPE_NOT_SET = 0,
  };

  static const PaddingLayerParams* internal_default_instance();

  void Swap(PaddingLayerParams* other);

  // implements Message ----------------------------------------------

  inline PaddingLayerParams* New() const { return New(NULL); }

  PaddingLayerParams* New(::google::protobuf::Arena* arena) const;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from);
  void CopyFrom(const PaddingLayerParams& from);
  void MergeFrom(const PaddingLayerParams& from);
  void Clear();
  bool IsInitialized() const;

  size_t ByteSizeLong() const;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input);
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const;
  void DiscardUnknownFields();
  int GetCachedSize() const { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(PaddingLayerParams* other);
  void UnsafeMergeFrom(const PaddingLayerParams& from);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return _arena_ptr_;
  }
  inline ::google::protobuf::Arena* MaybeArenaPtr() const {
    return _arena_ptr_;
  }
  public:

  ::std::string GetTypeName() const;

  // nested types ----------------------------------------------------

  typedef PaddingLayerParams_PaddingConstant PaddingConstant;
  typedef PaddingLayerParams_PaddingReflection PaddingReflection;
  typedef PaddingLayerParams_PaddingReplication PaddingReplication;

  // accessors -------------------------------------------------------

  // optional .CoreML.Specification.PaddingLayerParams.PaddingConstant constant = 1;
  bool has_constant() const;
  void clear_constant();
  static const int kConstantFieldNumber = 1;
  const ::CoreML::Specification::PaddingLayerParams_PaddingConstant& constant() const;
  ::CoreML::Specification::PaddingLayerParams_PaddingConstant* mutable_constant();
  ::CoreML::Specification::PaddingLayerParams_PaddingConstant* release_constant();
  void set_allocated_constant(::CoreML::Specification::PaddingLayerParams_PaddingConstant* constant);

  // optional .CoreML.Specification.PaddingLayerParams.PaddingReflection reflection = 2;
  bool has_reflection() const;
  void clear_reflection();
  static const int kReflectionFieldNumber = 2;
  const ::CoreML::Specification::PaddingLayerParams_PaddingReflection& reflection() const;
  ::CoreML::Specification::PaddingLayerParams_PaddingReflection* mutable_reflection();
  ::CoreML::Specification::PaddingLayerParams_PaddingReflection* release_reflection();
  void set_allocated_reflection(::CoreML::Specification::PaddingLayerParams_PaddingReflection* reflection);

  // optional .CoreML.Specification.PaddingLayerParams.PaddingReplication replication = 3;
  bool has_replication() const;
  void clear_replication();
  static const int kReplicationFieldNumber = 3;
  const ::CoreML::Specification::PaddingLayerParams_PaddingReplication& replication() const;
  ::CoreML::Specification::PaddingLayerParams_PaddingReplication* mutable_replication();
  ::CoreML::Specification::PaddingLayerParams_PaddingReplication* release_replication();
  void set_allocated_replication(::CoreML::Specification::PaddingLayerParams_PaddingReplication* replication);

  // optional .CoreML.Specification.BorderAmounts paddingAmounts = 10;
  bool has_paddingamounts() const;
  void clear_paddingamounts();
  static const int kPaddingAmountsFieldNumber = 10;
  const ::CoreML::Specification::BorderAmounts& paddingamounts() const;
  ::CoreML::Specification::BorderAmounts* mutable_paddingamounts();
  ::CoreML::Specification::BorderAmounts* release_paddingamounts();
  void set_allocated_paddingamounts(::CoreML::Specification::BorderAmounts* paddingamounts);

  PaddingTypeCase PaddingType_case() const;
  // @@protoc_insertion_point(class_scope:CoreML.Specification.PaddingLayerParams)
 private:
  inline void set_has_constant();
  inline void set_has_reflection();
  inline void set_has_replication();

  inline bool has_PaddingType() const;
  void clear_PaddingType();
  inline void clear_has_PaddingType();

  ::google::protobuf::internal::ArenaStringPtr _unknown_fields_;
  ::google::protobuf::Arena* _arena_ptr_;

  ::CoreML::Specification::BorderAmounts* paddingamounts_;
  union PaddingTypeUnion {
    PaddingTypeUnion() {}
    ::CoreML::Specification::PaddingLayerParams_PaddingConstant* constant_;
    ::CoreML::Specification::PaddingLayerParams_PaddingReflection* reflection_;
    ::CoreML::Specification::PaddingLayerParams_PaddingReplication* replication_;
  } PaddingType_;
  mutable int _cached_size_;
  ::google::protobuf::uint32 _oneof_case_[1];

  friend void  protobuf_InitDefaults_NeuralNetwork_2eproto_impl();
  friend void  protobuf_AddDesc_NeuralNetwork_2eproto_impl();
  friend void protobuf_AssignDesc_NeuralNetwork_2eproto();
  friend void protobuf_ShutdownFile_NeuralNetwork_2eproto();

  void InitAsDefaultInstance();
};
extern ::google::protobuf::internal::ExplicitlyConstructed<PaddingLayerParams> PaddingLayerParams_default_instance_;

// -------------------------------------------------------------------

class ConcatLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ConcatLayerParams) */ {
 public:
  ConcatLayerParams();
  virtual ~ConcatLayerParams();

  ConcatLayerParams(const ConcatLayerParams& from);

  inline ConcatLayerParams& operator=(const ConcatLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const ConcatLayerParams& default_instance();

  static const ConcatLayerParams* internal_default_instance();

  void Swap(ConcatLayerParams* other);

  // implements Message ----------------------------------------------

  inline ConcatLayerParams* New() const { return New(NULL); }

  ConcatLayerParams* New(::google::protobuf::Arena* arena) const;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from);
  void CopyFrom(const ConcatLayerParams& from);
  void MergeFrom(const ConcatLayerParams& from);
  void Clear();
  bool IsInitialized() const;

  size_t ByteSizeLong() const;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input);
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const;
  void DiscardUnknownFields();
  int GetCachedSize() const { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ConcatLayerParams* other);
  void UnsafeMergeFrom(const ConcatLayerParams& from);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return _arena_ptr_;
  }
  inline ::google::protobuf::Arena* MaybeArenaPtr() const {
    return _arena_ptr_;
  }
  public:

  ::std::string GetTypeName() const;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // optional bool sequenceConcat = 100;
  void clear_sequenceconcat();
  static const int kSequenceConcatFieldNumber = 100;
  bool sequenceconcat() const;
  void set_sequenceconcat(bool value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ConcatLayerParams)
 private:

  ::google::protobuf::internal::ArenaStringPtr _unknown_fields_;
  ::google::protobuf::Arena* _arena_ptr_;

  bool sequenceconcat_;
  mutable int _cached_size_;
  friend void  protobuf_InitDefaults_NeuralNetwork_2eproto_impl();
  friend void  protobuf_AddDesc_NeuralNetwork_2eproto_impl();
  friend void protobuf_AssignDesc_NeuralNetwork_2eproto();
  friend void protobuf_ShutdownFile_NeuralNetwork_2eproto();

  void InitAsDefaultInstance();
};
extern ::google::protobuf::internal::ExplicitlyConstructed<ConcatLayerParams> ConcatLayerParams_default_instance_;

// -------------------------------------------------------------------

class LRNLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.LRNLayerParams) */ {
 public:
  LRNLayerParams();
  virtual ~LRNLayerParams();

  LRNLayerParams(const LRNLayerParams& from);

  inline LRNLayerParams& operator=(const LRNLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const LRNLayerParams& default_instance();

  static const LRNLayerParams* internal_default_instance();

  void Swap(LRNLayerParams* other);

  // implements Message ----------------------------------------------

  inline LRNLayerParams* New() const { return New(NULL); }

  LRNLayerParams* New(::google::protobuf::Arena* arena) const;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from);
  void CopyFrom(const LRNLayerParams& from);
  void MergeFrom(const LRNLayerParams& from);
  void Clear();
  bool IsInitialized() const;

  size_t ByteSizeLong() const;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input);
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const;
  void DiscardUnknownFields();
  int GetCachedSize() const { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(LRNLayerParams* other);
  void UnsafeMergeFrom(const LRNLayerParams& from);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return _arena_ptr_;
  }
  inline ::google::protobuf::Arena* MaybeArenaPtr() const {
    return _arena_ptr_;
  }
  public:

  ::std::string GetTypeName() const;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // optional float alpha = 1;
  void clear_alpha();
  static const int kAlphaFieldNumber = 1;
  float alpha() const;
  void set_alpha(float value);

  // optional float beta = 2;
  void clear_beta();
  static const int kBetaFieldNumber = 2;
  float beta() const;
  void set_beta(float value);

  // optional uint64 localSize = 3;
  void clear_localsize();
  static const int kLocalSizeFieldNumber = 3;
  ::google::protobuf::uint64 localsize() const;
  void set_localsize(::google::protobuf::uint64 value);

  // optional float k = 4;
  void clear_k();
  static const int kKFieldNumber = 4;
  float k() const;
  void set_k(float value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.LRNLayerParams)
 private:

  ::google::protobuf::internal::ArenaStringPtr _unknown_fields_;
  ::google::protobuf::Arena* _arena_ptr_;

  float alpha_;
  float beta_;
  ::google::protobuf::uint64 localsize_;
  float k_;
  mutable int _cached_size_;
  friend void  protobuf_InitDefaults_NeuralNetwork_2eproto_impl();
  friend void  protobuf_AddDesc_NeuralNetwork_2eproto_impl();
  friend void protobuf_AssignDesc_NeuralNetwork_2eproto();
  friend void protobuf_ShutdownFile_NeuralNetwork_2eproto();

  void InitAsDefaultInstance();
};
extern ::google::protobuf::internal::ExplicitlyConstructed<LRNLayerParams> LRNLayerParams_default_instance_;

// -------------------------------------------------------------------

class SoftmaxLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.SoftmaxLayerParams) */ {
 public:
  SoftmaxLayerParams();
  virtual ~SoftmaxLayerParams();

  SoftmaxLayerParams(const SoftmaxLayerParams& from);

  inline SoftmaxLayerParams& operator=(const SoftmaxLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const SoftmaxLayerParams& default_instance();

  static const SoftmaxLayerParams* internal_default_instance();

  void Swap(SoftmaxLayerParams* other);

  // implements Message ----------------------------------------------

  inline SoftmaxLayerParams* New() const { return New(NULL); }

  SoftmaxLayerParams* New(::google::protobuf::Arena* arena) const;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from);
  void CopyFrom(const SoftmaxLayerParams& from);
  void MergeFrom(const SoftmaxLayerParams& from);
  void Clear();
  bool IsInitialized() const;

  size_t ByteSizeLong() const;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input);
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const;
  void DiscardUnknownFields();
  int GetCachedSize() const { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(SoftmaxLayerParams* other);
  void UnsafeMergeFrom(const SoftmaxLayerParams& from);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return _arena_ptr_;
  }
  inline ::google::protobuf::Arena* MaybeArenaPtr() const {
    return _arena_ptr_;
  }
  public:

  ::std::string GetTypeName() const;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.SoftmaxLayerParams)
 private:

  ::google::protobuf::internal::ArenaStringPtr _unknown_fields_;
  ::google::protobuf::Arena* _arena_ptr_;

  mutable int _cached_size_;
  friend void  protobuf_InitDefaults_NeuralNetwork_2eproto_impl();
  friend void  protobuf_AddDesc_NeuralNetwork_2eproto_impl();
  friend void protobuf_AssignDesc_NeuralNetwork_2eproto();
  friend void protobuf_ShutdownFile_NeuralNetwork_2eproto();

  void InitAsDefaultInstance();
};
extern ::google::protobuf::internal::ExplicitlyConstructed<SoftmaxLayerParams> SoftmaxLayerParams_default_instance_;

// -------------------------------------------------------------------

class SplitLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.SplitLayerParams) */ {
 public:
  SplitLayerParams();
  virtual ~SplitLayerParams();

  SplitLayerParams(const SplitLayerParams& from);

  inline SplitLayerParams& operator=(const SplitLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const SplitLayerParams& default_instance();

  static const SplitLayerParams* internal_default_instance();

  void Swap(SplitLayerParams* other);

  // implements Message ----------------------------------------------

  inline SplitLayerParams* New() const { return New(NULL); }

  SplitLayerParams* New(::google::protobuf::Arena* arena) const;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from);
  void CopyFrom(const SplitLayerParams& from);
  void MergeFrom(const SplitLayerParams& from);
  void Clear();
  bool IsInitialized() const;

  size_t ByteSizeLong() const;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input);
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const;
  void DiscardUnknownFields();
  int GetCachedSize() const { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(SplitLayerParams* other);
  void UnsafeMergeFrom(const SplitLayerParams& from);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return _arena_ptr_;
  }
  inline ::google::protobuf::Arena* MaybeArenaPtr() const {
    return _arena_ptr_;
  }
  public:

  ::std::string GetTypeName() const;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // optional uint64 nOutputs = 1;
  void clear_noutputs();
  static const int kNOutputsFieldNumber = 1;
  ::google::protobuf::uint64 noutputs() const;
  void set_noutputs(::google::protobuf::uint64 value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.SplitLayerParams)
 private:

  ::google::protobuf::internal::ArenaStringPtr _unknown_fields_;
  ::google::protobuf::Arena* _arena_ptr_;

  ::google::protobuf::uint64 noutputs_;
  mutable int _cached_size_;
  friend void  protobuf_InitDefaults_NeuralNetwork_2eproto_impl();
  friend void  protobuf_AddDesc_NeuralNetwork_2eproto_impl();
  friend void protobuf_AssignDesc_NeuralNetwork_2eproto();
  friend void protobuf_ShutdownFile_NeuralNetwork_2eproto();

  void InitAsDefaultInstance();
};
extern ::google::protobuf::internal::ExplicitlyConstructed<SplitLayerParams> SplitLayerParams_default_instance_;

// -------------------------------------------------------------------

class AddLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.AddLayerParams) */ {
 public:
  AddLayerParams();
  virtual ~AddLayerParams();

  AddLayerParams(const AddLayerParams& from);

  inline AddLayerParams& operator=(const AddLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const AddLayerParams& default_instance();

  static const AddLayerParams* internal_default_instance();

  void Swap(AddLayerParams* other);

  // implements Message ----------------------------------------------

  inline AddLayerParams* New() const { return New(NULL); }

  AddLayerParams* New(::google::protobuf::Arena* arena) const;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from);
  void CopyFrom(const AddLayerParams& from);
  void MergeFrom(const AddLayerParams& from);
  void Clear();
  bool IsInitialized() const;

  size_t ByteSizeLong() const;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input);
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const;
  void DiscardUnknownFields();
  int GetCachedSize() const { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(AddLayerParams* other);
  void UnsafeMergeFrom(const AddLayerParams& from);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return _arena_ptr_;
  }
  inline ::google::protobuf::Arena* MaybeArenaPtr() const {
    return _arena_ptr_;
  }
  public:

  ::std::string GetTypeName() const;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // optional float alpha = 1;
  void clear_alpha();
  static const int kAlphaFieldNumber = 1;
  float alpha() const;
  void set_alpha(float value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.AddLayerParams)
 private:

  ::google::protobuf::internal::ArenaStringPtr _unknown_fields_;
  ::google::protobuf::Arena* _arena_ptr_;

  float alpha_;
  mutable int _cached_size_;
  friend void  protobuf_InitDefaults_NeuralNetwork_2eproto_impl();
  friend void  protobuf_AddDesc_NeuralNetwork_2eproto_impl();
  friend void protobuf_AssignDesc_NeuralNetwork_2eproto();
  friend void protobuf_ShutdownFile_NeuralNetwork_2eproto();

  void InitAsDefaultInstance();
};
extern ::google::protobuf::internal::ExplicitlyConstructed<AddLayerParams> AddLayerParams_default_instance_;

// -------------------------------------------------------------------

class MultiplyLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.MultiplyLayerParams) */ {
 public:
  MultiplyLayerParams();
  virtual ~MultiplyLayerParams();

  MultiplyLayerParams(const MultiplyLayerParams& from);

  inline MultiplyLayerParams& operator=(const MultiplyLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const MultiplyLayerParams& default_instance();

  static const MultiplyLayerParams* internal_default_instance();

  void Swap(MultiplyLayerParams* other);

  // implements Message ----------------------------------------------

  inline MultiplyLayerParams* New() const { return New(NULL); }

  MultiplyLayerParams* New(::google::protobuf::Arena* arena) const;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from);
  void CopyFrom(const MultiplyLayerParams& from);
  void MergeFrom(const MultiplyLayerParams& from);
  void Clear();
  bool IsInitialized() const;

  size_t ByteSizeLong() const;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input);
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const;
  void DiscardUnknownFields();
  int GetCachedSize() const { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(MultiplyLayerParams* other);
  void UnsafeMergeFrom(const MultiplyLayerParams& from);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return _arena_ptr_;
  }
  inline ::google::protobuf::Arena* MaybeArenaPtr() const {
    return _arena_ptr_;
  }
  public:

  ::std::string GetTypeName() const;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // optional float alpha = 1;
  void clear_alpha();
  static const int kAlphaFieldNumber = 1;
  float alpha() const;
  void set_alpha(float value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.MultiplyLayerParams)
 private:

  ::google::protobuf::internal::ArenaStringPtr _unknown_fields_;
  ::google::protobuf::Arena* _arena_ptr_;

  float alpha_;
  mutable int _cached_size_;
  friend void  protobuf_InitDefaults_NeuralNetwork_2eproto_impl();
  friend void  protobuf_AddDesc_NeuralNetwork_2eproto_impl();
  friend void protobuf_AssignDesc_NeuralNetwork_2eproto();
  friend void protobuf_ShutdownFile_NeuralNetwork_2eproto();

  void InitAsDefaultInstance();
};
extern ::google::protobuf::internal::ExplicitlyConstructed<MultiplyLayerParams> MultiplyLayerParams_default_instance_;

// -------------------------------------------------------------------

class UnaryFunctionLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.UnaryFunctionLayerParams) */ {
 public:
  UnaryFunctionLayerParams();
  virtual ~UnaryFunctionLayerParams();

  UnaryFunctionLayerParams(const UnaryFunctionLayerParams& from);

  inline UnaryFunctionLayerParams& operator=(const UnaryFunctionLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const UnaryFunctionLayerParams& default_instance();

  static const UnaryFunctionLayerParams* internal_default_instance();

  void Swap(UnaryFunctionLayerParams* other);

  // implements Message ----------------------------------------------

  inline UnaryFunctionLayerParams* New() const { return New(NULL); }

  UnaryFunctionLayerParams* New(::google::protobuf::Arena* arena) const;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from);
  void CopyFrom(const UnaryFunctionLayerParams& from);
  void MergeFrom(const UnaryFunctionLayerParams& from);
  void Clear();
  bool IsInitialized() const;

  size_t ByteSizeLong() const;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input);
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const;
  void DiscardUnknownFields();
  int GetCachedSize() const { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(UnaryFunctionLayerParams* other);
  void UnsafeMergeFrom(const UnaryFunctionLayerParams& from);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return _arena_ptr_;
  }
  inline ::google::protobuf::Arena* MaybeArenaPtr() const {
    return _arena_ptr_;
  }
  public:

  ::std::string GetTypeName() const;

  // nested types ----------------------------------------------------

  typedef UnaryFunctionLayerParams_Operation Operation;
  static const Operation SQRT =
    UnaryFunctionLayerParams_Operation_SQRT;
  static const Operation RSQRT =
    UnaryFunctionLayerParams_Operation_RSQRT;
  static const Operation INVERSE =
    UnaryFunctionLayerParams_Operation_INVERSE;
  static const Operation POWER =
    UnaryFunctionLayerParams_Operation_POWER;
  static const Operation EXP =
    UnaryFunctionLayerParams_Operation_EXP;
  static const Operation LOG =
    UnaryFunctionLayerParams_Operation_LOG;
  static const Operation ABS =
    UnaryFunctionLayerParams_Operation_ABS;
  static const Operation THRESHOLD =
    UnaryFunctionLayerParams_Operation_THRESHOLD;
  static inline bool Operation_IsValid(int value) {
    return UnaryFunctionLayerParams_Operation_IsValid(value);
  }
  static const Operation Operation_MIN =
    UnaryFunctionLayerParams_Operation_Operation_MIN;
  static const Operation Operation_MAX =
    UnaryFunctionLayerParams_Operation_Operation_MAX;
  static const int Operation_ARRAYSIZE =
    UnaryFunctionLayerParams_Operation_Operation_ARRAYSIZE;

  // accessors -------------------------------------------------------

  // optional .CoreML.Specification.UnaryFunctionLayerParams.Operation type = 1;
  void clear_type();
  static const int kTypeFieldNumber = 1;
  ::CoreML::Specification::UnaryFunctionLayerParams_Operation type() const;
  void set_type(::CoreML::Specification::UnaryFunctionLayerParams_Operation value);

  // optional float alpha = 2;
  void clear_alpha();
  static const int kAlphaFieldNumber = 2;
  float alpha() const;
  void set_alpha(float value);

  // optional float epsilon = 3;
  void clear_epsilon();
  static const int kEpsilonFieldNumber = 3;
  float epsilon() const;
  void set_epsilon(float value);

  // optional float shift = 4;
  void clear_shift();
  static const int kShiftFieldNumber = 4;
  float shift() const;
  void set_shift(float value);

  // optional float scale = 5;
  void clear_scale();
  static const int kScaleFieldNumber = 5;
  float scale() const;
  void set_scale(float value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.UnaryFunctionLayerParams)
 private:

  ::google::protobuf::internal::ArenaStringPtr _unknown_fields_;
  ::google::protobuf::Arena* _arena_ptr_;

  int type_;
  float alpha_;
  float epsilon_;
  float shift_;
  float scale_;
  mutable int _cached_size_;
  friend void  protobuf_InitDefaults_NeuralNetwork_2eproto_impl();
  friend void  protobuf_AddDesc_NeuralNetwork_2eproto_impl();
  friend void protobuf_AssignDesc_NeuralNetwork_2eproto();
  friend void protobuf_ShutdownFile_NeuralNetwork_2eproto();

  void InitAsDefaultInstance();
};
extern ::google::protobuf::internal::ExplicitlyConstructed<UnaryFunctionLayerParams> UnaryFunctionLayerParams_default_instance_;

// -------------------------------------------------------------------

class UpsampleLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.UpsampleLayerParams) */ {
 public:
  UpsampleLayerParams();
  virtual ~UpsampleLayerParams();

  UpsampleLayerParams(const UpsampleLayerParams& from);

  inline UpsampleLayerParams& operator=(const UpsampleLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const UpsampleLayerParams& default_instance();

  static const UpsampleLayerParams* internal_default_instance();

  void Swap(UpsampleLayerParams* other);

  // implements Message ----------------------------------------------

  inline UpsampleLayerParams* New() const { return New(NULL); }

  UpsampleLayerParams* New(::google::protobuf::Arena* arena) const;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from);
  void CopyFrom(const UpsampleLayerParams& from);
  void MergeFrom(const UpsampleLayerParams& from);
  void Clear();
  bool IsInitialized() const;

  size_t ByteSizeLong() const;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input);
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const;
  void DiscardUnknownFields();
  int GetCachedSize() const { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(UpsampleLayerParams* other);
  void UnsafeMergeFrom(const UpsampleLayerParams& from);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return _arena_ptr_;
  }
  inline ::google::protobuf::Arena* MaybeArenaPtr() const {
    return _arena_ptr_;
  }
  public:

  ::std::string GetTypeName() const;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated uint64 scalingFactor = 1;
  int scalingfactor_size() const;
  void clear_scalingfactor();
  static const int kScalingFactorFieldNumber = 1;
  ::google::protobuf::uint64 scalingfactor(int index) const;
  void set_scalingfactor(int index, ::google::protobuf::uint64 value);
  void add_scalingfactor(::google::protobuf::uint64 value);
  const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
      scalingfactor() const;
  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
      mutable_scalingfactor();

  // @@protoc_insertion_point(class_scope:CoreML.Specification.UpsampleLayerParams)
 private:

  ::google::protobuf::internal::ArenaStringPtr _unknown_fields_;
  ::google::protobuf::Arena* _arena_ptr_;

  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 > scalingfactor_;
  mutable int _scalingfactor_cached_byte_size_;
  mutable int _cached_size_;
  friend void  protobuf_InitDefaults_NeuralNetwork_2eproto_impl();
  friend void  protobuf_AddDesc_NeuralNetwork_2eproto_impl();
  friend void protobuf_AssignDesc_NeuralNetwork_2eproto();
  friend void protobuf_ShutdownFile_NeuralNetwork_2eproto();

  void InitAsDefaultInstance();
};
extern ::google::protobuf::internal::ExplicitlyConstructed<UpsampleLayerParams> UpsampleLayerParams_default_instance_;

// -------------------------------------------------------------------

class BiasLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.BiasLayerParams) */ {
 public:
  BiasLayerParams();
  virtual ~BiasLayerParams();

  BiasLayerParams(const BiasLayerParams& from);

  inline BiasLayerParams& operator=(const BiasLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const BiasLayerParams& default_instance();

  static const BiasLayerParams* internal_default_instance();

  void Swap(BiasLayerParams* other);

  // implements Message ----------------------------------------------

  inline BiasLayerParams* New() const { return New(NULL); }

  BiasLayerParams* New(::google::protobuf::Arena* arena) const;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from);
  void CopyFrom(const BiasLayerParams& from);
  void MergeFrom(const BiasLayerParams& from);
  void Clear();
  bool IsInitialized() const;

  size_t ByteSizeLong() const;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input);
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const;
  void DiscardUnknownFields();
  int GetCachedSize() const { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(BiasLayerParams* other);
  void UnsafeMergeFrom(const BiasLayerParams& from);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return _arena_ptr_;
  }
  inline ::google::protobuf::Arena* MaybeArenaPtr() const {
    return _arena_ptr_;
  }
  public:

  ::std::string GetTypeName() const;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated uint64 shape = 1;
  int shape_size() const;
  void clear_shape();
  static const int kShapeFieldNumber = 1;
  ::google::protobuf::uint64 shape(int index) const;
  void set_shape(int index, ::google::protobuf::uint64 value);
  void add_shape(::google::protobuf::uint64 value);
  const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
      shape() const;
  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
      mutable_shape();

  // optional .CoreML.Specification.WeightParams bias = 2;
  bool has_bias() const;
  void clear_bias();
  static const int kBiasFieldNumber = 2;
  const ::CoreML::Specification::WeightParams& bias() const;
  ::CoreML::Specification::WeightParams* mutable_bias();
  ::CoreML::Specification::WeightParams* release_bias();
  void set_allocated_bias(::CoreML::Specification::WeightParams* bias);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.BiasLayerParams)
 private:

  ::google::protobuf::internal::ArenaStringPtr _unknown_fields_;
  ::google::protobuf::Arena* _arena_ptr_;

  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 > shape_;
  mutable int _shape_cached_byte_size_;
  ::CoreML::Specification::WeightParams* bias_;
  mutable int _cached_size_;
  friend void  protobuf_InitDefaults_NeuralNetwork_2eproto_impl();
  friend void  protobuf_AddDesc_NeuralNetwork_2eproto_impl();
  friend void protobuf_AssignDesc_NeuralNetwork_2eproto();
  friend void protobuf_ShutdownFile_NeuralNetwork_2eproto();

  void InitAsDefaultInstance();
};
extern ::google::protobuf::internal::ExplicitlyConstructed<BiasLayerParams> BiasLayerParams_default_instance_;

// -------------------------------------------------------------------

class ScaleLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ScaleLayerParams) */ {
 public:
  ScaleLayerParams();
  virtual ~ScaleLayerParams();

  ScaleLayerParams(const ScaleLayerParams& from);

  inline ScaleLayerParams& operator=(const ScaleLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const ScaleLayerParams& default_instance();

  static const ScaleLayerParams* internal_default_instance();

  void Swap(ScaleLayerParams* other);

  // implements Message ----------------------------------------------

  inline ScaleLayerParams* New() const { return New(NULL); }

  ScaleLayerParams* New(::google::protobuf::Arena* arena) const;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from);
  void CopyFrom(const ScaleLayerParams& from);
  void MergeFrom(const ScaleLayerParams& from);
  void Clear();
  bool IsInitialized() const;

  size_t ByteSizeLong() const;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input);
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const;
  void DiscardUnknownFields();
  int GetCachedSize() const { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ScaleLayerParams* other);
  void UnsafeMergeFrom(const ScaleLayerParams& from);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return _arena_ptr_;
  }
  inline ::google::protobuf::Arena* MaybeArenaPtr() const {
    return _arena_ptr_;
  }
  public:

  ::std::string GetTypeName() const;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated uint64 shapeScale = 1;
  int shapescale_size() const;
  void clear_shapescale();
  static const int kShapeScaleFieldNumber = 1;
  ::google::protobuf::uint64 shapescale(int index) const;
  void set_shapescale(int index, ::google::protobuf::uint64 value);
  void add_shapescale(::google::protobuf::uint64 value);
  const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
      shapescale() const;
  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
      mutable_shapescale();

  // optional .CoreML.Specification.WeightParams scale = 2;
  bool has_scale() const;
  void clear_scale();
  static const int kScaleFieldNumber = 2;
  const ::CoreML::Specification::WeightParams& scale() const;
  ::CoreML::Specification::WeightParams* mutable_scale();
  ::CoreML::Specification::WeightParams* release_scale();
  void set_allocated_scale(::CoreML::Specification::WeightParams* scale);

  // optional bool hasBias = 3;
  void clear_hasbias();
  static const int kHasBiasFieldNumber = 3;
  bool hasbias() const;
  void set_hasbias(bool value);

  // repeated uint64 shapeBias = 4;
  int shapebias_size() const;
  void clear_shapebias();
  static const int kShapeBiasFieldNumber = 4;
  ::google::protobuf::uint64 shapebias(int index) const;
  void set_shapebias(int index, ::google::protobuf::uint64 value);
  void add_shapebias(::google::protobuf::uint64 value);
  const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
      shapebias() const;
  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
      mutable_shapebias();

  // optional .CoreML.Specification.WeightParams bias = 5;
  bool has_bias() const;
  void clear_bias();
  static const int kBiasFieldNumber = 5;
  const ::CoreML::Specification::WeightParams& bias() const;
  ::CoreML::Specification::WeightParams* mutable_bias();
  ::CoreML::Specification::WeightParams* release_bias();
  void set_allocated_bias(::CoreML::Specification::WeightParams* bias);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ScaleLayerParams)
 private:

  ::google::protobuf::internal::ArenaStringPtr _unknown_fields_;
  ::google::protobuf::Arena* _arena_ptr_;

  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 > shapescale_;
  mutable int _shapescale_cached_byte_size_;
  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 > shapebias_;
  mutable int _shapebias_cached_byte_size_;
  ::CoreML::Specification::WeightParams* scale_;
  ::CoreML::Specification::WeightParams* bias_;
  bool hasbias_;
  mutable int _cached_size_;
  friend void  protobuf_InitDefaults_NeuralNetwork_2eproto_impl();
  friend void  protobuf_AddDesc_NeuralNetwork_2eproto_impl();
  friend void protobuf_AssignDesc_NeuralNetwork_2eproto();
  friend void protobuf_ShutdownFile_NeuralNetwork_2eproto();

  void InitAsDefaultInstance();
};
extern ::google::protobuf::internal::ExplicitlyConstructed<ScaleLayerParams> ScaleLayerParams_default_instance_;

// -------------------------------------------------------------------

class LoadConstantLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.LoadConstantLayerParams) */ {
 public:
  LoadConstantLayerParams();
  virtual ~LoadConstantLayerParams();

  LoadConstantLayerParams(const LoadConstantLayerParams& from);

  inline LoadConstantLayerParams& operator=(const LoadConstantLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const LoadConstantLayerParams& default_instance();

  static const LoadConstantLayerParams* internal_default_instance();

  void Swap(LoadConstantLayerParams* other);

  // implements Message ----------------------------------------------

  inline LoadConstantLayerParams* New() const { return New(NULL); }

  LoadConstantLayerParams* New(::google::protobuf::Arena* arena) const;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from);
  void CopyFrom(const LoadConstantLayerParams& from);
  void MergeFrom(const LoadConstantLayerParams& from);
  void Clear();
  bool IsInitialized() const;

  size_t ByteSizeLong() const;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input);
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const;
  void DiscardUnknownFields();
  int GetCachedSize() const { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(LoadConstantLayerParams* other);
  void UnsafeMergeFrom(const LoadConstantLayerParams& from);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return _arena_ptr_;
  }
  inline ::google::protobuf::Arena* MaybeArenaPtr() const {
    return _arena_ptr_;
  }
  public:

  ::std::string GetTypeName() const;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated uint64 shape = 1;
  int shape_size() const;
  void clear_shape();
  static const int kShapeFieldNumber = 1;
  ::google::protobuf::uint64 shape(int index) const;
  void set_shape(int index, ::google::protobuf::uint64 value);
  void add_shape(::google::protobuf::uint64 value);
  const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
      shape() const;
  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
      mutable_shape();

  // optional .CoreML.Specification.WeightParams data = 2;
  bool has_data() const;
  void clear_data();
  static const int kDataFieldNumber = 2;
  const ::CoreML::Specification::WeightParams& data() const;
  ::CoreML::Specification::WeightParams* mutable_data();
  ::CoreML::Specification::WeightParams* release_data();
  void set_allocated_data(::CoreML::Specification::WeightParams* data);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.LoadConstantLayerParams)
 private:

  ::google::protobuf::internal::ArenaStringPtr _unknown_fields_;
  ::google::protobuf::Arena* _arena_ptr_;

  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 > shape_;
  mutable int _shape_cached_byte_size_;
  ::CoreML::Specification::WeightParams* data_;
  mutable int _cached_size_;
  friend void  protobuf_InitDefaults_NeuralNetwork_2eproto_impl();
  friend void  protobuf_AddDesc_NeuralNetwork_2eproto_impl();
  friend void protobuf_AssignDesc_NeuralNetwork_2eproto();
  friend void protobuf_ShutdownFile_NeuralNetwork_2eproto();

  void InitAsDefaultInstance();
};
extern ::google::protobuf::internal::ExplicitlyConstructed<LoadConstantLayerParams> LoadConstantLayerParams_default_instance_;

// -------------------------------------------------------------------

class L2NormalizeLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.L2NormalizeLayerParams) */ {
 public:
  L2NormalizeLayerParams();
  virtual ~L2NormalizeLayerParams();

  L2NormalizeLayerParams(const L2NormalizeLayerParams& from);

  inline L2NormalizeLayerParams& operator=(const L2NormalizeLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const L2NormalizeLayerParams& default_instance();

  static const L2NormalizeLayerParams* internal_default_instance();

  void Swap(L2NormalizeLayerParams* other);

  // implements Message ----------------------------------------------

  inline L2NormalizeLayerParams* New() const { return New(NULL); }

  L2NormalizeLayerParams* New(::google::protobuf::Arena* arena) const;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from);
  void CopyFrom(const L2NormalizeLayerParams& from);
  void MergeFrom(const L2NormalizeLayerParams& from);
  void Clear();
  bool IsInitialized() const;

  size_t ByteSizeLong() const;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input);
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const;
  void DiscardUnknownFields();
  int GetCachedSize() const { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(L2NormalizeLayerParams* other);
  void UnsafeMergeFrom(const L2NormalizeLayerParams& from);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return _arena_ptr_;
  }
  inline ::google::protobuf::Arena* MaybeArenaPtr() const {
    return _arena_ptr_;
  }
  public:

  ::std::string GetTypeName() const;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // optional float epsilon = 1;
  void clear_epsilon();
  static const int kEpsilonFieldNumber = 1;
  float epsilon() const;
  void set_epsilon(float value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.L2NormalizeLayerParams)
 private:

  ::google::protobuf::internal::ArenaStringPtr _unknown_fields_;
  ::google::protobuf::Arena* _arena_ptr_;

  float epsilon_;
  mutable int _cached_size_;
  friend void  protobuf_InitDefaults_NeuralNetwork_2eproto_impl();
  friend void  protobuf_AddDesc_NeuralNetwork_2eproto_impl();
  friend void protobuf_AssignDesc_NeuralNetwork_2eproto();
  friend void protobuf_ShutdownFile_NeuralNetwork_2eproto();

  void InitAsDefaultInstance();
};
extern ::google::protobuf::internal::ExplicitlyConstructed<L2NormalizeLayerParams> L2NormalizeLayerParams_default_instance_;

// -------------------------------------------------------------------

class FlattenLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.FlattenLayerParams) */ {
 public:
  FlattenLayerParams();
  virtual ~FlattenLayerParams();

  FlattenLayerParams(const FlattenLayerParams& from);

  inline FlattenLayerParams& operator=(const FlattenLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const FlattenLayerParams& default_instance();

  static const FlattenLayerParams* internal_default_instance();

  void Swap(FlattenLayerParams* other);

  // implements Message ----------------------------------------------

  inline FlattenLayerParams* New() const { return New(NULL); }

  FlattenLayerParams* New(::google::protobuf::Arena* arena) const;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from);
  void CopyFrom(const FlattenLayerParams& from);
  void MergeFrom(const FlattenLayerParams& from);
  void Clear();
  bool IsInitialized() const;

  size_t ByteSizeLong() const;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input);
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const;
  void DiscardUnknownFields();
  int GetCachedSize() const { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(FlattenLayerParams* other);
  void UnsafeMergeFrom(const FlattenLayerParams& from);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return _arena_ptr_;
  }
  inline ::google::protobuf::Arena* MaybeArenaPtr() const {
    return _arena_ptr_;
  }
  public:

  ::std::string GetTypeName() const;

  // nested types ----------------------------------------------------

  typedef FlattenLayerParams_FlattenOrder FlattenOrder;
  static const FlattenOrder CHANNEL_FIRST =
    FlattenLayerParams_FlattenOrder_CHANNEL_FIRST;
  static const FlattenOrder CHANNEL_LAST =
    FlattenLayerParams_FlattenOrder_CHANNEL_LAST;
  static inline bool FlattenOrder_IsValid(int value) {
    return FlattenLayerParams_FlattenOrder_IsValid(value);
  }
  static const FlattenOrder FlattenOrder_MIN =
    FlattenLayerParams_FlattenOrder_FlattenOrder_MIN;
  static const FlattenOrder FlattenOrder_MAX =
    FlattenLayerParams_FlattenOrder_FlattenOrder_MAX;
  static const int FlattenOrder_ARRAYSIZE =
    FlattenLayerParams_FlattenOrder_FlattenOrder_ARRAYSIZE;

  // accessors -------------------------------------------------------

  // optional .CoreML.Specification.FlattenLayerParams.FlattenOrder mode = 1;
  void clear_mode();
  static const int kModeFieldNumber = 1;
  ::CoreML::Specification::FlattenLayerParams_FlattenOrder mode() const;
  void set_mode(::CoreML::Specification::FlattenLayerParams_FlattenOrder value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.FlattenLayerParams)
 private:

  ::google::protobuf::internal::ArenaStringPtr _unknown_fields_;
  ::google::protobuf::Arena* _arena_ptr_;

  int mode_;
  mutable int _cached_size_;
  friend void  protobuf_InitDefaults_NeuralNetwork_2eproto_impl();
  friend void  protobuf_AddDesc_NeuralNetwork_2eproto_impl();
  friend void protobuf_AssignDesc_NeuralNetwork_2eproto();
  friend void protobuf_ShutdownFile_NeuralNetwork_2eproto();

  void InitAsDefaultInstance();
};
extern ::google::protobuf::internal::ExplicitlyConstructed<FlattenLayerParams> FlattenLayerParams_default_instance_;

// -------------------------------------------------------------------

class ReshapeLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ReshapeLayerParams) */ {
 public:
  ReshapeLayerParams();
  virtual ~ReshapeLayerParams();

  ReshapeLayerParams(const ReshapeLayerParams& from);

  inline ReshapeLayerParams& operator=(const ReshapeLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const ReshapeLayerParams& default_instance();

  static const ReshapeLayerParams* internal_default_instance();

  void Swap(ReshapeLayerParams* other);

  // implements Message ----------------------------------------------

  inline ReshapeLayerParams* New() const { return New(NULL); }

  ReshapeLayerParams* New(::google::protobuf::Arena* arena) const;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from);
  void CopyFrom(const ReshapeLayerParams& from);
  void MergeFrom(const ReshapeLayerParams& from);
  void Clear();
  bool IsInitialized() const;

  size_t ByteSizeLong() const;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input);
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const;
  void DiscardUnknownFields();
  int GetCachedSize() const { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ReshapeLayerParams* other);
  void UnsafeMergeFrom(const ReshapeLayerParams& from);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return _arena_ptr_;
  }
  inline ::google::protobuf::Arena* MaybeArenaPtr() const {
    return _arena_ptr_;
  }
  public:

  ::std::string GetTypeName() const;

  // nested types ----------------------------------------------------

  typedef ReshapeLayerParams_ReshapeOrder ReshapeOrder;
  static const ReshapeOrder CHANNEL_FIRST =
    ReshapeLayerParams_ReshapeOrder_CHANNEL_FIRST;
  static const ReshapeOrder CHANNEL_LAST =
    ReshapeLayerParams_ReshapeOrder_CHANNEL_LAST;
  static inline bool ReshapeOrder_IsValid(int value) {
    return ReshapeLayerParams_ReshapeOrder_IsValid(value);
  }
  static const ReshapeOrder ReshapeOrder_MIN =
    ReshapeLayerParams_ReshapeOrder_ReshapeOrder_MIN;
  static const ReshapeOrder ReshapeOrder_MAX =
    ReshapeLayerParams_ReshapeOrder_ReshapeOrder_MAX;
  static const int ReshapeOrder_ARRAYSIZE =
    ReshapeLayerParams_ReshapeOrder_ReshapeOrder_ARRAYSIZE;

  // accessors -------------------------------------------------------

  // repeated int64 targetShape = 1;
  int targetshape_size() const;
  void clear_targetshape();
  static const int kTargetShapeFieldNumber = 1;
  ::google::protobuf::int64 targetshape(int index) const;
  void set_targetshape(int index, ::google::protobuf::int64 value);
  void add_targetshape(::google::protobuf::int64 value);
  const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
      targetshape() const;
  ::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
      mutable_targetshape();

  // optional .CoreML.Specification.ReshapeLayerParams.ReshapeOrder mode = 2;
  void clear_mode();
  static const int kModeFieldNumber = 2;
  ::CoreML::Specification::ReshapeLayerParams_ReshapeOrder mode() const;
  void set_mode(::CoreML::Specification::ReshapeLayerParams_ReshapeOrder value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ReshapeLayerParams)
 private:

  ::google::protobuf::internal::ArenaStringPtr _unknown_fields_;
  ::google::protobuf::Arena* _arena_ptr_;

  ::google::protobuf::RepeatedField< ::google::protobuf::int64 > targetshape_;
  mutable int _targetshape_cached_byte_size_;
  int mode_;
  mutable int _cached_size_;
  friend void  protobuf_InitDefaults_NeuralNetwork_2eproto_impl();
  friend void  protobuf_AddDesc_NeuralNetwork_2eproto_impl();
  friend void protobuf_AssignDesc_NeuralNetwork_2eproto();
  friend void protobuf_ShutdownFile_NeuralNetwork_2eproto();

  void InitAsDefaultInstance();
};
extern ::google::protobuf::internal::ExplicitlyConstructed<ReshapeLayerParams> ReshapeLayerParams_default_instance_;

// -------------------------------------------------------------------

class PermuteLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.PermuteLayerParams) */ {
 public:
  PermuteLayerParams();
  virtual ~PermuteLayerParams();

  PermuteLayerParams(const PermuteLayerParams& from);

  inline PermuteLayerParams& operator=(const PermuteLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const PermuteLayerParams& default_instance();

  static const PermuteLayerParams* internal_default_instance();

  void Swap(PermuteLayerParams* other);

  // implements Message ----------------------------------------------

  inline PermuteLayerParams* New() const { return New(NULL); }

  PermuteLayerParams* New(::google::protobuf::Arena* arena) const;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from);
  void CopyFrom(const PermuteLayerParams& from);
  void MergeFrom(const PermuteLayerParams& from);
  void Clear();
  bool IsInitialized() const;

  size_t ByteSizeLong() const;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input);
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const;
  void DiscardUnknownFields();
  int GetCachedSize() const { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(PermuteLayerParams* other);
  void UnsafeMergeFrom(const PermuteLayerParams& from);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return _arena_ptr_;
  }
  inline ::google::protobuf::Arena* MaybeArenaPtr() const {
    return _arena_ptr_;
  }
  public:

  ::std::string GetTypeName() const;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated uint64 axis = 1;
  int axis_size() const;
  void clear_axis();
  static const int kAxisFieldNumber = 1;
  ::google::protobuf::uint64 axis(int index) const;
  void set_axis(int index, ::google::protobuf::uint64 value);
  void add_axis(::google::protobuf::uint64 value);
  const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
      axis() const;
  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
      mutable_axis();

  // @@protoc_insertion_point(class_scope:CoreML.Specification.PermuteLayerParams)
 private:

  ::google::protobuf::internal::ArenaStringPtr _unknown_fields_;
  ::google::protobuf::Arena* _arena_ptr_;

  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 > axis_;
  mutable int _axis_cached_byte_size_;
  mutable int _cached_size_;
  friend void  protobuf_InitDefaults_NeuralNetwork_2eproto_impl();
  friend void  protobuf_AddDesc_NeuralNetwork_2eproto_impl();
  friend void protobuf_AssignDesc_NeuralNetwork_2eproto();
  friend void protobuf_ShutdownFile_NeuralNetwork_2eproto();

  void InitAsDefaultInstance();
};
extern ::google::protobuf::internal::ExplicitlyConstructed<PermuteLayerParams> PermuteLayerParams_default_instance_;

// -------------------------------------------------------------------

class ReduceLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.ReduceLayerParams) */ {
 public:
  ReduceLayerParams();
  virtual ~ReduceLayerParams();

  ReduceLayerParams(const ReduceLayerParams& from);

  inline ReduceLayerParams& operator=(const ReduceLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const ReduceLayerParams& default_instance();

  static const ReduceLayerParams* internal_default_instance();

  void Swap(ReduceLayerParams* other);

  // implements Message ----------------------------------------------

  inline ReduceLayerParams* New() const { return New(NULL); }

  ReduceLayerParams* New(::google::protobuf::Arena* arena) const;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from);
  void CopyFrom(const ReduceLayerParams& from);
  void MergeFrom(const ReduceLayerParams& from);
  void Clear();
  bool IsInitialized() const;

  size_t ByteSizeLong() const;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input);
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const;
  void DiscardUnknownFields();
  int GetCachedSize() const { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ReduceLayerParams* other);
  void UnsafeMergeFrom(const ReduceLayerParams& from);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return _arena_ptr_;
  }
  inline ::google::protobuf::Arena* MaybeArenaPtr() const {
    return _arena_ptr_;
  }
  public:

  ::std::string GetTypeName() const;

  // nested types ----------------------------------------------------

  typedef ReduceLayerParams_ReduceOperation ReduceOperation;
  static const ReduceOperation SUM =
    ReduceLayerParams_ReduceOperation_SUM;
  static const ReduceOperation AVG =
    ReduceLayerParams_ReduceOperation_AVG;
  static const ReduceOperation PROD =
    ReduceLayerParams_ReduceOperation_PROD;
  static const ReduceOperation LOGSUM =
    ReduceLayerParams_ReduceOperation_LOGSUM;
  static const ReduceOperation SUMSQUARE =
    ReduceLayerParams_ReduceOperation_SUMSQUARE;
  static const ReduceOperation L1 =
    ReduceLayerParams_ReduceOperation_L1;
  static const ReduceOperation L2 =
    ReduceLayerParams_ReduceOperation_L2;
  static inline bool ReduceOperation_IsValid(int value) {
    return ReduceLayerParams_ReduceOperation_IsValid(value);
  }
  static const ReduceOperation ReduceOperation_MIN =
    ReduceLayerParams_ReduceOperation_ReduceOperation_MIN;
  static const ReduceOperation ReduceOperation_MAX =
    ReduceLayerParams_ReduceOperation_ReduceOperation_MAX;
  static const int ReduceOperation_ARRAYSIZE =
    ReduceLayerParams_ReduceOperation_ReduceOperation_ARRAYSIZE;

  // accessors -------------------------------------------------------

  // optional .CoreML.Specification.ReduceLayerParams.ReduceOperation mode = 1;
  void clear_mode();
  static const int kModeFieldNumber = 1;
  ::CoreML::Specification::ReduceLayerParams_ReduceOperation mode() const;
  void set_mode(::CoreML::Specification::ReduceLayerParams_ReduceOperation value);

  // optional float epsilon = 2;
  void clear_epsilon();
  static const int kEpsilonFieldNumber = 2;
  float epsilon() const;
  void set_epsilon(float value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.ReduceLayerParams)
 private:

  ::google::protobuf::internal::ArenaStringPtr _unknown_fields_;
  ::google::protobuf::Arena* _arena_ptr_;

  int mode_;
  float epsilon_;
  mutable int _cached_size_;
  friend void  protobuf_InitDefaults_NeuralNetwork_2eproto_impl();
  friend void  protobuf_AddDesc_NeuralNetwork_2eproto_impl();
  friend void protobuf_AssignDesc_NeuralNetwork_2eproto();
  friend void protobuf_ShutdownFile_NeuralNetwork_2eproto();

  void InitAsDefaultInstance();
};
extern ::google::protobuf::internal::ExplicitlyConstructed<ReduceLayerParams> ReduceLayerParams_default_instance_;

// -------------------------------------------------------------------

class CropLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.CropLayerParams) */ {
 public:
  CropLayerParams();
  virtual ~CropLayerParams();

  CropLayerParams(const CropLayerParams& from);

  inline CropLayerParams& operator=(const CropLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const CropLayerParams& default_instance();

  static const CropLayerParams* internal_default_instance();

  void Swap(CropLayerParams* other);

  // implements Message ----------------------------------------------

  inline CropLayerParams* New() const { return New(NULL); }

  CropLayerParams* New(::google::protobuf::Arena* arena) const;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from);
  void CopyFrom(const CropLayerParams& from);
  void MergeFrom(const CropLayerParams& from);
  void Clear();
  bool IsInitialized() const;

  size_t ByteSizeLong() const;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input);
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const;
  void DiscardUnknownFields();
  int GetCachedSize() const { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(CropLayerParams* other);
  void UnsafeMergeFrom(const CropLayerParams& from);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return _arena_ptr_;
  }
  inline ::google::protobuf::Arena* MaybeArenaPtr() const {
    return _arena_ptr_;
  }
  public:

  ::std::string GetTypeName() const;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // optional .CoreML.Specification.BorderAmounts cropAmounts = 1;
  bool has_cropamounts() const;
  void clear_cropamounts();
  static const int kCropAmountsFieldNumber = 1;
  const ::CoreML::Specification::BorderAmounts& cropamounts() const;
  ::CoreML::Specification::BorderAmounts* mutable_cropamounts();
  ::CoreML::Specification::BorderAmounts* release_cropamounts();
  void set_allocated_cropamounts(::CoreML::Specification::BorderAmounts* cropamounts);

  // repeated uint64 offset = 5;
  int offset_size() const;
  void clear_offset();
  static const int kOffsetFieldNumber = 5;
  ::google::protobuf::uint64 offset(int index) const;
  void set_offset(int index, ::google::protobuf::uint64 value);
  void add_offset(::google::protobuf::uint64 value);
  const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
      offset() const;
  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
      mutable_offset();

  // @@protoc_insertion_point(class_scope:CoreML.Specification.CropLayerParams)
 private:

  ::google::protobuf::internal::ArenaStringPtr _unknown_fields_;
  ::google::protobuf::Arena* _arena_ptr_;

  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 > offset_;
  mutable int _offset_cached_byte_size_;
  ::CoreML::Specification::BorderAmounts* cropamounts_;
  mutable int _cached_size_;
  friend void  protobuf_InitDefaults_NeuralNetwork_2eproto_impl();
  friend void  protobuf_AddDesc_NeuralNetwork_2eproto_impl();
  friend void protobuf_AssignDesc_NeuralNetwork_2eproto();
  friend void protobuf_ShutdownFile_NeuralNetwork_2eproto();

  void InitAsDefaultInstance();
};
extern ::google::protobuf::internal::ExplicitlyConstructed<CropLayerParams> CropLayerParams_default_instance_;

// -------------------------------------------------------------------

class AverageLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.AverageLayerParams) */ {
 public:
  AverageLayerParams();
  virtual ~AverageLayerParams();

  AverageLayerParams(const AverageLayerParams& from);

  inline AverageLayerParams& operator=(const AverageLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const AverageLayerParams& default_instance();

  static const AverageLayerParams* internal_default_instance();

  void Swap(AverageLayerParams* other);

  // implements Message ----------------------------------------------

  inline AverageLayerParams* New() const { return New(NULL); }

  AverageLayerParams* New(::google::protobuf::Arena* arena) const;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from);
  void CopyFrom(const AverageLayerParams& from);
  void MergeFrom(const AverageLayerParams& from);
  void Clear();
  bool IsInitialized() const;

  size_t ByteSizeLong() const;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input);
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const;
  void DiscardUnknownFields();
  int GetCachedSize() const { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(AverageLayerParams* other);
  void UnsafeMergeFrom(const AverageLayerParams& from);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return _arena_ptr_;
  }
  inline ::google::protobuf::Arena* MaybeArenaPtr() const {
    return _arena_ptr_;
  }
  public:

  ::std::string GetTypeName() const;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.AverageLayerParams)
 private:

  ::google::protobuf::internal::ArenaStringPtr _unknown_fields_;
  ::google::protobuf::Arena* _arena_ptr_;

  mutable int _cached_size_;
  friend void  protobuf_InitDefaults_NeuralNetwork_2eproto_impl();
  friend void  protobuf_AddDesc_NeuralNetwork_2eproto_impl();
  friend void protobuf_AssignDesc_NeuralNetwork_2eproto();
  friend void protobuf_ShutdownFile_NeuralNetwork_2eproto();

  void InitAsDefaultInstance();
};
extern ::google::protobuf::internal::ExplicitlyConstructed<AverageLayerParams> AverageLayerParams_default_instance_;

// -------------------------------------------------------------------

class MaxLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.MaxLayerParams) */ {
 public:
  MaxLayerParams();
  virtual ~MaxLayerParams();

  MaxLayerParams(const MaxLayerParams& from);

  inline MaxLayerParams& operator=(const MaxLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const MaxLayerParams& default_instance();

  static const MaxLayerParams* internal_default_instance();

  void Swap(MaxLayerParams* other);

  // implements Message ----------------------------------------------

  inline MaxLayerParams* New() const { return New(NULL); }

  MaxLayerParams* New(::google::protobuf::Arena* arena) const;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from);
  void CopyFrom(const MaxLayerParams& from);
  void MergeFrom(const MaxLayerParams& from);
  void Clear();
  bool IsInitialized() const;

  size_t ByteSizeLong() const;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input);
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const;
  void DiscardUnknownFields();
  int GetCachedSize() const { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(MaxLayerParams* other);
  void UnsafeMergeFrom(const MaxLayerParams& from);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return _arena_ptr_;
  }
  inline ::google::protobuf::Arena* MaybeArenaPtr() const {
    return _arena_ptr_;
  }
  public:

  ::std::string GetTypeName() const;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.MaxLayerParams)
 private:

  ::google::protobuf::internal::ArenaStringPtr _unknown_fields_;
  ::google::protobuf::Arena* _arena_ptr_;

  mutable int _cached_size_;
  friend void  protobuf_InitDefaults_NeuralNetwork_2eproto_impl();
  friend void  protobuf_AddDesc_NeuralNetwork_2eproto_impl();
  friend void protobuf_AssignDesc_NeuralNetwork_2eproto();
  friend void protobuf_ShutdownFile_NeuralNetwork_2eproto();

  void InitAsDefaultInstance();
};
extern ::google::protobuf::internal::ExplicitlyConstructed<MaxLayerParams> MaxLayerParams_default_instance_;

// -------------------------------------------------------------------

class MinLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.MinLayerParams) */ {
 public:
  MinLayerParams();
  virtual ~MinLayerParams();

  MinLayerParams(const MinLayerParams& from);

  inline MinLayerParams& operator=(const MinLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const MinLayerParams& default_instance();

  static const MinLayerParams* internal_default_instance();

  void Swap(MinLayerParams* other);

  // implements Message ----------------------------------------------

  inline MinLayerParams* New() const { return New(NULL); }

  MinLayerParams* New(::google::protobuf::Arena* arena) const;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from);
  void CopyFrom(const MinLayerParams& from);
  void MergeFrom(const MinLayerParams& from);
  void Clear();
  bool IsInitialized() const;

  size_t ByteSizeLong() const;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input);
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const;
  void DiscardUnknownFields();
  int GetCachedSize() const { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(MinLayerParams* other);
  void UnsafeMergeFrom(const MinLayerParams& from);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return _arena_ptr_;
  }
  inline ::google::protobuf::Arena* MaybeArenaPtr() const {
    return _arena_ptr_;
  }
  public:

  ::std::string GetTypeName() const;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:CoreML.Specification.MinLayerParams)
 private:

  ::google::protobuf::internal::ArenaStringPtr _unknown_fields_;
  ::google::protobuf::Arena* _arena_ptr_;

  mutable int _cached_size_;
  friend void  protobuf_InitDefaults_NeuralNetwork_2eproto_impl();
  friend void  protobuf_AddDesc_NeuralNetwork_2eproto_impl();
  friend void protobuf_AssignDesc_NeuralNetwork_2eproto();
  friend void protobuf_ShutdownFile_NeuralNetwork_2eproto();

  void InitAsDefaultInstance();
};
extern ::google::protobuf::internal::ExplicitlyConstructed<MinLayerParams> MinLayerParams_default_instance_;

// -------------------------------------------------------------------

class DotProductLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.DotProductLayerParams) */ {
 public:
  DotProductLayerParams();
  virtual ~DotProductLayerParams();

  DotProductLayerParams(const DotProductLayerParams& from);

  inline DotProductLayerParams& operator=(const DotProductLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const DotProductLayerParams& default_instance();

  static const DotProductLayerParams* internal_default_instance();

  void Swap(DotProductLayerParams* other);

  // implements Message ----------------------------------------------

  inline DotProductLayerParams* New() const { return New(NULL); }

  DotProductLayerParams* New(::google::protobuf::Arena* arena) const;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from);
  void CopyFrom(const DotProductLayerParams& from);
  void MergeFrom(const DotProductLayerParams& from);
  void Clear();
  bool IsInitialized() const;

  size_t ByteSizeLong() const;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input);
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const;
  void DiscardUnknownFields();
  int GetCachedSize() const { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(DotProductLayerParams* other);
  void UnsafeMergeFrom(const DotProductLayerParams& from);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return _arena_ptr_;
  }
  inline ::google::protobuf::Arena* MaybeArenaPtr() const {
    return _arena_ptr_;
  }
  public:

  ::std::string GetTypeName() const;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // optional bool cosineSimilarity = 1;
  void clear_cosinesimilarity();
  static const int kCosineSimilarityFieldNumber = 1;
  bool cosinesimilarity() const;
  void set_cosinesimilarity(bool value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.DotProductLayerParams)
 private:

  ::google::protobuf::internal::ArenaStringPtr _unknown_fields_;
  ::google::protobuf::Arena* _arena_ptr_;

  bool cosinesimilarity_;
  mutable int _cached_size_;
  friend void  protobuf_InitDefaults_NeuralNetwork_2eproto_impl();
  friend void  protobuf_AddDesc_NeuralNetwork_2eproto_impl();
  friend void protobuf_AssignDesc_NeuralNetwork_2eproto();
  friend void protobuf_ShutdownFile_NeuralNetwork_2eproto();

  void InitAsDefaultInstance();
};
extern ::google::protobuf::internal::ExplicitlyConstructed<DotProductLayerParams> DotProductLayerParams_default_instance_;

// -------------------------------------------------------------------

class MeanVarianceNormalizeLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.MeanVarianceNormalizeLayerParams) */ {
 public:
  MeanVarianceNormalizeLayerParams();
  virtual ~MeanVarianceNormalizeLayerParams();

  MeanVarianceNormalizeLayerParams(const MeanVarianceNormalizeLayerParams& from);

  inline MeanVarianceNormalizeLayerParams& operator=(const MeanVarianceNormalizeLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const MeanVarianceNormalizeLayerParams& default_instance();

  static const MeanVarianceNormalizeLayerParams* internal_default_instance();

  void Swap(MeanVarianceNormalizeLayerParams* other);

  // implements Message ----------------------------------------------

  inline MeanVarianceNormalizeLayerParams* New() const { return New(NULL); }

  MeanVarianceNormalizeLayerParams* New(::google::protobuf::Arena* arena) const;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from);
  void CopyFrom(const MeanVarianceNormalizeLayerParams& from);
  void MergeFrom(const MeanVarianceNormalizeLayerParams& from);
  void Clear();
  bool IsInitialized() const;

  size_t ByteSizeLong() const;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input);
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const;
  void DiscardUnknownFields();
  int GetCachedSize() const { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(MeanVarianceNormalizeLayerParams* other);
  void UnsafeMergeFrom(const MeanVarianceNormalizeLayerParams& from);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return _arena_ptr_;
  }
  inline ::google::protobuf::Arena* MaybeArenaPtr() const {
    return _arena_ptr_;
  }
  public:

  ::std::string GetTypeName() const;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // optional bool acrossChannels = 1;
  void clear_acrosschannels();
  static const int kAcrossChannelsFieldNumber = 1;
  bool acrosschannels() const;
  void set_acrosschannels(bool value);

  // optional bool normalizeVariance = 2;
  void clear_normalizevariance();
  static const int kNormalizeVarianceFieldNumber = 2;
  bool normalizevariance() const;
  void set_normalizevariance(bool value);

  // optional float epsilon = 3;
  void clear_epsilon();
  static const int kEpsilonFieldNumber = 3;
  float epsilon() const;
  void set_epsilon(float value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.MeanVarianceNormalizeLayerParams)
 private:

  ::google::protobuf::internal::ArenaStringPtr _unknown_fields_;
  ::google::protobuf::Arena* _arena_ptr_;

  bool acrosschannels_;
  bool normalizevariance_;
  float epsilon_;
  mutable int _cached_size_;
  friend void  protobuf_InitDefaults_NeuralNetwork_2eproto_impl();
  friend void  protobuf_AddDesc_NeuralNetwork_2eproto_impl();
  friend void protobuf_AssignDesc_NeuralNetwork_2eproto();
  friend void protobuf_ShutdownFile_NeuralNetwork_2eproto();

  void InitAsDefaultInstance();
};
extern ::google::protobuf::internal::ExplicitlyConstructed<MeanVarianceNormalizeLayerParams> MeanVarianceNormalizeLayerParams_default_instance_;

// -------------------------------------------------------------------

class SequenceRepeatLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.SequenceRepeatLayerParams) */ {
 public:
  SequenceRepeatLayerParams();
  virtual ~SequenceRepeatLayerParams();

  SequenceRepeatLayerParams(const SequenceRepeatLayerParams& from);

  inline SequenceRepeatLayerParams& operator=(const SequenceRepeatLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const SequenceRepeatLayerParams& default_instance();

  static const SequenceRepeatLayerParams* internal_default_instance();

  void Swap(SequenceRepeatLayerParams* other);

  // implements Message ----------------------------------------------

  inline SequenceRepeatLayerParams* New() const { return New(NULL); }

  SequenceRepeatLayerParams* New(::google::protobuf::Arena* arena) const;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from);
  void CopyFrom(const SequenceRepeatLayerParams& from);
  void MergeFrom(const SequenceRepeatLayerParams& from);
  void Clear();
  bool IsInitialized() const;

  size_t ByteSizeLong() const;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input);
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const;
  void DiscardUnknownFields();
  int GetCachedSize() const { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(SequenceRepeatLayerParams* other);
  void UnsafeMergeFrom(const SequenceRepeatLayerParams& from);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return _arena_ptr_;
  }
  inline ::google::protobuf::Arena* MaybeArenaPtr() const {
    return _arena_ptr_;
  }
  public:

  ::std::string GetTypeName() const;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // optional uint64 nRepetitions = 1;
  void clear_nrepetitions();
  static const int kNRepetitionsFieldNumber = 1;
  ::google::protobuf::uint64 nrepetitions() const;
  void set_nrepetitions(::google::protobuf::uint64 value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.SequenceRepeatLayerParams)
 private:

  ::google::protobuf::internal::ArenaStringPtr _unknown_fields_;
  ::google::protobuf::Arena* _arena_ptr_;

  ::google::protobuf::uint64 nrepetitions_;
  mutable int _cached_size_;
  friend void  protobuf_InitDefaults_NeuralNetwork_2eproto_impl();
  friend void  protobuf_AddDesc_NeuralNetwork_2eproto_impl();
  friend void protobuf_AssignDesc_NeuralNetwork_2eproto();
  friend void protobuf_ShutdownFile_NeuralNetwork_2eproto();

  void InitAsDefaultInstance();
};
extern ::google::protobuf::internal::ExplicitlyConstructed<SequenceRepeatLayerParams> SequenceRepeatLayerParams_default_instance_;

// -------------------------------------------------------------------

class SimpleRecurrentLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.SimpleRecurrentLayerParams) */ {
 public:
  SimpleRecurrentLayerParams();
  virtual ~SimpleRecurrentLayerParams();

  SimpleRecurrentLayerParams(const SimpleRecurrentLayerParams& from);

  inline SimpleRecurrentLayerParams& operator=(const SimpleRecurrentLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const SimpleRecurrentLayerParams& default_instance();

  static const SimpleRecurrentLayerParams* internal_default_instance();

  void Swap(SimpleRecurrentLayerParams* other);

  // implements Message ----------------------------------------------

  inline SimpleRecurrentLayerParams* New() const { return New(NULL); }

  SimpleRecurrentLayerParams* New(::google::protobuf::Arena* arena) const;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from);
  void CopyFrom(const SimpleRecurrentLayerParams& from);
  void MergeFrom(const SimpleRecurrentLayerParams& from);
  void Clear();
  bool IsInitialized() const;

  size_t ByteSizeLong() const;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input);
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const;
  void DiscardUnknownFields();
  int GetCachedSize() const { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(SimpleRecurrentLayerParams* other);
  void UnsafeMergeFrom(const SimpleRecurrentLayerParams& from);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return _arena_ptr_;
  }
  inline ::google::protobuf::Arena* MaybeArenaPtr() const {
    return _arena_ptr_;
  }
  public:

  ::std::string GetTypeName() const;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // optional uint64 inputVectorSize = 1;
  void clear_inputvectorsize();
  static const int kInputVectorSizeFieldNumber = 1;
  ::google::protobuf::uint64 inputvectorsize() const;
  void set_inputvectorsize(::google::protobuf::uint64 value);

  // optional uint64 outputVectorSize = 2;
  void clear_outputvectorsize();
  static const int kOutputVectorSizeFieldNumber = 2;
  ::google::protobuf::uint64 outputvectorsize() const;
  void set_outputvectorsize(::google::protobuf::uint64 value);

  // optional .CoreML.Specification.ActivationParams activation = 10;
  bool has_activation() const;
  void clear_activation();
  static const int kActivationFieldNumber = 10;
  const ::CoreML::Specification::ActivationParams& activation() const;
  ::CoreML::Specification::ActivationParams* mutable_activation();
  ::CoreML::Specification::ActivationParams* release_activation();
  void set_allocated_activation(::CoreML::Specification::ActivationParams* activation);

  // optional bool sequenceOutput = 15;
  void clear_sequenceoutput();
  static const int kSequenceOutputFieldNumber = 15;
  bool sequenceoutput() const;
  void set_sequenceoutput(bool value);

  // optional bool hasBiasVector = 20;
  void clear_hasbiasvector();
  static const int kHasBiasVectorFieldNumber = 20;
  bool hasbiasvector() const;
  void set_hasbiasvector(bool value);

  // optional .CoreML.Specification.WeightParams weightMatrix = 30;
  bool has_weightmatrix() const;
  void clear_weightmatrix();
  static const int kWeightMatrixFieldNumber = 30;
  const ::CoreML::Specification::WeightParams& weightmatrix() const;
  ::CoreML::Specification::WeightParams* mutable_weightmatrix();
  ::CoreML::Specification::WeightParams* release_weightmatrix();
  void set_allocated_weightmatrix(::CoreML::Specification::WeightParams* weightmatrix);

  // optional .CoreML.Specification.WeightParams recursionMatrix = 31;
  bool has_recursionmatrix() const;
  void clear_recursionmatrix();
  static const int kRecursionMatrixFieldNumber = 31;
  const ::CoreML::Specification::WeightParams& recursionmatrix() const;
  ::CoreML::Specification::WeightParams* mutable_recursionmatrix();
  ::CoreML::Specification::WeightParams* release_recursionmatrix();
  void set_allocated_recursionmatrix(::CoreML::Specification::WeightParams* recursionmatrix);

  // optional .CoreML.Specification.WeightParams biasVector = 32;
  bool has_biasvector() const;
  void clear_biasvector();
  static const int kBiasVectorFieldNumber = 32;
  const ::CoreML::Specification::WeightParams& biasvector() const;
  ::CoreML::Specification::WeightParams* mutable_biasvector();
  ::CoreML::Specification::WeightParams* release_biasvector();
  void set_allocated_biasvector(::CoreML::Specification::WeightParams* biasvector);

  // optional bool reverseInput = 100;
  void clear_reverseinput();
  static const int kReverseInputFieldNumber = 100;
  bool reverseinput() const;
  void set_reverseinput(bool value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.SimpleRecurrentLayerParams)
 private:

  ::google::protobuf::internal::ArenaStringPtr _unknown_fields_;
  ::google::protobuf::Arena* _arena_ptr_;

  ::CoreML::Specification::ActivationParams* activation_;
  ::CoreML::Specification::WeightParams* weightmatrix_;
  ::CoreML::Specification::WeightParams* recursionmatrix_;
  ::CoreML::Specification::WeightParams* biasvector_;
  ::google::protobuf::uint64 inputvectorsize_;
  ::google::protobuf::uint64 outputvectorsize_;
  bool sequenceoutput_;
  bool hasbiasvector_;
  bool reverseinput_;
  mutable int _cached_size_;
  friend void  protobuf_InitDefaults_NeuralNetwork_2eproto_impl();
  friend void  protobuf_AddDesc_NeuralNetwork_2eproto_impl();
  friend void protobuf_AssignDesc_NeuralNetwork_2eproto();
  friend void protobuf_ShutdownFile_NeuralNetwork_2eproto();

  void InitAsDefaultInstance();
};
extern ::google::protobuf::internal::ExplicitlyConstructed<SimpleRecurrentLayerParams> SimpleRecurrentLayerParams_default_instance_;

// -------------------------------------------------------------------

class GRULayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.GRULayerParams) */ {
 public:
  GRULayerParams();
  virtual ~GRULayerParams();

  GRULayerParams(const GRULayerParams& from);

  inline GRULayerParams& operator=(const GRULayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const GRULayerParams& default_instance();

  static const GRULayerParams* internal_default_instance();

  void Swap(GRULayerParams* other);

  // implements Message ----------------------------------------------

  inline GRULayerParams* New() const { return New(NULL); }

  GRULayerParams* New(::google::protobuf::Arena* arena) const;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from);
  void CopyFrom(const GRULayerParams& from);
  void MergeFrom(const GRULayerParams& from);
  void Clear();
  bool IsInitialized() const;

  size_t ByteSizeLong() const;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input);
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const;
  void DiscardUnknownFields();
  int GetCachedSize() const { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(GRULayerParams* other);
  void UnsafeMergeFrom(const GRULayerParams& from);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return _arena_ptr_;
  }
  inline ::google::protobuf::Arena* MaybeArenaPtr() const {
    return _arena_ptr_;
  }
  public:

  ::std::string GetTypeName() const;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // optional uint64 inputVectorSize = 1;
  void clear_inputvectorsize();
  static const int kInputVectorSizeFieldNumber = 1;
  ::google::protobuf::uint64 inputvectorsize() const;
  void set_inputvectorsize(::google::protobuf::uint64 value);

  // optional uint64 outputVectorSize = 2;
  void clear_outputvectorsize();
  static const int kOutputVectorSizeFieldNumber = 2;
  ::google::protobuf::uint64 outputvectorsize() const;
  void set_outputvectorsize(::google::protobuf::uint64 value);

  // repeated .CoreML.Specification.ActivationParams activations = 10;
  int activations_size() const;
  void clear_activations();
  static const int kActivationsFieldNumber = 10;
  const ::CoreML::Specification::ActivationParams& activations(int index) const;
  ::CoreML::Specification::ActivationParams* mutable_activations(int index);
  ::CoreML::Specification::ActivationParams* add_activations();
  ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::ActivationParams >*
      mutable_activations();
  const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::ActivationParams >&
      activations() const;

  // optional bool sequenceOutput = 15;
  void clear_sequenceoutput();
  static const int kSequenceOutputFieldNumber = 15;
  bool sequenceoutput() const;
  void set_sequenceoutput(bool value);

  // optional bool hasBiasVectors = 20;
  void clear_hasbiasvectors();
  static const int kHasBiasVectorsFieldNumber = 20;
  bool hasbiasvectors() const;
  void set_hasbiasvectors(bool value);

  // optional .CoreML.Specification.WeightParams updateGateWeightMatrix = 30;
  bool has_updategateweightmatrix() const;
  void clear_updategateweightmatrix();
  static const int kUpdateGateWeightMatrixFieldNumber = 30;
  const ::CoreML::Specification::WeightParams& updategateweightmatrix() const;
  ::CoreML::Specification::WeightParams* mutable_updategateweightmatrix();
  ::CoreML::Specification::WeightParams* release_updategateweightmatrix();
  void set_allocated_updategateweightmatrix(::CoreML::Specification::WeightParams* updategateweightmatrix);

  // optional .CoreML.Specification.WeightParams resetGateWeightMatrix = 31;
  bool has_resetgateweightmatrix() const;
  void clear_resetgateweightmatrix();
  static const int kResetGateWeightMatrixFieldNumber = 31;
  const ::CoreML::Specification::WeightParams& resetgateweightmatrix() const;
  ::CoreML::Specification::WeightParams* mutable_resetgateweightmatrix();
  ::CoreML::Specification::WeightParams* release_resetgateweightmatrix();
  void set_allocated_resetgateweightmatrix(::CoreML::Specification::WeightParams* resetgateweightmatrix);

  // optional .CoreML.Specification.WeightParams outputGateWeightMatrix = 32;
  bool has_outputgateweightmatrix() const;
  void clear_outputgateweightmatrix();
  static const int kOutputGateWeightMatrixFieldNumber = 32;
  const ::CoreML::Specification::WeightParams& outputgateweightmatrix() const;
  ::CoreML::Specification::WeightParams* mutable_outputgateweightmatrix();
  ::CoreML::Specification::WeightParams* release_outputgateweightmatrix();
  void set_allocated_outputgateweightmatrix(::CoreML::Specification::WeightParams* outputgateweightmatrix);

  // optional .CoreML.Specification.WeightParams updateGateRecursionMatrix = 50;
  bool has_updategaterecursionmatrix() const;
  void clear_updategaterecursionmatrix();
  static const int kUpdateGateRecursionMatrixFieldNumber = 50;
  const ::CoreML::Specification::WeightParams& updategaterecursionmatrix() const;
  ::CoreML::Specification::WeightParams* mutable_updategaterecursionmatrix();
  ::CoreML::Specification::WeightParams* release_updategaterecursionmatrix();
  void set_allocated_updategaterecursionmatrix(::CoreML::Specification::WeightParams* updategaterecursionmatrix);

  // optional .CoreML.Specification.WeightParams resetGateRecursionMatrix = 51;
  bool has_resetgaterecursionmatrix() const;
  void clear_resetgaterecursionmatrix();
  static const int kResetGateRecursionMatrixFieldNumber = 51;
  const ::CoreML::Specification::WeightParams& resetgaterecursionmatrix() const;
  ::CoreML::Specification::WeightParams* mutable_resetgaterecursionmatrix();
  ::CoreML::Specification::WeightParams* release_resetgaterecursionmatrix();
  void set_allocated_resetgaterecursionmatrix(::CoreML::Specification::WeightParams* resetgaterecursionmatrix);

  // optional .CoreML.Specification.WeightParams outputGateRecursionMatrix = 52;
  bool has_outputgaterecursionmatrix() const;
  void clear_outputgaterecursionmatrix();
  static const int kOutputGateRecursionMatrixFieldNumber = 52;
  const ::CoreML::Specification::WeightParams& outputgaterecursionmatrix() const;
  ::CoreML::Specification::WeightParams* mutable_outputgaterecursionmatrix();
  ::CoreML::Specification::WeightParams* release_outputgaterecursionmatrix();
  void set_allocated_outputgaterecursionmatrix(::CoreML::Specification::WeightParams* outputgaterecursionmatrix);

  // optional .CoreML.Specification.WeightParams updateGateBiasVector = 70;
  bool has_updategatebiasvector() const;
  void clear_updategatebiasvector();
  static const int kUpdateGateBiasVectorFieldNumber = 70;
  const ::CoreML::Specification::WeightParams& updategatebiasvector() const;
  ::CoreML::Specification::WeightParams* mutable_updategatebiasvector();
  ::CoreML::Specification::WeightParams* release_updategatebiasvector();
  void set_allocated_updategatebiasvector(::CoreML::Specification::WeightParams* updategatebiasvector);

  // optional .CoreML.Specification.WeightParams resetGateBiasVector = 71;
  bool has_resetgatebiasvector() const;
  void clear_resetgatebiasvector();
  static const int kResetGateBiasVectorFieldNumber = 71;
  const ::CoreML::Specification::WeightParams& resetgatebiasvector() const;
  ::CoreML::Specification::WeightParams* mutable_resetgatebiasvector();
  ::CoreML::Specification::WeightParams* release_resetgatebiasvector();
  void set_allocated_resetgatebiasvector(::CoreML::Specification::WeightParams* resetgatebiasvector);

  // optional .CoreML.Specification.WeightParams outputGateBiasVector = 72;
  bool has_outputgatebiasvector() const;
  void clear_outputgatebiasvector();
  static const int kOutputGateBiasVectorFieldNumber = 72;
  const ::CoreML::Specification::WeightParams& outputgatebiasvector() const;
  ::CoreML::Specification::WeightParams* mutable_outputgatebiasvector();
  ::CoreML::Specification::WeightParams* release_outputgatebiasvector();
  void set_allocated_outputgatebiasvector(::CoreML::Specification::WeightParams* outputgatebiasvector);

  // optional bool reverseInput = 100;
  void clear_reverseinput();
  static const int kReverseInputFieldNumber = 100;
  bool reverseinput() const;
  void set_reverseinput(bool value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.GRULayerParams)
 private:

  ::google::protobuf::internal::ArenaStringPtr _unknown_fields_;
  ::google::protobuf::Arena* _arena_ptr_;

  ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::ActivationParams > activations_;
  ::CoreML::Specification::WeightParams* updategateweightmatrix_;
  ::CoreML::Specification::WeightParams* resetgateweightmatrix_;
  ::CoreML::Specification::WeightParams* outputgateweightmatrix_;
  ::CoreML::Specification::WeightParams* updategaterecursionmatrix_;
  ::CoreML::Specification::WeightParams* resetgaterecursionmatrix_;
  ::CoreML::Specification::WeightParams* outputgaterecursionmatrix_;
  ::CoreML::Specification::WeightParams* updategatebiasvector_;
  ::CoreML::Specification::WeightParams* resetgatebiasvector_;
  ::CoreML::Specification::WeightParams* outputgatebiasvector_;
  ::google::protobuf::uint64 inputvectorsize_;
  ::google::protobuf::uint64 outputvectorsize_;
  bool sequenceoutput_;
  bool hasbiasvectors_;
  bool reverseinput_;
  mutable int _cached_size_;
  friend void  protobuf_InitDefaults_NeuralNetwork_2eproto_impl();
  friend void  protobuf_AddDesc_NeuralNetwork_2eproto_impl();
  friend void protobuf_AssignDesc_NeuralNetwork_2eproto();
  friend void protobuf_ShutdownFile_NeuralNetwork_2eproto();

  void InitAsDefaultInstance();
};
extern ::google::protobuf::internal::ExplicitlyConstructed<GRULayerParams> GRULayerParams_default_instance_;

// -------------------------------------------------------------------

class LSTMParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.LSTMParams) */ {
 public:
  LSTMParams();
  virtual ~LSTMParams();

  LSTMParams(const LSTMParams& from);

  inline LSTMParams& operator=(const LSTMParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const LSTMParams& default_instance();

  static const LSTMParams* internal_default_instance();

  void Swap(LSTMParams* other);

  // implements Message ----------------------------------------------

  inline LSTMParams* New() const { return New(NULL); }

  LSTMParams* New(::google::protobuf::Arena* arena) const;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from);
  void CopyFrom(const LSTMParams& from);
  void MergeFrom(const LSTMParams& from);
  void Clear();
  bool IsInitialized() const;

  size_t ByteSizeLong() const;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input);
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const;
  void DiscardUnknownFields();
  int GetCachedSize() const { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(LSTMParams* other);
  void UnsafeMergeFrom(const LSTMParams& from);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return _arena_ptr_;
  }
  inline ::google::protobuf::Arena* MaybeArenaPtr() const {
    return _arena_ptr_;
  }
  public:

  ::std::string GetTypeName() const;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // optional bool sequenceOutput = 10;
  void clear_sequenceoutput();
  static const int kSequenceOutputFieldNumber = 10;
  bool sequenceoutput() const;
  void set_sequenceoutput(bool value);

  // optional bool hasBiasVectors = 20;
  void clear_hasbiasvectors();
  static const int kHasBiasVectorsFieldNumber = 20;
  bool hasbiasvectors() const;
  void set_hasbiasvectors(bool value);

  // optional bool forgetBias = 30;
  void clear_forgetbias();
  static const int kForgetBiasFieldNumber = 30;
  bool forgetbias() const;
  void set_forgetbias(bool value);

  // optional bool hasPeepholeVectors = 40;
  void clear_haspeepholevectors();
  static const int kHasPeepholeVectorsFieldNumber = 40;
  bool haspeepholevectors() const;
  void set_haspeepholevectors(bool value);

  // optional bool coupledInputAndForgetGate = 50;
  void clear_coupledinputandforgetgate();
  static const int kCoupledInputAndForgetGateFieldNumber = 50;
  bool coupledinputandforgetgate() const;
  void set_coupledinputandforgetgate(bool value);

  // optional float cellClipThreshold = 60;
  void clear_cellclipthreshold();
  static const int kCellClipThresholdFieldNumber = 60;
  float cellclipthreshold() const;
  void set_cellclipthreshold(float value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.LSTMParams)
 private:

  ::google::protobuf::internal::ArenaStringPtr _unknown_fields_;
  ::google::protobuf::Arena* _arena_ptr_;

  bool sequenceoutput_;
  bool hasbiasvectors_;
  bool forgetbias_;
  bool haspeepholevectors_;
  bool coupledinputandforgetgate_;
  float cellclipthreshold_;
  mutable int _cached_size_;
  friend void  protobuf_InitDefaults_NeuralNetwork_2eproto_impl();
  friend void  protobuf_AddDesc_NeuralNetwork_2eproto_impl();
  friend void protobuf_AssignDesc_NeuralNetwork_2eproto();
  friend void protobuf_ShutdownFile_NeuralNetwork_2eproto();

  void InitAsDefaultInstance();
};
extern ::google::protobuf::internal::ExplicitlyConstructed<LSTMParams> LSTMParams_default_instance_;

// -------------------------------------------------------------------

class LSTMWeightParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.LSTMWeightParams) */ {
 public:
  LSTMWeightParams();
  virtual ~LSTMWeightParams();

  LSTMWeightParams(const LSTMWeightParams& from);

  inline LSTMWeightParams& operator=(const LSTMWeightParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const LSTMWeightParams& default_instance();

  static const LSTMWeightParams* internal_default_instance();

  void Swap(LSTMWeightParams* other);

  // implements Message ----------------------------------------------

  inline LSTMWeightParams* New() const { return New(NULL); }

  LSTMWeightParams* New(::google::protobuf::Arena* arena) const;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from);
  void CopyFrom(const LSTMWeightParams& from);
  void MergeFrom(const LSTMWeightParams& from);
  void Clear();
  bool IsInitialized() const;

  size_t ByteSizeLong() const;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input);
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const;
  void DiscardUnknownFields();
  int GetCachedSize() const { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(LSTMWeightParams* other);
  void UnsafeMergeFrom(const LSTMWeightParams& from);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return _arena_ptr_;
  }
  inline ::google::protobuf::Arena* MaybeArenaPtr() const {
    return _arena_ptr_;
  }
  public:

  ::std::string GetTypeName() const;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // optional .CoreML.Specification.WeightParams inputGateWeightMatrix = 1;
  bool has_inputgateweightmatrix() const;
  void clear_inputgateweightmatrix();
  static const int kInputGateWeightMatrixFieldNumber = 1;
  const ::CoreML::Specification::WeightParams& inputgateweightmatrix() const;
  ::CoreML::Specification::WeightParams* mutable_inputgateweightmatrix();
  ::CoreML::Specification::WeightParams* release_inputgateweightmatrix();
  void set_allocated_inputgateweightmatrix(::CoreML::Specification::WeightParams* inputgateweightmatrix);

  // optional .CoreML.Specification.WeightParams forgetGateWeightMatrix = 2;
  bool has_forgetgateweightmatrix() const;
  void clear_forgetgateweightmatrix();
  static const int kForgetGateWeightMatrixFieldNumber = 2;
  const ::CoreML::Specification::WeightParams& forgetgateweightmatrix() const;
  ::CoreML::Specification::WeightParams* mutable_forgetgateweightmatrix();
  ::CoreML::Specification::WeightParams* release_forgetgateweightmatrix();
  void set_allocated_forgetgateweightmatrix(::CoreML::Specification::WeightParams* forgetgateweightmatrix);

  // optional .CoreML.Specification.WeightParams blockInputWeightMatrix = 3;
  bool has_blockinputweightmatrix() const;
  void clear_blockinputweightmatrix();
  static const int kBlockInputWeightMatrixFieldNumber = 3;
  const ::CoreML::Specification::WeightParams& blockinputweightmatrix() const;
  ::CoreML::Specification::WeightParams* mutable_blockinputweightmatrix();
  ::CoreML::Specification::WeightParams* release_blockinputweightmatrix();
  void set_allocated_blockinputweightmatrix(::CoreML::Specification::WeightParams* blockinputweightmatrix);

  // optional .CoreML.Specification.WeightParams outputGateWeightMatrix = 4;
  bool has_outputgateweightmatrix() const;
  void clear_outputgateweightmatrix();
  static const int kOutputGateWeightMatrixFieldNumber = 4;
  const ::CoreML::Specification::WeightParams& outputgateweightmatrix() const;
  ::CoreML::Specification::WeightParams* mutable_outputgateweightmatrix();
  ::CoreML::Specification::WeightParams* release_outputgateweightmatrix();
  void set_allocated_outputgateweightmatrix(::CoreML::Specification::WeightParams* outputgateweightmatrix);

  // optional .CoreML.Specification.WeightParams inputGateRecursionMatrix = 20;
  bool has_inputgaterecursionmatrix() const;
  void clear_inputgaterecursionmatrix();
  static const int kInputGateRecursionMatrixFieldNumber = 20;
  const ::CoreML::Specification::WeightParams& inputgaterecursionmatrix() const;
  ::CoreML::Specification::WeightParams* mutable_inputgaterecursionmatrix();
  ::CoreML::Specification::WeightParams* release_inputgaterecursionmatrix();
  void set_allocated_inputgaterecursionmatrix(::CoreML::Specification::WeightParams* inputgaterecursionmatrix);

  // optional .CoreML.Specification.WeightParams forgetGateRecursionMatrix = 21;
  bool has_forgetgaterecursionmatrix() const;
  void clear_forgetgaterecursionmatrix();
  static const int kForgetGateRecursionMatrixFieldNumber = 21;
  const ::CoreML::Specification::WeightParams& forgetgaterecursionmatrix() const;
  ::CoreML::Specification::WeightParams* mutable_forgetgaterecursionmatrix();
  ::CoreML::Specification::WeightParams* release_forgetgaterecursionmatrix();
  void set_allocated_forgetgaterecursionmatrix(::CoreML::Specification::WeightParams* forgetgaterecursionmatrix);

  // optional .CoreML.Specification.WeightParams blockInputRecursionMatrix = 22;
  bool has_blockinputrecursionmatrix() const;
  void clear_blockinputrecursionmatrix();
  static const int kBlockInputRecursionMatrixFieldNumber = 22;
  const ::CoreML::Specification::WeightParams& blockinputrecursionmatrix() const;
  ::CoreML::Specification::WeightParams* mutable_blockinputrecursionmatrix();
  ::CoreML::Specification::WeightParams* release_blockinputrecursionmatrix();
  void set_allocated_blockinputrecursionmatrix(::CoreML::Specification::WeightParams* blockinputrecursionmatrix);

  // optional .CoreML.Specification.WeightParams outputGateRecursionMatrix = 23;
  bool has_outputgaterecursionmatrix() const;
  void clear_outputgaterecursionmatrix();
  static const int kOutputGateRecursionMatrixFieldNumber = 23;
  const ::CoreML::Specification::WeightParams& outputgaterecursionmatrix() const;
  ::CoreML::Specification::WeightParams* mutable_outputgaterecursionmatrix();
  ::CoreML::Specification::WeightParams* release_outputgaterecursionmatrix();
  void set_allocated_outputgaterecursionmatrix(::CoreML::Specification::WeightParams* outputgaterecursionmatrix);

  // optional .CoreML.Specification.WeightParams inputGateBiasVector = 40;
  bool has_inputgatebiasvector() const;
  void clear_inputgatebiasvector();
  static const int kInputGateBiasVectorFieldNumber = 40;
  const ::CoreML::Specification::WeightParams& inputgatebiasvector() const;
  ::CoreML::Specification::WeightParams* mutable_inputgatebiasvector();
  ::CoreML::Specification::WeightParams* release_inputgatebiasvector();
  void set_allocated_inputgatebiasvector(::CoreML::Specification::WeightParams* inputgatebiasvector);

  // optional .CoreML.Specification.WeightParams forgetGateBiasVector = 41;
  bool has_forgetgatebiasvector() const;
  void clear_forgetgatebiasvector();
  static const int kForgetGateBiasVectorFieldNumber = 41;
  const ::CoreML::Specification::WeightParams& forgetgatebiasvector() const;
  ::CoreML::Specification::WeightParams* mutable_forgetgatebiasvector();
  ::CoreML::Specification::WeightParams* release_forgetgatebiasvector();
  void set_allocated_forgetgatebiasvector(::CoreML::Specification::WeightParams* forgetgatebiasvector);

  // optional .CoreML.Specification.WeightParams blockInputBiasVector = 42;
  bool has_blockinputbiasvector() const;
  void clear_blockinputbiasvector();
  static const int kBlockInputBiasVectorFieldNumber = 42;
  const ::CoreML::Specification::WeightParams& blockinputbiasvector() const;
  ::CoreML::Specification::WeightParams* mutable_blockinputbiasvector();
  ::CoreML::Specification::WeightParams* release_blockinputbiasvector();
  void set_allocated_blockinputbiasvector(::CoreML::Specification::WeightParams* blockinputbiasvector);

  // optional .CoreML.Specification.WeightParams outputGateBiasVector = 43;
  bool has_outputgatebiasvector() const;
  void clear_outputgatebiasvector();
  static const int kOutputGateBiasVectorFieldNumber = 43;
  const ::CoreML::Specification::WeightParams& outputgatebiasvector() const;
  ::CoreML::Specification::WeightParams* mutable_outputgatebiasvector();
  ::CoreML::Specification::WeightParams* release_outputgatebiasvector();
  void set_allocated_outputgatebiasvector(::CoreML::Specification::WeightParams* outputgatebiasvector);

  // optional .CoreML.Specification.WeightParams inputGatePeepholeVector = 60;
  bool has_inputgatepeepholevector() const;
  void clear_inputgatepeepholevector();
  static const int kInputGatePeepholeVectorFieldNumber = 60;
  const ::CoreML::Specification::WeightParams& inputgatepeepholevector() const;
  ::CoreML::Specification::WeightParams* mutable_inputgatepeepholevector();
  ::CoreML::Specification::WeightParams* release_inputgatepeepholevector();
  void set_allocated_inputgatepeepholevector(::CoreML::Specification::WeightParams* inputgatepeepholevector);

  // optional .CoreML.Specification.WeightParams forgetGatePeepholeVector = 61;
  bool has_forgetgatepeepholevector() const;
  void clear_forgetgatepeepholevector();
  static const int kForgetGatePeepholeVectorFieldNumber = 61;
  const ::CoreML::Specification::WeightParams& forgetgatepeepholevector() const;
  ::CoreML::Specification::WeightParams* mutable_forgetgatepeepholevector();
  ::CoreML::Specification::WeightParams* release_forgetgatepeepholevector();
  void set_allocated_forgetgatepeepholevector(::CoreML::Specification::WeightParams* forgetgatepeepholevector);

  // optional .CoreML.Specification.WeightParams outputGatePeepholeVector = 62;
  bool has_outputgatepeepholevector() const;
  void clear_outputgatepeepholevector();
  static const int kOutputGatePeepholeVectorFieldNumber = 62;
  const ::CoreML::Specification::WeightParams& outputgatepeepholevector() const;
  ::CoreML::Specification::WeightParams* mutable_outputgatepeepholevector();
  ::CoreML::Specification::WeightParams* release_outputgatepeepholevector();
  void set_allocated_outputgatepeepholevector(::CoreML::Specification::WeightParams* outputgatepeepholevector);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.LSTMWeightParams)
 private:

  ::google::protobuf::internal::ArenaStringPtr _unknown_fields_;
  ::google::protobuf::Arena* _arena_ptr_;

  ::CoreML::Specification::WeightParams* inputgateweightmatrix_;
  ::CoreML::Specification::WeightParams* forgetgateweightmatrix_;
  ::CoreML::Specification::WeightParams* blockinputweightmatrix_;
  ::CoreML::Specification::WeightParams* outputgateweightmatrix_;
  ::CoreML::Specification::WeightParams* inputgaterecursionmatrix_;
  ::CoreML::Specification::WeightParams* forgetgaterecursionmatrix_;
  ::CoreML::Specification::WeightParams* blockinputrecursionmatrix_;
  ::CoreML::Specification::WeightParams* outputgaterecursionmatrix_;
  ::CoreML::Specification::WeightParams* inputgatebiasvector_;
  ::CoreML::Specification::WeightParams* forgetgatebiasvector_;
  ::CoreML::Specification::WeightParams* blockinputbiasvector_;
  ::CoreML::Specification::WeightParams* outputgatebiasvector_;
  ::CoreML::Specification::WeightParams* inputgatepeepholevector_;
  ::CoreML::Specification::WeightParams* forgetgatepeepholevector_;
  ::CoreML::Specification::WeightParams* outputgatepeepholevector_;
  mutable int _cached_size_;
  friend void  protobuf_InitDefaults_NeuralNetwork_2eproto_impl();
  friend void  protobuf_AddDesc_NeuralNetwork_2eproto_impl();
  friend void protobuf_AssignDesc_NeuralNetwork_2eproto();
  friend void protobuf_ShutdownFile_NeuralNetwork_2eproto();

  void InitAsDefaultInstance();
};
extern ::google::protobuf::internal::ExplicitlyConstructed<LSTMWeightParams> LSTMWeightParams_default_instance_;

// -------------------------------------------------------------------

class UniDirectionalLSTMLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.UniDirectionalLSTMLayerParams) */ {
 public:
  UniDirectionalLSTMLayerParams();
  virtual ~UniDirectionalLSTMLayerParams();

  UniDirectionalLSTMLayerParams(const UniDirectionalLSTMLayerParams& from);

  inline UniDirectionalLSTMLayerParams& operator=(const UniDirectionalLSTMLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const UniDirectionalLSTMLayerParams& default_instance();

  static const UniDirectionalLSTMLayerParams* internal_default_instance();

  void Swap(UniDirectionalLSTMLayerParams* other);

  // implements Message ----------------------------------------------

  inline UniDirectionalLSTMLayerParams* New() const { return New(NULL); }

  UniDirectionalLSTMLayerParams* New(::google::protobuf::Arena* arena) const;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from);
  void CopyFrom(const UniDirectionalLSTMLayerParams& from);
  void MergeFrom(const UniDirectionalLSTMLayerParams& from);
  void Clear();
  bool IsInitialized() const;

  size_t ByteSizeLong() const;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input);
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const;
  void DiscardUnknownFields();
  int GetCachedSize() const { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(UniDirectionalLSTMLayerParams* other);
  void UnsafeMergeFrom(const UniDirectionalLSTMLayerParams& from);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return _arena_ptr_;
  }
  inline ::google::protobuf::Arena* MaybeArenaPtr() const {
    return _arena_ptr_;
  }
  public:

  ::std::string GetTypeName() const;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // optional uint64 inputVectorSize = 1;
  void clear_inputvectorsize();
  static const int kInputVectorSizeFieldNumber = 1;
  ::google::protobuf::uint64 inputvectorsize() const;
  void set_inputvectorsize(::google::protobuf::uint64 value);

  // optional uint64 outputVectorSize = 2;
  void clear_outputvectorsize();
  static const int kOutputVectorSizeFieldNumber = 2;
  ::google::protobuf::uint64 outputvectorsize() const;
  void set_outputvectorsize(::google::protobuf::uint64 value);

  // repeated .CoreML.Specification.ActivationParams activations = 10;
  int activations_size() const;
  void clear_activations();
  static const int kActivationsFieldNumber = 10;
  const ::CoreML::Specification::ActivationParams& activations(int index) const;
  ::CoreML::Specification::ActivationParams* mutable_activations(int index);
  ::CoreML::Specification::ActivationParams* add_activations();
  ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::ActivationParams >*
      mutable_activations();
  const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::ActivationParams >&
      activations() const;

  // optional .CoreML.Specification.LSTMParams params = 15;
  bool has_params() const;
  void clear_params();
  static const int kParamsFieldNumber = 15;
  const ::CoreML::Specification::LSTMParams& params() const;
  ::CoreML::Specification::LSTMParams* mutable_params();
  ::CoreML::Specification::LSTMParams* release_params();
  void set_allocated_params(::CoreML::Specification::LSTMParams* params);

  // optional .CoreML.Specification.LSTMWeightParams weightParams = 20;
  bool has_weightparams() const;
  void clear_weightparams();
  static const int kWeightParamsFieldNumber = 20;
  const ::CoreML::Specification::LSTMWeightParams& weightparams() const;
  ::CoreML::Specification::LSTMWeightParams* mutable_weightparams();
  ::CoreML::Specification::LSTMWeightParams* release_weightparams();
  void set_allocated_weightparams(::CoreML::Specification::LSTMWeightParams* weightparams);

  // optional bool reverseInput = 100;
  void clear_reverseinput();
  static const int kReverseInputFieldNumber = 100;
  bool reverseinput() const;
  void set_reverseinput(bool value);

  // @@protoc_insertion_point(class_scope:CoreML.Specification.UniDirectionalLSTMLayerParams)
 private:

  ::google::protobuf::internal::ArenaStringPtr _unknown_fields_;
  ::google::protobuf::Arena* _arena_ptr_;

  ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::ActivationParams > activations_;
  ::CoreML::Specification::LSTMParams* params_;
  ::CoreML::Specification::LSTMWeightParams* weightparams_;
  ::google::protobuf::uint64 inputvectorsize_;
  ::google::protobuf::uint64 outputvectorsize_;
  bool reverseinput_;
  mutable int _cached_size_;
  friend void  protobuf_InitDefaults_NeuralNetwork_2eproto_impl();
  friend void  protobuf_AddDesc_NeuralNetwork_2eproto_impl();
  friend void protobuf_AssignDesc_NeuralNetwork_2eproto();
  friend void protobuf_ShutdownFile_NeuralNetwork_2eproto();

  void InitAsDefaultInstance();
};
extern ::google::protobuf::internal::ExplicitlyConstructed<UniDirectionalLSTMLayerParams> UniDirectionalLSTMLayerParams_default_instance_;

// -------------------------------------------------------------------

class BiDirectionalLSTMLayerParams : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.BiDirectionalLSTMLayerParams) */ {
 public:
  BiDirectionalLSTMLayerParams();
  virtual ~BiDirectionalLSTMLayerParams();

  BiDirectionalLSTMLayerParams(const BiDirectionalLSTMLayerParams& from);

  inline BiDirectionalLSTMLayerParams& operator=(const BiDirectionalLSTMLayerParams& from) {
    CopyFrom(from);
    return *this;
  }

  static const BiDirectionalLSTMLayerParams& default_instance();

  static const BiDirectionalLSTMLayerParams* internal_default_instance();

  void Swap(BiDirectionalLSTMLayerParams* other);

  // implements Message ----------------------------------------------

  inline BiDirectionalLSTMLayerParams* New() const { return New(NULL); }

  BiDirectionalLSTMLayerParams* New(::google::protobuf::Arena* arena) const;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from);
  void CopyFrom(const BiDirectionalLSTMLayerParams& from);
  void MergeFrom(const BiDirectionalLSTMLayerParams& from);
  void Clear();
  bool IsInitialized() const;

  size_t ByteSizeLong() const;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input);
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const;
  void DiscardUnknownFields();
  int GetCachedSize() const { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(BiDirectionalLSTMLayerParams* other);
  void UnsafeMergeFrom(const BiDirectionalLSTMLayerParams& from);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return _arena_ptr_;
  }
  inline ::google::protobuf::Arena* MaybeArenaPtr() const {
    return _arena_ptr_;
  }
  public:

  ::std::string GetTypeName() const;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // optional uint64 inputVectorSize = 1;
  void clear_inputvectorsize();
  static const int kInputVectorSizeFieldNumber = 1;
  ::google::protobuf::uint64 inputvectorsize() const;
  void set_inputvectorsize(::google::protobuf::uint64 value);

  // optional uint64 outputVectorSize = 2;
  void clear_outputvectorsize();
  static const int kOutputVectorSizeFieldNumber = 2;
  ::google::protobuf::uint64 outputvectorsize() const;
  void set_outputvectorsize(::google::protobuf::uint64 value);

  // repeated .CoreML.Specification.ActivationParams activationsForwardLSTM = 10;
  int activationsforwardlstm_size() const;
  void clear_activationsforwardlstm();
  static const int kActivationsForwardLSTMFieldNumber = 10;
  const ::CoreML::Specification::ActivationParams& activationsforwardlstm(int index) const;
  ::CoreML::Specification::ActivationParams* mutable_activationsforwardlstm(int index);
  ::CoreML::Specification::ActivationParams* add_activationsforwardlstm();
  ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::ActivationParams >*
      mutable_activationsforwardlstm();
  const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::ActivationParams >&
      activationsforwardlstm() const;

  // repeated .CoreML.Specification.ActivationParams activationsBackwardLSTM = 11;
  int activationsbackwardlstm_size() const;
  void clear_activationsbackwardlstm();
  static const int kActivationsBackwardLSTMFieldNumber = 11;
  const ::CoreML::Specification::ActivationParams& activationsbackwardlstm(int index) const;
  ::CoreML::Specification::ActivationParams* mutable_activationsbackwardlstm(int index);
  ::CoreML::Specification::ActivationParams* add_activationsbackwardlstm();
  ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::ActivationParams >*
      mutable_activationsbackwardlstm();
  const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::ActivationParams >&
      activationsbackwardlstm() const;

  // optional .CoreML.Specification.LSTMParams params = 15;
  bool has_params() const;
  void clear_params();
  static const int kParamsFieldNumber = 15;
  const ::CoreML::Specification::LSTMParams& params() const;
  ::CoreML::Specification::LSTMParams* mutable_params();
  ::CoreML::Specification::LSTMParams* release_params();
  void set_allocated_params(::CoreML::Specification::LSTMParams* params);

  // repeated .CoreML.Specification.LSTMWeightParams weightParams = 20;
  int weightparams_size() const;
  void clear_weightparams();
  static const int kWeightParamsFieldNumber = 20;
  const ::CoreML::Specification::LSTMWeightParams& weightparams(int index) const;
  ::CoreML::Specification::LSTMWeightParams* mutable_weightparams(int index);
  ::CoreML::Specification::LSTMWeightParams* add_weightparams();
  ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::LSTMWeightParams >*
      mutable_weightparams();
  const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::LSTMWeightParams >&
      weightparams() const;

  // @@protoc_insertion_point(class_scope:CoreML.Specification.BiDirectionalLSTMLayerParams)
 private:

  ::google::protobuf::internal::ArenaStringPtr _unknown_fields_;
  ::google::protobuf::Arena* _arena_ptr_;

  ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::ActivationParams > activationsforwardlstm_;
  ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::ActivationParams > activationsbackwardlstm_;
  ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::LSTMWeightParams > weightparams_;
  ::CoreML::Specification::LSTMParams* params_;
  ::google::protobuf::uint64 inputvectorsize_;
  ::google::protobuf::uint64 outputvectorsize_;
  mutable int _cached_size_;
  friend void  protobuf_InitDefaults_NeuralNetwork_2eproto_impl();
  friend void  protobuf_AddDesc_NeuralNetwork_2eproto_impl();
  friend void protobuf_AssignDesc_NeuralNetwork_2eproto();
  friend void protobuf_ShutdownFile_NeuralNetwork_2eproto();

  void InitAsDefaultInstance();
};
extern ::google::protobuf::internal::ExplicitlyConstructed<BiDirectionalLSTMLayerParams> BiDirectionalLSTMLayerParams_default_instance_;

// -------------------------------------------------------------------

class NeuralNetworkClassifier : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.NeuralNetworkClassifier) */ {
 public:
  NeuralNetworkClassifier();
  virtual ~NeuralNetworkClassifier();

  NeuralNetworkClassifier(const NeuralNetworkClassifier& from);

  inline NeuralNetworkClassifier& operator=(const NeuralNetworkClassifier& from) {
    CopyFrom(from);
    return *this;
  }

  static const NeuralNetworkClassifier& default_instance();

  enum ClassLabelsCase {
    kStringClassLabels = 100,
    kInt64ClassLabels = 101,
    CLASSLABELS_NOT_SET = 0,
  };

  static const NeuralNetworkClassifier* internal_default_instance();

  void Swap(NeuralNetworkClassifier* other);

  // implements Message ----------------------------------------------

  inline NeuralNetworkClassifier* New() const { return New(NULL); }

  NeuralNetworkClassifier* New(::google::protobuf::Arena* arena) const;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from);
  void CopyFrom(const NeuralNetworkClassifier& from);
  void MergeFrom(const NeuralNetworkClassifier& from);
  void Clear();
  bool IsInitialized() const;

  size_t ByteSizeLong() const;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input);
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const;
  void DiscardUnknownFields();
  int GetCachedSize() const { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(NeuralNetworkClassifier* other);
  void UnsafeMergeFrom(const NeuralNetworkClassifier& from);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return _arena_ptr_;
  }
  inline ::google::protobuf::Arena* MaybeArenaPtr() const {
    return _arena_ptr_;
  }
  public:

  ::std::string GetTypeName() const;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated .CoreML.Specification.NeuralNetworkLayer layers = 1;
  int layers_size() const;
  void clear_layers();
  static const int kLayersFieldNumber = 1;
  const ::CoreML::Specification::NeuralNetworkLayer& layers(int index) const;
  ::CoreML::Specification::NeuralNetworkLayer* mutable_layers(int index);
  ::CoreML::Specification::NeuralNetworkLayer* add_layers();
  ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkLayer >*
      mutable_layers();
  const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkLayer >&
      layers() const;

  // repeated .CoreML.Specification.NeuralNetworkPreprocessing preprocessing = 2;
  int preprocessing_size() const;
  void clear_preprocessing();
  static const int kPreprocessingFieldNumber = 2;
  const ::CoreML::Specification::NeuralNetworkPreprocessing& preprocessing(int index) const;
  ::CoreML::Specification::NeuralNetworkPreprocessing* mutable_preprocessing(int index);
  ::CoreML::Specification::NeuralNetworkPreprocessing* add_preprocessing();
  ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkPreprocessing >*
      mutable_preprocessing();
  const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkPreprocessing >&
      preprocessing() const;

  // optional .CoreML.Specification.StringVector stringClassLabels = 100;
  bool has_stringclasslabels() const;
  void clear_stringclasslabels();
  static const int kStringClassLabelsFieldNumber = 100;
  const ::CoreML::Specification::StringVector& stringclasslabels() const;
  ::CoreML::Specification::StringVector* mutable_stringclasslabels();
  ::CoreML::Specification::StringVector* release_stringclasslabels();
  void set_allocated_stringclasslabels(::CoreML::Specification::StringVector* stringclasslabels);

  // optional .CoreML.Specification.Int64Vector int64ClassLabels = 101;
  bool has_int64classlabels() const;
  void clear_int64classlabels();
  static const int kInt64ClassLabelsFieldNumber = 101;
  const ::CoreML::Specification::Int64Vector& int64classlabels() const;
  ::CoreML::Specification::Int64Vector* mutable_int64classlabels();
  ::CoreML::Specification::Int64Vector* release_int64classlabels();
  void set_allocated_int64classlabels(::CoreML::Specification::Int64Vector* int64classlabels);

  // optional string labelProbabilityLayerName = 200;
  void clear_labelprobabilitylayername();
  static const int kLabelProbabilityLayerNameFieldNumber = 200;
  const ::std::string& labelprobabilitylayername() const;
  void set_labelprobabilitylayername(const ::std::string& value);
  void set_labelprobabilitylayername(const char* value);
  void set_labelprobabilitylayername(const char* value, size_t size);
  ::std::string* mutable_labelprobabilitylayername();
  ::std::string* release_labelprobabilitylayername();
  void set_allocated_labelprobabilitylayername(::std::string* labelprobabilitylayername);

  ClassLabelsCase ClassLabels_case() const;
  // @@protoc_insertion_point(class_scope:CoreML.Specification.NeuralNetworkClassifier)
 private:
  inline void set_has_stringclasslabels();
  inline void set_has_int64classlabels();

  inline bool has_ClassLabels() const;
  void clear_ClassLabels();
  inline void clear_has_ClassLabels();

  ::google::protobuf::internal::ArenaStringPtr _unknown_fields_;
  ::google::protobuf::Arena* _arena_ptr_;

  ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkLayer > layers_;
  ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkPreprocessing > preprocessing_;
  ::google::protobuf::internal::ArenaStringPtr labelprobabilitylayername_;
  union ClassLabelsUnion {
    ClassLabelsUnion() {}
    ::CoreML::Specification::StringVector* stringclasslabels_;
    ::CoreML::Specification::Int64Vector* int64classlabels_;
  } ClassLabels_;
  mutable int _cached_size_;
  ::google::protobuf::uint32 _oneof_case_[1];

  friend void  protobuf_InitDefaults_NeuralNetwork_2eproto_impl();
  friend void  protobuf_AddDesc_NeuralNetwork_2eproto_impl();
  friend void protobuf_AssignDesc_NeuralNetwork_2eproto();
  friend void protobuf_ShutdownFile_NeuralNetwork_2eproto();

  void InitAsDefaultInstance();
};
extern ::google::protobuf::internal::ExplicitlyConstructed<NeuralNetworkClassifier> NeuralNetworkClassifier_default_instance_;

// -------------------------------------------------------------------

class NeuralNetworkRegressor : public ::google::protobuf::MessageLite /* @@protoc_insertion_point(class_definition:CoreML.Specification.NeuralNetworkRegressor) */ {
 public:
  NeuralNetworkRegressor();
  virtual ~NeuralNetworkRegressor();

  NeuralNetworkRegressor(const NeuralNetworkRegressor& from);

  inline NeuralNetworkRegressor& operator=(const NeuralNetworkRegressor& from) {
    CopyFrom(from);
    return *this;
  }

  static const NeuralNetworkRegressor& default_instance();

  static const NeuralNetworkRegressor* internal_default_instance();

  void Swap(NeuralNetworkRegressor* other);

  // implements Message ----------------------------------------------

  inline NeuralNetworkRegressor* New() const { return New(NULL); }

  NeuralNetworkRegressor* New(::google::protobuf::Arena* arena) const;
  void CheckTypeAndMergeFrom(const ::google::protobuf::MessageLite& from);
  void CopyFrom(const NeuralNetworkRegressor& from);
  void MergeFrom(const NeuralNetworkRegressor& from);
  void Clear();
  bool IsInitialized() const;

  size_t ByteSizeLong() const;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input);
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const;
  void DiscardUnknownFields();
  int GetCachedSize() const { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(NeuralNetworkRegressor* other);
  void UnsafeMergeFrom(const NeuralNetworkRegressor& from);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return _arena_ptr_;
  }
  inline ::google::protobuf::Arena* MaybeArenaPtr() const {
    return _arena_ptr_;
  }
  public:

  ::std::string GetTypeName() const;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated .CoreML.Specification.NeuralNetworkLayer layers = 1;
  int layers_size() const;
  void clear_layers();
  static const int kLayersFieldNumber = 1;
  const ::CoreML::Specification::NeuralNetworkLayer& layers(int index) const;
  ::CoreML::Specification::NeuralNetworkLayer* mutable_layers(int index);
  ::CoreML::Specification::NeuralNetworkLayer* add_layers();
  ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkLayer >*
      mutable_layers();
  const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkLayer >&
      layers() const;

  // repeated .CoreML.Specification.NeuralNetworkPreprocessing preprocessing = 2;
  int preprocessing_size() const;
  void clear_preprocessing();
  static const int kPreprocessingFieldNumber = 2;
  const ::CoreML::Specification::NeuralNetworkPreprocessing& preprocessing(int index) const;
  ::CoreML::Specification::NeuralNetworkPreprocessing* mutable_preprocessing(int index);
  ::CoreML::Specification::NeuralNetworkPreprocessing* add_preprocessing();
  ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkPreprocessing >*
      mutable_preprocessing();
  const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkPreprocessing >&
      preprocessing() const;

  // @@protoc_insertion_point(class_scope:CoreML.Specification.NeuralNetworkRegressor)
 private:

  ::google::protobuf::internal::ArenaStringPtr _unknown_fields_;
  ::google::protobuf::Arena* _arena_ptr_;

  ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkLayer > layers_;
  ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkPreprocessing > preprocessing_;
  mutable int _cached_size_;
  friend void  protobuf_InitDefaults_NeuralNetwork_2eproto_impl();
  friend void  protobuf_AddDesc_NeuralNetwork_2eproto_impl();
  friend void protobuf_AssignDesc_NeuralNetwork_2eproto();
  friend void protobuf_ShutdownFile_NeuralNetwork_2eproto();

  void InitAsDefaultInstance();
};
extern ::google::protobuf::internal::ExplicitlyConstructed<NeuralNetworkRegressor> NeuralNetworkRegressor_default_instance_;

// ===================================================================


// ===================================================================

#if !PROTOBUF_INLINE_NOT_IN_HEADERS
// NeuralNetwork

// repeated .CoreML.Specification.NeuralNetworkLayer layers = 1;
inline int NeuralNetwork::layers_size() const {
  return layers_.size();
}
inline void NeuralNetwork::clear_layers() {
  layers_.Clear();
}
inline const ::CoreML::Specification::NeuralNetworkLayer& NeuralNetwork::layers(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetwork.layers)
  return layers_.Get(index);
}
inline ::CoreML::Specification::NeuralNetworkLayer* NeuralNetwork::mutable_layers(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetwork.layers)
  return layers_.Mutable(index);
}
inline ::CoreML::Specification::NeuralNetworkLayer* NeuralNetwork::add_layers() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.NeuralNetwork.layers)
  return layers_.Add();
}
inline ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkLayer >*
NeuralNetwork::mutable_layers() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.NeuralNetwork.layers)
  return &layers_;
}
inline const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkLayer >&
NeuralNetwork::layers() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.NeuralNetwork.layers)
  return layers_;
}

// repeated .CoreML.Specification.NeuralNetworkPreprocessing preprocessing = 2;
inline int NeuralNetwork::preprocessing_size() const {
  return preprocessing_.size();
}
inline void NeuralNetwork::clear_preprocessing() {
  preprocessing_.Clear();
}
inline const ::CoreML::Specification::NeuralNetworkPreprocessing& NeuralNetwork::preprocessing(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetwork.preprocessing)
  return preprocessing_.Get(index);
}
inline ::CoreML::Specification::NeuralNetworkPreprocessing* NeuralNetwork::mutable_preprocessing(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetwork.preprocessing)
  return preprocessing_.Mutable(index);
}
inline ::CoreML::Specification::NeuralNetworkPreprocessing* NeuralNetwork::add_preprocessing() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.NeuralNetwork.preprocessing)
  return preprocessing_.Add();
}
inline ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkPreprocessing >*
NeuralNetwork::mutable_preprocessing() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.NeuralNetwork.preprocessing)
  return &preprocessing_;
}
inline const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkPreprocessing >&
NeuralNetwork::preprocessing() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.NeuralNetwork.preprocessing)
  return preprocessing_;
}

inline const NeuralNetwork* NeuralNetwork::internal_default_instance() {
  return &NeuralNetwork_default_instance_.get();
}
// -------------------------------------------------------------------

// NeuralNetworkImageScaler

// optional float channelScale = 10;
inline void NeuralNetworkImageScaler::clear_channelscale() {
  channelscale_ = 0;
}
inline float NeuralNetworkImageScaler::channelscale() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkImageScaler.channelScale)
  return channelscale_;
}
inline void NeuralNetworkImageScaler::set_channelscale(float value) {

  channelscale_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkImageScaler.channelScale)
}

// optional float blueBias = 20;
inline void NeuralNetworkImageScaler::clear_bluebias() {
  bluebias_ = 0;
}
inline float NeuralNetworkImageScaler::bluebias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkImageScaler.blueBias)
  return bluebias_;
}
inline void NeuralNetworkImageScaler::set_bluebias(float value) {

  bluebias_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkImageScaler.blueBias)
}

// optional float greenBias = 21;
inline void NeuralNetworkImageScaler::clear_greenbias() {
  greenbias_ = 0;
}
inline float NeuralNetworkImageScaler::greenbias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkImageScaler.greenBias)
  return greenbias_;
}
inline void NeuralNetworkImageScaler::set_greenbias(float value) {

  greenbias_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkImageScaler.greenBias)
}

// optional float redBias = 22;
inline void NeuralNetworkImageScaler::clear_redbias() {
  redbias_ = 0;
}
inline float NeuralNetworkImageScaler::redbias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkImageScaler.redBias)
  return redbias_;
}
inline void NeuralNetworkImageScaler::set_redbias(float value) {

  redbias_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkImageScaler.redBias)
}

// optional float grayBias = 30;
inline void NeuralNetworkImageScaler::clear_graybias() {
  graybias_ = 0;
}
inline float NeuralNetworkImageScaler::graybias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkImageScaler.grayBias)
  return graybias_;
}
inline void NeuralNetworkImageScaler::set_graybias(float value) {

  graybias_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkImageScaler.grayBias)
}

inline const NeuralNetworkImageScaler* NeuralNetworkImageScaler::internal_default_instance() {
  return &NeuralNetworkImageScaler_default_instance_.get();
}
// -------------------------------------------------------------------

// NeuralNetworkMeanImage

// repeated float meanImage = 1;
inline int NeuralNetworkMeanImage::meanimage_size() const {
  return meanimage_.size();
}
inline void NeuralNetworkMeanImage::clear_meanimage() {
  meanimage_.Clear();
}
inline float NeuralNetworkMeanImage::meanimage(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkMeanImage.meanImage)
  return meanimage_.Get(index);
}
inline void NeuralNetworkMeanImage::set_meanimage(int index, float value) {
  meanimage_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkMeanImage.meanImage)
}
inline void NeuralNetworkMeanImage::add_meanimage(float value) {
  meanimage_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.NeuralNetworkMeanImage.meanImage)
}
inline const ::google::protobuf::RepeatedField< float >&
NeuralNetworkMeanImage::meanimage() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.NeuralNetworkMeanImage.meanImage)
  return meanimage_;
}
inline ::google::protobuf::RepeatedField< float >*
NeuralNetworkMeanImage::mutable_meanimage() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.NeuralNetworkMeanImage.meanImage)
  return &meanimage_;
}

inline const NeuralNetworkMeanImage* NeuralNetworkMeanImage::internal_default_instance() {
  return &NeuralNetworkMeanImage_default_instance_.get();
}
// -------------------------------------------------------------------

// NeuralNetworkPreprocessing

// optional string featureName = 1;
inline void NeuralNetworkPreprocessing::clear_featurename() {
  featurename_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline const ::std::string& NeuralNetworkPreprocessing::featurename() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkPreprocessing.featureName)
  return featurename_.GetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline void NeuralNetworkPreprocessing::set_featurename(const ::std::string& value) {

  featurename_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkPreprocessing.featureName)
}
inline void NeuralNetworkPreprocessing::set_featurename(const char* value) {

  featurename_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:CoreML.Specification.NeuralNetworkPreprocessing.featureName)
}
inline void NeuralNetworkPreprocessing::set_featurename(const char* value, size_t size) {

  featurename_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:CoreML.Specification.NeuralNetworkPreprocessing.featureName)
}
inline ::std::string* NeuralNetworkPreprocessing::mutable_featurename() {

  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkPreprocessing.featureName)
  return featurename_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline ::std::string* NeuralNetworkPreprocessing::release_featurename() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkPreprocessing.featureName)

  return featurename_.ReleaseNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline void NeuralNetworkPreprocessing::set_allocated_featurename(::std::string* featurename) {
  if (featurename != NULL) {

  } else {

  }
  featurename_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), featurename);
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkPreprocessing.featureName)
}

// optional .CoreML.Specification.NeuralNetworkImageScaler scaler = 10;
inline bool NeuralNetworkPreprocessing::has_scaler() const {
  return preprocessor_case() == kScaler;
}
inline void NeuralNetworkPreprocessing::set_has_scaler() {
  _oneof_case_[0] = kScaler;
}
inline void NeuralNetworkPreprocessing::clear_scaler() {
  if (has_scaler()) {
    delete preprocessor_.scaler_;
    clear_has_preprocessor();
  }
}
inline  const ::CoreML::Specification::NeuralNetworkImageScaler& NeuralNetworkPreprocessing::scaler() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkPreprocessing.scaler)
  return has_scaler()
      ? *preprocessor_.scaler_
      : ::CoreML::Specification::NeuralNetworkImageScaler::default_instance();
}
inline ::CoreML::Specification::NeuralNetworkImageScaler* NeuralNetworkPreprocessing::mutable_scaler() {
  if (!has_scaler()) {
    clear_preprocessor();
    set_has_scaler();
    preprocessor_.scaler_ = new ::CoreML::Specification::NeuralNetworkImageScaler;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkPreprocessing.scaler)
  return preprocessor_.scaler_;
}
inline ::CoreML::Specification::NeuralNetworkImageScaler* NeuralNetworkPreprocessing::release_scaler() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkPreprocessing.scaler)
  if (has_scaler()) {
    clear_has_preprocessor();
    ::CoreML::Specification::NeuralNetworkImageScaler* temp = preprocessor_.scaler_;
    preprocessor_.scaler_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkPreprocessing::set_allocated_scaler(::CoreML::Specification::NeuralNetworkImageScaler* scaler) {
  clear_preprocessor();
  if (scaler) {
    set_has_scaler();
    preprocessor_.scaler_ = scaler;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkPreprocessing.scaler)
}

// optional .CoreML.Specification.NeuralNetworkMeanImage meanImage = 11;
inline bool NeuralNetworkPreprocessing::has_meanimage() const {
  return preprocessor_case() == kMeanImage;
}
inline void NeuralNetworkPreprocessing::set_has_meanimage() {
  _oneof_case_[0] = kMeanImage;
}
inline void NeuralNetworkPreprocessing::clear_meanimage() {
  if (has_meanimage()) {
    delete preprocessor_.meanimage_;
    clear_has_preprocessor();
  }
}
inline  const ::CoreML::Specification::NeuralNetworkMeanImage& NeuralNetworkPreprocessing::meanimage() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkPreprocessing.meanImage)
  return has_meanimage()
      ? *preprocessor_.meanimage_
      : ::CoreML::Specification::NeuralNetworkMeanImage::default_instance();
}
inline ::CoreML::Specification::NeuralNetworkMeanImage* NeuralNetworkPreprocessing::mutable_meanimage() {
  if (!has_meanimage()) {
    clear_preprocessor();
    set_has_meanimage();
    preprocessor_.meanimage_ = new ::CoreML::Specification::NeuralNetworkMeanImage;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkPreprocessing.meanImage)
  return preprocessor_.meanimage_;
}
inline ::CoreML::Specification::NeuralNetworkMeanImage* NeuralNetworkPreprocessing::release_meanimage() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkPreprocessing.meanImage)
  if (has_meanimage()) {
    clear_has_preprocessor();
    ::CoreML::Specification::NeuralNetworkMeanImage* temp = preprocessor_.meanimage_;
    preprocessor_.meanimage_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkPreprocessing::set_allocated_meanimage(::CoreML::Specification::NeuralNetworkMeanImage* meanimage) {
  clear_preprocessor();
  if (meanimage) {
    set_has_meanimage();
    preprocessor_.meanimage_ = meanimage;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkPreprocessing.meanImage)
}

inline bool NeuralNetworkPreprocessing::has_preprocessor() const {
  return preprocessor_case() != PREPROCESSOR_NOT_SET;
}
inline void NeuralNetworkPreprocessing::clear_has_preprocessor() {
  _oneof_case_[0] = PREPROCESSOR_NOT_SET;
}
inline NeuralNetworkPreprocessing::PreprocessorCase NeuralNetworkPreprocessing::preprocessor_case() const {
  return NeuralNetworkPreprocessing::PreprocessorCase(_oneof_case_[0]);
}
inline const NeuralNetworkPreprocessing* NeuralNetworkPreprocessing::internal_default_instance() {
  return &NeuralNetworkPreprocessing_default_instance_.get();
}
// -------------------------------------------------------------------

// ActivationReLU

inline const ActivationReLU* ActivationReLU::internal_default_instance() {
  return &ActivationReLU_default_instance_.get();
}
// -------------------------------------------------------------------

// ActivationLeakyReLU

// optional float alpha = 1;
inline void ActivationLeakyReLU::clear_alpha() {
  alpha_ = 0;
}
inline float ActivationLeakyReLU::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationLeakyReLU.alpha)
  return alpha_;
}
inline void ActivationLeakyReLU::set_alpha(float value) {

  alpha_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ActivationLeakyReLU.alpha)
}

inline const ActivationLeakyReLU* ActivationLeakyReLU::internal_default_instance() {
  return &ActivationLeakyReLU_default_instance_.get();
}
// -------------------------------------------------------------------

// ActivationTanh

inline const ActivationTanh* ActivationTanh::internal_default_instance() {
  return &ActivationTanh_default_instance_.get();
}
// -------------------------------------------------------------------

// ActivationScaledTanh

// optional float alpha = 1;
inline void ActivationScaledTanh::clear_alpha() {
  alpha_ = 0;
}
inline float ActivationScaledTanh::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationScaledTanh.alpha)
  return alpha_;
}
inline void ActivationScaledTanh::set_alpha(float value) {

  alpha_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ActivationScaledTanh.alpha)
}

// optional float beta = 2;
inline void ActivationScaledTanh::clear_beta() {
  beta_ = 0;
}
inline float ActivationScaledTanh::beta() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationScaledTanh.beta)
  return beta_;
}
inline void ActivationScaledTanh::set_beta(float value) {

  beta_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ActivationScaledTanh.beta)
}

inline const ActivationScaledTanh* ActivationScaledTanh::internal_default_instance() {
  return &ActivationScaledTanh_default_instance_.get();
}
// -------------------------------------------------------------------

// ActivationSigmoid

inline const ActivationSigmoid* ActivationSigmoid::internal_default_instance() {
  return &ActivationSigmoid_default_instance_.get();
}
// -------------------------------------------------------------------

// ActivationLinear

// optional float alpha = 1;
inline void ActivationLinear::clear_alpha() {
  alpha_ = 0;
}
inline float ActivationLinear::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationLinear.alpha)
  return alpha_;
}
inline void ActivationLinear::set_alpha(float value) {

  alpha_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ActivationLinear.alpha)
}

// optional float beta = 2;
inline void ActivationLinear::clear_beta() {
  beta_ = 0;
}
inline float ActivationLinear::beta() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationLinear.beta)
  return beta_;
}
inline void ActivationLinear::set_beta(float value) {

  beta_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ActivationLinear.beta)
}

inline const ActivationLinear* ActivationLinear::internal_default_instance() {
  return &ActivationLinear_default_instance_.get();
}
// -------------------------------------------------------------------

// ActivationSigmoidHard

// optional float alpha = 1;
inline void ActivationSigmoidHard::clear_alpha() {
  alpha_ = 0;
}
inline float ActivationSigmoidHard::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationSigmoidHard.alpha)
  return alpha_;
}
inline void ActivationSigmoidHard::set_alpha(float value) {

  alpha_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ActivationSigmoidHard.alpha)
}

// optional float beta = 2;
inline void ActivationSigmoidHard::clear_beta() {
  beta_ = 0;
}
inline float ActivationSigmoidHard::beta() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationSigmoidHard.beta)
  return beta_;
}
inline void ActivationSigmoidHard::set_beta(float value) {

  beta_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ActivationSigmoidHard.beta)
}

inline const ActivationSigmoidHard* ActivationSigmoidHard::internal_default_instance() {
  return &ActivationSigmoidHard_default_instance_.get();
}
// -------------------------------------------------------------------

// ActivationPReLU

// optional .CoreML.Specification.WeightParams alpha = 1;
inline bool ActivationPReLU::has_alpha() const {
  return this != internal_default_instance() && alpha_ != NULL;
}
inline void ActivationPReLU::clear_alpha() {
  if (GetArenaNoVirtual() == NULL && alpha_ != NULL) delete alpha_;
  alpha_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& ActivationPReLU::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationPReLU.alpha)
  return alpha_ != NULL ? *alpha_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* ActivationPReLU::mutable_alpha() {

  if (alpha_ == NULL) {
    alpha_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationPReLU.alpha)
  return alpha_;
}
inline ::CoreML::Specification::WeightParams* ActivationPReLU::release_alpha() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationPReLU.alpha)

  ::CoreML::Specification::WeightParams* temp = alpha_;
  alpha_ = NULL;
  return temp;
}
inline void ActivationPReLU::set_allocated_alpha(::CoreML::Specification::WeightParams* alpha) {
  delete alpha_;
  alpha_ = alpha;
  if (alpha) {

  } else {

  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationPReLU.alpha)
}

inline const ActivationPReLU* ActivationPReLU::internal_default_instance() {
  return &ActivationPReLU_default_instance_.get();
}
// -------------------------------------------------------------------

// ActivationELU

// optional float alpha = 1;
inline void ActivationELU::clear_alpha() {
  alpha_ = 0;
}
inline float ActivationELU::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationELU.alpha)
  return alpha_;
}
inline void ActivationELU::set_alpha(float value) {

  alpha_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ActivationELU.alpha)
}

inline const ActivationELU* ActivationELU::internal_default_instance() {
  return &ActivationELU_default_instance_.get();
}
// -------------------------------------------------------------------

// ActivationThresholdedReLU

// optional float alpha = 1;
inline void ActivationThresholdedReLU::clear_alpha() {
  alpha_ = 0;
}
inline float ActivationThresholdedReLU::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationThresholdedReLU.alpha)
  return alpha_;
}
inline void ActivationThresholdedReLU::set_alpha(float value) {

  alpha_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ActivationThresholdedReLU.alpha)
}

inline const ActivationThresholdedReLU* ActivationThresholdedReLU::internal_default_instance() {
  return &ActivationThresholdedReLU_default_instance_.get();
}
// -------------------------------------------------------------------

// ActivationSoftsign

inline const ActivationSoftsign* ActivationSoftsign::internal_default_instance() {
  return &ActivationSoftsign_default_instance_.get();
}
// -------------------------------------------------------------------

// ActivationSoftplus

inline const ActivationSoftplus* ActivationSoftplus::internal_default_instance() {
  return &ActivationSoftplus_default_instance_.get();
}
// -------------------------------------------------------------------

// ActivationParametricSoftplus

// optional .CoreML.Specification.WeightParams alpha = 1;
inline bool ActivationParametricSoftplus::has_alpha() const {
  return this != internal_default_instance() && alpha_ != NULL;
}
inline void ActivationParametricSoftplus::clear_alpha() {
  if (GetArenaNoVirtual() == NULL && alpha_ != NULL) delete alpha_;
  alpha_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& ActivationParametricSoftplus::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParametricSoftplus.alpha)
  return alpha_ != NULL ? *alpha_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* ActivationParametricSoftplus::mutable_alpha() {

  if (alpha_ == NULL) {
    alpha_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParametricSoftplus.alpha)
  return alpha_;
}
inline ::CoreML::Specification::WeightParams* ActivationParametricSoftplus::release_alpha() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParametricSoftplus.alpha)

  ::CoreML::Specification::WeightParams* temp = alpha_;
  alpha_ = NULL;
  return temp;
}
inline void ActivationParametricSoftplus::set_allocated_alpha(::CoreML::Specification::WeightParams* alpha) {
  delete alpha_;
  alpha_ = alpha;
  if (alpha) {

  } else {

  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParametricSoftplus.alpha)
}

// optional .CoreML.Specification.WeightParams beta = 2;
inline bool ActivationParametricSoftplus::has_beta() const {
  return this != internal_default_instance() && beta_ != NULL;
}
inline void ActivationParametricSoftplus::clear_beta() {
  if (GetArenaNoVirtual() == NULL && beta_ != NULL) delete beta_;
  beta_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& ActivationParametricSoftplus::beta() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParametricSoftplus.beta)
  return beta_ != NULL ? *beta_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* ActivationParametricSoftplus::mutable_beta() {

  if (beta_ == NULL) {
    beta_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParametricSoftplus.beta)
  return beta_;
}
inline ::CoreML::Specification::WeightParams* ActivationParametricSoftplus::release_beta() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParametricSoftplus.beta)

  ::CoreML::Specification::WeightParams* temp = beta_;
  beta_ = NULL;
  return temp;
}
inline void ActivationParametricSoftplus::set_allocated_beta(::CoreML::Specification::WeightParams* beta) {
  delete beta_;
  beta_ = beta;
  if (beta) {

  } else {

  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParametricSoftplus.beta)
}

inline const ActivationParametricSoftplus* ActivationParametricSoftplus::internal_default_instance() {
  return &ActivationParametricSoftplus_default_instance_.get();
}
// -------------------------------------------------------------------

// ActivationParams

// optional .CoreML.Specification.ActivationLinear linear = 5;
inline bool ActivationParams::has_linear() const {
  return NonlinearityType_case() == kLinear;
}
inline void ActivationParams::set_has_linear() {
  _oneof_case_[0] = kLinear;
}
inline void ActivationParams::clear_linear() {
  if (has_linear()) {
    delete NonlinearityType_.linear_;
    clear_has_NonlinearityType();
  }
}
inline  const ::CoreML::Specification::ActivationLinear& ActivationParams::linear() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.linear)
  return has_linear()
      ? *NonlinearityType_.linear_
      : ::CoreML::Specification::ActivationLinear::default_instance();
}
inline ::CoreML::Specification::ActivationLinear* ActivationParams::mutable_linear() {
  if (!has_linear()) {
    clear_NonlinearityType();
    set_has_linear();
    NonlinearityType_.linear_ = new ::CoreML::Specification::ActivationLinear;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.linear)
  return NonlinearityType_.linear_;
}
inline ::CoreML::Specification::ActivationLinear* ActivationParams::release_linear() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.linear)
  if (has_linear()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationLinear* temp = NonlinearityType_.linear_;
    NonlinearityType_.linear_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void ActivationParams::set_allocated_linear(::CoreML::Specification::ActivationLinear* linear) {
  clear_NonlinearityType();
  if (linear) {
    set_has_linear();
    NonlinearityType_.linear_ = linear;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParams.linear)
}

// optional .CoreML.Specification.ActivationReLU ReLU = 10;
inline bool ActivationParams::has_relu() const {
  return NonlinearityType_case() == kReLU;
}
inline void ActivationParams::set_has_relu() {
  _oneof_case_[0] = kReLU;
}
inline void ActivationParams::clear_relu() {
  if (has_relu()) {
    delete NonlinearityType_.relu_;
    clear_has_NonlinearityType();
  }
}
inline  const ::CoreML::Specification::ActivationReLU& ActivationParams::relu() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.ReLU)
  return has_relu()
      ? *NonlinearityType_.relu_
      : ::CoreML::Specification::ActivationReLU::default_instance();
}
inline ::CoreML::Specification::ActivationReLU* ActivationParams::mutable_relu() {
  if (!has_relu()) {
    clear_NonlinearityType();
    set_has_relu();
    NonlinearityType_.relu_ = new ::CoreML::Specification::ActivationReLU;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.ReLU)
  return NonlinearityType_.relu_;
}
inline ::CoreML::Specification::ActivationReLU* ActivationParams::release_relu() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.ReLU)
  if (has_relu()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationReLU* temp = NonlinearityType_.relu_;
    NonlinearityType_.relu_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void ActivationParams::set_allocated_relu(::CoreML::Specification::ActivationReLU* relu) {
  clear_NonlinearityType();
  if (relu) {
    set_has_relu();
    NonlinearityType_.relu_ = relu;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParams.ReLU)
}

// optional .CoreML.Specification.ActivationLeakyReLU leakyReLU = 15;
inline bool ActivationParams::has_leakyrelu() const {
  return NonlinearityType_case() == kLeakyReLU;
}
inline void ActivationParams::set_has_leakyrelu() {
  _oneof_case_[0] = kLeakyReLU;
}
inline void ActivationParams::clear_leakyrelu() {
  if (has_leakyrelu()) {
    delete NonlinearityType_.leakyrelu_;
    clear_has_NonlinearityType();
  }
}
inline  const ::CoreML::Specification::ActivationLeakyReLU& ActivationParams::leakyrelu() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.leakyReLU)
  return has_leakyrelu()
      ? *NonlinearityType_.leakyrelu_
      : ::CoreML::Specification::ActivationLeakyReLU::default_instance();
}
inline ::CoreML::Specification::ActivationLeakyReLU* ActivationParams::mutable_leakyrelu() {
  if (!has_leakyrelu()) {
    clear_NonlinearityType();
    set_has_leakyrelu();
    NonlinearityType_.leakyrelu_ = new ::CoreML::Specification::ActivationLeakyReLU;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.leakyReLU)
  return NonlinearityType_.leakyrelu_;
}
inline ::CoreML::Specification::ActivationLeakyReLU* ActivationParams::release_leakyrelu() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.leakyReLU)
  if (has_leakyrelu()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationLeakyReLU* temp = NonlinearityType_.leakyrelu_;
    NonlinearityType_.leakyrelu_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void ActivationParams::set_allocated_leakyrelu(::CoreML::Specification::ActivationLeakyReLU* leakyrelu) {
  clear_NonlinearityType();
  if (leakyrelu) {
    set_has_leakyrelu();
    NonlinearityType_.leakyrelu_ = leakyrelu;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParams.leakyReLU)
}

// optional .CoreML.Specification.ActivationThresholdedReLU thresholdedReLU = 20;
inline bool ActivationParams::has_thresholdedrelu() const {
  return NonlinearityType_case() == kThresholdedReLU;
}
inline void ActivationParams::set_has_thresholdedrelu() {
  _oneof_case_[0] = kThresholdedReLU;
}
inline void ActivationParams::clear_thresholdedrelu() {
  if (has_thresholdedrelu()) {
    delete NonlinearityType_.thresholdedrelu_;
    clear_has_NonlinearityType();
  }
}
inline  const ::CoreML::Specification::ActivationThresholdedReLU& ActivationParams::thresholdedrelu() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.thresholdedReLU)
  return has_thresholdedrelu()
      ? *NonlinearityType_.thresholdedrelu_
      : ::CoreML::Specification::ActivationThresholdedReLU::default_instance();
}
inline ::CoreML::Specification::ActivationThresholdedReLU* ActivationParams::mutable_thresholdedrelu() {
  if (!has_thresholdedrelu()) {
    clear_NonlinearityType();
    set_has_thresholdedrelu();
    NonlinearityType_.thresholdedrelu_ = new ::CoreML::Specification::ActivationThresholdedReLU;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.thresholdedReLU)
  return NonlinearityType_.thresholdedrelu_;
}
inline ::CoreML::Specification::ActivationThresholdedReLU* ActivationParams::release_thresholdedrelu() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.thresholdedReLU)
  if (has_thresholdedrelu()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationThresholdedReLU* temp = NonlinearityType_.thresholdedrelu_;
    NonlinearityType_.thresholdedrelu_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void ActivationParams::set_allocated_thresholdedrelu(::CoreML::Specification::ActivationThresholdedReLU* thresholdedrelu) {
  clear_NonlinearityType();
  if (thresholdedrelu) {
    set_has_thresholdedrelu();
    NonlinearityType_.thresholdedrelu_ = thresholdedrelu;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParams.thresholdedReLU)
}

// optional .CoreML.Specification.ActivationPReLU PReLU = 25;
inline bool ActivationParams::has_prelu() const {
  return NonlinearityType_case() == kPReLU;
}
inline void ActivationParams::set_has_prelu() {
  _oneof_case_[0] = kPReLU;
}
inline void ActivationParams::clear_prelu() {
  if (has_prelu()) {
    delete NonlinearityType_.prelu_;
    clear_has_NonlinearityType();
  }
}
inline  const ::CoreML::Specification::ActivationPReLU& ActivationParams::prelu() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.PReLU)
  return has_prelu()
      ? *NonlinearityType_.prelu_
      : ::CoreML::Specification::ActivationPReLU::default_instance();
}
inline ::CoreML::Specification::ActivationPReLU* ActivationParams::mutable_prelu() {
  if (!has_prelu()) {
    clear_NonlinearityType();
    set_has_prelu();
    NonlinearityType_.prelu_ = new ::CoreML::Specification::ActivationPReLU;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.PReLU)
  return NonlinearityType_.prelu_;
}
inline ::CoreML::Specification::ActivationPReLU* ActivationParams::release_prelu() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.PReLU)
  if (has_prelu()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationPReLU* temp = NonlinearityType_.prelu_;
    NonlinearityType_.prelu_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void ActivationParams::set_allocated_prelu(::CoreML::Specification::ActivationPReLU* prelu) {
  clear_NonlinearityType();
  if (prelu) {
    set_has_prelu();
    NonlinearityType_.prelu_ = prelu;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParams.PReLU)
}

// optional .CoreML.Specification.ActivationTanh tanh = 30;
inline bool ActivationParams::has_tanh() const {
  return NonlinearityType_case() == kTanh;
}
inline void ActivationParams::set_has_tanh() {
  _oneof_case_[0] = kTanh;
}
inline void ActivationParams::clear_tanh() {
  if (has_tanh()) {
    delete NonlinearityType_.tanh_;
    clear_has_NonlinearityType();
  }
}
inline  const ::CoreML::Specification::ActivationTanh& ActivationParams::tanh() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.tanh)
  return has_tanh()
      ? *NonlinearityType_.tanh_
      : ::CoreML::Specification::ActivationTanh::default_instance();
}
inline ::CoreML::Specification::ActivationTanh* ActivationParams::mutable_tanh() {
  if (!has_tanh()) {
    clear_NonlinearityType();
    set_has_tanh();
    NonlinearityType_.tanh_ = new ::CoreML::Specification::ActivationTanh;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.tanh)
  return NonlinearityType_.tanh_;
}
inline ::CoreML::Specification::ActivationTanh* ActivationParams::release_tanh() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.tanh)
  if (has_tanh()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationTanh* temp = NonlinearityType_.tanh_;
    NonlinearityType_.tanh_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void ActivationParams::set_allocated_tanh(::CoreML::Specification::ActivationTanh* tanh) {
  clear_NonlinearityType();
  if (tanh) {
    set_has_tanh();
    NonlinearityType_.tanh_ = tanh;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParams.tanh)
}

// optional .CoreML.Specification.ActivationScaledTanh scaledTanh = 31;
inline bool ActivationParams::has_scaledtanh() const {
  return NonlinearityType_case() == kScaledTanh;
}
inline void ActivationParams::set_has_scaledtanh() {
  _oneof_case_[0] = kScaledTanh;
}
inline void ActivationParams::clear_scaledtanh() {
  if (has_scaledtanh()) {
    delete NonlinearityType_.scaledtanh_;
    clear_has_NonlinearityType();
  }
}
inline  const ::CoreML::Specification::ActivationScaledTanh& ActivationParams::scaledtanh() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.scaledTanh)
  return has_scaledtanh()
      ? *NonlinearityType_.scaledtanh_
      : ::CoreML::Specification::ActivationScaledTanh::default_instance();
}
inline ::CoreML::Specification::ActivationScaledTanh* ActivationParams::mutable_scaledtanh() {
  if (!has_scaledtanh()) {
    clear_NonlinearityType();
    set_has_scaledtanh();
    NonlinearityType_.scaledtanh_ = new ::CoreML::Specification::ActivationScaledTanh;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.scaledTanh)
  return NonlinearityType_.scaledtanh_;
}
inline ::CoreML::Specification::ActivationScaledTanh* ActivationParams::release_scaledtanh() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.scaledTanh)
  if (has_scaledtanh()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationScaledTanh* temp = NonlinearityType_.scaledtanh_;
    NonlinearityType_.scaledtanh_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void ActivationParams::set_allocated_scaledtanh(::CoreML::Specification::ActivationScaledTanh* scaledtanh) {
  clear_NonlinearityType();
  if (scaledtanh) {
    set_has_scaledtanh();
    NonlinearityType_.scaledtanh_ = scaledtanh;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParams.scaledTanh)
}

// optional .CoreML.Specification.ActivationSigmoid sigmoid = 40;
inline bool ActivationParams::has_sigmoid() const {
  return NonlinearityType_case() == kSigmoid;
}
inline void ActivationParams::set_has_sigmoid() {
  _oneof_case_[0] = kSigmoid;
}
inline void ActivationParams::clear_sigmoid() {
  if (has_sigmoid()) {
    delete NonlinearityType_.sigmoid_;
    clear_has_NonlinearityType();
  }
}
inline  const ::CoreML::Specification::ActivationSigmoid& ActivationParams::sigmoid() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.sigmoid)
  return has_sigmoid()
      ? *NonlinearityType_.sigmoid_
      : ::CoreML::Specification::ActivationSigmoid::default_instance();
}
inline ::CoreML::Specification::ActivationSigmoid* ActivationParams::mutable_sigmoid() {
  if (!has_sigmoid()) {
    clear_NonlinearityType();
    set_has_sigmoid();
    NonlinearityType_.sigmoid_ = new ::CoreML::Specification::ActivationSigmoid;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.sigmoid)
  return NonlinearityType_.sigmoid_;
}
inline ::CoreML::Specification::ActivationSigmoid* ActivationParams::release_sigmoid() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.sigmoid)
  if (has_sigmoid()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationSigmoid* temp = NonlinearityType_.sigmoid_;
    NonlinearityType_.sigmoid_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void ActivationParams::set_allocated_sigmoid(::CoreML::Specification::ActivationSigmoid* sigmoid) {
  clear_NonlinearityType();
  if (sigmoid) {
    set_has_sigmoid();
    NonlinearityType_.sigmoid_ = sigmoid;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParams.sigmoid)
}

// optional .CoreML.Specification.ActivationSigmoidHard sigmoidHard = 41;
inline bool ActivationParams::has_sigmoidhard() const {
  return NonlinearityType_case() == kSigmoidHard;
}
inline void ActivationParams::set_has_sigmoidhard() {
  _oneof_case_[0] = kSigmoidHard;
}
inline void ActivationParams::clear_sigmoidhard() {
  if (has_sigmoidhard()) {
    delete NonlinearityType_.sigmoidhard_;
    clear_has_NonlinearityType();
  }
}
inline  const ::CoreML::Specification::ActivationSigmoidHard& ActivationParams::sigmoidhard() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.sigmoidHard)
  return has_sigmoidhard()
      ? *NonlinearityType_.sigmoidhard_
      : ::CoreML::Specification::ActivationSigmoidHard::default_instance();
}
inline ::CoreML::Specification::ActivationSigmoidHard* ActivationParams::mutable_sigmoidhard() {
  if (!has_sigmoidhard()) {
    clear_NonlinearityType();
    set_has_sigmoidhard();
    NonlinearityType_.sigmoidhard_ = new ::CoreML::Specification::ActivationSigmoidHard;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.sigmoidHard)
  return NonlinearityType_.sigmoidhard_;
}
inline ::CoreML::Specification::ActivationSigmoidHard* ActivationParams::release_sigmoidhard() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.sigmoidHard)
  if (has_sigmoidhard()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationSigmoidHard* temp = NonlinearityType_.sigmoidhard_;
    NonlinearityType_.sigmoidhard_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void ActivationParams::set_allocated_sigmoidhard(::CoreML::Specification::ActivationSigmoidHard* sigmoidhard) {
  clear_NonlinearityType();
  if (sigmoidhard) {
    set_has_sigmoidhard();
    NonlinearityType_.sigmoidhard_ = sigmoidhard;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParams.sigmoidHard)
}

// optional .CoreML.Specification.ActivationELU ELU = 50;
inline bool ActivationParams::has_elu() const {
  return NonlinearityType_case() == kELU;
}
inline void ActivationParams::set_has_elu() {
  _oneof_case_[0] = kELU;
}
inline void ActivationParams::clear_elu() {
  if (has_elu()) {
    delete NonlinearityType_.elu_;
    clear_has_NonlinearityType();
  }
}
inline  const ::CoreML::Specification::ActivationELU& ActivationParams::elu() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.ELU)
  return has_elu()
      ? *NonlinearityType_.elu_
      : ::CoreML::Specification::ActivationELU::default_instance();
}
inline ::CoreML::Specification::ActivationELU* ActivationParams::mutable_elu() {
  if (!has_elu()) {
    clear_NonlinearityType();
    set_has_elu();
    NonlinearityType_.elu_ = new ::CoreML::Specification::ActivationELU;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.ELU)
  return NonlinearityType_.elu_;
}
inline ::CoreML::Specification::ActivationELU* ActivationParams::release_elu() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.ELU)
  if (has_elu()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationELU* temp = NonlinearityType_.elu_;
    NonlinearityType_.elu_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void ActivationParams::set_allocated_elu(::CoreML::Specification::ActivationELU* elu) {
  clear_NonlinearityType();
  if (elu) {
    set_has_elu();
    NonlinearityType_.elu_ = elu;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParams.ELU)
}

// optional .CoreML.Specification.ActivationSoftsign softsign = 60;
inline bool ActivationParams::has_softsign() const {
  return NonlinearityType_case() == kSoftsign;
}
inline void ActivationParams::set_has_softsign() {
  _oneof_case_[0] = kSoftsign;
}
inline void ActivationParams::clear_softsign() {
  if (has_softsign()) {
    delete NonlinearityType_.softsign_;
    clear_has_NonlinearityType();
  }
}
inline  const ::CoreML::Specification::ActivationSoftsign& ActivationParams::softsign() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.softsign)
  return has_softsign()
      ? *NonlinearityType_.softsign_
      : ::CoreML::Specification::ActivationSoftsign::default_instance();
}
inline ::CoreML::Specification::ActivationSoftsign* ActivationParams::mutable_softsign() {
  if (!has_softsign()) {
    clear_NonlinearityType();
    set_has_softsign();
    NonlinearityType_.softsign_ = new ::CoreML::Specification::ActivationSoftsign;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.softsign)
  return NonlinearityType_.softsign_;
}
inline ::CoreML::Specification::ActivationSoftsign* ActivationParams::release_softsign() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.softsign)
  if (has_softsign()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationSoftsign* temp = NonlinearityType_.softsign_;
    NonlinearityType_.softsign_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void ActivationParams::set_allocated_softsign(::CoreML::Specification::ActivationSoftsign* softsign) {
  clear_NonlinearityType();
  if (softsign) {
    set_has_softsign();
    NonlinearityType_.softsign_ = softsign;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParams.softsign)
}

// optional .CoreML.Specification.ActivationSoftplus softplus = 70;
inline bool ActivationParams::has_softplus() const {
  return NonlinearityType_case() == kSoftplus;
}
inline void ActivationParams::set_has_softplus() {
  _oneof_case_[0] = kSoftplus;
}
inline void ActivationParams::clear_softplus() {
  if (has_softplus()) {
    delete NonlinearityType_.softplus_;
    clear_has_NonlinearityType();
  }
}
inline  const ::CoreML::Specification::ActivationSoftplus& ActivationParams::softplus() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.softplus)
  return has_softplus()
      ? *NonlinearityType_.softplus_
      : ::CoreML::Specification::ActivationSoftplus::default_instance();
}
inline ::CoreML::Specification::ActivationSoftplus* ActivationParams::mutable_softplus() {
  if (!has_softplus()) {
    clear_NonlinearityType();
    set_has_softplus();
    NonlinearityType_.softplus_ = new ::CoreML::Specification::ActivationSoftplus;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.softplus)
  return NonlinearityType_.softplus_;
}
inline ::CoreML::Specification::ActivationSoftplus* ActivationParams::release_softplus() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.softplus)
  if (has_softplus()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationSoftplus* temp = NonlinearityType_.softplus_;
    NonlinearityType_.softplus_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void ActivationParams::set_allocated_softplus(::CoreML::Specification::ActivationSoftplus* softplus) {
  clear_NonlinearityType();
  if (softplus) {
    set_has_softplus();
    NonlinearityType_.softplus_ = softplus;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParams.softplus)
}

// optional .CoreML.Specification.ActivationParametricSoftplus parametricSoftplus = 71;
inline bool ActivationParams::has_parametricsoftplus() const {
  return NonlinearityType_case() == kParametricSoftplus;
}
inline void ActivationParams::set_has_parametricsoftplus() {
  _oneof_case_[0] = kParametricSoftplus;
}
inline void ActivationParams::clear_parametricsoftplus() {
  if (has_parametricsoftplus()) {
    delete NonlinearityType_.parametricsoftplus_;
    clear_has_NonlinearityType();
  }
}
inline  const ::CoreML::Specification::ActivationParametricSoftplus& ActivationParams::parametricsoftplus() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.parametricSoftplus)
  return has_parametricsoftplus()
      ? *NonlinearityType_.parametricsoftplus_
      : ::CoreML::Specification::ActivationParametricSoftplus::default_instance();
}
inline ::CoreML::Specification::ActivationParametricSoftplus* ActivationParams::mutable_parametricsoftplus() {
  if (!has_parametricsoftplus()) {
    clear_NonlinearityType();
    set_has_parametricsoftplus();
    NonlinearityType_.parametricsoftplus_ = new ::CoreML::Specification::ActivationParametricSoftplus;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.parametricSoftplus)
  return NonlinearityType_.parametricsoftplus_;
}
inline ::CoreML::Specification::ActivationParametricSoftplus* ActivationParams::release_parametricsoftplus() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.parametricSoftplus)
  if (has_parametricsoftplus()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationParametricSoftplus* temp = NonlinearityType_.parametricsoftplus_;
    NonlinearityType_.parametricsoftplus_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void ActivationParams::set_allocated_parametricsoftplus(::CoreML::Specification::ActivationParametricSoftplus* parametricsoftplus) {
  clear_NonlinearityType();
  if (parametricsoftplus) {
    set_has_parametricsoftplus();
    NonlinearityType_.parametricsoftplus_ = parametricsoftplus;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParams.parametricSoftplus)
}

inline bool ActivationParams::has_NonlinearityType() const {
  return NonlinearityType_case() != NONLINEARITYTYPE_NOT_SET;
}
inline void ActivationParams::clear_has_NonlinearityType() {
  _oneof_case_[0] = NONLINEARITYTYPE_NOT_SET;
}
inline ActivationParams::NonlinearityTypeCase ActivationParams::NonlinearityType_case() const {
  return ActivationParams::NonlinearityTypeCase(_oneof_case_[0]);
}
inline const ActivationParams* ActivationParams::internal_default_instance() {
  return &ActivationParams_default_instance_.get();
}
// -------------------------------------------------------------------

// NeuralNetworkLayer

// optional string name = 1;
inline void NeuralNetworkLayer::clear_name() {
  name_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline const ::std::string& NeuralNetworkLayer::name() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.name)
  return name_.GetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline void NeuralNetworkLayer::set_name(const ::std::string& value) {

  name_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkLayer.name)
}
inline void NeuralNetworkLayer::set_name(const char* value) {

  name_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:CoreML.Specification.NeuralNetworkLayer.name)
}
inline void NeuralNetworkLayer::set_name(const char* value, size_t size) {

  name_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:CoreML.Specification.NeuralNetworkLayer.name)
}
inline ::std::string* NeuralNetworkLayer::mutable_name() {

  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.name)
  return name_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline ::std::string* NeuralNetworkLayer::release_name() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.name)

  return name_.ReleaseNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline void NeuralNetworkLayer::set_allocated_name(::std::string* name) {
  if (name != NULL) {

  } else {

  }
  name_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), name);
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.name)
}

// repeated string input = 2;
inline int NeuralNetworkLayer::input_size() const {
  return input_.size();
}
inline void NeuralNetworkLayer::clear_input() {
  input_.Clear();
}
inline const ::std::string& NeuralNetworkLayer::input(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.input)
  return input_.Get(index);
}
inline ::std::string* NeuralNetworkLayer::mutable_input(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.input)
  return input_.Mutable(index);
}
inline void NeuralNetworkLayer::set_input(int index, const ::std::string& value) {
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkLayer.input)
  input_.Mutable(index)->assign(value);
}
inline void NeuralNetworkLayer::set_input(int index, const char* value) {
  input_.Mutable(index)->assign(value);
  // @@protoc_insertion_point(field_set_char:CoreML.Specification.NeuralNetworkLayer.input)
}
inline void NeuralNetworkLayer::set_input(int index, const char* value, size_t size) {
  input_.Mutable(index)->assign(
    reinterpret_cast<const char*>(value), size);
  // @@protoc_insertion_point(field_set_pointer:CoreML.Specification.NeuralNetworkLayer.input)
}
inline ::std::string* NeuralNetworkLayer::add_input() {
  // @@protoc_insertion_point(field_add_mutable:CoreML.Specification.NeuralNetworkLayer.input)
  return input_.Add();
}
inline void NeuralNetworkLayer::add_input(const ::std::string& value) {
  input_.Add()->assign(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.NeuralNetworkLayer.input)
}
inline void NeuralNetworkLayer::add_input(const char* value) {
  input_.Add()->assign(value);
  // @@protoc_insertion_point(field_add_char:CoreML.Specification.NeuralNetworkLayer.input)
}
inline void NeuralNetworkLayer::add_input(const char* value, size_t size) {
  input_.Add()->assign(reinterpret_cast<const char*>(value), size);
  // @@protoc_insertion_point(field_add_pointer:CoreML.Specification.NeuralNetworkLayer.input)
}
inline const ::google::protobuf::RepeatedPtrField< ::std::string>&
NeuralNetworkLayer::input() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.NeuralNetworkLayer.input)
  return input_;
}
inline ::google::protobuf::RepeatedPtrField< ::std::string>*
NeuralNetworkLayer::mutable_input() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.NeuralNetworkLayer.input)
  return &input_;
}

// repeated string output = 3;
inline int NeuralNetworkLayer::output_size() const {
  return output_.size();
}
inline void NeuralNetworkLayer::clear_output() {
  output_.Clear();
}
inline const ::std::string& NeuralNetworkLayer::output(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.output)
  return output_.Get(index);
}
inline ::std::string* NeuralNetworkLayer::mutable_output(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.output)
  return output_.Mutable(index);
}
inline void NeuralNetworkLayer::set_output(int index, const ::std::string& value) {
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkLayer.output)
  output_.Mutable(index)->assign(value);
}
inline void NeuralNetworkLayer::set_output(int index, const char* value) {
  output_.Mutable(index)->assign(value);
  // @@protoc_insertion_point(field_set_char:CoreML.Specification.NeuralNetworkLayer.output)
}
inline void NeuralNetworkLayer::set_output(int index, const char* value, size_t size) {
  output_.Mutable(index)->assign(
    reinterpret_cast<const char*>(value), size);
  // @@protoc_insertion_point(field_set_pointer:CoreML.Specification.NeuralNetworkLayer.output)
}
inline ::std::string* NeuralNetworkLayer::add_output() {
  // @@protoc_insertion_point(field_add_mutable:CoreML.Specification.NeuralNetworkLayer.output)
  return output_.Add();
}
inline void NeuralNetworkLayer::add_output(const ::std::string& value) {
  output_.Add()->assign(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.NeuralNetworkLayer.output)
}
inline void NeuralNetworkLayer::add_output(const char* value) {
  output_.Add()->assign(value);
  // @@protoc_insertion_point(field_add_char:CoreML.Specification.NeuralNetworkLayer.output)
}
inline void NeuralNetworkLayer::add_output(const char* value, size_t size) {
  output_.Add()->assign(reinterpret_cast<const char*>(value), size);
  // @@protoc_insertion_point(field_add_pointer:CoreML.Specification.NeuralNetworkLayer.output)
}
inline const ::google::protobuf::RepeatedPtrField< ::std::string>&
NeuralNetworkLayer::output() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.NeuralNetworkLayer.output)
  return output_;
}
inline ::google::protobuf::RepeatedPtrField< ::std::string>*
NeuralNetworkLayer::mutable_output() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.NeuralNetworkLayer.output)
  return &output_;
}

// optional .CoreML.Specification.ConvolutionLayerParams convolution = 100;
inline bool NeuralNetworkLayer::has_convolution() const {
  return layer_case() == kConvolution;
}
inline void NeuralNetworkLayer::set_has_convolution() {
  _oneof_case_[0] = kConvolution;
}
inline void NeuralNetworkLayer::clear_convolution() {
  if (has_convolution()) {
    delete layer_.convolution_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::ConvolutionLayerParams& NeuralNetworkLayer::convolution() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.convolution)
  return has_convolution()
      ? *layer_.convolution_
      : ::CoreML::Specification::ConvolutionLayerParams::default_instance();
}
inline ::CoreML::Specification::ConvolutionLayerParams* NeuralNetworkLayer::mutable_convolution() {
  if (!has_convolution()) {
    clear_layer();
    set_has_convolution();
    layer_.convolution_ = new ::CoreML::Specification::ConvolutionLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.convolution)
  return layer_.convolution_;
}
inline ::CoreML::Specification::ConvolutionLayerParams* NeuralNetworkLayer::release_convolution() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.convolution)
  if (has_convolution()) {
    clear_has_layer();
    ::CoreML::Specification::ConvolutionLayerParams* temp = layer_.convolution_;
    layer_.convolution_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_convolution(::CoreML::Specification::ConvolutionLayerParams* convolution) {
  clear_layer();
  if (convolution) {
    set_has_convolution();
    layer_.convolution_ = convolution;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.convolution)
}

// optional .CoreML.Specification.PoolingLayerParams pooling = 120;
inline bool NeuralNetworkLayer::has_pooling() const {
  return layer_case() == kPooling;
}
inline void NeuralNetworkLayer::set_has_pooling() {
  _oneof_case_[0] = kPooling;
}
inline void NeuralNetworkLayer::clear_pooling() {
  if (has_pooling()) {
    delete layer_.pooling_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::PoolingLayerParams& NeuralNetworkLayer::pooling() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.pooling)
  return has_pooling()
      ? *layer_.pooling_
      : ::CoreML::Specification::PoolingLayerParams::default_instance();
}
inline ::CoreML::Specification::PoolingLayerParams* NeuralNetworkLayer::mutable_pooling() {
  if (!has_pooling()) {
    clear_layer();
    set_has_pooling();
    layer_.pooling_ = new ::CoreML::Specification::PoolingLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.pooling)
  return layer_.pooling_;
}
inline ::CoreML::Specification::PoolingLayerParams* NeuralNetworkLayer::release_pooling() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.pooling)
  if (has_pooling()) {
    clear_has_layer();
    ::CoreML::Specification::PoolingLayerParams* temp = layer_.pooling_;
    layer_.pooling_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_pooling(::CoreML::Specification::PoolingLayerParams* pooling) {
  clear_layer();
  if (pooling) {
    set_has_pooling();
    layer_.pooling_ = pooling;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.pooling)
}

// optional .CoreML.Specification.ActivationParams activation = 130;
inline bool NeuralNetworkLayer::has_activation() const {
  return layer_case() == kActivation;
}
inline void NeuralNetworkLayer::set_has_activation() {
  _oneof_case_[0] = kActivation;
}
inline void NeuralNetworkLayer::clear_activation() {
  if (has_activation()) {
    delete layer_.activation_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::ActivationParams& NeuralNetworkLayer::activation() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.activation)
  return has_activation()
      ? *layer_.activation_
      : ::CoreML::Specification::ActivationParams::default_instance();
}
inline ::CoreML::Specification::ActivationParams* NeuralNetworkLayer::mutable_activation() {
  if (!has_activation()) {
    clear_layer();
    set_has_activation();
    layer_.activation_ = new ::CoreML::Specification::ActivationParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.activation)
  return layer_.activation_;
}
inline ::CoreML::Specification::ActivationParams* NeuralNetworkLayer::release_activation() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.activation)
  if (has_activation()) {
    clear_has_layer();
    ::CoreML::Specification::ActivationParams* temp = layer_.activation_;
    layer_.activation_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_activation(::CoreML::Specification::ActivationParams* activation) {
  clear_layer();
  if (activation) {
    set_has_activation();
    layer_.activation_ = activation;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.activation)
}

// optional .CoreML.Specification.InnerProductLayerParams innerProduct = 140;
inline bool NeuralNetworkLayer::has_innerproduct() const {
  return layer_case() == kInnerProduct;
}
inline void NeuralNetworkLayer::set_has_innerproduct() {
  _oneof_case_[0] = kInnerProduct;
}
inline void NeuralNetworkLayer::clear_innerproduct() {
  if (has_innerproduct()) {
    delete layer_.innerproduct_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::InnerProductLayerParams& NeuralNetworkLayer::innerproduct() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.innerProduct)
  return has_innerproduct()
      ? *layer_.innerproduct_
      : ::CoreML::Specification::InnerProductLayerParams::default_instance();
}
inline ::CoreML::Specification::InnerProductLayerParams* NeuralNetworkLayer::mutable_innerproduct() {
  if (!has_innerproduct()) {
    clear_layer();
    set_has_innerproduct();
    layer_.innerproduct_ = new ::CoreML::Specification::InnerProductLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.innerProduct)
  return layer_.innerproduct_;
}
inline ::CoreML::Specification::InnerProductLayerParams* NeuralNetworkLayer::release_innerproduct() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.innerProduct)
  if (has_innerproduct()) {
    clear_has_layer();
    ::CoreML::Specification::InnerProductLayerParams* temp = layer_.innerproduct_;
    layer_.innerproduct_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_innerproduct(::CoreML::Specification::InnerProductLayerParams* innerproduct) {
  clear_layer();
  if (innerproduct) {
    set_has_innerproduct();
    layer_.innerproduct_ = innerproduct;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.innerProduct)
}

// optional .CoreML.Specification.EmbeddingLayerParams embedding = 150;
inline bool NeuralNetworkLayer::has_embedding() const {
  return layer_case() == kEmbedding;
}
inline void NeuralNetworkLayer::set_has_embedding() {
  _oneof_case_[0] = kEmbedding;
}
inline void NeuralNetworkLayer::clear_embedding() {
  if (has_embedding()) {
    delete layer_.embedding_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::EmbeddingLayerParams& NeuralNetworkLayer::embedding() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.embedding)
  return has_embedding()
      ? *layer_.embedding_
      : ::CoreML::Specification::EmbeddingLayerParams::default_instance();
}
inline ::CoreML::Specification::EmbeddingLayerParams* NeuralNetworkLayer::mutable_embedding() {
  if (!has_embedding()) {
    clear_layer();
    set_has_embedding();
    layer_.embedding_ = new ::CoreML::Specification::EmbeddingLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.embedding)
  return layer_.embedding_;
}
inline ::CoreML::Specification::EmbeddingLayerParams* NeuralNetworkLayer::release_embedding() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.embedding)
  if (has_embedding()) {
    clear_has_layer();
    ::CoreML::Specification::EmbeddingLayerParams* temp = layer_.embedding_;
    layer_.embedding_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_embedding(::CoreML::Specification::EmbeddingLayerParams* embedding) {
  clear_layer();
  if (embedding) {
    set_has_embedding();
    layer_.embedding_ = embedding;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.embedding)
}

// optional .CoreML.Specification.BatchnormLayerParams batchnorm = 160;
inline bool NeuralNetworkLayer::has_batchnorm() const {
  return layer_case() == kBatchnorm;
}
inline void NeuralNetworkLayer::set_has_batchnorm() {
  _oneof_case_[0] = kBatchnorm;
}
inline void NeuralNetworkLayer::clear_batchnorm() {
  if (has_batchnorm()) {
    delete layer_.batchnorm_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::BatchnormLayerParams& NeuralNetworkLayer::batchnorm() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.batchnorm)
  return has_batchnorm()
      ? *layer_.batchnorm_
      : ::CoreML::Specification::BatchnormLayerParams::default_instance();
}
inline ::CoreML::Specification::BatchnormLayerParams* NeuralNetworkLayer::mutable_batchnorm() {
  if (!has_batchnorm()) {
    clear_layer();
    set_has_batchnorm();
    layer_.batchnorm_ = new ::CoreML::Specification::BatchnormLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.batchnorm)
  return layer_.batchnorm_;
}
inline ::CoreML::Specification::BatchnormLayerParams* NeuralNetworkLayer::release_batchnorm() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.batchnorm)
  if (has_batchnorm()) {
    clear_has_layer();
    ::CoreML::Specification::BatchnormLayerParams* temp = layer_.batchnorm_;
    layer_.batchnorm_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_batchnorm(::CoreML::Specification::BatchnormLayerParams* batchnorm) {
  clear_layer();
  if (batchnorm) {
    set_has_batchnorm();
    layer_.batchnorm_ = batchnorm;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.batchnorm)
}

// optional .CoreML.Specification.MeanVarianceNormalizeLayerParams mvn = 165;
inline bool NeuralNetworkLayer::has_mvn() const {
  return layer_case() == kMvn;
}
inline void NeuralNetworkLayer::set_has_mvn() {
  _oneof_case_[0] = kMvn;
}
inline void NeuralNetworkLayer::clear_mvn() {
  if (has_mvn()) {
    delete layer_.mvn_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::MeanVarianceNormalizeLayerParams& NeuralNetworkLayer::mvn() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.mvn)
  return has_mvn()
      ? *layer_.mvn_
      : ::CoreML::Specification::MeanVarianceNormalizeLayerParams::default_instance();
}
inline ::CoreML::Specification::MeanVarianceNormalizeLayerParams* NeuralNetworkLayer::mutable_mvn() {
  if (!has_mvn()) {
    clear_layer();
    set_has_mvn();
    layer_.mvn_ = new ::CoreML::Specification::MeanVarianceNormalizeLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.mvn)
  return layer_.mvn_;
}
inline ::CoreML::Specification::MeanVarianceNormalizeLayerParams* NeuralNetworkLayer::release_mvn() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.mvn)
  if (has_mvn()) {
    clear_has_layer();
    ::CoreML::Specification::MeanVarianceNormalizeLayerParams* temp = layer_.mvn_;
    layer_.mvn_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_mvn(::CoreML::Specification::MeanVarianceNormalizeLayerParams* mvn) {
  clear_layer();
  if (mvn) {
    set_has_mvn();
    layer_.mvn_ = mvn;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.mvn)
}

// optional .CoreML.Specification.L2NormalizeLayerParams l2normalize = 170;
inline bool NeuralNetworkLayer::has_l2normalize() const {
  return layer_case() == kL2Normalize;
}
inline void NeuralNetworkLayer::set_has_l2normalize() {
  _oneof_case_[0] = kL2Normalize;
}
inline void NeuralNetworkLayer::clear_l2normalize() {
  if (has_l2normalize()) {
    delete layer_.l2normalize_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::L2NormalizeLayerParams& NeuralNetworkLayer::l2normalize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.l2normalize)
  return has_l2normalize()
      ? *layer_.l2normalize_
      : ::CoreML::Specification::L2NormalizeLayerParams::default_instance();
}
inline ::CoreML::Specification::L2NormalizeLayerParams* NeuralNetworkLayer::mutable_l2normalize() {
  if (!has_l2normalize()) {
    clear_layer();
    set_has_l2normalize();
    layer_.l2normalize_ = new ::CoreML::Specification::L2NormalizeLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.l2normalize)
  return layer_.l2normalize_;
}
inline ::CoreML::Specification::L2NormalizeLayerParams* NeuralNetworkLayer::release_l2normalize() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.l2normalize)
  if (has_l2normalize()) {
    clear_has_layer();
    ::CoreML::Specification::L2NormalizeLayerParams* temp = layer_.l2normalize_;
    layer_.l2normalize_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_l2normalize(::CoreML::Specification::L2NormalizeLayerParams* l2normalize) {
  clear_layer();
  if (l2normalize) {
    set_has_l2normalize();
    layer_.l2normalize_ = l2normalize;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.l2normalize)
}

// optional .CoreML.Specification.SoftmaxLayerParams softmax = 175;
inline bool NeuralNetworkLayer::has_softmax() const {
  return layer_case() == kSoftmax;
}
inline void NeuralNetworkLayer::set_has_softmax() {
  _oneof_case_[0] = kSoftmax;
}
inline void NeuralNetworkLayer::clear_softmax() {
  if (has_softmax()) {
    delete layer_.softmax_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::SoftmaxLayerParams& NeuralNetworkLayer::softmax() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.softmax)
  return has_softmax()
      ? *layer_.softmax_
      : ::CoreML::Specification::SoftmaxLayerParams::default_instance();
}
inline ::CoreML::Specification::SoftmaxLayerParams* NeuralNetworkLayer::mutable_softmax() {
  if (!has_softmax()) {
    clear_layer();
    set_has_softmax();
    layer_.softmax_ = new ::CoreML::Specification::SoftmaxLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.softmax)
  return layer_.softmax_;
}
inline ::CoreML::Specification::SoftmaxLayerParams* NeuralNetworkLayer::release_softmax() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.softmax)
  if (has_softmax()) {
    clear_has_layer();
    ::CoreML::Specification::SoftmaxLayerParams* temp = layer_.softmax_;
    layer_.softmax_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_softmax(::CoreML::Specification::SoftmaxLayerParams* softmax) {
  clear_layer();
  if (softmax) {
    set_has_softmax();
    layer_.softmax_ = softmax;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.softmax)
}

// optional .CoreML.Specification.LRNLayerParams lrn = 180;
inline bool NeuralNetworkLayer::has_lrn() const {
  return layer_case() == kLrn;
}
inline void NeuralNetworkLayer::set_has_lrn() {
  _oneof_case_[0] = kLrn;
}
inline void NeuralNetworkLayer::clear_lrn() {
  if (has_lrn()) {
    delete layer_.lrn_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::LRNLayerParams& NeuralNetworkLayer::lrn() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.lrn)
  return has_lrn()
      ? *layer_.lrn_
      : ::CoreML::Specification::LRNLayerParams::default_instance();
}
inline ::CoreML::Specification::LRNLayerParams* NeuralNetworkLayer::mutable_lrn() {
  if (!has_lrn()) {
    clear_layer();
    set_has_lrn();
    layer_.lrn_ = new ::CoreML::Specification::LRNLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.lrn)
  return layer_.lrn_;
}
inline ::CoreML::Specification::LRNLayerParams* NeuralNetworkLayer::release_lrn() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.lrn)
  if (has_lrn()) {
    clear_has_layer();
    ::CoreML::Specification::LRNLayerParams* temp = layer_.lrn_;
    layer_.lrn_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_lrn(::CoreML::Specification::LRNLayerParams* lrn) {
  clear_layer();
  if (lrn) {
    set_has_lrn();
    layer_.lrn_ = lrn;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.lrn)
}

// optional .CoreML.Specification.CropLayerParams crop = 190;
inline bool NeuralNetworkLayer::has_crop() const {
  return layer_case() == kCrop;
}
inline void NeuralNetworkLayer::set_has_crop() {
  _oneof_case_[0] = kCrop;
}
inline void NeuralNetworkLayer::clear_crop() {
  if (has_crop()) {
    delete layer_.crop_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::CropLayerParams& NeuralNetworkLayer::crop() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.crop)
  return has_crop()
      ? *layer_.crop_
      : ::CoreML::Specification::CropLayerParams::default_instance();
}
inline ::CoreML::Specification::CropLayerParams* NeuralNetworkLayer::mutable_crop() {
  if (!has_crop()) {
    clear_layer();
    set_has_crop();
    layer_.crop_ = new ::CoreML::Specification::CropLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.crop)
  return layer_.crop_;
}
inline ::CoreML::Specification::CropLayerParams* NeuralNetworkLayer::release_crop() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.crop)
  if (has_crop()) {
    clear_has_layer();
    ::CoreML::Specification::CropLayerParams* temp = layer_.crop_;
    layer_.crop_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_crop(::CoreML::Specification::CropLayerParams* crop) {
  clear_layer();
  if (crop) {
    set_has_crop();
    layer_.crop_ = crop;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.crop)
}

// optional .CoreML.Specification.PaddingLayerParams padding = 200;
inline bool NeuralNetworkLayer::has_padding() const {
  return layer_case() == kPadding;
}
inline void NeuralNetworkLayer::set_has_padding() {
  _oneof_case_[0] = kPadding;
}
inline void NeuralNetworkLayer::clear_padding() {
  if (has_padding()) {
    delete layer_.padding_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::PaddingLayerParams& NeuralNetworkLayer::padding() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.padding)
  return has_padding()
      ? *layer_.padding_
      : ::CoreML::Specification::PaddingLayerParams::default_instance();
}
inline ::CoreML::Specification::PaddingLayerParams* NeuralNetworkLayer::mutable_padding() {
  if (!has_padding()) {
    clear_layer();
    set_has_padding();
    layer_.padding_ = new ::CoreML::Specification::PaddingLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.padding)
  return layer_.padding_;
}
inline ::CoreML::Specification::PaddingLayerParams* NeuralNetworkLayer::release_padding() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.padding)
  if (has_padding()) {
    clear_has_layer();
    ::CoreML::Specification::PaddingLayerParams* temp = layer_.padding_;
    layer_.padding_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_padding(::CoreML::Specification::PaddingLayerParams* padding) {
  clear_layer();
  if (padding) {
    set_has_padding();
    layer_.padding_ = padding;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.padding)
}

// optional .CoreML.Specification.UpsampleLayerParams upsample = 210;
inline bool NeuralNetworkLayer::has_upsample() const {
  return layer_case() == kUpsample;
}
inline void NeuralNetworkLayer::set_has_upsample() {
  _oneof_case_[0] = kUpsample;
}
inline void NeuralNetworkLayer::clear_upsample() {
  if (has_upsample()) {
    delete layer_.upsample_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::UpsampleLayerParams& NeuralNetworkLayer::upsample() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.upsample)
  return has_upsample()
      ? *layer_.upsample_
      : ::CoreML::Specification::UpsampleLayerParams::default_instance();
}
inline ::CoreML::Specification::UpsampleLayerParams* NeuralNetworkLayer::mutable_upsample() {
  if (!has_upsample()) {
    clear_layer();
    set_has_upsample();
    layer_.upsample_ = new ::CoreML::Specification::UpsampleLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.upsample)
  return layer_.upsample_;
}
inline ::CoreML::Specification::UpsampleLayerParams* NeuralNetworkLayer::release_upsample() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.upsample)
  if (has_upsample()) {
    clear_has_layer();
    ::CoreML::Specification::UpsampleLayerParams* temp = layer_.upsample_;
    layer_.upsample_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_upsample(::CoreML::Specification::UpsampleLayerParams* upsample) {
  clear_layer();
  if (upsample) {
    set_has_upsample();
    layer_.upsample_ = upsample;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.upsample)
}

// optional .CoreML.Specification.UnaryFunctionLayerParams unary = 220;
inline bool NeuralNetworkLayer::has_unary() const {
  return layer_case() == kUnary;
}
inline void NeuralNetworkLayer::set_has_unary() {
  _oneof_case_[0] = kUnary;
}
inline void NeuralNetworkLayer::clear_unary() {
  if (has_unary()) {
    delete layer_.unary_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::UnaryFunctionLayerParams& NeuralNetworkLayer::unary() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.unary)
  return has_unary()
      ? *layer_.unary_
      : ::CoreML::Specification::UnaryFunctionLayerParams::default_instance();
}
inline ::CoreML::Specification::UnaryFunctionLayerParams* NeuralNetworkLayer::mutable_unary() {
  if (!has_unary()) {
    clear_layer();
    set_has_unary();
    layer_.unary_ = new ::CoreML::Specification::UnaryFunctionLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.unary)
  return layer_.unary_;
}
inline ::CoreML::Specification::UnaryFunctionLayerParams* NeuralNetworkLayer::release_unary() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.unary)
  if (has_unary()) {
    clear_has_layer();
    ::CoreML::Specification::UnaryFunctionLayerParams* temp = layer_.unary_;
    layer_.unary_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_unary(::CoreML::Specification::UnaryFunctionLayerParams* unary) {
  clear_layer();
  if (unary) {
    set_has_unary();
    layer_.unary_ = unary;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.unary)
}

// optional .CoreML.Specification.AddLayerParams add = 230;
inline bool NeuralNetworkLayer::has_add() const {
  return layer_case() == kAdd;
}
inline void NeuralNetworkLayer::set_has_add() {
  _oneof_case_[0] = kAdd;
}
inline void NeuralNetworkLayer::clear_add() {
  if (has_add()) {
    delete layer_.add_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::AddLayerParams& NeuralNetworkLayer::add() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.add)
  return has_add()
      ? *layer_.add_
      : ::CoreML::Specification::AddLayerParams::default_instance();
}
inline ::CoreML::Specification::AddLayerParams* NeuralNetworkLayer::mutable_add() {
  if (!has_add()) {
    clear_layer();
    set_has_add();
    layer_.add_ = new ::CoreML::Specification::AddLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.add)
  return layer_.add_;
}
inline ::CoreML::Specification::AddLayerParams* NeuralNetworkLayer::release_add() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.add)
  if (has_add()) {
    clear_has_layer();
    ::CoreML::Specification::AddLayerParams* temp = layer_.add_;
    layer_.add_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_add(::CoreML::Specification::AddLayerParams* add) {
  clear_layer();
  if (add) {
    set_has_add();
    layer_.add_ = add;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.add)
}

// optional .CoreML.Specification.MultiplyLayerParams multiply = 231;
inline bool NeuralNetworkLayer::has_multiply() const {
  return layer_case() == kMultiply;
}
inline void NeuralNetworkLayer::set_has_multiply() {
  _oneof_case_[0] = kMultiply;
}
inline void NeuralNetworkLayer::clear_multiply() {
  if (has_multiply()) {
    delete layer_.multiply_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::MultiplyLayerParams& NeuralNetworkLayer::multiply() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.multiply)
  return has_multiply()
      ? *layer_.multiply_
      : ::CoreML::Specification::MultiplyLayerParams::default_instance();
}
inline ::CoreML::Specification::MultiplyLayerParams* NeuralNetworkLayer::mutable_multiply() {
  if (!has_multiply()) {
    clear_layer();
    set_has_multiply();
    layer_.multiply_ = new ::CoreML::Specification::MultiplyLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.multiply)
  return layer_.multiply_;
}
inline ::CoreML::Specification::MultiplyLayerParams* NeuralNetworkLayer::release_multiply() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.multiply)
  if (has_multiply()) {
    clear_has_layer();
    ::CoreML::Specification::MultiplyLayerParams* temp = layer_.multiply_;
    layer_.multiply_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_multiply(::CoreML::Specification::MultiplyLayerParams* multiply) {
  clear_layer();
  if (multiply) {
    set_has_multiply();
    layer_.multiply_ = multiply;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.multiply)
}

// optional .CoreML.Specification.AverageLayerParams average = 240;
inline bool NeuralNetworkLayer::has_average() const {
  return layer_case() == kAverage;
}
inline void NeuralNetworkLayer::set_has_average() {
  _oneof_case_[0] = kAverage;
}
inline void NeuralNetworkLayer::clear_average() {
  if (has_average()) {
    delete layer_.average_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::AverageLayerParams& NeuralNetworkLayer::average() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.average)
  return has_average()
      ? *layer_.average_
      : ::CoreML::Specification::AverageLayerParams::default_instance();
}
inline ::CoreML::Specification::AverageLayerParams* NeuralNetworkLayer::mutable_average() {
  if (!has_average()) {
    clear_layer();
    set_has_average();
    layer_.average_ = new ::CoreML::Specification::AverageLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.average)
  return layer_.average_;
}
inline ::CoreML::Specification::AverageLayerParams* NeuralNetworkLayer::release_average() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.average)
  if (has_average()) {
    clear_has_layer();
    ::CoreML::Specification::AverageLayerParams* temp = layer_.average_;
    layer_.average_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_average(::CoreML::Specification::AverageLayerParams* average) {
  clear_layer();
  if (average) {
    set_has_average();
    layer_.average_ = average;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.average)
}

// optional .CoreML.Specification.ScaleLayerParams scale = 245;
inline bool NeuralNetworkLayer::has_scale() const {
  return layer_case() == kScale;
}
inline void NeuralNetworkLayer::set_has_scale() {
  _oneof_case_[0] = kScale;
}
inline void NeuralNetworkLayer::clear_scale() {
  if (has_scale()) {
    delete layer_.scale_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::ScaleLayerParams& NeuralNetworkLayer::scale() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.scale)
  return has_scale()
      ? *layer_.scale_
      : ::CoreML::Specification::ScaleLayerParams::default_instance();
}
inline ::CoreML::Specification::ScaleLayerParams* NeuralNetworkLayer::mutable_scale() {
  if (!has_scale()) {
    clear_layer();
    set_has_scale();
    layer_.scale_ = new ::CoreML::Specification::ScaleLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.scale)
  return layer_.scale_;
}
inline ::CoreML::Specification::ScaleLayerParams* NeuralNetworkLayer::release_scale() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.scale)
  if (has_scale()) {
    clear_has_layer();
    ::CoreML::Specification::ScaleLayerParams* temp = layer_.scale_;
    layer_.scale_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_scale(::CoreML::Specification::ScaleLayerParams* scale) {
  clear_layer();
  if (scale) {
    set_has_scale();
    layer_.scale_ = scale;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.scale)
}

// optional .CoreML.Specification.BiasLayerParams bias = 250;
inline bool NeuralNetworkLayer::has_bias() const {
  return layer_case() == kBias;
}
inline void NeuralNetworkLayer::set_has_bias() {
  _oneof_case_[0] = kBias;
}
inline void NeuralNetworkLayer::clear_bias() {
  if (has_bias()) {
    delete layer_.bias_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::BiasLayerParams& NeuralNetworkLayer::bias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.bias)
  return has_bias()
      ? *layer_.bias_
      : ::CoreML::Specification::BiasLayerParams::default_instance();
}
inline ::CoreML::Specification::BiasLayerParams* NeuralNetworkLayer::mutable_bias() {
  if (!has_bias()) {
    clear_layer();
    set_has_bias();
    layer_.bias_ = new ::CoreML::Specification::BiasLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.bias)
  return layer_.bias_;
}
inline ::CoreML::Specification::BiasLayerParams* NeuralNetworkLayer::release_bias() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.bias)
  if (has_bias()) {
    clear_has_layer();
    ::CoreML::Specification::BiasLayerParams* temp = layer_.bias_;
    layer_.bias_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_bias(::CoreML::Specification::BiasLayerParams* bias) {
  clear_layer();
  if (bias) {
    set_has_bias();
    layer_.bias_ = bias;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.bias)
}

// optional .CoreML.Specification.MaxLayerParams max = 260;
inline bool NeuralNetworkLayer::has_max() const {
  return layer_case() == kMax;
}
inline void NeuralNetworkLayer::set_has_max() {
  _oneof_case_[0] = kMax;
}
inline void NeuralNetworkLayer::clear_max() {
  if (has_max()) {
    delete layer_.max_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::MaxLayerParams& NeuralNetworkLayer::max() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.max)
  return has_max()
      ? *layer_.max_
      : ::CoreML::Specification::MaxLayerParams::default_instance();
}
inline ::CoreML::Specification::MaxLayerParams* NeuralNetworkLayer::mutable_max() {
  if (!has_max()) {
    clear_layer();
    set_has_max();
    layer_.max_ = new ::CoreML::Specification::MaxLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.max)
  return layer_.max_;
}
inline ::CoreML::Specification::MaxLayerParams* NeuralNetworkLayer::release_max() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.max)
  if (has_max()) {
    clear_has_layer();
    ::CoreML::Specification::MaxLayerParams* temp = layer_.max_;
    layer_.max_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_max(::CoreML::Specification::MaxLayerParams* max) {
  clear_layer();
  if (max) {
    set_has_max();
    layer_.max_ = max;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.max)
}

// optional .CoreML.Specification.MinLayerParams min = 261;
inline bool NeuralNetworkLayer::has_min() const {
  return layer_case() == kMin;
}
inline void NeuralNetworkLayer::set_has_min() {
  _oneof_case_[0] = kMin;
}
inline void NeuralNetworkLayer::clear_min() {
  if (has_min()) {
    delete layer_.min_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::MinLayerParams& NeuralNetworkLayer::min() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.min)
  return has_min()
      ? *layer_.min_
      : ::CoreML::Specification::MinLayerParams::default_instance();
}
inline ::CoreML::Specification::MinLayerParams* NeuralNetworkLayer::mutable_min() {
  if (!has_min()) {
    clear_layer();
    set_has_min();
    layer_.min_ = new ::CoreML::Specification::MinLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.min)
  return layer_.min_;
}
inline ::CoreML::Specification::MinLayerParams* NeuralNetworkLayer::release_min() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.min)
  if (has_min()) {
    clear_has_layer();
    ::CoreML::Specification::MinLayerParams* temp = layer_.min_;
    layer_.min_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_min(::CoreML::Specification::MinLayerParams* min) {
  clear_layer();
  if (min) {
    set_has_min();
    layer_.min_ = min;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.min)
}

// optional .CoreML.Specification.DotProductLayerParams dot = 270;
inline bool NeuralNetworkLayer::has_dot() const {
  return layer_case() == kDot;
}
inline void NeuralNetworkLayer::set_has_dot() {
  _oneof_case_[0] = kDot;
}
inline void NeuralNetworkLayer::clear_dot() {
  if (has_dot()) {
    delete layer_.dot_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::DotProductLayerParams& NeuralNetworkLayer::dot() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.dot)
  return has_dot()
      ? *layer_.dot_
      : ::CoreML::Specification::DotProductLayerParams::default_instance();
}
inline ::CoreML::Specification::DotProductLayerParams* NeuralNetworkLayer::mutable_dot() {
  if (!has_dot()) {
    clear_layer();
    set_has_dot();
    layer_.dot_ = new ::CoreML::Specification::DotProductLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.dot)
  return layer_.dot_;
}
inline ::CoreML::Specification::DotProductLayerParams* NeuralNetworkLayer::release_dot() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.dot)
  if (has_dot()) {
    clear_has_layer();
    ::CoreML::Specification::DotProductLayerParams* temp = layer_.dot_;
    layer_.dot_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_dot(::CoreML::Specification::DotProductLayerParams* dot) {
  clear_layer();
  if (dot) {
    set_has_dot();
    layer_.dot_ = dot;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.dot)
}

// optional .CoreML.Specification.ReduceLayerParams reduce = 280;
inline bool NeuralNetworkLayer::has_reduce() const {
  return layer_case() == kReduce;
}
inline void NeuralNetworkLayer::set_has_reduce() {
  _oneof_case_[0] = kReduce;
}
inline void NeuralNetworkLayer::clear_reduce() {
  if (has_reduce()) {
    delete layer_.reduce_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::ReduceLayerParams& NeuralNetworkLayer::reduce() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.reduce)
  return has_reduce()
      ? *layer_.reduce_
      : ::CoreML::Specification::ReduceLayerParams::default_instance();
}
inline ::CoreML::Specification::ReduceLayerParams* NeuralNetworkLayer::mutable_reduce() {
  if (!has_reduce()) {
    clear_layer();
    set_has_reduce();
    layer_.reduce_ = new ::CoreML::Specification::ReduceLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.reduce)
  return layer_.reduce_;
}
inline ::CoreML::Specification::ReduceLayerParams* NeuralNetworkLayer::release_reduce() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.reduce)
  if (has_reduce()) {
    clear_has_layer();
    ::CoreML::Specification::ReduceLayerParams* temp = layer_.reduce_;
    layer_.reduce_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_reduce(::CoreML::Specification::ReduceLayerParams* reduce) {
  clear_layer();
  if (reduce) {
    set_has_reduce();
    layer_.reduce_ = reduce;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.reduce)
}

// optional .CoreML.Specification.LoadConstantLayerParams loadConstant = 290;
inline bool NeuralNetworkLayer::has_loadconstant() const {
  return layer_case() == kLoadConstant;
}
inline void NeuralNetworkLayer::set_has_loadconstant() {
  _oneof_case_[0] = kLoadConstant;
}
inline void NeuralNetworkLayer::clear_loadconstant() {
  if (has_loadconstant()) {
    delete layer_.loadconstant_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::LoadConstantLayerParams& NeuralNetworkLayer::loadconstant() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.loadConstant)
  return has_loadconstant()
      ? *layer_.loadconstant_
      : ::CoreML::Specification::LoadConstantLayerParams::default_instance();
}
inline ::CoreML::Specification::LoadConstantLayerParams* NeuralNetworkLayer::mutable_loadconstant() {
  if (!has_loadconstant()) {
    clear_layer();
    set_has_loadconstant();
    layer_.loadconstant_ = new ::CoreML::Specification::LoadConstantLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.loadConstant)
  return layer_.loadconstant_;
}
inline ::CoreML::Specification::LoadConstantLayerParams* NeuralNetworkLayer::release_loadconstant() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.loadConstant)
  if (has_loadconstant()) {
    clear_has_layer();
    ::CoreML::Specification::LoadConstantLayerParams* temp = layer_.loadconstant_;
    layer_.loadconstant_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_loadconstant(::CoreML::Specification::LoadConstantLayerParams* loadconstant) {
  clear_layer();
  if (loadconstant) {
    set_has_loadconstant();
    layer_.loadconstant_ = loadconstant;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.loadConstant)
}

// optional .CoreML.Specification.ReshapeLayerParams reshape = 300;
inline bool NeuralNetworkLayer::has_reshape() const {
  return layer_case() == kReshape;
}
inline void NeuralNetworkLayer::set_has_reshape() {
  _oneof_case_[0] = kReshape;
}
inline void NeuralNetworkLayer::clear_reshape() {
  if (has_reshape()) {
    delete layer_.reshape_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::ReshapeLayerParams& NeuralNetworkLayer::reshape() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.reshape)
  return has_reshape()
      ? *layer_.reshape_
      : ::CoreML::Specification::ReshapeLayerParams::default_instance();
}
inline ::CoreML::Specification::ReshapeLayerParams* NeuralNetworkLayer::mutable_reshape() {
  if (!has_reshape()) {
    clear_layer();
    set_has_reshape();
    layer_.reshape_ = new ::CoreML::Specification::ReshapeLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.reshape)
  return layer_.reshape_;
}
inline ::CoreML::Specification::ReshapeLayerParams* NeuralNetworkLayer::release_reshape() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.reshape)
  if (has_reshape()) {
    clear_has_layer();
    ::CoreML::Specification::ReshapeLayerParams* temp = layer_.reshape_;
    layer_.reshape_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_reshape(::CoreML::Specification::ReshapeLayerParams* reshape) {
  clear_layer();
  if (reshape) {
    set_has_reshape();
    layer_.reshape_ = reshape;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.reshape)
}

// optional .CoreML.Specification.FlattenLayerParams flatten = 301;
inline bool NeuralNetworkLayer::has_flatten() const {
  return layer_case() == kFlatten;
}
inline void NeuralNetworkLayer::set_has_flatten() {
  _oneof_case_[0] = kFlatten;
}
inline void NeuralNetworkLayer::clear_flatten() {
  if (has_flatten()) {
    delete layer_.flatten_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::FlattenLayerParams& NeuralNetworkLayer::flatten() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.flatten)
  return has_flatten()
      ? *layer_.flatten_
      : ::CoreML::Specification::FlattenLayerParams::default_instance();
}
inline ::CoreML::Specification::FlattenLayerParams* NeuralNetworkLayer::mutable_flatten() {
  if (!has_flatten()) {
    clear_layer();
    set_has_flatten();
    layer_.flatten_ = new ::CoreML::Specification::FlattenLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.flatten)
  return layer_.flatten_;
}
inline ::CoreML::Specification::FlattenLayerParams* NeuralNetworkLayer::release_flatten() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.flatten)
  if (has_flatten()) {
    clear_has_layer();
    ::CoreML::Specification::FlattenLayerParams* temp = layer_.flatten_;
    layer_.flatten_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_flatten(::CoreML::Specification::FlattenLayerParams* flatten) {
  clear_layer();
  if (flatten) {
    set_has_flatten();
    layer_.flatten_ = flatten;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.flatten)
}

// optional .CoreML.Specification.PermuteLayerParams permute = 310;
inline bool NeuralNetworkLayer::has_permute() const {
  return layer_case() == kPermute;
}
inline void NeuralNetworkLayer::set_has_permute() {
  _oneof_case_[0] = kPermute;
}
inline void NeuralNetworkLayer::clear_permute() {
  if (has_permute()) {
    delete layer_.permute_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::PermuteLayerParams& NeuralNetworkLayer::permute() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.permute)
  return has_permute()
      ? *layer_.permute_
      : ::CoreML::Specification::PermuteLayerParams::default_instance();
}
inline ::CoreML::Specification::PermuteLayerParams* NeuralNetworkLayer::mutable_permute() {
  if (!has_permute()) {
    clear_layer();
    set_has_permute();
    layer_.permute_ = new ::CoreML::Specification::PermuteLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.permute)
  return layer_.permute_;
}
inline ::CoreML::Specification::PermuteLayerParams* NeuralNetworkLayer::release_permute() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.permute)
  if (has_permute()) {
    clear_has_layer();
    ::CoreML::Specification::PermuteLayerParams* temp = layer_.permute_;
    layer_.permute_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_permute(::CoreML::Specification::PermuteLayerParams* permute) {
  clear_layer();
  if (permute) {
    set_has_permute();
    layer_.permute_ = permute;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.permute)
}

// optional .CoreML.Specification.ConcatLayerParams concat = 320;
inline bool NeuralNetworkLayer::has_concat() const {
  return layer_case() == kConcat;
}
inline void NeuralNetworkLayer::set_has_concat() {
  _oneof_case_[0] = kConcat;
}
inline void NeuralNetworkLayer::clear_concat() {
  if (has_concat()) {
    delete layer_.concat_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::ConcatLayerParams& NeuralNetworkLayer::concat() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.concat)
  return has_concat()
      ? *layer_.concat_
      : ::CoreML::Specification::ConcatLayerParams::default_instance();
}
inline ::CoreML::Specification::ConcatLayerParams* NeuralNetworkLayer::mutable_concat() {
  if (!has_concat()) {
    clear_layer();
    set_has_concat();
    layer_.concat_ = new ::CoreML::Specification::ConcatLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.concat)
  return layer_.concat_;
}
inline ::CoreML::Specification::ConcatLayerParams* NeuralNetworkLayer::release_concat() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.concat)
  if (has_concat()) {
    clear_has_layer();
    ::CoreML::Specification::ConcatLayerParams* temp = layer_.concat_;
    layer_.concat_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_concat(::CoreML::Specification::ConcatLayerParams* concat) {
  clear_layer();
  if (concat) {
    set_has_concat();
    layer_.concat_ = concat;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.concat)
}

// optional .CoreML.Specification.SplitLayerParams split = 330;
inline bool NeuralNetworkLayer::has_split() const {
  return layer_case() == kSplit;
}
inline void NeuralNetworkLayer::set_has_split() {
  _oneof_case_[0] = kSplit;
}
inline void NeuralNetworkLayer::clear_split() {
  if (has_split()) {
    delete layer_.split_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::SplitLayerParams& NeuralNetworkLayer::split() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.split)
  return has_split()
      ? *layer_.split_
      : ::CoreML::Specification::SplitLayerParams::default_instance();
}
inline ::CoreML::Specification::SplitLayerParams* NeuralNetworkLayer::mutable_split() {
  if (!has_split()) {
    clear_layer();
    set_has_split();
    layer_.split_ = new ::CoreML::Specification::SplitLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.split)
  return layer_.split_;
}
inline ::CoreML::Specification::SplitLayerParams* NeuralNetworkLayer::release_split() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.split)
  if (has_split()) {
    clear_has_layer();
    ::CoreML::Specification::SplitLayerParams* temp = layer_.split_;
    layer_.split_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_split(::CoreML::Specification::SplitLayerParams* split) {
  clear_layer();
  if (split) {
    set_has_split();
    layer_.split_ = split;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.split)
}

// optional .CoreML.Specification.SequenceRepeatLayerParams sequenceRepeat = 340;
inline bool NeuralNetworkLayer::has_sequencerepeat() const {
  return layer_case() == kSequenceRepeat;
}
inline void NeuralNetworkLayer::set_has_sequencerepeat() {
  _oneof_case_[0] = kSequenceRepeat;
}
inline void NeuralNetworkLayer::clear_sequencerepeat() {
  if (has_sequencerepeat()) {
    delete layer_.sequencerepeat_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::SequenceRepeatLayerParams& NeuralNetworkLayer::sequencerepeat() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.sequenceRepeat)
  return has_sequencerepeat()
      ? *layer_.sequencerepeat_
      : ::CoreML::Specification::SequenceRepeatLayerParams::default_instance();
}
inline ::CoreML::Specification::SequenceRepeatLayerParams* NeuralNetworkLayer::mutable_sequencerepeat() {
  if (!has_sequencerepeat()) {
    clear_layer();
    set_has_sequencerepeat();
    layer_.sequencerepeat_ = new ::CoreML::Specification::SequenceRepeatLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.sequenceRepeat)
  return layer_.sequencerepeat_;
}
inline ::CoreML::Specification::SequenceRepeatLayerParams* NeuralNetworkLayer::release_sequencerepeat() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.sequenceRepeat)
  if (has_sequencerepeat()) {
    clear_has_layer();
    ::CoreML::Specification::SequenceRepeatLayerParams* temp = layer_.sequencerepeat_;
    layer_.sequencerepeat_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_sequencerepeat(::CoreML::Specification::SequenceRepeatLayerParams* sequencerepeat) {
  clear_layer();
  if (sequencerepeat) {
    set_has_sequencerepeat();
    layer_.sequencerepeat_ = sequencerepeat;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.sequenceRepeat)
}

// optional .CoreML.Specification.SimpleRecurrentLayerParams simpleRecurrent = 400;
inline bool NeuralNetworkLayer::has_simplerecurrent() const {
  return layer_case() == kSimpleRecurrent;
}
inline void NeuralNetworkLayer::set_has_simplerecurrent() {
  _oneof_case_[0] = kSimpleRecurrent;
}
inline void NeuralNetworkLayer::clear_simplerecurrent() {
  if (has_simplerecurrent()) {
    delete layer_.simplerecurrent_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::SimpleRecurrentLayerParams& NeuralNetworkLayer::simplerecurrent() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.simpleRecurrent)
  return has_simplerecurrent()
      ? *layer_.simplerecurrent_
      : ::CoreML::Specification::SimpleRecurrentLayerParams::default_instance();
}
inline ::CoreML::Specification::SimpleRecurrentLayerParams* NeuralNetworkLayer::mutable_simplerecurrent() {
  if (!has_simplerecurrent()) {
    clear_layer();
    set_has_simplerecurrent();
    layer_.simplerecurrent_ = new ::CoreML::Specification::SimpleRecurrentLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.simpleRecurrent)
  return layer_.simplerecurrent_;
}
inline ::CoreML::Specification::SimpleRecurrentLayerParams* NeuralNetworkLayer::release_simplerecurrent() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.simpleRecurrent)
  if (has_simplerecurrent()) {
    clear_has_layer();
    ::CoreML::Specification::SimpleRecurrentLayerParams* temp = layer_.simplerecurrent_;
    layer_.simplerecurrent_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_simplerecurrent(::CoreML::Specification::SimpleRecurrentLayerParams* simplerecurrent) {
  clear_layer();
  if (simplerecurrent) {
    set_has_simplerecurrent();
    layer_.simplerecurrent_ = simplerecurrent;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.simpleRecurrent)
}

// optional .CoreML.Specification.GRULayerParams gru = 410;
inline bool NeuralNetworkLayer::has_gru() const {
  return layer_case() == kGru;
}
inline void NeuralNetworkLayer::set_has_gru() {
  _oneof_case_[0] = kGru;
}
inline void NeuralNetworkLayer::clear_gru() {
  if (has_gru()) {
    delete layer_.gru_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::GRULayerParams& NeuralNetworkLayer::gru() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.gru)
  return has_gru()
      ? *layer_.gru_
      : ::CoreML::Specification::GRULayerParams::default_instance();
}
inline ::CoreML::Specification::GRULayerParams* NeuralNetworkLayer::mutable_gru() {
  if (!has_gru()) {
    clear_layer();
    set_has_gru();
    layer_.gru_ = new ::CoreML::Specification::GRULayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.gru)
  return layer_.gru_;
}
inline ::CoreML::Specification::GRULayerParams* NeuralNetworkLayer::release_gru() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.gru)
  if (has_gru()) {
    clear_has_layer();
    ::CoreML::Specification::GRULayerParams* temp = layer_.gru_;
    layer_.gru_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_gru(::CoreML::Specification::GRULayerParams* gru) {
  clear_layer();
  if (gru) {
    set_has_gru();
    layer_.gru_ = gru;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.gru)
}

// optional .CoreML.Specification.UniDirectionalLSTMLayerParams uniDirectionalLSTM = 420;
inline bool NeuralNetworkLayer::has_unidirectionallstm() const {
  return layer_case() == kUniDirectionalLSTM;
}
inline void NeuralNetworkLayer::set_has_unidirectionallstm() {
  _oneof_case_[0] = kUniDirectionalLSTM;
}
inline void NeuralNetworkLayer::clear_unidirectionallstm() {
  if (has_unidirectionallstm()) {
    delete layer_.unidirectionallstm_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::UniDirectionalLSTMLayerParams& NeuralNetworkLayer::unidirectionallstm() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.uniDirectionalLSTM)
  return has_unidirectionallstm()
      ? *layer_.unidirectionallstm_
      : ::CoreML::Specification::UniDirectionalLSTMLayerParams::default_instance();
}
inline ::CoreML::Specification::UniDirectionalLSTMLayerParams* NeuralNetworkLayer::mutable_unidirectionallstm() {
  if (!has_unidirectionallstm()) {
    clear_layer();
    set_has_unidirectionallstm();
    layer_.unidirectionallstm_ = new ::CoreML::Specification::UniDirectionalLSTMLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.uniDirectionalLSTM)
  return layer_.unidirectionallstm_;
}
inline ::CoreML::Specification::UniDirectionalLSTMLayerParams* NeuralNetworkLayer::release_unidirectionallstm() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.uniDirectionalLSTM)
  if (has_unidirectionallstm()) {
    clear_has_layer();
    ::CoreML::Specification::UniDirectionalLSTMLayerParams* temp = layer_.unidirectionallstm_;
    layer_.unidirectionallstm_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_unidirectionallstm(::CoreML::Specification::UniDirectionalLSTMLayerParams* unidirectionallstm) {
  clear_layer();
  if (unidirectionallstm) {
    set_has_unidirectionallstm();
    layer_.unidirectionallstm_ = unidirectionallstm;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.uniDirectionalLSTM)
}

// optional .CoreML.Specification.BiDirectionalLSTMLayerParams biDirectionalLSTM = 430;
inline bool NeuralNetworkLayer::has_bidirectionallstm() const {
  return layer_case() == kBiDirectionalLSTM;
}
inline void NeuralNetworkLayer::set_has_bidirectionallstm() {
  _oneof_case_[0] = kBiDirectionalLSTM;
}
inline void NeuralNetworkLayer::clear_bidirectionallstm() {
  if (has_bidirectionallstm()) {
    delete layer_.bidirectionallstm_;
    clear_has_layer();
  }
}
inline  const ::CoreML::Specification::BiDirectionalLSTMLayerParams& NeuralNetworkLayer::bidirectionallstm() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.biDirectionalLSTM)
  return has_bidirectionallstm()
      ? *layer_.bidirectionallstm_
      : ::CoreML::Specification::BiDirectionalLSTMLayerParams::default_instance();
}
inline ::CoreML::Specification::BiDirectionalLSTMLayerParams* NeuralNetworkLayer::mutable_bidirectionallstm() {
  if (!has_bidirectionallstm()) {
    clear_layer();
    set_has_bidirectionallstm();
    layer_.bidirectionallstm_ = new ::CoreML::Specification::BiDirectionalLSTMLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.biDirectionalLSTM)
  return layer_.bidirectionallstm_;
}
inline ::CoreML::Specification::BiDirectionalLSTMLayerParams* NeuralNetworkLayer::release_bidirectionallstm() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.biDirectionalLSTM)
  if (has_bidirectionallstm()) {
    clear_has_layer();
    ::CoreML::Specification::BiDirectionalLSTMLayerParams* temp = layer_.bidirectionallstm_;
    layer_.bidirectionallstm_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkLayer::set_allocated_bidirectionallstm(::CoreML::Specification::BiDirectionalLSTMLayerParams* bidirectionallstm) {
  clear_layer();
  if (bidirectionallstm) {
    set_has_bidirectionallstm();
    layer_.bidirectionallstm_ = bidirectionallstm;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.biDirectionalLSTM)
}

inline bool NeuralNetworkLayer::has_layer() const {
  return layer_case() != LAYER_NOT_SET;
}
inline void NeuralNetworkLayer::clear_has_layer() {
  _oneof_case_[0] = LAYER_NOT_SET;
}
inline NeuralNetworkLayer::LayerCase NeuralNetworkLayer::layer_case() const {
  return NeuralNetworkLayer::LayerCase(_oneof_case_[0]);
}
inline const NeuralNetworkLayer* NeuralNetworkLayer::internal_default_instance() {
  return &NeuralNetworkLayer_default_instance_.get();
}
// -------------------------------------------------------------------

// BorderAmounts_EdgeSizes

// optional uint64 startEdgeSize = 1;
inline void BorderAmounts_EdgeSizes::clear_startedgesize() {
  startedgesize_ = GOOGLE_ULONGLONG(0);
}
inline ::google::protobuf::uint64 BorderAmounts_EdgeSizes::startedgesize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BorderAmounts.EdgeSizes.startEdgeSize)
  return startedgesize_;
}
inline void BorderAmounts_EdgeSizes::set_startedgesize(::google::protobuf::uint64 value) {

  startedgesize_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.BorderAmounts.EdgeSizes.startEdgeSize)
}

// optional uint64 endEdgeSize = 2;
inline void BorderAmounts_EdgeSizes::clear_endedgesize() {
  endedgesize_ = GOOGLE_ULONGLONG(0);
}
inline ::google::protobuf::uint64 BorderAmounts_EdgeSizes::endedgesize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BorderAmounts.EdgeSizes.endEdgeSize)
  return endedgesize_;
}
inline void BorderAmounts_EdgeSizes::set_endedgesize(::google::protobuf::uint64 value) {

  endedgesize_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.BorderAmounts.EdgeSizes.endEdgeSize)
}

inline const BorderAmounts_EdgeSizes* BorderAmounts_EdgeSizes::internal_default_instance() {
  return &BorderAmounts_EdgeSizes_default_instance_.get();
}
// -------------------------------------------------------------------

// BorderAmounts

// repeated .CoreML.Specification.BorderAmounts.EdgeSizes borderAmounts = 10;
inline int BorderAmounts::borderamounts_size() const {
  return borderamounts_.size();
}
inline void BorderAmounts::clear_borderamounts() {
  borderamounts_.Clear();
}
inline const ::CoreML::Specification::BorderAmounts_EdgeSizes& BorderAmounts::borderamounts(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BorderAmounts.borderAmounts)
  return borderamounts_.Get(index);
}
inline ::CoreML::Specification::BorderAmounts_EdgeSizes* BorderAmounts::mutable_borderamounts(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BorderAmounts.borderAmounts)
  return borderamounts_.Mutable(index);
}
inline ::CoreML::Specification::BorderAmounts_EdgeSizes* BorderAmounts::add_borderamounts() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.BorderAmounts.borderAmounts)
  return borderamounts_.Add();
}
inline ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::BorderAmounts_EdgeSizes >*
BorderAmounts::mutable_borderamounts() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.BorderAmounts.borderAmounts)
  return &borderamounts_;
}
inline const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::BorderAmounts_EdgeSizes >&
BorderAmounts::borderamounts() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.BorderAmounts.borderAmounts)
  return borderamounts_;
}

inline const BorderAmounts* BorderAmounts::internal_default_instance() {
  return &BorderAmounts_default_instance_.get();
}
// -------------------------------------------------------------------

// ValidPadding

// optional .CoreML.Specification.BorderAmounts paddingAmounts = 1;
inline bool ValidPadding::has_paddingamounts() const {
  return this != internal_default_instance() && paddingamounts_ != NULL;
}
inline void ValidPadding::clear_paddingamounts() {
  if (GetArenaNoVirtual() == NULL && paddingamounts_ != NULL) delete paddingamounts_;
  paddingamounts_ = NULL;
}
inline const ::CoreML::Specification::BorderAmounts& ValidPadding::paddingamounts() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ValidPadding.paddingAmounts)
  return paddingamounts_ != NULL ? *paddingamounts_
                         : *::CoreML::Specification::BorderAmounts::internal_default_instance();
}
inline ::CoreML::Specification::BorderAmounts* ValidPadding::mutable_paddingamounts() {

  if (paddingamounts_ == NULL) {
    paddingamounts_ = new ::CoreML::Specification::BorderAmounts;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ValidPadding.paddingAmounts)
  return paddingamounts_;
}
inline ::CoreML::Specification::BorderAmounts* ValidPadding::release_paddingamounts() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ValidPadding.paddingAmounts)

  ::CoreML::Specification::BorderAmounts* temp = paddingamounts_;
  paddingamounts_ = NULL;
  return temp;
}
inline void ValidPadding::set_allocated_paddingamounts(::CoreML::Specification::BorderAmounts* paddingamounts) {
  delete paddingamounts_;
  paddingamounts_ = paddingamounts;
  if (paddingamounts) {

  } else {

  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ValidPadding.paddingAmounts)
}

inline const ValidPadding* ValidPadding::internal_default_instance() {
  return &ValidPadding_default_instance_.get();
}
// -------------------------------------------------------------------

// SamePadding

// optional .CoreML.Specification.SamePadding.SamePaddingMode asymmetryMode = 1;
inline void SamePadding::clear_asymmetrymode() {
  asymmetrymode_ = 0;
}
inline ::CoreML::Specification::SamePadding_SamePaddingMode SamePadding::asymmetrymode() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SamePadding.asymmetryMode)
  return static_cast< ::CoreML::Specification::SamePadding_SamePaddingMode >(asymmetrymode_);
}
inline void SamePadding::set_asymmetrymode(::CoreML::Specification::SamePadding_SamePaddingMode value) {

  asymmetrymode_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SamePadding.asymmetryMode)
}

inline const SamePadding* SamePadding::internal_default_instance() {
  return &SamePadding_default_instance_.get();
}
// -------------------------------------------------------------------

// WeightParams

// repeated float floatValue = 1;
inline int WeightParams::floatvalue_size() const {
  return floatvalue_.size();
}
inline void WeightParams::clear_floatvalue() {
  floatvalue_.Clear();
}
inline float WeightParams::floatvalue(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.WeightParams.floatValue)
  return floatvalue_.Get(index);
}
inline void WeightParams::set_floatvalue(int index, float value) {
  floatvalue_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.WeightParams.floatValue)
}
inline void WeightParams::add_floatvalue(float value) {
  floatvalue_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.WeightParams.floatValue)
}
inline const ::google::protobuf::RepeatedField< float >&
WeightParams::floatvalue() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.WeightParams.floatValue)
  return floatvalue_;
}
inline ::google::protobuf::RepeatedField< float >*
WeightParams::mutable_floatvalue() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.WeightParams.floatValue)
  return &floatvalue_;
}

inline const WeightParams* WeightParams::internal_default_instance() {
  return &WeightParams_default_instance_.get();
}
// -------------------------------------------------------------------

// ConvolutionLayerParams

// optional uint64 outputChannels = 1;
inline void ConvolutionLayerParams::clear_outputchannels() {
  outputchannels_ = GOOGLE_ULONGLONG(0);
}
inline ::google::protobuf::uint64 ConvolutionLayerParams::outputchannels() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.outputChannels)
  return outputchannels_;
}
inline void ConvolutionLayerParams::set_outputchannels(::google::protobuf::uint64 value) {

  outputchannels_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConvolutionLayerParams.outputChannels)
}

// optional uint64 kernelChannels = 2;
inline void ConvolutionLayerParams::clear_kernelchannels() {
  kernelchannels_ = GOOGLE_ULONGLONG(0);
}
inline ::google::protobuf::uint64 ConvolutionLayerParams::kernelchannels() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.kernelChannels)
  return kernelchannels_;
}
inline void ConvolutionLayerParams::set_kernelchannels(::google::protobuf::uint64 value) {

  kernelchannels_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConvolutionLayerParams.kernelChannels)
}

// optional uint64 nGroups = 10;
inline void ConvolutionLayerParams::clear_ngroups() {
  ngroups_ = GOOGLE_ULONGLONG(0);
}
inline ::google::protobuf::uint64 ConvolutionLayerParams::ngroups() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.nGroups)
  return ngroups_;
}
inline void ConvolutionLayerParams::set_ngroups(::google::protobuf::uint64 value) {

  ngroups_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConvolutionLayerParams.nGroups)
}

// repeated uint64 kernelSize = 20;
inline int ConvolutionLayerParams::kernelsize_size() const {
  return kernelsize_.size();
}
inline void ConvolutionLayerParams::clear_kernelsize() {
  kernelsize_.Clear();
}
inline ::google::protobuf::uint64 ConvolutionLayerParams::kernelsize(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.kernelSize)
  return kernelsize_.Get(index);
}
inline void ConvolutionLayerParams::set_kernelsize(int index, ::google::protobuf::uint64 value) {
  kernelsize_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConvolutionLayerParams.kernelSize)
}
inline void ConvolutionLayerParams::add_kernelsize(::google::protobuf::uint64 value) {
  kernelsize_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ConvolutionLayerParams.kernelSize)
}
inline const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
ConvolutionLayerParams::kernelsize() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ConvolutionLayerParams.kernelSize)
  return kernelsize_;
}
inline ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
ConvolutionLayerParams::mutable_kernelsize() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ConvolutionLayerParams.kernelSize)
  return &kernelsize_;
}

// repeated uint64 stride = 30;
inline int ConvolutionLayerParams::stride_size() const {
  return stride_.size();
}
inline void ConvolutionLayerParams::clear_stride() {
  stride_.Clear();
}
inline ::google::protobuf::uint64 ConvolutionLayerParams::stride(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.stride)
  return stride_.Get(index);
}
inline void ConvolutionLayerParams::set_stride(int index, ::google::protobuf::uint64 value) {
  stride_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConvolutionLayerParams.stride)
}
inline void ConvolutionLayerParams::add_stride(::google::protobuf::uint64 value) {
  stride_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ConvolutionLayerParams.stride)
}
inline const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
ConvolutionLayerParams::stride() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ConvolutionLayerParams.stride)
  return stride_;
}
inline ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
ConvolutionLayerParams::mutable_stride() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ConvolutionLayerParams.stride)
  return &stride_;
}

// repeated uint64 dilationFactor = 40;
inline int ConvolutionLayerParams::dilationfactor_size() const {
  return dilationfactor_.size();
}
inline void ConvolutionLayerParams::clear_dilationfactor() {
  dilationfactor_.Clear();
}
inline ::google::protobuf::uint64 ConvolutionLayerParams::dilationfactor(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.dilationFactor)
  return dilationfactor_.Get(index);
}
inline void ConvolutionLayerParams::set_dilationfactor(int index, ::google::protobuf::uint64 value) {
  dilationfactor_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConvolutionLayerParams.dilationFactor)
}
inline void ConvolutionLayerParams::add_dilationfactor(::google::protobuf::uint64 value) {
  dilationfactor_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ConvolutionLayerParams.dilationFactor)
}
inline const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
ConvolutionLayerParams::dilationfactor() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ConvolutionLayerParams.dilationFactor)
  return dilationfactor_;
}
inline ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
ConvolutionLayerParams::mutable_dilationfactor() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ConvolutionLayerParams.dilationFactor)
  return &dilationfactor_;
}

// optional .CoreML.Specification.ValidPadding valid = 50;
inline bool ConvolutionLayerParams::has_valid() const {
  return ConvolutionPaddingType_case() == kValid;
}
inline void ConvolutionLayerParams::set_has_valid() {
  _oneof_case_[0] = kValid;
}
inline void ConvolutionLayerParams::clear_valid() {
  if (has_valid()) {
    delete ConvolutionPaddingType_.valid_;
    clear_has_ConvolutionPaddingType();
  }
}
inline  const ::CoreML::Specification::ValidPadding& ConvolutionLayerParams::valid() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.valid)
  return has_valid()
      ? *ConvolutionPaddingType_.valid_
      : ::CoreML::Specification::ValidPadding::default_instance();
}
inline ::CoreML::Specification::ValidPadding* ConvolutionLayerParams::mutable_valid() {
  if (!has_valid()) {
    clear_ConvolutionPaddingType();
    set_has_valid();
    ConvolutionPaddingType_.valid_ = new ::CoreML::Specification::ValidPadding;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ConvolutionLayerParams.valid)
  return ConvolutionPaddingType_.valid_;
}
inline ::CoreML::Specification::ValidPadding* ConvolutionLayerParams::release_valid() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ConvolutionLayerParams.valid)
  if (has_valid()) {
    clear_has_ConvolutionPaddingType();
    ::CoreML::Specification::ValidPadding* temp = ConvolutionPaddingType_.valid_;
    ConvolutionPaddingType_.valid_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void ConvolutionLayerParams::set_allocated_valid(::CoreML::Specification::ValidPadding* valid) {
  clear_ConvolutionPaddingType();
  if (valid) {
    set_has_valid();
    ConvolutionPaddingType_.valid_ = valid;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ConvolutionLayerParams.valid)
}

// optional .CoreML.Specification.SamePadding same = 51;
inline bool ConvolutionLayerParams::has_same() const {
  return ConvolutionPaddingType_case() == kSame;
}
inline void ConvolutionLayerParams::set_has_same() {
  _oneof_case_[0] = kSame;
}
inline void ConvolutionLayerParams::clear_same() {
  if (has_same()) {
    delete ConvolutionPaddingType_.same_;
    clear_has_ConvolutionPaddingType();
  }
}
inline  const ::CoreML::Specification::SamePadding& ConvolutionLayerParams::same() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.same)
  return has_same()
      ? *ConvolutionPaddingType_.same_
      : ::CoreML::Specification::SamePadding::default_instance();
}
inline ::CoreML::Specification::SamePadding* ConvolutionLayerParams::mutable_same() {
  if (!has_same()) {
    clear_ConvolutionPaddingType();
    set_has_same();
    ConvolutionPaddingType_.same_ = new ::CoreML::Specification::SamePadding;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ConvolutionLayerParams.same)
  return ConvolutionPaddingType_.same_;
}
inline ::CoreML::Specification::SamePadding* ConvolutionLayerParams::release_same() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ConvolutionLayerParams.same)
  if (has_same()) {
    clear_has_ConvolutionPaddingType();
    ::CoreML::Specification::SamePadding* temp = ConvolutionPaddingType_.same_;
    ConvolutionPaddingType_.same_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void ConvolutionLayerParams::set_allocated_same(::CoreML::Specification::SamePadding* same) {
  clear_ConvolutionPaddingType();
  if (same) {
    set_has_same();
    ConvolutionPaddingType_.same_ = same;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ConvolutionLayerParams.same)
}

// optional bool isDeconvolution = 60;
inline void ConvolutionLayerParams::clear_isdeconvolution() {
  isdeconvolution_ = false;
}
inline bool ConvolutionLayerParams::isdeconvolution() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.isDeconvolution)
  return isdeconvolution_;
}
inline void ConvolutionLayerParams::set_isdeconvolution(bool value) {

  isdeconvolution_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConvolutionLayerParams.isDeconvolution)
}

// optional bool hasBias = 70;
inline void ConvolutionLayerParams::clear_hasbias() {
  hasbias_ = false;
}
inline bool ConvolutionLayerParams::hasbias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.hasBias)
  return hasbias_;
}
inline void ConvolutionLayerParams::set_hasbias(bool value) {

  hasbias_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConvolutionLayerParams.hasBias)
}

// optional .CoreML.Specification.WeightParams weights = 90;
inline bool ConvolutionLayerParams::has_weights() const {
  return this != internal_default_instance() && weights_ != NULL;
}
inline void ConvolutionLayerParams::clear_weights() {
  if (GetArenaNoVirtual() == NULL && weights_ != NULL) delete weights_;
  weights_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& ConvolutionLayerParams::weights() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.weights)
  return weights_ != NULL ? *weights_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* ConvolutionLayerParams::mutable_weights() {

  if (weights_ == NULL) {
    weights_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ConvolutionLayerParams.weights)
  return weights_;
}
inline ::CoreML::Specification::WeightParams* ConvolutionLayerParams::release_weights() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ConvolutionLayerParams.weights)

  ::CoreML::Specification::WeightParams* temp = weights_;
  weights_ = NULL;
  return temp;
}
inline void ConvolutionLayerParams::set_allocated_weights(::CoreML::Specification::WeightParams* weights) {
  delete weights_;
  weights_ = weights;
  if (weights) {

  } else {

  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ConvolutionLayerParams.weights)
}

// optional .CoreML.Specification.WeightParams bias = 91;
inline bool ConvolutionLayerParams::has_bias() const {
  return this != internal_default_instance() && bias_ != NULL;
}
inline void ConvolutionLayerParams::clear_bias() {
  if (GetArenaNoVirtual() == NULL && bias_ != NULL) delete bias_;
  bias_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& ConvolutionLayerParams::bias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.bias)
  return bias_ != NULL ? *bias_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* ConvolutionLayerParams::mutable_bias() {

  if (bias_ == NULL) {
    bias_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ConvolutionLayerParams.bias)
  return bias_;
}
inline ::CoreML::Specification::WeightParams* ConvolutionLayerParams::release_bias() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ConvolutionLayerParams.bias)

  ::CoreML::Specification::WeightParams* temp = bias_;
  bias_ = NULL;
  return temp;
}
inline void ConvolutionLayerParams::set_allocated_bias(::CoreML::Specification::WeightParams* bias) {
  delete bias_;
  bias_ = bias;
  if (bias) {

  } else {

  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ConvolutionLayerParams.bias)
}

// repeated uint64 outputShape = 100;
inline int ConvolutionLayerParams::outputshape_size() const {
  return outputshape_.size();
}
inline void ConvolutionLayerParams::clear_outputshape() {
  outputshape_.Clear();
}
inline ::google::protobuf::uint64 ConvolutionLayerParams::outputshape(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.outputShape)
  return outputshape_.Get(index);
}
inline void ConvolutionLayerParams::set_outputshape(int index, ::google::protobuf::uint64 value) {
  outputshape_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConvolutionLayerParams.outputShape)
}
inline void ConvolutionLayerParams::add_outputshape(::google::protobuf::uint64 value) {
  outputshape_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ConvolutionLayerParams.outputShape)
}
inline const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
ConvolutionLayerParams::outputshape() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ConvolutionLayerParams.outputShape)
  return outputshape_;
}
inline ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
ConvolutionLayerParams::mutable_outputshape() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ConvolutionLayerParams.outputShape)
  return &outputshape_;
}

inline bool ConvolutionLayerParams::has_ConvolutionPaddingType() const {
  return ConvolutionPaddingType_case() != CONVOLUTIONPADDINGTYPE_NOT_SET;
}
inline void ConvolutionLayerParams::clear_has_ConvolutionPaddingType() {
  _oneof_case_[0] = CONVOLUTIONPADDINGTYPE_NOT_SET;
}
inline ConvolutionLayerParams::ConvolutionPaddingTypeCase ConvolutionLayerParams::ConvolutionPaddingType_case() const {
  return ConvolutionLayerParams::ConvolutionPaddingTypeCase(_oneof_case_[0]);
}
inline const ConvolutionLayerParams* ConvolutionLayerParams::internal_default_instance() {
  return &ConvolutionLayerParams_default_instance_.get();
}
// -------------------------------------------------------------------

// InnerProductLayerParams

// optional uint64 inputChannels = 1;
inline void InnerProductLayerParams::clear_inputchannels() {
  inputchannels_ = GOOGLE_ULONGLONG(0);
}
inline ::google::protobuf::uint64 InnerProductLayerParams::inputchannels() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.InnerProductLayerParams.inputChannels)
  return inputchannels_;
}
inline void InnerProductLayerParams::set_inputchannels(::google::protobuf::uint64 value) {

  inputchannels_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.InnerProductLayerParams.inputChannels)
}

// optional uint64 outputChannels = 2;
inline void InnerProductLayerParams::clear_outputchannels() {
  outputchannels_ = GOOGLE_ULONGLONG(0);
}
inline ::google::protobuf::uint64 InnerProductLayerParams::outputchannels() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.InnerProductLayerParams.outputChannels)
  return outputchannels_;
}
inline void InnerProductLayerParams::set_outputchannels(::google::protobuf::uint64 value) {

  outputchannels_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.InnerProductLayerParams.outputChannels)
}

// optional bool hasBias = 10;
inline void InnerProductLayerParams::clear_hasbias() {
  hasbias_ = false;
}
inline bool InnerProductLayerParams::hasbias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.InnerProductLayerParams.hasBias)
  return hasbias_;
}
inline void InnerProductLayerParams::set_hasbias(bool value) {

  hasbias_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.InnerProductLayerParams.hasBias)
}

// optional .CoreML.Specification.WeightParams weights = 20;
inline bool InnerProductLayerParams::has_weights() const {
  return this != internal_default_instance() && weights_ != NULL;
}
inline void InnerProductLayerParams::clear_weights() {
  if (GetArenaNoVirtual() == NULL && weights_ != NULL) delete weights_;
  weights_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& InnerProductLayerParams::weights() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.InnerProductLayerParams.weights)
  return weights_ != NULL ? *weights_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* InnerProductLayerParams::mutable_weights() {

  if (weights_ == NULL) {
    weights_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.InnerProductLayerParams.weights)
  return weights_;
}
inline ::CoreML::Specification::WeightParams* InnerProductLayerParams::release_weights() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.InnerProductLayerParams.weights)

  ::CoreML::Specification::WeightParams* temp = weights_;
  weights_ = NULL;
  return temp;
}
inline void InnerProductLayerParams::set_allocated_weights(::CoreML::Specification::WeightParams* weights) {
  delete weights_;
  weights_ = weights;
  if (weights) {

  } else {

  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.InnerProductLayerParams.weights)
}

// optional .CoreML.Specification.WeightParams bias = 21;
inline bool InnerProductLayerParams::has_bias() const {
  return this != internal_default_instance() && bias_ != NULL;
}
inline void InnerProductLayerParams::clear_bias() {
  if (GetArenaNoVirtual() == NULL && bias_ != NULL) delete bias_;
  bias_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& InnerProductLayerParams::bias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.InnerProductLayerParams.bias)
  return bias_ != NULL ? *bias_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* InnerProductLayerParams::mutable_bias() {

  if (bias_ == NULL) {
    bias_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.InnerProductLayerParams.bias)
  return bias_;
}
inline ::CoreML::Specification::WeightParams* InnerProductLayerParams::release_bias() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.InnerProductLayerParams.bias)

  ::CoreML::Specification::WeightParams* temp = bias_;
  bias_ = NULL;
  return temp;
}
inline void InnerProductLayerParams::set_allocated_bias(::CoreML::Specification::WeightParams* bias) {
  delete bias_;
  bias_ = bias;
  if (bias) {

  } else {

  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.InnerProductLayerParams.bias)
}

inline const InnerProductLayerParams* InnerProductLayerParams::internal_default_instance() {
  return &InnerProductLayerParams_default_instance_.get();
}
// -------------------------------------------------------------------

// EmbeddingLayerParams

// optional uint64 inputDim = 1;
inline void EmbeddingLayerParams::clear_inputdim() {
  inputdim_ = GOOGLE_ULONGLONG(0);
}
inline ::google::protobuf::uint64 EmbeddingLayerParams::inputdim() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.EmbeddingLayerParams.inputDim)
  return inputdim_;
}
inline void EmbeddingLayerParams::set_inputdim(::google::protobuf::uint64 value) {

  inputdim_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.EmbeddingLayerParams.inputDim)
}

// optional uint64 outputChannels = 2;
inline void EmbeddingLayerParams::clear_outputchannels() {
  outputchannels_ = GOOGLE_ULONGLONG(0);
}
inline ::google::protobuf::uint64 EmbeddingLayerParams::outputchannels() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.EmbeddingLayerParams.outputChannels)
  return outputchannels_;
}
inline void EmbeddingLayerParams::set_outputchannels(::google::protobuf::uint64 value) {

  outputchannels_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.EmbeddingLayerParams.outputChannels)
}

// optional bool hasBias = 10;
inline void EmbeddingLayerParams::clear_hasbias() {
  hasbias_ = false;
}
inline bool EmbeddingLayerParams::hasbias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.EmbeddingLayerParams.hasBias)
  return hasbias_;
}
inline void EmbeddingLayerParams::set_hasbias(bool value) {

  hasbias_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.EmbeddingLayerParams.hasBias)
}

// optional .CoreML.Specification.WeightParams weights = 20;
inline bool EmbeddingLayerParams::has_weights() const {
  return this != internal_default_instance() && weights_ != NULL;
}
inline void EmbeddingLayerParams::clear_weights() {
  if (GetArenaNoVirtual() == NULL && weights_ != NULL) delete weights_;
  weights_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& EmbeddingLayerParams::weights() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.EmbeddingLayerParams.weights)
  return weights_ != NULL ? *weights_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* EmbeddingLayerParams::mutable_weights() {

  if (weights_ == NULL) {
    weights_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.EmbeddingLayerParams.weights)
  return weights_;
}
inline ::CoreML::Specification::WeightParams* EmbeddingLayerParams::release_weights() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.EmbeddingLayerParams.weights)

  ::CoreML::Specification::WeightParams* temp = weights_;
  weights_ = NULL;
  return temp;
}
inline void EmbeddingLayerParams::set_allocated_weights(::CoreML::Specification::WeightParams* weights) {
  delete weights_;
  weights_ = weights;
  if (weights) {

  } else {

  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.EmbeddingLayerParams.weights)
}

// optional .CoreML.Specification.WeightParams bias = 21;
inline bool EmbeddingLayerParams::has_bias() const {
  return this != internal_default_instance() && bias_ != NULL;
}
inline void EmbeddingLayerParams::clear_bias() {
  if (GetArenaNoVirtual() == NULL && bias_ != NULL) delete bias_;
  bias_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& EmbeddingLayerParams::bias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.EmbeddingLayerParams.bias)
  return bias_ != NULL ? *bias_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* EmbeddingLayerParams::mutable_bias() {

  if (bias_ == NULL) {
    bias_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.EmbeddingLayerParams.bias)
  return bias_;
}
inline ::CoreML::Specification::WeightParams* EmbeddingLayerParams::release_bias() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.EmbeddingLayerParams.bias)

  ::CoreML::Specification::WeightParams* temp = bias_;
  bias_ = NULL;
  return temp;
}
inline void EmbeddingLayerParams::set_allocated_bias(::CoreML::Specification::WeightParams* bias) {
  delete bias_;
  bias_ = bias;
  if (bias) {

  } else {

  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.EmbeddingLayerParams.bias)
}

inline const EmbeddingLayerParams* EmbeddingLayerParams::internal_default_instance() {
  return &EmbeddingLayerParams_default_instance_.get();
}
// -------------------------------------------------------------------

// BatchnormLayerParams

// optional uint64 channels = 1;
inline void BatchnormLayerParams::clear_channels() {
  channels_ = GOOGLE_ULONGLONG(0);
}
inline ::google::protobuf::uint64 BatchnormLayerParams::channels() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchnormLayerParams.channels)
  return channels_;
}
inline void BatchnormLayerParams::set_channels(::google::protobuf::uint64 value) {

  channels_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.BatchnormLayerParams.channels)
}

// optional bool computeMeanVar = 5;
inline void BatchnormLayerParams::clear_computemeanvar() {
  computemeanvar_ = false;
}
inline bool BatchnormLayerParams::computemeanvar() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchnormLayerParams.computeMeanVar)
  return computemeanvar_;
}
inline void BatchnormLayerParams::set_computemeanvar(bool value) {

  computemeanvar_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.BatchnormLayerParams.computeMeanVar)
}

// optional bool instanceNormalization = 6;
inline void BatchnormLayerParams::clear_instancenormalization() {
  instancenormalization_ = false;
}
inline bool BatchnormLayerParams::instancenormalization() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchnormLayerParams.instanceNormalization)
  return instancenormalization_;
}
inline void BatchnormLayerParams::set_instancenormalization(bool value) {

  instancenormalization_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.BatchnormLayerParams.instanceNormalization)
}

// optional float epsilon = 10;
inline void BatchnormLayerParams::clear_epsilon() {
  epsilon_ = 0;
}
inline float BatchnormLayerParams::epsilon() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchnormLayerParams.epsilon)
  return epsilon_;
}
inline void BatchnormLayerParams::set_epsilon(float value) {

  epsilon_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.BatchnormLayerParams.epsilon)
}

// optional .CoreML.Specification.WeightParams gamma = 15;
inline bool BatchnormLayerParams::has_gamma() const {
  return this != internal_default_instance() && gamma_ != NULL;
}
inline void BatchnormLayerParams::clear_gamma() {
  if (GetArenaNoVirtual() == NULL && gamma_ != NULL) delete gamma_;
  gamma_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& BatchnormLayerParams::gamma() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchnormLayerParams.gamma)
  return gamma_ != NULL ? *gamma_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* BatchnormLayerParams::mutable_gamma() {

  if (gamma_ == NULL) {
    gamma_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BatchnormLayerParams.gamma)
  return gamma_;
}
inline ::CoreML::Specification::WeightParams* BatchnormLayerParams::release_gamma() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.BatchnormLayerParams.gamma)

  ::CoreML::Specification::WeightParams* temp = gamma_;
  gamma_ = NULL;
  return temp;
}
inline void BatchnormLayerParams::set_allocated_gamma(::CoreML::Specification::WeightParams* gamma) {
  delete gamma_;
  gamma_ = gamma;
  if (gamma) {

  } else {

  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.BatchnormLayerParams.gamma)
}

// optional .CoreML.Specification.WeightParams beta = 16;
inline bool BatchnormLayerParams::has_beta() const {
  return this != internal_default_instance() && beta_ != NULL;
}
inline void BatchnormLayerParams::clear_beta() {
  if (GetArenaNoVirtual() == NULL && beta_ != NULL) delete beta_;
  beta_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& BatchnormLayerParams::beta() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchnormLayerParams.beta)
  return beta_ != NULL ? *beta_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* BatchnormLayerParams::mutable_beta() {

  if (beta_ == NULL) {
    beta_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BatchnormLayerParams.beta)
  return beta_;
}
inline ::CoreML::Specification::WeightParams* BatchnormLayerParams::release_beta() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.BatchnormLayerParams.beta)

  ::CoreML::Specification::WeightParams* temp = beta_;
  beta_ = NULL;
  return temp;
}
inline void BatchnormLayerParams::set_allocated_beta(::CoreML::Specification::WeightParams* beta) {
  delete beta_;
  beta_ = beta;
  if (beta) {

  } else {

  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.BatchnormLayerParams.beta)
}

// optional .CoreML.Specification.WeightParams mean = 17;
inline bool BatchnormLayerParams::has_mean() const {
  return this != internal_default_instance() && mean_ != NULL;
}
inline void BatchnormLayerParams::clear_mean() {
  if (GetArenaNoVirtual() == NULL && mean_ != NULL) delete mean_;
  mean_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& BatchnormLayerParams::mean() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchnormLayerParams.mean)
  return mean_ != NULL ? *mean_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* BatchnormLayerParams::mutable_mean() {

  if (mean_ == NULL) {
    mean_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BatchnormLayerParams.mean)
  return mean_;
}
inline ::CoreML::Specification::WeightParams* BatchnormLayerParams::release_mean() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.BatchnormLayerParams.mean)

  ::CoreML::Specification::WeightParams* temp = mean_;
  mean_ = NULL;
  return temp;
}
inline void BatchnormLayerParams::set_allocated_mean(::CoreML::Specification::WeightParams* mean) {
  delete mean_;
  mean_ = mean;
  if (mean) {

  } else {

  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.BatchnormLayerParams.mean)
}

// optional .CoreML.Specification.WeightParams variance = 18;
inline bool BatchnormLayerParams::has_variance() const {
  return this != internal_default_instance() && variance_ != NULL;
}
inline void BatchnormLayerParams::clear_variance() {
  if (GetArenaNoVirtual() == NULL && variance_ != NULL) delete variance_;
  variance_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& BatchnormLayerParams::variance() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchnormLayerParams.variance)
  return variance_ != NULL ? *variance_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* BatchnormLayerParams::mutable_variance() {

  if (variance_ == NULL) {
    variance_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BatchnormLayerParams.variance)
  return variance_;
}
inline ::CoreML::Specification::WeightParams* BatchnormLayerParams::release_variance() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.BatchnormLayerParams.variance)

  ::CoreML::Specification::WeightParams* temp = variance_;
  variance_ = NULL;
  return temp;
}
inline void BatchnormLayerParams::set_allocated_variance(::CoreML::Specification::WeightParams* variance) {
  delete variance_;
  variance_ = variance;
  if (variance) {

  } else {

  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.BatchnormLayerParams.variance)
}

inline const BatchnormLayerParams* BatchnormLayerParams::internal_default_instance() {
  return &BatchnormLayerParams_default_instance_.get();
}
// -------------------------------------------------------------------

// PoolingLayerParams_ValidCompletePadding

// repeated uint64 paddingAmounts = 10;
inline int PoolingLayerParams_ValidCompletePadding::paddingamounts_size() const {
  return paddingamounts_.size();
}
inline void PoolingLayerParams_ValidCompletePadding::clear_paddingamounts() {
  paddingamounts_.Clear();
}
inline ::google::protobuf::uint64 PoolingLayerParams_ValidCompletePadding::paddingamounts(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PoolingLayerParams.ValidCompletePadding.paddingAmounts)
  return paddingamounts_.Get(index);
}
inline void PoolingLayerParams_ValidCompletePadding::set_paddingamounts(int index, ::google::protobuf::uint64 value) {
  paddingamounts_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.PoolingLayerParams.ValidCompletePadding.paddingAmounts)
}
inline void PoolingLayerParams_ValidCompletePadding::add_paddingamounts(::google::protobuf::uint64 value) {
  paddingamounts_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.PoolingLayerParams.ValidCompletePadding.paddingAmounts)
}
inline const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
PoolingLayerParams_ValidCompletePadding::paddingamounts() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.PoolingLayerParams.ValidCompletePadding.paddingAmounts)
  return paddingamounts_;
}
inline ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
PoolingLayerParams_ValidCompletePadding::mutable_paddingamounts() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.PoolingLayerParams.ValidCompletePadding.paddingAmounts)
  return &paddingamounts_;
}

inline const PoolingLayerParams_ValidCompletePadding* PoolingLayerParams_ValidCompletePadding::internal_default_instance() {
  return &PoolingLayerParams_ValidCompletePadding_default_instance_.get();
}
// -------------------------------------------------------------------

// PoolingLayerParams

// optional .CoreML.Specification.PoolingLayerParams.PoolingType type = 1;
inline void PoolingLayerParams::clear_type() {
  type_ = 0;
}
inline ::CoreML::Specification::PoolingLayerParams_PoolingType PoolingLayerParams::type() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PoolingLayerParams.type)
  return static_cast< ::CoreML::Specification::PoolingLayerParams_PoolingType >(type_);
}
inline void PoolingLayerParams::set_type(::CoreML::Specification::PoolingLayerParams_PoolingType value) {

  type_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.PoolingLayerParams.type)
}

// repeated uint64 kernelSize = 10;
inline int PoolingLayerParams::kernelsize_size() const {
  return kernelsize_.size();
}
inline void PoolingLayerParams::clear_kernelsize() {
  kernelsize_.Clear();
}
inline ::google::protobuf::uint64 PoolingLayerParams::kernelsize(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PoolingLayerParams.kernelSize)
  return kernelsize_.Get(index);
}
inline void PoolingLayerParams::set_kernelsize(int index, ::google::protobuf::uint64 value) {
  kernelsize_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.PoolingLayerParams.kernelSize)
}
inline void PoolingLayerParams::add_kernelsize(::google::protobuf::uint64 value) {
  kernelsize_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.PoolingLayerParams.kernelSize)
}
inline const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
PoolingLayerParams::kernelsize() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.PoolingLayerParams.kernelSize)
  return kernelsize_;
}
inline ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
PoolingLayerParams::mutable_kernelsize() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.PoolingLayerParams.kernelSize)
  return &kernelsize_;
}

// repeated uint64 stride = 20;
inline int PoolingLayerParams::stride_size() const {
  return stride_.size();
}
inline void PoolingLayerParams::clear_stride() {
  stride_.Clear();
}
inline ::google::protobuf::uint64 PoolingLayerParams::stride(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PoolingLayerParams.stride)
  return stride_.Get(index);
}
inline void PoolingLayerParams::set_stride(int index, ::google::protobuf::uint64 value) {
  stride_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.PoolingLayerParams.stride)
}
inline void PoolingLayerParams::add_stride(::google::protobuf::uint64 value) {
  stride_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.PoolingLayerParams.stride)
}
inline const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
PoolingLayerParams::stride() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.PoolingLayerParams.stride)
  return stride_;
}
inline ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
PoolingLayerParams::mutable_stride() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.PoolingLayerParams.stride)
  return &stride_;
}

// optional .CoreML.Specification.ValidPadding valid = 30;
inline bool PoolingLayerParams::has_valid() const {
  return PoolingPaddingType_case() == kValid;
}
inline void PoolingLayerParams::set_has_valid() {
  _oneof_case_[0] = kValid;
}
inline void PoolingLayerParams::clear_valid() {
  if (has_valid()) {
    delete PoolingPaddingType_.valid_;
    clear_has_PoolingPaddingType();
  }
}
inline  const ::CoreML::Specification::ValidPadding& PoolingLayerParams::valid() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PoolingLayerParams.valid)
  return has_valid()
      ? *PoolingPaddingType_.valid_
      : ::CoreML::Specification::ValidPadding::default_instance();
}
inline ::CoreML::Specification::ValidPadding* PoolingLayerParams::mutable_valid() {
  if (!has_valid()) {
    clear_PoolingPaddingType();
    set_has_valid();
    PoolingPaddingType_.valid_ = new ::CoreML::Specification::ValidPadding;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.PoolingLayerParams.valid)
  return PoolingPaddingType_.valid_;
}
inline ::CoreML::Specification::ValidPadding* PoolingLayerParams::release_valid() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.PoolingLayerParams.valid)
  if (has_valid()) {
    clear_has_PoolingPaddingType();
    ::CoreML::Specification::ValidPadding* temp = PoolingPaddingType_.valid_;
    PoolingPaddingType_.valid_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void PoolingLayerParams::set_allocated_valid(::CoreML::Specification::ValidPadding* valid) {
  clear_PoolingPaddingType();
  if (valid) {
    set_has_valid();
    PoolingPaddingType_.valid_ = valid;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.PoolingLayerParams.valid)
}

// optional .CoreML.Specification.SamePadding same = 31;
inline bool PoolingLayerParams::has_same() const {
  return PoolingPaddingType_case() == kSame;
}
inline void PoolingLayerParams::set_has_same() {
  _oneof_case_[0] = kSame;
}
inline void PoolingLayerParams::clear_same() {
  if (has_same()) {
    delete PoolingPaddingType_.same_;
    clear_has_PoolingPaddingType();
  }
}
inline  const ::CoreML::Specification::SamePadding& PoolingLayerParams::same() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PoolingLayerParams.same)
  return has_same()
      ? *PoolingPaddingType_.same_
      : ::CoreML::Specification::SamePadding::default_instance();
}
inline ::CoreML::Specification::SamePadding* PoolingLayerParams::mutable_same() {
  if (!has_same()) {
    clear_PoolingPaddingType();
    set_has_same();
    PoolingPaddingType_.same_ = new ::CoreML::Specification::SamePadding;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.PoolingLayerParams.same)
  return PoolingPaddingType_.same_;
}
inline ::CoreML::Specification::SamePadding* PoolingLayerParams::release_same() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.PoolingLayerParams.same)
  if (has_same()) {
    clear_has_PoolingPaddingType();
    ::CoreML::Specification::SamePadding* temp = PoolingPaddingType_.same_;
    PoolingPaddingType_.same_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void PoolingLayerParams::set_allocated_same(::CoreML::Specification::SamePadding* same) {
  clear_PoolingPaddingType();
  if (same) {
    set_has_same();
    PoolingPaddingType_.same_ = same;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.PoolingLayerParams.same)
}

// optional .CoreML.Specification.PoolingLayerParams.ValidCompletePadding includeLastPixel = 32;
inline bool PoolingLayerParams::has_includelastpixel() const {
  return PoolingPaddingType_case() == kIncludeLastPixel;
}
inline void PoolingLayerParams::set_has_includelastpixel() {
  _oneof_case_[0] = kIncludeLastPixel;
}
inline void PoolingLayerParams::clear_includelastpixel() {
  if (has_includelastpixel()) {
    delete PoolingPaddingType_.includelastpixel_;
    clear_has_PoolingPaddingType();
  }
}
inline  const ::CoreML::Specification::PoolingLayerParams_ValidCompletePadding& PoolingLayerParams::includelastpixel() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PoolingLayerParams.includeLastPixel)
  return has_includelastpixel()
      ? *PoolingPaddingType_.includelastpixel_
      : ::CoreML::Specification::PoolingLayerParams_ValidCompletePadding::default_instance();
}
inline ::CoreML::Specification::PoolingLayerParams_ValidCompletePadding* PoolingLayerParams::mutable_includelastpixel() {
  if (!has_includelastpixel()) {
    clear_PoolingPaddingType();
    set_has_includelastpixel();
    PoolingPaddingType_.includelastpixel_ = new ::CoreML::Specification::PoolingLayerParams_ValidCompletePadding;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.PoolingLayerParams.includeLastPixel)
  return PoolingPaddingType_.includelastpixel_;
}
inline ::CoreML::Specification::PoolingLayerParams_ValidCompletePadding* PoolingLayerParams::release_includelastpixel() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.PoolingLayerParams.includeLastPixel)
  if (has_includelastpixel()) {
    clear_has_PoolingPaddingType();
    ::CoreML::Specification::PoolingLayerParams_ValidCompletePadding* temp = PoolingPaddingType_.includelastpixel_;
    PoolingPaddingType_.includelastpixel_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void PoolingLayerParams::set_allocated_includelastpixel(::CoreML::Specification::PoolingLayerParams_ValidCompletePadding* includelastpixel) {
  clear_PoolingPaddingType();
  if (includelastpixel) {
    set_has_includelastpixel();
    PoolingPaddingType_.includelastpixel_ = includelastpixel;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.PoolingLayerParams.includeLastPixel)
}

// optional bool avgPoolExcludePadding = 50;
inline void PoolingLayerParams::clear_avgpoolexcludepadding() {
  avgpoolexcludepadding_ = false;
}
inline bool PoolingLayerParams::avgpoolexcludepadding() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PoolingLayerParams.avgPoolExcludePadding)
  return avgpoolexcludepadding_;
}
inline void PoolingLayerParams::set_avgpoolexcludepadding(bool value) {

  avgpoolexcludepadding_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.PoolingLayerParams.avgPoolExcludePadding)
}

// optional bool globalPooling = 60;
inline void PoolingLayerParams::clear_globalpooling() {
  globalpooling_ = false;
}
inline bool PoolingLayerParams::globalpooling() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PoolingLayerParams.globalPooling)
  return globalpooling_;
}
inline void PoolingLayerParams::set_globalpooling(bool value) {

  globalpooling_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.PoolingLayerParams.globalPooling)
}

inline bool PoolingLayerParams::has_PoolingPaddingType() const {
  return PoolingPaddingType_case() != POOLINGPADDINGTYPE_NOT_SET;
}
inline void PoolingLayerParams::clear_has_PoolingPaddingType() {
  _oneof_case_[0] = POOLINGPADDINGTYPE_NOT_SET;
}
inline PoolingLayerParams::PoolingPaddingTypeCase PoolingLayerParams::PoolingPaddingType_case() const {
  return PoolingLayerParams::PoolingPaddingTypeCase(_oneof_case_[0]);
}
inline const PoolingLayerParams* PoolingLayerParams::internal_default_instance() {
  return &PoolingLayerParams_default_instance_.get();
}
// -------------------------------------------------------------------

// PaddingLayerParams_PaddingConstant

// optional float value = 1;
inline void PaddingLayerParams_PaddingConstant::clear_value() {
  value_ = 0;
}
inline float PaddingLayerParams_PaddingConstant::value() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PaddingLayerParams.PaddingConstant.value)
  return value_;
}
inline void PaddingLayerParams_PaddingConstant::set_value(float value) {

  value_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.PaddingLayerParams.PaddingConstant.value)
}

inline const PaddingLayerParams_PaddingConstant* PaddingLayerParams_PaddingConstant::internal_default_instance() {
  return &PaddingLayerParams_PaddingConstant_default_instance_.get();
}
// -------------------------------------------------------------------

// PaddingLayerParams_PaddingReflection

inline const PaddingLayerParams_PaddingReflection* PaddingLayerParams_PaddingReflection::internal_default_instance() {
  return &PaddingLayerParams_PaddingReflection_default_instance_.get();
}
// -------------------------------------------------------------------

// PaddingLayerParams_PaddingReplication

inline const PaddingLayerParams_PaddingReplication* PaddingLayerParams_PaddingReplication::internal_default_instance() {
  return &PaddingLayerParams_PaddingReplication_default_instance_.get();
}
// -------------------------------------------------------------------

// PaddingLayerParams

// optional .CoreML.Specification.PaddingLayerParams.PaddingConstant constant = 1;
inline bool PaddingLayerParams::has_constant() const {
  return PaddingType_case() == kConstant;
}
inline void PaddingLayerParams::set_has_constant() {
  _oneof_case_[0] = kConstant;
}
inline void PaddingLayerParams::clear_constant() {
  if (has_constant()) {
    delete PaddingType_.constant_;
    clear_has_PaddingType();
  }
}
inline  const ::CoreML::Specification::PaddingLayerParams_PaddingConstant& PaddingLayerParams::constant() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PaddingLayerParams.constant)
  return has_constant()
      ? *PaddingType_.constant_
      : ::CoreML::Specification::PaddingLayerParams_PaddingConstant::default_instance();
}
inline ::CoreML::Specification::PaddingLayerParams_PaddingConstant* PaddingLayerParams::mutable_constant() {
  if (!has_constant()) {
    clear_PaddingType();
    set_has_constant();
    PaddingType_.constant_ = new ::CoreML::Specification::PaddingLayerParams_PaddingConstant;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.PaddingLayerParams.constant)
  return PaddingType_.constant_;
}
inline ::CoreML::Specification::PaddingLayerParams_PaddingConstant* PaddingLayerParams::release_constant() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.PaddingLayerParams.constant)
  if (has_constant()) {
    clear_has_PaddingType();
    ::CoreML::Specification::PaddingLayerParams_PaddingConstant* temp = PaddingType_.constant_;
    PaddingType_.constant_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void PaddingLayerParams::set_allocated_constant(::CoreML::Specification::PaddingLayerParams_PaddingConstant* constant) {
  clear_PaddingType();
  if (constant) {
    set_has_constant();
    PaddingType_.constant_ = constant;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.PaddingLayerParams.constant)
}

// optional .CoreML.Specification.PaddingLayerParams.PaddingReflection reflection = 2;
inline bool PaddingLayerParams::has_reflection() const {
  return PaddingType_case() == kReflection;
}
inline void PaddingLayerParams::set_has_reflection() {
  _oneof_case_[0] = kReflection;
}
inline void PaddingLayerParams::clear_reflection() {
  if (has_reflection()) {
    delete PaddingType_.reflection_;
    clear_has_PaddingType();
  }
}
inline  const ::CoreML::Specification::PaddingLayerParams_PaddingReflection& PaddingLayerParams::reflection() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PaddingLayerParams.reflection)
  return has_reflection()
      ? *PaddingType_.reflection_
      : ::CoreML::Specification::PaddingLayerParams_PaddingReflection::default_instance();
}
inline ::CoreML::Specification::PaddingLayerParams_PaddingReflection* PaddingLayerParams::mutable_reflection() {
  if (!has_reflection()) {
    clear_PaddingType();
    set_has_reflection();
    PaddingType_.reflection_ = new ::CoreML::Specification::PaddingLayerParams_PaddingReflection;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.PaddingLayerParams.reflection)
  return PaddingType_.reflection_;
}
inline ::CoreML::Specification::PaddingLayerParams_PaddingReflection* PaddingLayerParams::release_reflection() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.PaddingLayerParams.reflection)
  if (has_reflection()) {
    clear_has_PaddingType();
    ::CoreML::Specification::PaddingLayerParams_PaddingReflection* temp = PaddingType_.reflection_;
    PaddingType_.reflection_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void PaddingLayerParams::set_allocated_reflection(::CoreML::Specification::PaddingLayerParams_PaddingReflection* reflection) {
  clear_PaddingType();
  if (reflection) {
    set_has_reflection();
    PaddingType_.reflection_ = reflection;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.PaddingLayerParams.reflection)
}

// optional .CoreML.Specification.PaddingLayerParams.PaddingReplication replication = 3;
inline bool PaddingLayerParams::has_replication() const {
  return PaddingType_case() == kReplication;
}
inline void PaddingLayerParams::set_has_replication() {
  _oneof_case_[0] = kReplication;
}
inline void PaddingLayerParams::clear_replication() {
  if (has_replication()) {
    delete PaddingType_.replication_;
    clear_has_PaddingType();
  }
}
inline  const ::CoreML::Specification::PaddingLayerParams_PaddingReplication& PaddingLayerParams::replication() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PaddingLayerParams.replication)
  return has_replication()
      ? *PaddingType_.replication_
      : ::CoreML::Specification::PaddingLayerParams_PaddingReplication::default_instance();
}
inline ::CoreML::Specification::PaddingLayerParams_PaddingReplication* PaddingLayerParams::mutable_replication() {
  if (!has_replication()) {
    clear_PaddingType();
    set_has_replication();
    PaddingType_.replication_ = new ::CoreML::Specification::PaddingLayerParams_PaddingReplication;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.PaddingLayerParams.replication)
  return PaddingType_.replication_;
}
inline ::CoreML::Specification::PaddingLayerParams_PaddingReplication* PaddingLayerParams::release_replication() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.PaddingLayerParams.replication)
  if (has_replication()) {
    clear_has_PaddingType();
    ::CoreML::Specification::PaddingLayerParams_PaddingReplication* temp = PaddingType_.replication_;
    PaddingType_.replication_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void PaddingLayerParams::set_allocated_replication(::CoreML::Specification::PaddingLayerParams_PaddingReplication* replication) {
  clear_PaddingType();
  if (replication) {
    set_has_replication();
    PaddingType_.replication_ = replication;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.PaddingLayerParams.replication)
}

// optional .CoreML.Specification.BorderAmounts paddingAmounts = 10;
inline bool PaddingLayerParams::has_paddingamounts() const {
  return this != internal_default_instance() && paddingamounts_ != NULL;
}
inline void PaddingLayerParams::clear_paddingamounts() {
  if (GetArenaNoVirtual() == NULL && paddingamounts_ != NULL) delete paddingamounts_;
  paddingamounts_ = NULL;
}
inline const ::CoreML::Specification::BorderAmounts& PaddingLayerParams::paddingamounts() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PaddingLayerParams.paddingAmounts)
  return paddingamounts_ != NULL ? *paddingamounts_
                         : *::CoreML::Specification::BorderAmounts::internal_default_instance();
}
inline ::CoreML::Specification::BorderAmounts* PaddingLayerParams::mutable_paddingamounts() {

  if (paddingamounts_ == NULL) {
    paddingamounts_ = new ::CoreML::Specification::BorderAmounts;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.PaddingLayerParams.paddingAmounts)
  return paddingamounts_;
}
inline ::CoreML::Specification::BorderAmounts* PaddingLayerParams::release_paddingamounts() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.PaddingLayerParams.paddingAmounts)

  ::CoreML::Specification::BorderAmounts* temp = paddingamounts_;
  paddingamounts_ = NULL;
  return temp;
}
inline void PaddingLayerParams::set_allocated_paddingamounts(::CoreML::Specification::BorderAmounts* paddingamounts) {
  delete paddingamounts_;
  paddingamounts_ = paddingamounts;
  if (paddingamounts) {

  } else {

  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.PaddingLayerParams.paddingAmounts)
}

inline bool PaddingLayerParams::has_PaddingType() const {
  return PaddingType_case() != PADDINGTYPE_NOT_SET;
}
inline void PaddingLayerParams::clear_has_PaddingType() {
  _oneof_case_[0] = PADDINGTYPE_NOT_SET;
}
inline PaddingLayerParams::PaddingTypeCase PaddingLayerParams::PaddingType_case() const {
  return PaddingLayerParams::PaddingTypeCase(_oneof_case_[0]);
}
inline const PaddingLayerParams* PaddingLayerParams::internal_default_instance() {
  return &PaddingLayerParams_default_instance_.get();
}
// -------------------------------------------------------------------

// ConcatLayerParams

// optional bool sequenceConcat = 100;
inline void ConcatLayerParams::clear_sequenceconcat() {
  sequenceconcat_ = false;
}
inline bool ConcatLayerParams::sequenceconcat() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConcatLayerParams.sequenceConcat)
  return sequenceconcat_;
}
inline void ConcatLayerParams::set_sequenceconcat(bool value) {

  sequenceconcat_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConcatLayerParams.sequenceConcat)
}

inline const ConcatLayerParams* ConcatLayerParams::internal_default_instance() {
  return &ConcatLayerParams_default_instance_.get();
}
// -------------------------------------------------------------------

// LRNLayerParams

// optional float alpha = 1;
inline void LRNLayerParams::clear_alpha() {
  alpha_ = 0;
}
inline float LRNLayerParams::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LRNLayerParams.alpha)
  return alpha_;
}
inline void LRNLayerParams::set_alpha(float value) {

  alpha_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.LRNLayerParams.alpha)
}

// optional float beta = 2;
inline void LRNLayerParams::clear_beta() {
  beta_ = 0;
}
inline float LRNLayerParams::beta() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LRNLayerParams.beta)
  return beta_;
}
inline void LRNLayerParams::set_beta(float value) {

  beta_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.LRNLayerParams.beta)
}

// optional uint64 localSize = 3;
inline void LRNLayerParams::clear_localsize() {
  localsize_ = GOOGLE_ULONGLONG(0);
}
inline ::google::protobuf::uint64 LRNLayerParams::localsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LRNLayerParams.localSize)
  return localsize_;
}
inline void LRNLayerParams::set_localsize(::google::protobuf::uint64 value) {

  localsize_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.LRNLayerParams.localSize)
}

// optional float k = 4;
inline void LRNLayerParams::clear_k() {
  k_ = 0;
}
inline float LRNLayerParams::k() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LRNLayerParams.k)
  return k_;
}
inline void LRNLayerParams::set_k(float value) {

  k_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.LRNLayerParams.k)
}

inline const LRNLayerParams* LRNLayerParams::internal_default_instance() {
  return &LRNLayerParams_default_instance_.get();
}
// -------------------------------------------------------------------

// SoftmaxLayerParams

inline const SoftmaxLayerParams* SoftmaxLayerParams::internal_default_instance() {
  return &SoftmaxLayerParams_default_instance_.get();
}
// -------------------------------------------------------------------

// SplitLayerParams

// optional uint64 nOutputs = 1;
inline void SplitLayerParams::clear_noutputs() {
  noutputs_ = GOOGLE_ULONGLONG(0);
}
inline ::google::protobuf::uint64 SplitLayerParams::noutputs() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SplitLayerParams.nOutputs)
  return noutputs_;
}
inline void SplitLayerParams::set_noutputs(::google::protobuf::uint64 value) {

  noutputs_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SplitLayerParams.nOutputs)
}

inline const SplitLayerParams* SplitLayerParams::internal_default_instance() {
  return &SplitLayerParams_default_instance_.get();
}
// -------------------------------------------------------------------

// AddLayerParams

// optional float alpha = 1;
inline void AddLayerParams::clear_alpha() {
  alpha_ = 0;
}
inline float AddLayerParams::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.AddLayerParams.alpha)
  return alpha_;
}
inline void AddLayerParams::set_alpha(float value) {

  alpha_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.AddLayerParams.alpha)
}

inline const AddLayerParams* AddLayerParams::internal_default_instance() {
  return &AddLayerParams_default_instance_.get();
}
// -------------------------------------------------------------------

// MultiplyLayerParams

// optional float alpha = 1;
inline void MultiplyLayerParams::clear_alpha() {
  alpha_ = 0;
}
inline float MultiplyLayerParams::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.MultiplyLayerParams.alpha)
  return alpha_;
}
inline void MultiplyLayerParams::set_alpha(float value) {

  alpha_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.MultiplyLayerParams.alpha)
}

inline const MultiplyLayerParams* MultiplyLayerParams::internal_default_instance() {
  return &MultiplyLayerParams_default_instance_.get();
}
// -------------------------------------------------------------------

// UnaryFunctionLayerParams

// optional .CoreML.Specification.UnaryFunctionLayerParams.Operation type = 1;
inline void UnaryFunctionLayerParams::clear_type() {
  type_ = 0;
}
inline ::CoreML::Specification::UnaryFunctionLayerParams_Operation UnaryFunctionLayerParams::type() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UnaryFunctionLayerParams.type)
  return static_cast< ::CoreML::Specification::UnaryFunctionLayerParams_Operation >(type_);
}
inline void UnaryFunctionLayerParams::set_type(::CoreML::Specification::UnaryFunctionLayerParams_Operation value) {

  type_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.UnaryFunctionLayerParams.type)
}

// optional float alpha = 2;
inline void UnaryFunctionLayerParams::clear_alpha() {
  alpha_ = 0;
}
inline float UnaryFunctionLayerParams::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UnaryFunctionLayerParams.alpha)
  return alpha_;
}
inline void UnaryFunctionLayerParams::set_alpha(float value) {

  alpha_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.UnaryFunctionLayerParams.alpha)
}

// optional float epsilon = 3;
inline void UnaryFunctionLayerParams::clear_epsilon() {
  epsilon_ = 0;
}
inline float UnaryFunctionLayerParams::epsilon() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UnaryFunctionLayerParams.epsilon)
  return epsilon_;
}
inline void UnaryFunctionLayerParams::set_epsilon(float value) {

  epsilon_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.UnaryFunctionLayerParams.epsilon)
}

// optional float shift = 4;
inline void UnaryFunctionLayerParams::clear_shift() {
  shift_ = 0;
}
inline float UnaryFunctionLayerParams::shift() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UnaryFunctionLayerParams.shift)
  return shift_;
}
inline void UnaryFunctionLayerParams::set_shift(float value) {

  shift_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.UnaryFunctionLayerParams.shift)
}

// optional float scale = 5;
inline void UnaryFunctionLayerParams::clear_scale() {
  scale_ = 0;
}
inline float UnaryFunctionLayerParams::scale() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UnaryFunctionLayerParams.scale)
  return scale_;
}
inline void UnaryFunctionLayerParams::set_scale(float value) {

  scale_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.UnaryFunctionLayerParams.scale)
}

inline const UnaryFunctionLayerParams* UnaryFunctionLayerParams::internal_default_instance() {
  return &UnaryFunctionLayerParams_default_instance_.get();
}
// -------------------------------------------------------------------

// UpsampleLayerParams

// repeated uint64 scalingFactor = 1;
inline int UpsampleLayerParams::scalingfactor_size() const {
  return scalingfactor_.size();
}
inline void UpsampleLayerParams::clear_scalingfactor() {
  scalingfactor_.Clear();
}
inline ::google::protobuf::uint64 UpsampleLayerParams::scalingfactor(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UpsampleLayerParams.scalingFactor)
  return scalingfactor_.Get(index);
}
inline void UpsampleLayerParams::set_scalingfactor(int index, ::google::protobuf::uint64 value) {
  scalingfactor_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.UpsampleLayerParams.scalingFactor)
}
inline void UpsampleLayerParams::add_scalingfactor(::google::protobuf::uint64 value) {
  scalingfactor_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.UpsampleLayerParams.scalingFactor)
}
inline const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
UpsampleLayerParams::scalingfactor() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.UpsampleLayerParams.scalingFactor)
  return scalingfactor_;
}
inline ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
UpsampleLayerParams::mutable_scalingfactor() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.UpsampleLayerParams.scalingFactor)
  return &scalingfactor_;
}

inline const UpsampleLayerParams* UpsampleLayerParams::internal_default_instance() {
  return &UpsampleLayerParams_default_instance_.get();
}
// -------------------------------------------------------------------

// BiasLayerParams

// repeated uint64 shape = 1;
inline int BiasLayerParams::shape_size() const {
  return shape_.size();
}
inline void BiasLayerParams::clear_shape() {
  shape_.Clear();
}
inline ::google::protobuf::uint64 BiasLayerParams::shape(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BiasLayerParams.shape)
  return shape_.Get(index);
}
inline void BiasLayerParams::set_shape(int index, ::google::protobuf::uint64 value) {
  shape_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.BiasLayerParams.shape)
}
inline void BiasLayerParams::add_shape(::google::protobuf::uint64 value) {
  shape_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.BiasLayerParams.shape)
}
inline const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
BiasLayerParams::shape() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.BiasLayerParams.shape)
  return shape_;
}
inline ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
BiasLayerParams::mutable_shape() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.BiasLayerParams.shape)
  return &shape_;
}

// optional .CoreML.Specification.WeightParams bias = 2;
inline bool BiasLayerParams::has_bias() const {
  return this != internal_default_instance() && bias_ != NULL;
}
inline void BiasLayerParams::clear_bias() {
  if (GetArenaNoVirtual() == NULL && bias_ != NULL) delete bias_;
  bias_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& BiasLayerParams::bias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BiasLayerParams.bias)
  return bias_ != NULL ? *bias_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* BiasLayerParams::mutable_bias() {

  if (bias_ == NULL) {
    bias_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BiasLayerParams.bias)
  return bias_;
}
inline ::CoreML::Specification::WeightParams* BiasLayerParams::release_bias() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.BiasLayerParams.bias)

  ::CoreML::Specification::WeightParams* temp = bias_;
  bias_ = NULL;
  return temp;
}
inline void BiasLayerParams::set_allocated_bias(::CoreML::Specification::WeightParams* bias) {
  delete bias_;
  bias_ = bias;
  if (bias) {

  } else {

  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.BiasLayerParams.bias)
}

inline const BiasLayerParams* BiasLayerParams::internal_default_instance() {
  return &BiasLayerParams_default_instance_.get();
}
// -------------------------------------------------------------------

// ScaleLayerParams

// repeated uint64 shapeScale = 1;
inline int ScaleLayerParams::shapescale_size() const {
  return shapescale_.size();
}
inline void ScaleLayerParams::clear_shapescale() {
  shapescale_.Clear();
}
inline ::google::protobuf::uint64 ScaleLayerParams::shapescale(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ScaleLayerParams.shapeScale)
  return shapescale_.Get(index);
}
inline void ScaleLayerParams::set_shapescale(int index, ::google::protobuf::uint64 value) {
  shapescale_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ScaleLayerParams.shapeScale)
}
inline void ScaleLayerParams::add_shapescale(::google::protobuf::uint64 value) {
  shapescale_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ScaleLayerParams.shapeScale)
}
inline const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
ScaleLayerParams::shapescale() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ScaleLayerParams.shapeScale)
  return shapescale_;
}
inline ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
ScaleLayerParams::mutable_shapescale() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ScaleLayerParams.shapeScale)
  return &shapescale_;
}

// optional .CoreML.Specification.WeightParams scale = 2;
inline bool ScaleLayerParams::has_scale() const {
  return this != internal_default_instance() && scale_ != NULL;
}
inline void ScaleLayerParams::clear_scale() {
  if (GetArenaNoVirtual() == NULL && scale_ != NULL) delete scale_;
  scale_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& ScaleLayerParams::scale() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ScaleLayerParams.scale)
  return scale_ != NULL ? *scale_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* ScaleLayerParams::mutable_scale() {

  if (scale_ == NULL) {
    scale_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ScaleLayerParams.scale)
  return scale_;
}
inline ::CoreML::Specification::WeightParams* ScaleLayerParams::release_scale() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ScaleLayerParams.scale)

  ::CoreML::Specification::WeightParams* temp = scale_;
  scale_ = NULL;
  return temp;
}
inline void ScaleLayerParams::set_allocated_scale(::CoreML::Specification::WeightParams* scale) {
  delete scale_;
  scale_ = scale;
  if (scale) {

  } else {

  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ScaleLayerParams.scale)
}

// optional bool hasBias = 3;
inline void ScaleLayerParams::clear_hasbias() {
  hasbias_ = false;
}
inline bool ScaleLayerParams::hasbias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ScaleLayerParams.hasBias)
  return hasbias_;
}
inline void ScaleLayerParams::set_hasbias(bool value) {

  hasbias_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ScaleLayerParams.hasBias)
}

// repeated uint64 shapeBias = 4;
inline int ScaleLayerParams::shapebias_size() const {
  return shapebias_.size();
}
inline void ScaleLayerParams::clear_shapebias() {
  shapebias_.Clear();
}
inline ::google::protobuf::uint64 ScaleLayerParams::shapebias(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ScaleLayerParams.shapeBias)
  return shapebias_.Get(index);
}
inline void ScaleLayerParams::set_shapebias(int index, ::google::protobuf::uint64 value) {
  shapebias_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ScaleLayerParams.shapeBias)
}
inline void ScaleLayerParams::add_shapebias(::google::protobuf::uint64 value) {
  shapebias_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ScaleLayerParams.shapeBias)
}
inline const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
ScaleLayerParams::shapebias() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ScaleLayerParams.shapeBias)
  return shapebias_;
}
inline ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
ScaleLayerParams::mutable_shapebias() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ScaleLayerParams.shapeBias)
  return &shapebias_;
}

// optional .CoreML.Specification.WeightParams bias = 5;
inline bool ScaleLayerParams::has_bias() const {
  return this != internal_default_instance() && bias_ != NULL;
}
inline void ScaleLayerParams::clear_bias() {
  if (GetArenaNoVirtual() == NULL && bias_ != NULL) delete bias_;
  bias_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& ScaleLayerParams::bias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ScaleLayerParams.bias)
  return bias_ != NULL ? *bias_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* ScaleLayerParams::mutable_bias() {

  if (bias_ == NULL) {
    bias_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ScaleLayerParams.bias)
  return bias_;
}
inline ::CoreML::Specification::WeightParams* ScaleLayerParams::release_bias() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ScaleLayerParams.bias)

  ::CoreML::Specification::WeightParams* temp = bias_;
  bias_ = NULL;
  return temp;
}
inline void ScaleLayerParams::set_allocated_bias(::CoreML::Specification::WeightParams* bias) {
  delete bias_;
  bias_ = bias;
  if (bias) {

  } else {

  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ScaleLayerParams.bias)
}

inline const ScaleLayerParams* ScaleLayerParams::internal_default_instance() {
  return &ScaleLayerParams_default_instance_.get();
}
// -------------------------------------------------------------------

// LoadConstantLayerParams

// repeated uint64 shape = 1;
inline int LoadConstantLayerParams::shape_size() const {
  return shape_.size();
}
inline void LoadConstantLayerParams::clear_shape() {
  shape_.Clear();
}
inline ::google::protobuf::uint64 LoadConstantLayerParams::shape(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LoadConstantLayerParams.shape)
  return shape_.Get(index);
}
inline void LoadConstantLayerParams::set_shape(int index, ::google::protobuf::uint64 value) {
  shape_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.LoadConstantLayerParams.shape)
}
inline void LoadConstantLayerParams::add_shape(::google::protobuf::uint64 value) {
  shape_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.LoadConstantLayerParams.shape)
}
inline const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
LoadConstantLayerParams::shape() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.LoadConstantLayerParams.shape)
  return shape_;
}
inline ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
LoadConstantLayerParams::mutable_shape() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.LoadConstantLayerParams.shape)
  return &shape_;
}

// optional .CoreML.Specification.WeightParams data = 2;
inline bool LoadConstantLayerParams::has_data() const {
  return this != internal_default_instance() && data_ != NULL;
}
inline void LoadConstantLayerParams::clear_data() {
  if (GetArenaNoVirtual() == NULL && data_ != NULL) delete data_;
  data_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& LoadConstantLayerParams::data() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LoadConstantLayerParams.data)
  return data_ != NULL ? *data_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* LoadConstantLayerParams::mutable_data() {

  if (data_ == NULL) {
    data_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LoadConstantLayerParams.data)
  return data_;
}
inline ::CoreML::Specification::WeightParams* LoadConstantLayerParams::release_data() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LoadConstantLayerParams.data)

  ::CoreML::Specification::WeightParams* temp = data_;
  data_ = NULL;
  return temp;
}
inline void LoadConstantLayerParams::set_allocated_data(::CoreML::Specification::WeightParams* data) {
  delete data_;
  data_ = data;
  if (data) {

  } else {

  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LoadConstantLayerParams.data)
}

inline const LoadConstantLayerParams* LoadConstantLayerParams::internal_default_instance() {
  return &LoadConstantLayerParams_default_instance_.get();
}
// -------------------------------------------------------------------

// L2NormalizeLayerParams

// optional float epsilon = 1;
inline void L2NormalizeLayerParams::clear_epsilon() {
  epsilon_ = 0;
}
inline float L2NormalizeLayerParams::epsilon() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.L2NormalizeLayerParams.epsilon)
  return epsilon_;
}
inline void L2NormalizeLayerParams::set_epsilon(float value) {

  epsilon_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.L2NormalizeLayerParams.epsilon)
}

inline const L2NormalizeLayerParams* L2NormalizeLayerParams::internal_default_instance() {
  return &L2NormalizeLayerParams_default_instance_.get();
}
// -------------------------------------------------------------------

// FlattenLayerParams

// optional .CoreML.Specification.FlattenLayerParams.FlattenOrder mode = 1;
inline void FlattenLayerParams::clear_mode() {
  mode_ = 0;
}
inline ::CoreML::Specification::FlattenLayerParams_FlattenOrder FlattenLayerParams::mode() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.FlattenLayerParams.mode)
  return static_cast< ::CoreML::Specification::FlattenLayerParams_FlattenOrder >(mode_);
}
inline void FlattenLayerParams::set_mode(::CoreML::Specification::FlattenLayerParams_FlattenOrder value) {

  mode_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.FlattenLayerParams.mode)
}

inline const FlattenLayerParams* FlattenLayerParams::internal_default_instance() {
  return &FlattenLayerParams_default_instance_.get();
}
// -------------------------------------------------------------------

// ReshapeLayerParams

// repeated int64 targetShape = 1;
inline int ReshapeLayerParams::targetshape_size() const {
  return targetshape_.size();
}
inline void ReshapeLayerParams::clear_targetshape() {
  targetshape_.Clear();
}
inline ::google::protobuf::int64 ReshapeLayerParams::targetshape(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReshapeLayerParams.targetShape)
  return targetshape_.Get(index);
}
inline void ReshapeLayerParams::set_targetshape(int index, ::google::protobuf::int64 value) {
  targetshape_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReshapeLayerParams.targetShape)
}
inline void ReshapeLayerParams::add_targetshape(::google::protobuf::int64 value) {
  targetshape_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ReshapeLayerParams.targetShape)
}
inline const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
ReshapeLayerParams::targetshape() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ReshapeLayerParams.targetShape)
  return targetshape_;
}
inline ::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
ReshapeLayerParams::mutable_targetshape() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ReshapeLayerParams.targetShape)
  return &targetshape_;
}

// optional .CoreML.Specification.ReshapeLayerParams.ReshapeOrder mode = 2;
inline void ReshapeLayerParams::clear_mode() {
  mode_ = 0;
}
inline ::CoreML::Specification::ReshapeLayerParams_ReshapeOrder ReshapeLayerParams::mode() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReshapeLayerParams.mode)
  return static_cast< ::CoreML::Specification::ReshapeLayerParams_ReshapeOrder >(mode_);
}
inline void ReshapeLayerParams::set_mode(::CoreML::Specification::ReshapeLayerParams_ReshapeOrder value) {

  mode_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReshapeLayerParams.mode)
}

inline const ReshapeLayerParams* ReshapeLayerParams::internal_default_instance() {
  return &ReshapeLayerParams_default_instance_.get();
}
// -------------------------------------------------------------------

// PermuteLayerParams

// repeated uint64 axis = 1;
inline int PermuteLayerParams::axis_size() const {
  return axis_.size();
}
inline void PermuteLayerParams::clear_axis() {
  axis_.Clear();
}
inline ::google::protobuf::uint64 PermuteLayerParams::axis(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PermuteLayerParams.axis)
  return axis_.Get(index);
}
inline void PermuteLayerParams::set_axis(int index, ::google::protobuf::uint64 value) {
  axis_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.PermuteLayerParams.axis)
}
inline void PermuteLayerParams::add_axis(::google::protobuf::uint64 value) {
  axis_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.PermuteLayerParams.axis)
}
inline const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
PermuteLayerParams::axis() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.PermuteLayerParams.axis)
  return axis_;
}
inline ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
PermuteLayerParams::mutable_axis() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.PermuteLayerParams.axis)
  return &axis_;
}

inline const PermuteLayerParams* PermuteLayerParams::internal_default_instance() {
  return &PermuteLayerParams_default_instance_.get();
}
// -------------------------------------------------------------------

// ReduceLayerParams

// optional .CoreML.Specification.ReduceLayerParams.ReduceOperation mode = 1;
inline void ReduceLayerParams::clear_mode() {
  mode_ = 0;
}
inline ::CoreML::Specification::ReduceLayerParams_ReduceOperation ReduceLayerParams::mode() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceLayerParams.mode)
  return static_cast< ::CoreML::Specification::ReduceLayerParams_ReduceOperation >(mode_);
}
inline void ReduceLayerParams::set_mode(::CoreML::Specification::ReduceLayerParams_ReduceOperation value) {

  mode_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceLayerParams.mode)
}

// optional float epsilon = 2;
inline void ReduceLayerParams::clear_epsilon() {
  epsilon_ = 0;
}
inline float ReduceLayerParams::epsilon() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceLayerParams.epsilon)
  return epsilon_;
}
inline void ReduceLayerParams::set_epsilon(float value) {

  epsilon_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceLayerParams.epsilon)
}

inline const ReduceLayerParams* ReduceLayerParams::internal_default_instance() {
  return &ReduceLayerParams_default_instance_.get();
}
// -------------------------------------------------------------------

// CropLayerParams

// optional .CoreML.Specification.BorderAmounts cropAmounts = 1;
inline bool CropLayerParams::has_cropamounts() const {
  return this != internal_default_instance() && cropamounts_ != NULL;
}
inline void CropLayerParams::clear_cropamounts() {
  if (GetArenaNoVirtual() == NULL && cropamounts_ != NULL) delete cropamounts_;
  cropamounts_ = NULL;
}
inline const ::CoreML::Specification::BorderAmounts& CropLayerParams::cropamounts() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CropLayerParams.cropAmounts)
  return cropamounts_ != NULL ? *cropamounts_
                         : *::CoreML::Specification::BorderAmounts::internal_default_instance();
}
inline ::CoreML::Specification::BorderAmounts* CropLayerParams::mutable_cropamounts() {

  if (cropamounts_ == NULL) {
    cropamounts_ = new ::CoreML::Specification::BorderAmounts;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.CropLayerParams.cropAmounts)
  return cropamounts_;
}
inline ::CoreML::Specification::BorderAmounts* CropLayerParams::release_cropamounts() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.CropLayerParams.cropAmounts)

  ::CoreML::Specification::BorderAmounts* temp = cropamounts_;
  cropamounts_ = NULL;
  return temp;
}
inline void CropLayerParams::set_allocated_cropamounts(::CoreML::Specification::BorderAmounts* cropamounts) {
  delete cropamounts_;
  cropamounts_ = cropamounts;
  if (cropamounts) {

  } else {

  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.CropLayerParams.cropAmounts)
}

// repeated uint64 offset = 5;
inline int CropLayerParams::offset_size() const {
  return offset_.size();
}
inline void CropLayerParams::clear_offset() {
  offset_.Clear();
}
inline ::google::protobuf::uint64 CropLayerParams::offset(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CropLayerParams.offset)
  return offset_.Get(index);
}
inline void CropLayerParams::set_offset(int index, ::google::protobuf::uint64 value) {
  offset_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.CropLayerParams.offset)
}
inline void CropLayerParams::add_offset(::google::protobuf::uint64 value) {
  offset_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.CropLayerParams.offset)
}
inline const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
CropLayerParams::offset() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.CropLayerParams.offset)
  return offset_;
}
inline ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
CropLayerParams::mutable_offset() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.CropLayerParams.offset)
  return &offset_;
}

inline const CropLayerParams* CropLayerParams::internal_default_instance() {
  return &CropLayerParams_default_instance_.get();
}
// -------------------------------------------------------------------

// AverageLayerParams

inline const AverageLayerParams* AverageLayerParams::internal_default_instance() {
  return &AverageLayerParams_default_instance_.get();
}
// -------------------------------------------------------------------

// MaxLayerParams

inline const MaxLayerParams* MaxLayerParams::internal_default_instance() {
  return &MaxLayerParams_default_instance_.get();
}
// -------------------------------------------------------------------

// MinLayerParams

inline const MinLayerParams* MinLayerParams::internal_default_instance() {
  return &MinLayerParams_default_instance_.get();
}
// -------------------------------------------------------------------

// DotProductLayerParams

// optional bool cosineSimilarity = 1;
inline void DotProductLayerParams::clear_cosinesimilarity() {
  cosinesimilarity_ = false;
}
inline bool DotProductLayerParams::cosinesimilarity() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.DotProductLayerParams.cosineSimilarity)
  return cosinesimilarity_;
}
inline void DotProductLayerParams::set_cosinesimilarity(bool value) {

  cosinesimilarity_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.DotProductLayerParams.cosineSimilarity)
}

inline const DotProductLayerParams* DotProductLayerParams::internal_default_instance() {
  return &DotProductLayerParams_default_instance_.get();
}
// -------------------------------------------------------------------

// MeanVarianceNormalizeLayerParams

// optional bool acrossChannels = 1;
inline void MeanVarianceNormalizeLayerParams::clear_acrosschannels() {
  acrosschannels_ = false;
}
inline bool MeanVarianceNormalizeLayerParams::acrosschannels() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.MeanVarianceNormalizeLayerParams.acrossChannels)
  return acrosschannels_;
}
inline void MeanVarianceNormalizeLayerParams::set_acrosschannels(bool value) {

  acrosschannels_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.MeanVarianceNormalizeLayerParams.acrossChannels)
}

// optional bool normalizeVariance = 2;
inline void MeanVarianceNormalizeLayerParams::clear_normalizevariance() {
  normalizevariance_ = false;
}
inline bool MeanVarianceNormalizeLayerParams::normalizevariance() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.MeanVarianceNormalizeLayerParams.normalizeVariance)
  return normalizevariance_;
}
inline void MeanVarianceNormalizeLayerParams::set_normalizevariance(bool value) {

  normalizevariance_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.MeanVarianceNormalizeLayerParams.normalizeVariance)
}

// optional float epsilon = 3;
inline void MeanVarianceNormalizeLayerParams::clear_epsilon() {
  epsilon_ = 0;
}
inline float MeanVarianceNormalizeLayerParams::epsilon() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.MeanVarianceNormalizeLayerParams.epsilon)
  return epsilon_;
}
inline void MeanVarianceNormalizeLayerParams::set_epsilon(float value) {

  epsilon_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.MeanVarianceNormalizeLayerParams.epsilon)
}

inline const MeanVarianceNormalizeLayerParams* MeanVarianceNormalizeLayerParams::internal_default_instance() {
  return &MeanVarianceNormalizeLayerParams_default_instance_.get();
}
// -------------------------------------------------------------------

// SequenceRepeatLayerParams

// optional uint64 nRepetitions = 1;
inline void SequenceRepeatLayerParams::clear_nrepetitions() {
  nrepetitions_ = GOOGLE_ULONGLONG(0);
}
inline ::google::protobuf::uint64 SequenceRepeatLayerParams::nrepetitions() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SequenceRepeatLayerParams.nRepetitions)
  return nrepetitions_;
}
inline void SequenceRepeatLayerParams::set_nrepetitions(::google::protobuf::uint64 value) {

  nrepetitions_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SequenceRepeatLayerParams.nRepetitions)
}

inline const SequenceRepeatLayerParams* SequenceRepeatLayerParams::internal_default_instance() {
  return &SequenceRepeatLayerParams_default_instance_.get();
}
// -------------------------------------------------------------------

// SimpleRecurrentLayerParams

// optional uint64 inputVectorSize = 1;
inline void SimpleRecurrentLayerParams::clear_inputvectorsize() {
  inputvectorsize_ = GOOGLE_ULONGLONG(0);
}
inline ::google::protobuf::uint64 SimpleRecurrentLayerParams::inputvectorsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SimpleRecurrentLayerParams.inputVectorSize)
  return inputvectorsize_;
}
inline void SimpleRecurrentLayerParams::set_inputvectorsize(::google::protobuf::uint64 value) {

  inputvectorsize_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SimpleRecurrentLayerParams.inputVectorSize)
}

// optional uint64 outputVectorSize = 2;
inline void SimpleRecurrentLayerParams::clear_outputvectorsize() {
  outputvectorsize_ = GOOGLE_ULONGLONG(0);
}
inline ::google::protobuf::uint64 SimpleRecurrentLayerParams::outputvectorsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SimpleRecurrentLayerParams.outputVectorSize)
  return outputvectorsize_;
}
inline void SimpleRecurrentLayerParams::set_outputvectorsize(::google::protobuf::uint64 value) {

  outputvectorsize_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SimpleRecurrentLayerParams.outputVectorSize)
}

// optional .CoreML.Specification.ActivationParams activation = 10;
inline bool SimpleRecurrentLayerParams::has_activation() const {
  return this != internal_default_instance() && activation_ != NULL;
}
inline void SimpleRecurrentLayerParams::clear_activation() {
  if (GetArenaNoVirtual() == NULL && activation_ != NULL) delete activation_;
  activation_ = NULL;
}
inline const ::CoreML::Specification::ActivationParams& SimpleRecurrentLayerParams::activation() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SimpleRecurrentLayerParams.activation)
  return activation_ != NULL ? *activation_
                         : *::CoreML::Specification::ActivationParams::internal_default_instance();
}
inline ::CoreML::Specification::ActivationParams* SimpleRecurrentLayerParams::mutable_activation() {

  if (activation_ == NULL) {
    activation_ = new ::CoreML::Specification::ActivationParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.SimpleRecurrentLayerParams.activation)
  return activation_;
}
inline ::CoreML::Specification::ActivationParams* SimpleRecurrentLayerParams::release_activation() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.SimpleRecurrentLayerParams.activation)

  ::CoreML::Specification::ActivationParams* temp = activation_;
  activation_ = NULL;
  return temp;
}
inline void SimpleRecurrentLayerParams::set_allocated_activation(::CoreML::Specification::ActivationParams* activation) {
  delete activation_;
  activation_ = activation;
  if (activation) {

  } else {

  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.SimpleRecurrentLayerParams.activation)
}

// optional bool sequenceOutput = 15;
inline void SimpleRecurrentLayerParams::clear_sequenceoutput() {
  sequenceoutput_ = false;
}
inline bool SimpleRecurrentLayerParams::sequenceoutput() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SimpleRecurrentLayerParams.sequenceOutput)
  return sequenceoutput_;
}
inline void SimpleRecurrentLayerParams::set_sequenceoutput(bool value) {

  sequenceoutput_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SimpleRecurrentLayerParams.sequenceOutput)
}

// optional bool hasBiasVector = 20;
inline void SimpleRecurrentLayerParams::clear_hasbiasvector() {
  hasbiasvector_ = false;
}
inline bool SimpleRecurrentLayerParams::hasbiasvector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SimpleRecurrentLayerParams.hasBiasVector)
  return hasbiasvector_;
}
inline void SimpleRecurrentLayerParams::set_hasbiasvector(bool value) {

  hasbiasvector_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SimpleRecurrentLayerParams.hasBiasVector)
}

// optional .CoreML.Specification.WeightParams weightMatrix = 30;
inline bool SimpleRecurrentLayerParams::has_weightmatrix() const {
  return this != internal_default_instance() && weightmatrix_ != NULL;
}
inline void SimpleRecurrentLayerParams::clear_weightmatrix() {
  if (GetArenaNoVirtual() == NULL && weightmatrix_ != NULL) delete weightmatrix_;
  weightmatrix_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& SimpleRecurrentLayerParams::weightmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SimpleRecurrentLayerParams.weightMatrix)
  return weightmatrix_ != NULL ? *weightmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* SimpleRecurrentLayerParams::mutable_weightmatrix() {

  if (weightmatrix_ == NULL) {
    weightmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.SimpleRecurrentLayerParams.weightMatrix)
  return weightmatrix_;
}
inline ::CoreML::Specification::WeightParams* SimpleRecurrentLayerParams::release_weightmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.SimpleRecurrentLayerParams.weightMatrix)

  ::CoreML::Specification::WeightParams* temp = weightmatrix_;
  weightmatrix_ = NULL;
  return temp;
}
inline void SimpleRecurrentLayerParams::set_allocated_weightmatrix(::CoreML::Specification::WeightParams* weightmatrix) {
  delete weightmatrix_;
  weightmatrix_ = weightmatrix;
  if (weightmatrix) {

  } else {

  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.SimpleRecurrentLayerParams.weightMatrix)
}

// optional .CoreML.Specification.WeightParams recursionMatrix = 31;
inline bool SimpleRecurrentLayerParams::has_recursionmatrix() const {
  return this != internal_default_instance() && recursionmatrix_ != NULL;
}
inline void SimpleRecurrentLayerParams::clear_recursionmatrix() {
  if (GetArenaNoVirtual() == NULL && recursionmatrix_ != NULL) delete recursionmatrix_;
  recursionmatrix_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& SimpleRecurrentLayerParams::recursionmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SimpleRecurrentLayerParams.recursionMatrix)
  return recursionmatrix_ != NULL ? *recursionmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* SimpleRecurrentLayerParams::mutable_recursionmatrix() {

  if (recursionmatrix_ == NULL) {
    recursionmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.SimpleRecurrentLayerParams.recursionMatrix)
  return recursionmatrix_;
}
inline ::CoreML::Specification::WeightParams* SimpleRecurrentLayerParams::release_recursionmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.SimpleRecurrentLayerParams.recursionMatrix)

  ::CoreML::Specification::WeightParams* temp = recursionmatrix_;
  recursionmatrix_ = NULL;
  return temp;
}
inline void SimpleRecurrentLayerParams::set_allocated_recursionmatrix(::CoreML::Specification::WeightParams* recursionmatrix) {
  delete recursionmatrix_;
  recursionmatrix_ = recursionmatrix;
  if (recursionmatrix) {

  } else {

  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.SimpleRecurrentLayerParams.recursionMatrix)
}

// optional .CoreML.Specification.WeightParams biasVector = 32;
inline bool SimpleRecurrentLayerParams::has_biasvector() const {
  return this != internal_default_instance() && biasvector_ != NULL;
}
inline void SimpleRecurrentLayerParams::clear_biasvector() {
  if (GetArenaNoVirtual() == NULL && biasvector_ != NULL) delete biasvector_;
  biasvector_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& SimpleRecurrentLayerParams::biasvector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SimpleRecurrentLayerParams.biasVector)
  return biasvector_ != NULL ? *biasvector_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* SimpleRecurrentLayerParams::mutable_biasvector() {

  if (biasvector_ == NULL) {
    biasvector_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.SimpleRecurrentLayerParams.biasVector)
  return biasvector_;
}
inline ::CoreML::Specification::WeightParams* SimpleRecurrentLayerParams::release_biasvector() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.SimpleRecurrentLayerParams.biasVector)

  ::CoreML::Specification::WeightParams* temp = biasvector_;
  biasvector_ = NULL;
  return temp;
}
inline void SimpleRecurrentLayerParams::set_allocated_biasvector(::CoreML::Specification::WeightParams* biasvector) {
  delete biasvector_;
  biasvector_ = biasvector;
  if (biasvector) {

  } else {

  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.SimpleRecurrentLayerParams.biasVector)
}

// optional bool reverseInput = 100;
inline void SimpleRecurrentLayerParams::clear_reverseinput() {
  reverseinput_ = false;
}
inline bool SimpleRecurrentLayerParams::reverseinput() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SimpleRecurrentLayerParams.reverseInput)
  return reverseinput_;
}
inline void SimpleRecurrentLayerParams::set_reverseinput(bool value) {

  reverseinput_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SimpleRecurrentLayerParams.reverseInput)
}

inline const SimpleRecurrentLayerParams* SimpleRecurrentLayerParams::internal_default_instance() {
  return &SimpleRecurrentLayerParams_default_instance_.get();
}
// -------------------------------------------------------------------

// GRULayerParams

// optional uint64 inputVectorSize = 1;
inline void GRULayerParams::clear_inputvectorsize() {
  inputvectorsize_ = GOOGLE_ULONGLONG(0);
}
inline ::google::protobuf::uint64 GRULayerParams::inputvectorsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.inputVectorSize)
  return inputvectorsize_;
}
inline void GRULayerParams::set_inputvectorsize(::google::protobuf::uint64 value) {

  inputvectorsize_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.GRULayerParams.inputVectorSize)
}

// optional uint64 outputVectorSize = 2;
inline void GRULayerParams::clear_outputvectorsize() {
  outputvectorsize_ = GOOGLE_ULONGLONG(0);
}
inline ::google::protobuf::uint64 GRULayerParams::outputvectorsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.outputVectorSize)
  return outputvectorsize_;
}
inline void GRULayerParams::set_outputvectorsize(::google::protobuf::uint64 value) {

  outputvectorsize_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.GRULayerParams.outputVectorSize)
}

// repeated .CoreML.Specification.ActivationParams activations = 10;
inline int GRULayerParams::activations_size() const {
  return activations_.size();
}
inline void GRULayerParams::clear_activations() {
  activations_.Clear();
}
inline const ::CoreML::Specification::ActivationParams& GRULayerParams::activations(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.activations)
  return activations_.Get(index);
}
inline ::CoreML::Specification::ActivationParams* GRULayerParams::mutable_activations(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.GRULayerParams.activations)
  return activations_.Mutable(index);
}
inline ::CoreML::Specification::ActivationParams* GRULayerParams::add_activations() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.GRULayerParams.activations)
  return activations_.Add();
}
inline ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::ActivationParams >*
GRULayerParams::mutable_activations() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.GRULayerParams.activations)
  return &activations_;
}
inline const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::ActivationParams >&
GRULayerParams::activations() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.GRULayerParams.activations)
  return activations_;
}

// optional bool sequenceOutput = 15;
inline void GRULayerParams::clear_sequenceoutput() {
  sequenceoutput_ = false;
}
inline bool GRULayerParams::sequenceoutput() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.sequenceOutput)
  return sequenceoutput_;
}
inline void GRULayerParams::set_sequenceoutput(bool value) {

  sequenceoutput_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.GRULayerParams.sequenceOutput)
}

// optional bool hasBiasVectors = 20;
inline void GRULayerParams::clear_hasbiasvectors() {
  hasbiasvectors_ = false;
}
inline bool GRULayerParams::hasbiasvectors() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.hasBiasVectors)
  return hasbiasvectors_;
}
inline void GRULayerParams::set_hasbiasvectors(bool value) {

  hasbiasvectors_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.GRULayerParams.hasBiasVectors)
}

// optional .CoreML.Specification.WeightParams updateGateWeightMatrix = 30;
inline bool GRULayerParams::has_updategateweightmatrix() const {
  return this != internal_default_instance() && updategateweightmatrix_ != NULL;
}
inline void GRULayerParams::clear_updategateweightmatrix() {
  if (GetArenaNoVirtual() == NULL && updategateweightmatrix_ != NULL) delete updategateweightmatrix_;
  updategateweightmatrix_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& GRULayerParams::updategateweightmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.updateGateWeightMatrix)
  return updategateweightmatrix_ != NULL ? *updategateweightmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* GRULayerParams::mutable_updategateweightmatrix() {

  if (updategateweightmatrix_ == NULL) {
    updategateweightmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.GRULayerParams.updateGateWeightMatrix)
  return updategateweightmatrix_;
}
inline ::CoreML::Specification::WeightParams* GRULayerParams::release_updategateweightmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.GRULayerParams.updateGateWeightMatrix)

  ::CoreML::Specification::WeightParams* temp = updategateweightmatrix_;
  updategateweightmatrix_ = NULL;
  return temp;
}
inline void GRULayerParams::set_allocated_updategateweightmatrix(::CoreML::Specification::WeightParams* updategateweightmatrix) {
  delete updategateweightmatrix_;
  updategateweightmatrix_ = updategateweightmatrix;
  if (updategateweightmatrix) {

  } else {

  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.GRULayerParams.updateGateWeightMatrix)
}

// optional .CoreML.Specification.WeightParams resetGateWeightMatrix = 31;
inline bool GRULayerParams::has_resetgateweightmatrix() const {
  return this != internal_default_instance() && resetgateweightmatrix_ != NULL;
}
inline void GRULayerParams::clear_resetgateweightmatrix() {
  if (GetArenaNoVirtual() == NULL && resetgateweightmatrix_ != NULL) delete resetgateweightmatrix_;
  resetgateweightmatrix_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& GRULayerParams::resetgateweightmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.resetGateWeightMatrix)
  return resetgateweightmatrix_ != NULL ? *resetgateweightmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* GRULayerParams::mutable_resetgateweightmatrix() {

  if (resetgateweightmatrix_ == NULL) {
    resetgateweightmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.GRULayerParams.resetGateWeightMatrix)
  return resetgateweightmatrix_;
}
inline ::CoreML::Specification::WeightParams* GRULayerParams::release_resetgateweightmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.GRULayerParams.resetGateWeightMatrix)

  ::CoreML::Specification::WeightParams* temp = resetgateweightmatrix_;
  resetgateweightmatrix_ = NULL;
  return temp;
}
inline void GRULayerParams::set_allocated_resetgateweightmatrix(::CoreML::Specification::WeightParams* resetgateweightmatrix) {
  delete resetgateweightmatrix_;
  resetgateweightmatrix_ = resetgateweightmatrix;
  if (resetgateweightmatrix) {

  } else {

  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.GRULayerParams.resetGateWeightMatrix)
}

// optional .CoreML.Specification.WeightParams outputGateWeightMatrix = 32;
inline bool GRULayerParams::has_outputgateweightmatrix() const {
  return this != internal_default_instance() && outputgateweightmatrix_ != NULL;
}
inline void GRULayerParams::clear_outputgateweightmatrix() {
  if (GetArenaNoVirtual() == NULL && outputgateweightmatrix_ != NULL) delete outputgateweightmatrix_;
  outputgateweightmatrix_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& GRULayerParams::outputgateweightmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.outputGateWeightMatrix)
  return outputgateweightmatrix_ != NULL ? *outputgateweightmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* GRULayerParams::mutable_outputgateweightmatrix() {

  if (outputgateweightmatrix_ == NULL) {
    outputgateweightmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.GRULayerParams.outputGateWeightMatrix)
  return outputgateweightmatrix_;
}
inline ::CoreML::Specification::WeightParams* GRULayerParams::release_outputgateweightmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.GRULayerParams.outputGateWeightMatrix)

  ::CoreML::Specification::WeightParams* temp = outputgateweightmatrix_;
  outputgateweightmatrix_ = NULL;
  return temp;
}
inline void GRULayerParams::set_allocated_outputgateweightmatrix(::CoreML::Specification::WeightParams* outputgateweightmatrix) {
  delete outputgateweightmatrix_;
  outputgateweightmatrix_ = outputgateweightmatrix;
  if (outputgateweightmatrix) {

  } else {

  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.GRULayerParams.outputGateWeightMatrix)
}

// optional .CoreML.Specification.WeightParams updateGateRecursionMatrix = 50;
inline bool GRULayerParams::has_updategaterecursionmatrix() const {
  return this != internal_default_instance() && updategaterecursionmatrix_ != NULL;
}
inline void GRULayerParams::clear_updategaterecursionmatrix() {
  if (GetArenaNoVirtual() == NULL && updategaterecursionmatrix_ != NULL) delete updategaterecursionmatrix_;
  updategaterecursionmatrix_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& GRULayerParams::updategaterecursionmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.updateGateRecursionMatrix)
  return updategaterecursionmatrix_ != NULL ? *updategaterecursionmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* GRULayerParams::mutable_updategaterecursionmatrix() {

  if (updategaterecursionmatrix_ == NULL) {
    updategaterecursionmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.GRULayerParams.updateGateRecursionMatrix)
  return updategaterecursionmatrix_;
}
inline ::CoreML::Specification::WeightParams* GRULayerParams::release_updategaterecursionmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.GRULayerParams.updateGateRecursionMatrix)

  ::CoreML::Specification::WeightParams* temp = updategaterecursionmatrix_;
  updategaterecursionmatrix_ = NULL;
  return temp;
}
inline void GRULayerParams::set_allocated_updategaterecursionmatrix(::CoreML::Specification::WeightParams* updategaterecursionmatrix) {
  delete updategaterecursionmatrix_;
  updategaterecursionmatrix_ = updategaterecursionmatrix;
  if (updategaterecursionmatrix) {

  } else {

  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.GRULayerParams.updateGateRecursionMatrix)
}

// optional .CoreML.Specification.WeightParams resetGateRecursionMatrix = 51;
inline bool GRULayerParams::has_resetgaterecursionmatrix() const {
  return this != internal_default_instance() && resetgaterecursionmatrix_ != NULL;
}
inline void GRULayerParams::clear_resetgaterecursionmatrix() {
  if (GetArenaNoVirtual() == NULL && resetgaterecursionmatrix_ != NULL) delete resetgaterecursionmatrix_;
  resetgaterecursionmatrix_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& GRULayerParams::resetgaterecursionmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.resetGateRecursionMatrix)
  return resetgaterecursionmatrix_ != NULL ? *resetgaterecursionmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* GRULayerParams::mutable_resetgaterecursionmatrix() {

  if (resetgaterecursionmatrix_ == NULL) {
    resetgaterecursionmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.GRULayerParams.resetGateRecursionMatrix)
  return resetgaterecursionmatrix_;
}
inline ::CoreML::Specification::WeightParams* GRULayerParams::release_resetgaterecursionmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.GRULayerParams.resetGateRecursionMatrix)

  ::CoreML::Specification::WeightParams* temp = resetgaterecursionmatrix_;
  resetgaterecursionmatrix_ = NULL;
  return temp;
}
inline void GRULayerParams::set_allocated_resetgaterecursionmatrix(::CoreML::Specification::WeightParams* resetgaterecursionmatrix) {
  delete resetgaterecursionmatrix_;
  resetgaterecursionmatrix_ = resetgaterecursionmatrix;
  if (resetgaterecursionmatrix) {

  } else {

  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.GRULayerParams.resetGateRecursionMatrix)
}

// optional .CoreML.Specification.WeightParams outputGateRecursionMatrix = 52;
inline bool GRULayerParams::has_outputgaterecursionmatrix() const {
  return this != internal_default_instance() && outputgaterecursionmatrix_ != NULL;
}
inline void GRULayerParams::clear_outputgaterecursionmatrix() {
  if (GetArenaNoVirtual() == NULL && outputgaterecursionmatrix_ != NULL) delete outputgaterecursionmatrix_;
  outputgaterecursionmatrix_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& GRULayerParams::outputgaterecursionmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.outputGateRecursionMatrix)
  return outputgaterecursionmatrix_ != NULL ? *outputgaterecursionmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* GRULayerParams::mutable_outputgaterecursionmatrix() {

  if (outputgaterecursionmatrix_ == NULL) {
    outputgaterecursionmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.GRULayerParams.outputGateRecursionMatrix)
  return outputgaterecursionmatrix_;
}
inline ::CoreML::Specification::WeightParams* GRULayerParams::release_outputgaterecursionmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.GRULayerParams.outputGateRecursionMatrix)

  ::CoreML::Specification::WeightParams* temp = outputgaterecursionmatrix_;
  outputgaterecursionmatrix_ = NULL;
  return temp;
}
inline void GRULayerParams::set_allocated_outputgaterecursionmatrix(::CoreML::Specification::WeightParams* outputgaterecursionmatrix) {
  delete outputgaterecursionmatrix_;
  outputgaterecursionmatrix_ = outputgaterecursionmatrix;
  if (outputgaterecursionmatrix) {

  } else {

  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.GRULayerParams.outputGateRecursionMatrix)
}

// optional .CoreML.Specification.WeightParams updateGateBiasVector = 70;
inline bool GRULayerParams::has_updategatebiasvector() const {
  return this != internal_default_instance() && updategatebiasvector_ != NULL;
}
inline void GRULayerParams::clear_updategatebiasvector() {
  if (GetArenaNoVirtual() == NULL && updategatebiasvector_ != NULL) delete updategatebiasvector_;
  updategatebiasvector_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& GRULayerParams::updategatebiasvector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.updateGateBiasVector)
  return updategatebiasvector_ != NULL ? *updategatebiasvector_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* GRULayerParams::mutable_updategatebiasvector() {

  if (updategatebiasvector_ == NULL) {
    updategatebiasvector_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.GRULayerParams.updateGateBiasVector)
  return updategatebiasvector_;
}
inline ::CoreML::Specification::WeightParams* GRULayerParams::release_updategatebiasvector() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.GRULayerParams.updateGateBiasVector)

  ::CoreML::Specification::WeightParams* temp = updategatebiasvector_;
  updategatebiasvector_ = NULL;
  return temp;
}
inline void GRULayerParams::set_allocated_updategatebiasvector(::CoreML::Specification::WeightParams* updategatebiasvector) {
  delete updategatebiasvector_;
  updategatebiasvector_ = updategatebiasvector;
  if (updategatebiasvector) {

  } else {

  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.GRULayerParams.updateGateBiasVector)
}

// optional .CoreML.Specification.WeightParams resetGateBiasVector = 71;
inline bool GRULayerParams::has_resetgatebiasvector() const {
  return this != internal_default_instance() && resetgatebiasvector_ != NULL;
}
inline void GRULayerParams::clear_resetgatebiasvector() {
  if (GetArenaNoVirtual() == NULL && resetgatebiasvector_ != NULL) delete resetgatebiasvector_;
  resetgatebiasvector_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& GRULayerParams::resetgatebiasvector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.resetGateBiasVector)
  return resetgatebiasvector_ != NULL ? *resetgatebiasvector_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* GRULayerParams::mutable_resetgatebiasvector() {

  if (resetgatebiasvector_ == NULL) {
    resetgatebiasvector_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.GRULayerParams.resetGateBiasVector)
  return resetgatebiasvector_;
}
inline ::CoreML::Specification::WeightParams* GRULayerParams::release_resetgatebiasvector() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.GRULayerParams.resetGateBiasVector)

  ::CoreML::Specification::WeightParams* temp = resetgatebiasvector_;
  resetgatebiasvector_ = NULL;
  return temp;
}
inline void GRULayerParams::set_allocated_resetgatebiasvector(::CoreML::Specification::WeightParams* resetgatebiasvector) {
  delete resetgatebiasvector_;
  resetgatebiasvector_ = resetgatebiasvector;
  if (resetgatebiasvector) {

  } else {

  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.GRULayerParams.resetGateBiasVector)
}

// optional .CoreML.Specification.WeightParams outputGateBiasVector = 72;
inline bool GRULayerParams::has_outputgatebiasvector() const {
  return this != internal_default_instance() && outputgatebiasvector_ != NULL;
}
inline void GRULayerParams::clear_outputgatebiasvector() {
  if (GetArenaNoVirtual() == NULL && outputgatebiasvector_ != NULL) delete outputgatebiasvector_;
  outputgatebiasvector_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& GRULayerParams::outputgatebiasvector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.outputGateBiasVector)
  return outputgatebiasvector_ != NULL ? *outputgatebiasvector_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* GRULayerParams::mutable_outputgatebiasvector() {

  if (outputgatebiasvector_ == NULL) {
    outputgatebiasvector_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.GRULayerParams.outputGateBiasVector)
  return outputgatebiasvector_;
}
inline ::CoreML::Specification::WeightParams* GRULayerParams::release_outputgatebiasvector() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.GRULayerParams.outputGateBiasVector)

  ::CoreML::Specification::WeightParams* temp = outputgatebiasvector_;
  outputgatebiasvector_ = NULL;
  return temp;
}
inline void GRULayerParams::set_allocated_outputgatebiasvector(::CoreML::Specification::WeightParams* outputgatebiasvector) {
  delete outputgatebiasvector_;
  outputgatebiasvector_ = outputgatebiasvector;
  if (outputgatebiasvector) {

  } else {

  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.GRULayerParams.outputGateBiasVector)
}

// optional bool reverseInput = 100;
inline void GRULayerParams::clear_reverseinput() {
  reverseinput_ = false;
}
inline bool GRULayerParams::reverseinput() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.reverseInput)
  return reverseinput_;
}
inline void GRULayerParams::set_reverseinput(bool value) {

  reverseinput_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.GRULayerParams.reverseInput)
}

inline const GRULayerParams* GRULayerParams::internal_default_instance() {
  return &GRULayerParams_default_instance_.get();
}
// -------------------------------------------------------------------

// LSTMParams

// optional bool sequenceOutput = 10;
inline void LSTMParams::clear_sequenceoutput() {
  sequenceoutput_ = false;
}
inline bool LSTMParams::sequenceoutput() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMParams.sequenceOutput)
  return sequenceoutput_;
}
inline void LSTMParams::set_sequenceoutput(bool value) {

  sequenceoutput_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.LSTMParams.sequenceOutput)
}

// optional bool hasBiasVectors = 20;
inline void LSTMParams::clear_hasbiasvectors() {
  hasbiasvectors_ = false;
}
inline bool LSTMParams::hasbiasvectors() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMParams.hasBiasVectors)
  return hasbiasvectors_;
}
inline void LSTMParams::set_hasbiasvectors(bool value) {

  hasbiasvectors_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.LSTMParams.hasBiasVectors)
}

// optional bool forgetBias = 30;
inline void LSTMParams::clear_forgetbias() {
  forgetbias_ = false;
}
inline bool LSTMParams::forgetbias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMParams.forgetBias)
  return forgetbias_;
}
inline void LSTMParams::set_forgetbias(bool value) {

  forgetbias_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.LSTMParams.forgetBias)
}

// optional bool hasPeepholeVectors = 40;
inline void LSTMParams::clear_haspeepholevectors() {
  haspeepholevectors_ = false;
}
inline bool LSTMParams::haspeepholevectors() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMParams.hasPeepholeVectors)
  return haspeepholevectors_;
}
inline void LSTMParams::set_haspeepholevectors(bool value) {

  haspeepholevectors_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.LSTMParams.hasPeepholeVectors)
}

// optional bool coupledInputAndForgetGate = 50;
inline void LSTMParams::clear_coupledinputandforgetgate() {
  coupledinputandforgetgate_ = false;
}
inline bool LSTMParams::coupledinputandforgetgate() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMParams.coupledInputAndForgetGate)
  return coupledinputandforgetgate_;
}
inline void LSTMParams::set_coupledinputandforgetgate(bool value) {

  coupledinputandforgetgate_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.LSTMParams.coupledInputAndForgetGate)
}

// optional float cellClipThreshold = 60;
inline void LSTMParams::clear_cellclipthreshold() {
  cellclipthreshold_ = 0;
}
inline float LSTMParams::cellclipthreshold() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMParams.cellClipThreshold)
  return cellclipthreshold_;
}
inline void LSTMParams::set_cellclipthreshold(float value) {

  cellclipthreshold_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.LSTMParams.cellClipThreshold)
}

inline const LSTMParams* LSTMParams::internal_default_instance() {
  return &LSTMParams_default_instance_.get();
}
// -------------------------------------------------------------------

// LSTMWeightParams

// optional .CoreML.Specification.WeightParams inputGateWeightMatrix = 1;
inline bool LSTMWeightParams::has_inputgateweightmatrix() const {
  return this != internal_default_instance() && inputgateweightmatrix_ != NULL;
}
inline void LSTMWeightParams::clear_inputgateweightmatrix() {
  if (GetArenaNoVirtual() == NULL && inputgateweightmatrix_ != NULL) delete inputgateweightmatrix_;
  inputgateweightmatrix_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& LSTMWeightParams::inputgateweightmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.inputGateWeightMatrix)
  return inputgateweightmatrix_ != NULL ? *inputgateweightmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_inputgateweightmatrix() {

  if (inputgateweightmatrix_ == NULL) {
    inputgateweightmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.inputGateWeightMatrix)
  return inputgateweightmatrix_;
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::release_inputgateweightmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.inputGateWeightMatrix)

  ::CoreML::Specification::WeightParams* temp = inputgateweightmatrix_;
  inputgateweightmatrix_ = NULL;
  return temp;
}
inline void LSTMWeightParams::set_allocated_inputgateweightmatrix(::CoreML::Specification::WeightParams* inputgateweightmatrix) {
  delete inputgateweightmatrix_;
  inputgateweightmatrix_ = inputgateweightmatrix;
  if (inputgateweightmatrix) {

  } else {

  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.inputGateWeightMatrix)
}

// optional .CoreML.Specification.WeightParams forgetGateWeightMatrix = 2;
inline bool LSTMWeightParams::has_forgetgateweightmatrix() const {
  return this != internal_default_instance() && forgetgateweightmatrix_ != NULL;
}
inline void LSTMWeightParams::clear_forgetgateweightmatrix() {
  if (GetArenaNoVirtual() == NULL && forgetgateweightmatrix_ != NULL) delete forgetgateweightmatrix_;
  forgetgateweightmatrix_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& LSTMWeightParams::forgetgateweightmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.forgetGateWeightMatrix)
  return forgetgateweightmatrix_ != NULL ? *forgetgateweightmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_forgetgateweightmatrix() {

  if (forgetgateweightmatrix_ == NULL) {
    forgetgateweightmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.forgetGateWeightMatrix)
  return forgetgateweightmatrix_;
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::release_forgetgateweightmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.forgetGateWeightMatrix)

  ::CoreML::Specification::WeightParams* temp = forgetgateweightmatrix_;
  forgetgateweightmatrix_ = NULL;
  return temp;
}
inline void LSTMWeightParams::set_allocated_forgetgateweightmatrix(::CoreML::Specification::WeightParams* forgetgateweightmatrix) {
  delete forgetgateweightmatrix_;
  forgetgateweightmatrix_ = forgetgateweightmatrix;
  if (forgetgateweightmatrix) {

  } else {

  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.forgetGateWeightMatrix)
}

// optional .CoreML.Specification.WeightParams blockInputWeightMatrix = 3;
inline bool LSTMWeightParams::has_blockinputweightmatrix() const {
  return this != internal_default_instance() && blockinputweightmatrix_ != NULL;
}
inline void LSTMWeightParams::clear_blockinputweightmatrix() {
  if (GetArenaNoVirtual() == NULL && blockinputweightmatrix_ != NULL) delete blockinputweightmatrix_;
  blockinputweightmatrix_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& LSTMWeightParams::blockinputweightmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.blockInputWeightMatrix)
  return blockinputweightmatrix_ != NULL ? *blockinputweightmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_blockinputweightmatrix() {

  if (blockinputweightmatrix_ == NULL) {
    blockinputweightmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.blockInputWeightMatrix)
  return blockinputweightmatrix_;
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::release_blockinputweightmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.blockInputWeightMatrix)

  ::CoreML::Specification::WeightParams* temp = blockinputweightmatrix_;
  blockinputweightmatrix_ = NULL;
  return temp;
}
inline void LSTMWeightParams::set_allocated_blockinputweightmatrix(::CoreML::Specification::WeightParams* blockinputweightmatrix) {
  delete blockinputweightmatrix_;
  blockinputweightmatrix_ = blockinputweightmatrix;
  if (blockinputweightmatrix) {

  } else {

  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.blockInputWeightMatrix)
}

// optional .CoreML.Specification.WeightParams outputGateWeightMatrix = 4;
inline bool LSTMWeightParams::has_outputgateweightmatrix() const {
  return this != internal_default_instance() && outputgateweightmatrix_ != NULL;
}
inline void LSTMWeightParams::clear_outputgateweightmatrix() {
  if (GetArenaNoVirtual() == NULL && outputgateweightmatrix_ != NULL) delete outputgateweightmatrix_;
  outputgateweightmatrix_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& LSTMWeightParams::outputgateweightmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.outputGateWeightMatrix)
  return outputgateweightmatrix_ != NULL ? *outputgateweightmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_outputgateweightmatrix() {

  if (outputgateweightmatrix_ == NULL) {
    outputgateweightmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.outputGateWeightMatrix)
  return outputgateweightmatrix_;
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::release_outputgateweightmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.outputGateWeightMatrix)

  ::CoreML::Specification::WeightParams* temp = outputgateweightmatrix_;
  outputgateweightmatrix_ = NULL;
  return temp;
}
inline void LSTMWeightParams::set_allocated_outputgateweightmatrix(::CoreML::Specification::WeightParams* outputgateweightmatrix) {
  delete outputgateweightmatrix_;
  outputgateweightmatrix_ = outputgateweightmatrix;
  if (outputgateweightmatrix) {

  } else {

  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.outputGateWeightMatrix)
}

// optional .CoreML.Specification.WeightParams inputGateRecursionMatrix = 20;
inline bool LSTMWeightParams::has_inputgaterecursionmatrix() const {
  return this != internal_default_instance() && inputgaterecursionmatrix_ != NULL;
}
inline void LSTMWeightParams::clear_inputgaterecursionmatrix() {
  if (GetArenaNoVirtual() == NULL && inputgaterecursionmatrix_ != NULL) delete inputgaterecursionmatrix_;
  inputgaterecursionmatrix_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& LSTMWeightParams::inputgaterecursionmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.inputGateRecursionMatrix)
  return inputgaterecursionmatrix_ != NULL ? *inputgaterecursionmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_inputgaterecursionmatrix() {

  if (inputgaterecursionmatrix_ == NULL) {
    inputgaterecursionmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.inputGateRecursionMatrix)
  return inputgaterecursionmatrix_;
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::release_inputgaterecursionmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.inputGateRecursionMatrix)

  ::CoreML::Specification::WeightParams* temp = inputgaterecursionmatrix_;
  inputgaterecursionmatrix_ = NULL;
  return temp;
}
inline void LSTMWeightParams::set_allocated_inputgaterecursionmatrix(::CoreML::Specification::WeightParams* inputgaterecursionmatrix) {
  delete inputgaterecursionmatrix_;
  inputgaterecursionmatrix_ = inputgaterecursionmatrix;
  if (inputgaterecursionmatrix) {

  } else {

  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.inputGateRecursionMatrix)
}

// optional .CoreML.Specification.WeightParams forgetGateRecursionMatrix = 21;
inline bool LSTMWeightParams::has_forgetgaterecursionmatrix() const {
  return this != internal_default_instance() && forgetgaterecursionmatrix_ != NULL;
}
inline void LSTMWeightParams::clear_forgetgaterecursionmatrix() {
  if (GetArenaNoVirtual() == NULL && forgetgaterecursionmatrix_ != NULL) delete forgetgaterecursionmatrix_;
  forgetgaterecursionmatrix_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& LSTMWeightParams::forgetgaterecursionmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.forgetGateRecursionMatrix)
  return forgetgaterecursionmatrix_ != NULL ? *forgetgaterecursionmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_forgetgaterecursionmatrix() {

  if (forgetgaterecursionmatrix_ == NULL) {
    forgetgaterecursionmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.forgetGateRecursionMatrix)
  return forgetgaterecursionmatrix_;
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::release_forgetgaterecursionmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.forgetGateRecursionMatrix)

  ::CoreML::Specification::WeightParams* temp = forgetgaterecursionmatrix_;
  forgetgaterecursionmatrix_ = NULL;
  return temp;
}
inline void LSTMWeightParams::set_allocated_forgetgaterecursionmatrix(::CoreML::Specification::WeightParams* forgetgaterecursionmatrix) {
  delete forgetgaterecursionmatrix_;
  forgetgaterecursionmatrix_ = forgetgaterecursionmatrix;
  if (forgetgaterecursionmatrix) {

  } else {

  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.forgetGateRecursionMatrix)
}

// optional .CoreML.Specification.WeightParams blockInputRecursionMatrix = 22;
inline bool LSTMWeightParams::has_blockinputrecursionmatrix() const {
  return this != internal_default_instance() && blockinputrecursionmatrix_ != NULL;
}
inline void LSTMWeightParams::clear_blockinputrecursionmatrix() {
  if (GetArenaNoVirtual() == NULL && blockinputrecursionmatrix_ != NULL) delete blockinputrecursionmatrix_;
  blockinputrecursionmatrix_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& LSTMWeightParams::blockinputrecursionmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.blockInputRecursionMatrix)
  return blockinputrecursionmatrix_ != NULL ? *blockinputrecursionmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_blockinputrecursionmatrix() {

  if (blockinputrecursionmatrix_ == NULL) {
    blockinputrecursionmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.blockInputRecursionMatrix)
  return blockinputrecursionmatrix_;
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::release_blockinputrecursionmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.blockInputRecursionMatrix)

  ::CoreML::Specification::WeightParams* temp = blockinputrecursionmatrix_;
  blockinputrecursionmatrix_ = NULL;
  return temp;
}
inline void LSTMWeightParams::set_allocated_blockinputrecursionmatrix(::CoreML::Specification::WeightParams* blockinputrecursionmatrix) {
  delete blockinputrecursionmatrix_;
  blockinputrecursionmatrix_ = blockinputrecursionmatrix;
  if (blockinputrecursionmatrix) {

  } else {

  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.blockInputRecursionMatrix)
}

// optional .CoreML.Specification.WeightParams outputGateRecursionMatrix = 23;
inline bool LSTMWeightParams::has_outputgaterecursionmatrix() const {
  return this != internal_default_instance() && outputgaterecursionmatrix_ != NULL;
}
inline void LSTMWeightParams::clear_outputgaterecursionmatrix() {
  if (GetArenaNoVirtual() == NULL && outputgaterecursionmatrix_ != NULL) delete outputgaterecursionmatrix_;
  outputgaterecursionmatrix_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& LSTMWeightParams::outputgaterecursionmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.outputGateRecursionMatrix)
  return outputgaterecursionmatrix_ != NULL ? *outputgaterecursionmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_outputgaterecursionmatrix() {

  if (outputgaterecursionmatrix_ == NULL) {
    outputgaterecursionmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.outputGateRecursionMatrix)
  return outputgaterecursionmatrix_;
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::release_outputgaterecursionmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.outputGateRecursionMatrix)

  ::CoreML::Specification::WeightParams* temp = outputgaterecursionmatrix_;
  outputgaterecursionmatrix_ = NULL;
  return temp;
}
inline void LSTMWeightParams::set_allocated_outputgaterecursionmatrix(::CoreML::Specification::WeightParams* outputgaterecursionmatrix) {
  delete outputgaterecursionmatrix_;
  outputgaterecursionmatrix_ = outputgaterecursionmatrix;
  if (outputgaterecursionmatrix) {

  } else {

  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.outputGateRecursionMatrix)
}

// optional .CoreML.Specification.WeightParams inputGateBiasVector = 40;
inline bool LSTMWeightParams::has_inputgatebiasvector() const {
  return this != internal_default_instance() && inputgatebiasvector_ != NULL;
}
inline void LSTMWeightParams::clear_inputgatebiasvector() {
  if (GetArenaNoVirtual() == NULL && inputgatebiasvector_ != NULL) delete inputgatebiasvector_;
  inputgatebiasvector_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& LSTMWeightParams::inputgatebiasvector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.inputGateBiasVector)
  return inputgatebiasvector_ != NULL ? *inputgatebiasvector_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_inputgatebiasvector() {

  if (inputgatebiasvector_ == NULL) {
    inputgatebiasvector_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.inputGateBiasVector)
  return inputgatebiasvector_;
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::release_inputgatebiasvector() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.inputGateBiasVector)

  ::CoreML::Specification::WeightParams* temp = inputgatebiasvector_;
  inputgatebiasvector_ = NULL;
  return temp;
}
inline void LSTMWeightParams::set_allocated_inputgatebiasvector(::CoreML::Specification::WeightParams* inputgatebiasvector) {
  delete inputgatebiasvector_;
  inputgatebiasvector_ = inputgatebiasvector;
  if (inputgatebiasvector) {

  } else {

  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.inputGateBiasVector)
}

// optional .CoreML.Specification.WeightParams forgetGateBiasVector = 41;
inline bool LSTMWeightParams::has_forgetgatebiasvector() const {
  return this != internal_default_instance() && forgetgatebiasvector_ != NULL;
}
inline void LSTMWeightParams::clear_forgetgatebiasvector() {
  if (GetArenaNoVirtual() == NULL && forgetgatebiasvector_ != NULL) delete forgetgatebiasvector_;
  forgetgatebiasvector_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& LSTMWeightParams::forgetgatebiasvector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.forgetGateBiasVector)
  return forgetgatebiasvector_ != NULL ? *forgetgatebiasvector_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_forgetgatebiasvector() {

  if (forgetgatebiasvector_ == NULL) {
    forgetgatebiasvector_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.forgetGateBiasVector)
  return forgetgatebiasvector_;
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::release_forgetgatebiasvector() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.forgetGateBiasVector)

  ::CoreML::Specification::WeightParams* temp = forgetgatebiasvector_;
  forgetgatebiasvector_ = NULL;
  return temp;
}
inline void LSTMWeightParams::set_allocated_forgetgatebiasvector(::CoreML::Specification::WeightParams* forgetgatebiasvector) {
  delete forgetgatebiasvector_;
  forgetgatebiasvector_ = forgetgatebiasvector;
  if (forgetgatebiasvector) {

  } else {

  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.forgetGateBiasVector)
}

// optional .CoreML.Specification.WeightParams blockInputBiasVector = 42;
inline bool LSTMWeightParams::has_blockinputbiasvector() const {
  return this != internal_default_instance() && blockinputbiasvector_ != NULL;
}
inline void LSTMWeightParams::clear_blockinputbiasvector() {
  if (GetArenaNoVirtual() == NULL && blockinputbiasvector_ != NULL) delete blockinputbiasvector_;
  blockinputbiasvector_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& LSTMWeightParams::blockinputbiasvector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.blockInputBiasVector)
  return blockinputbiasvector_ != NULL ? *blockinputbiasvector_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_blockinputbiasvector() {

  if (blockinputbiasvector_ == NULL) {
    blockinputbiasvector_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.blockInputBiasVector)
  return blockinputbiasvector_;
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::release_blockinputbiasvector() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.blockInputBiasVector)

  ::CoreML::Specification::WeightParams* temp = blockinputbiasvector_;
  blockinputbiasvector_ = NULL;
  return temp;
}
inline void LSTMWeightParams::set_allocated_blockinputbiasvector(::CoreML::Specification::WeightParams* blockinputbiasvector) {
  delete blockinputbiasvector_;
  blockinputbiasvector_ = blockinputbiasvector;
  if (blockinputbiasvector) {

  } else {

  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.blockInputBiasVector)
}

// optional .CoreML.Specification.WeightParams outputGateBiasVector = 43;
inline bool LSTMWeightParams::has_outputgatebiasvector() const {
  return this != internal_default_instance() && outputgatebiasvector_ != NULL;
}
inline void LSTMWeightParams::clear_outputgatebiasvector() {
  if (GetArenaNoVirtual() == NULL && outputgatebiasvector_ != NULL) delete outputgatebiasvector_;
  outputgatebiasvector_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& LSTMWeightParams::outputgatebiasvector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.outputGateBiasVector)
  return outputgatebiasvector_ != NULL ? *outputgatebiasvector_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_outputgatebiasvector() {

  if (outputgatebiasvector_ == NULL) {
    outputgatebiasvector_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.outputGateBiasVector)
  return outputgatebiasvector_;
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::release_outputgatebiasvector() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.outputGateBiasVector)

  ::CoreML::Specification::WeightParams* temp = outputgatebiasvector_;
  outputgatebiasvector_ = NULL;
  return temp;
}
inline void LSTMWeightParams::set_allocated_outputgatebiasvector(::CoreML::Specification::WeightParams* outputgatebiasvector) {
  delete outputgatebiasvector_;
  outputgatebiasvector_ = outputgatebiasvector;
  if (outputgatebiasvector) {

  } else {

  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.outputGateBiasVector)
}

// optional .CoreML.Specification.WeightParams inputGatePeepholeVector = 60;
inline bool LSTMWeightParams::has_inputgatepeepholevector() const {
  return this != internal_default_instance() && inputgatepeepholevector_ != NULL;
}
inline void LSTMWeightParams::clear_inputgatepeepholevector() {
  if (GetArenaNoVirtual() == NULL && inputgatepeepholevector_ != NULL) delete inputgatepeepholevector_;
  inputgatepeepholevector_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& LSTMWeightParams::inputgatepeepholevector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.inputGatePeepholeVector)
  return inputgatepeepholevector_ != NULL ? *inputgatepeepholevector_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_inputgatepeepholevector() {

  if (inputgatepeepholevector_ == NULL) {
    inputgatepeepholevector_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.inputGatePeepholeVector)
  return inputgatepeepholevector_;
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::release_inputgatepeepholevector() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.inputGatePeepholeVector)

  ::CoreML::Specification::WeightParams* temp = inputgatepeepholevector_;
  inputgatepeepholevector_ = NULL;
  return temp;
}
inline void LSTMWeightParams::set_allocated_inputgatepeepholevector(::CoreML::Specification::WeightParams* inputgatepeepholevector) {
  delete inputgatepeepholevector_;
  inputgatepeepholevector_ = inputgatepeepholevector;
  if (inputgatepeepholevector) {

  } else {

  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.inputGatePeepholeVector)
}

// optional .CoreML.Specification.WeightParams forgetGatePeepholeVector = 61;
inline bool LSTMWeightParams::has_forgetgatepeepholevector() const {
  return this != internal_default_instance() && forgetgatepeepholevector_ != NULL;
}
inline void LSTMWeightParams::clear_forgetgatepeepholevector() {
  if (GetArenaNoVirtual() == NULL && forgetgatepeepholevector_ != NULL) delete forgetgatepeepholevector_;
  forgetgatepeepholevector_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& LSTMWeightParams::forgetgatepeepholevector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.forgetGatePeepholeVector)
  return forgetgatepeepholevector_ != NULL ? *forgetgatepeepholevector_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_forgetgatepeepholevector() {

  if (forgetgatepeepholevector_ == NULL) {
    forgetgatepeepholevector_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.forgetGatePeepholeVector)
  return forgetgatepeepholevector_;
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::release_forgetgatepeepholevector() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.forgetGatePeepholeVector)

  ::CoreML::Specification::WeightParams* temp = forgetgatepeepholevector_;
  forgetgatepeepholevector_ = NULL;
  return temp;
}
inline void LSTMWeightParams::set_allocated_forgetgatepeepholevector(::CoreML::Specification::WeightParams* forgetgatepeepholevector) {
  delete forgetgatepeepholevector_;
  forgetgatepeepholevector_ = forgetgatepeepholevector;
  if (forgetgatepeepholevector) {

  } else {

  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.forgetGatePeepholeVector)
}

// optional .CoreML.Specification.WeightParams outputGatePeepholeVector = 62;
inline bool LSTMWeightParams::has_outputgatepeepholevector() const {
  return this != internal_default_instance() && outputgatepeepholevector_ != NULL;
}
inline void LSTMWeightParams::clear_outputgatepeepholevector() {
  if (GetArenaNoVirtual() == NULL && outputgatepeepholevector_ != NULL) delete outputgatepeepholevector_;
  outputgatepeepholevector_ = NULL;
}
inline const ::CoreML::Specification::WeightParams& LSTMWeightParams::outputgatepeepholevector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.outputGatePeepholeVector)
  return outputgatepeepholevector_ != NULL ? *outputgatepeepholevector_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_outputgatepeepholevector() {

  if (outputgatepeepholevector_ == NULL) {
    outputgatepeepholevector_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.outputGatePeepholeVector)
  return outputgatepeepholevector_;
}
inline ::CoreML::Specification::WeightParams* LSTMWeightParams::release_outputgatepeepholevector() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.outputGatePeepholeVector)

  ::CoreML::Specification::WeightParams* temp = outputgatepeepholevector_;
  outputgatepeepholevector_ = NULL;
  return temp;
}
inline void LSTMWeightParams::set_allocated_outputgatepeepholevector(::CoreML::Specification::WeightParams* outputgatepeepholevector) {
  delete outputgatepeepholevector_;
  outputgatepeepholevector_ = outputgatepeepholevector;
  if (outputgatepeepholevector) {

  } else {

  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.outputGatePeepholeVector)
}

inline const LSTMWeightParams* LSTMWeightParams::internal_default_instance() {
  return &LSTMWeightParams_default_instance_.get();
}
// -------------------------------------------------------------------

// UniDirectionalLSTMLayerParams

// optional uint64 inputVectorSize = 1;
inline void UniDirectionalLSTMLayerParams::clear_inputvectorsize() {
  inputvectorsize_ = GOOGLE_ULONGLONG(0);
}
inline ::google::protobuf::uint64 UniDirectionalLSTMLayerParams::inputvectorsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UniDirectionalLSTMLayerParams.inputVectorSize)
  return inputvectorsize_;
}
inline void UniDirectionalLSTMLayerParams::set_inputvectorsize(::google::protobuf::uint64 value) {

  inputvectorsize_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.UniDirectionalLSTMLayerParams.inputVectorSize)
}

// optional uint64 outputVectorSize = 2;
inline void UniDirectionalLSTMLayerParams::clear_outputvectorsize() {
  outputvectorsize_ = GOOGLE_ULONGLONG(0);
}
inline ::google::protobuf::uint64 UniDirectionalLSTMLayerParams::outputvectorsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UniDirectionalLSTMLayerParams.outputVectorSize)
  return outputvectorsize_;
}
inline void UniDirectionalLSTMLayerParams::set_outputvectorsize(::google::protobuf::uint64 value) {

  outputvectorsize_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.UniDirectionalLSTMLayerParams.outputVectorSize)
}

// repeated .CoreML.Specification.ActivationParams activations = 10;
inline int UniDirectionalLSTMLayerParams::activations_size() const {
  return activations_.size();
}
inline void UniDirectionalLSTMLayerParams::clear_activations() {
  activations_.Clear();
}
inline const ::CoreML::Specification::ActivationParams& UniDirectionalLSTMLayerParams::activations(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UniDirectionalLSTMLayerParams.activations)
  return activations_.Get(index);
}
inline ::CoreML::Specification::ActivationParams* UniDirectionalLSTMLayerParams::mutable_activations(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.UniDirectionalLSTMLayerParams.activations)
  return activations_.Mutable(index);
}
inline ::CoreML::Specification::ActivationParams* UniDirectionalLSTMLayerParams::add_activations() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.UniDirectionalLSTMLayerParams.activations)
  return activations_.Add();
}
inline ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::ActivationParams >*
UniDirectionalLSTMLayerParams::mutable_activations() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.UniDirectionalLSTMLayerParams.activations)
  return &activations_;
}
inline const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::ActivationParams >&
UniDirectionalLSTMLayerParams::activations() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.UniDirectionalLSTMLayerParams.activations)
  return activations_;
}

// optional .CoreML.Specification.LSTMParams params = 15;
inline bool UniDirectionalLSTMLayerParams::has_params() const {
  return this != internal_default_instance() && params_ != NULL;
}
inline void UniDirectionalLSTMLayerParams::clear_params() {
  if (GetArenaNoVirtual() == NULL && params_ != NULL) delete params_;
  params_ = NULL;
}
inline const ::CoreML::Specification::LSTMParams& UniDirectionalLSTMLayerParams::params() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UniDirectionalLSTMLayerParams.params)
  return params_ != NULL ? *params_
                         : *::CoreML::Specification::LSTMParams::internal_default_instance();
}
inline ::CoreML::Specification::LSTMParams* UniDirectionalLSTMLayerParams::mutable_params() {

  if (params_ == NULL) {
    params_ = new ::CoreML::Specification::LSTMParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.UniDirectionalLSTMLayerParams.params)
  return params_;
}
inline ::CoreML::Specification::LSTMParams* UniDirectionalLSTMLayerParams::release_params() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.UniDirectionalLSTMLayerParams.params)

  ::CoreML::Specification::LSTMParams* temp = params_;
  params_ = NULL;
  return temp;
}
inline void UniDirectionalLSTMLayerParams::set_allocated_params(::CoreML::Specification::LSTMParams* params) {
  delete params_;
  params_ = params;
  if (params) {

  } else {

  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.UniDirectionalLSTMLayerParams.params)
}

// optional .CoreML.Specification.LSTMWeightParams weightParams = 20;
inline bool UniDirectionalLSTMLayerParams::has_weightparams() const {
  return this != internal_default_instance() && weightparams_ != NULL;
}
inline void UniDirectionalLSTMLayerParams::clear_weightparams() {
  if (GetArenaNoVirtual() == NULL && weightparams_ != NULL) delete weightparams_;
  weightparams_ = NULL;
}
inline const ::CoreML::Specification::LSTMWeightParams& UniDirectionalLSTMLayerParams::weightparams() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UniDirectionalLSTMLayerParams.weightParams)
  return weightparams_ != NULL ? *weightparams_
                         : *::CoreML::Specification::LSTMWeightParams::internal_default_instance();
}
inline ::CoreML::Specification::LSTMWeightParams* UniDirectionalLSTMLayerParams::mutable_weightparams() {

  if (weightparams_ == NULL) {
    weightparams_ = new ::CoreML::Specification::LSTMWeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.UniDirectionalLSTMLayerParams.weightParams)
  return weightparams_;
}
inline ::CoreML::Specification::LSTMWeightParams* UniDirectionalLSTMLayerParams::release_weightparams() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.UniDirectionalLSTMLayerParams.weightParams)

  ::CoreML::Specification::LSTMWeightParams* temp = weightparams_;
  weightparams_ = NULL;
  return temp;
}
inline void UniDirectionalLSTMLayerParams::set_allocated_weightparams(::CoreML::Specification::LSTMWeightParams* weightparams) {
  delete weightparams_;
  weightparams_ = weightparams;
  if (weightparams) {

  } else {

  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.UniDirectionalLSTMLayerParams.weightParams)
}

// optional bool reverseInput = 100;
inline void UniDirectionalLSTMLayerParams::clear_reverseinput() {
  reverseinput_ = false;
}
inline bool UniDirectionalLSTMLayerParams::reverseinput() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UniDirectionalLSTMLayerParams.reverseInput)
  return reverseinput_;
}
inline void UniDirectionalLSTMLayerParams::set_reverseinput(bool value) {

  reverseinput_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.UniDirectionalLSTMLayerParams.reverseInput)
}

inline const UniDirectionalLSTMLayerParams* UniDirectionalLSTMLayerParams::internal_default_instance() {
  return &UniDirectionalLSTMLayerParams_default_instance_.get();
}
// -------------------------------------------------------------------

// BiDirectionalLSTMLayerParams

// optional uint64 inputVectorSize = 1;
inline void BiDirectionalLSTMLayerParams::clear_inputvectorsize() {
  inputvectorsize_ = GOOGLE_ULONGLONG(0);
}
inline ::google::protobuf::uint64 BiDirectionalLSTMLayerParams::inputvectorsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BiDirectionalLSTMLayerParams.inputVectorSize)
  return inputvectorsize_;
}
inline void BiDirectionalLSTMLayerParams::set_inputvectorsize(::google::protobuf::uint64 value) {

  inputvectorsize_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.BiDirectionalLSTMLayerParams.inputVectorSize)
}

// optional uint64 outputVectorSize = 2;
inline void BiDirectionalLSTMLayerParams::clear_outputvectorsize() {
  outputvectorsize_ = GOOGLE_ULONGLONG(0);
}
inline ::google::protobuf::uint64 BiDirectionalLSTMLayerParams::outputvectorsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BiDirectionalLSTMLayerParams.outputVectorSize)
  return outputvectorsize_;
}
inline void BiDirectionalLSTMLayerParams::set_outputvectorsize(::google::protobuf::uint64 value) {

  outputvectorsize_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.BiDirectionalLSTMLayerParams.outputVectorSize)
}

// repeated .CoreML.Specification.ActivationParams activationsForwardLSTM = 10;
inline int BiDirectionalLSTMLayerParams::activationsforwardlstm_size() const {
  return activationsforwardlstm_.size();
}
inline void BiDirectionalLSTMLayerParams::clear_activationsforwardlstm() {
  activationsforwardlstm_.Clear();
}
inline const ::CoreML::Specification::ActivationParams& BiDirectionalLSTMLayerParams::activationsforwardlstm(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BiDirectionalLSTMLayerParams.activationsForwardLSTM)
  return activationsforwardlstm_.Get(index);
}
inline ::CoreML::Specification::ActivationParams* BiDirectionalLSTMLayerParams::mutable_activationsforwardlstm(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BiDirectionalLSTMLayerParams.activationsForwardLSTM)
  return activationsforwardlstm_.Mutable(index);
}
inline ::CoreML::Specification::ActivationParams* BiDirectionalLSTMLayerParams::add_activationsforwardlstm() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.BiDirectionalLSTMLayerParams.activationsForwardLSTM)
  return activationsforwardlstm_.Add();
}
inline ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::ActivationParams >*
BiDirectionalLSTMLayerParams::mutable_activationsforwardlstm() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.BiDirectionalLSTMLayerParams.activationsForwardLSTM)
  return &activationsforwardlstm_;
}
inline const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::ActivationParams >&
BiDirectionalLSTMLayerParams::activationsforwardlstm() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.BiDirectionalLSTMLayerParams.activationsForwardLSTM)
  return activationsforwardlstm_;
}

// repeated .CoreML.Specification.ActivationParams activationsBackwardLSTM = 11;
inline int BiDirectionalLSTMLayerParams::activationsbackwardlstm_size() const {
  return activationsbackwardlstm_.size();
}
inline void BiDirectionalLSTMLayerParams::clear_activationsbackwardlstm() {
  activationsbackwardlstm_.Clear();
}
inline const ::CoreML::Specification::ActivationParams& BiDirectionalLSTMLayerParams::activationsbackwardlstm(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BiDirectionalLSTMLayerParams.activationsBackwardLSTM)
  return activationsbackwardlstm_.Get(index);
}
inline ::CoreML::Specification::ActivationParams* BiDirectionalLSTMLayerParams::mutable_activationsbackwardlstm(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BiDirectionalLSTMLayerParams.activationsBackwardLSTM)
  return activationsbackwardlstm_.Mutable(index);
}
inline ::CoreML::Specification::ActivationParams* BiDirectionalLSTMLayerParams::add_activationsbackwardlstm() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.BiDirectionalLSTMLayerParams.activationsBackwardLSTM)
  return activationsbackwardlstm_.Add();
}
inline ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::ActivationParams >*
BiDirectionalLSTMLayerParams::mutable_activationsbackwardlstm() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.BiDirectionalLSTMLayerParams.activationsBackwardLSTM)
  return &activationsbackwardlstm_;
}
inline const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::ActivationParams >&
BiDirectionalLSTMLayerParams::activationsbackwardlstm() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.BiDirectionalLSTMLayerParams.activationsBackwardLSTM)
  return activationsbackwardlstm_;
}

// optional .CoreML.Specification.LSTMParams params = 15;
inline bool BiDirectionalLSTMLayerParams::has_params() const {
  return this != internal_default_instance() && params_ != NULL;
}
inline void BiDirectionalLSTMLayerParams::clear_params() {
  if (GetArenaNoVirtual() == NULL && params_ != NULL) delete params_;
  params_ = NULL;
}
inline const ::CoreML::Specification::LSTMParams& BiDirectionalLSTMLayerParams::params() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BiDirectionalLSTMLayerParams.params)
  return params_ != NULL ? *params_
                         : *::CoreML::Specification::LSTMParams::internal_default_instance();
}
inline ::CoreML::Specification::LSTMParams* BiDirectionalLSTMLayerParams::mutable_params() {

  if (params_ == NULL) {
    params_ = new ::CoreML::Specification::LSTMParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BiDirectionalLSTMLayerParams.params)
  return params_;
}
inline ::CoreML::Specification::LSTMParams* BiDirectionalLSTMLayerParams::release_params() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.BiDirectionalLSTMLayerParams.params)

  ::CoreML::Specification::LSTMParams* temp = params_;
  params_ = NULL;
  return temp;
}
inline void BiDirectionalLSTMLayerParams::set_allocated_params(::CoreML::Specification::LSTMParams* params) {
  delete params_;
  params_ = params;
  if (params) {

  } else {

  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.BiDirectionalLSTMLayerParams.params)
}

// repeated .CoreML.Specification.LSTMWeightParams weightParams = 20;
inline int BiDirectionalLSTMLayerParams::weightparams_size() const {
  return weightparams_.size();
}
inline void BiDirectionalLSTMLayerParams::clear_weightparams() {
  weightparams_.Clear();
}
inline const ::CoreML::Specification::LSTMWeightParams& BiDirectionalLSTMLayerParams::weightparams(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BiDirectionalLSTMLayerParams.weightParams)
  return weightparams_.Get(index);
}
inline ::CoreML::Specification::LSTMWeightParams* BiDirectionalLSTMLayerParams::mutable_weightparams(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BiDirectionalLSTMLayerParams.weightParams)
  return weightparams_.Mutable(index);
}
inline ::CoreML::Specification::LSTMWeightParams* BiDirectionalLSTMLayerParams::add_weightparams() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.BiDirectionalLSTMLayerParams.weightParams)
  return weightparams_.Add();
}
inline ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::LSTMWeightParams >*
BiDirectionalLSTMLayerParams::mutable_weightparams() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.BiDirectionalLSTMLayerParams.weightParams)
  return &weightparams_;
}
inline const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::LSTMWeightParams >&
BiDirectionalLSTMLayerParams::weightparams() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.BiDirectionalLSTMLayerParams.weightParams)
  return weightparams_;
}

inline const BiDirectionalLSTMLayerParams* BiDirectionalLSTMLayerParams::internal_default_instance() {
  return &BiDirectionalLSTMLayerParams_default_instance_.get();
}
// -------------------------------------------------------------------

// NeuralNetworkClassifier

// repeated .CoreML.Specification.NeuralNetworkLayer layers = 1;
inline int NeuralNetworkClassifier::layers_size() const {
  return layers_.size();
}
inline void NeuralNetworkClassifier::clear_layers() {
  layers_.Clear();
}
inline const ::CoreML::Specification::NeuralNetworkLayer& NeuralNetworkClassifier::layers(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkClassifier.layers)
  return layers_.Get(index);
}
inline ::CoreML::Specification::NeuralNetworkLayer* NeuralNetworkClassifier::mutable_layers(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkClassifier.layers)
  return layers_.Mutable(index);
}
inline ::CoreML::Specification::NeuralNetworkLayer* NeuralNetworkClassifier::add_layers() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.NeuralNetworkClassifier.layers)
  return layers_.Add();
}
inline ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkLayer >*
NeuralNetworkClassifier::mutable_layers() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.NeuralNetworkClassifier.layers)
  return &layers_;
}
inline const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkLayer >&
NeuralNetworkClassifier::layers() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.NeuralNetworkClassifier.layers)
  return layers_;
}

// repeated .CoreML.Specification.NeuralNetworkPreprocessing preprocessing = 2;
inline int NeuralNetworkClassifier::preprocessing_size() const {
  return preprocessing_.size();
}
inline void NeuralNetworkClassifier::clear_preprocessing() {
  preprocessing_.Clear();
}
inline const ::CoreML::Specification::NeuralNetworkPreprocessing& NeuralNetworkClassifier::preprocessing(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkClassifier.preprocessing)
  return preprocessing_.Get(index);
}
inline ::CoreML::Specification::NeuralNetworkPreprocessing* NeuralNetworkClassifier::mutable_preprocessing(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkClassifier.preprocessing)
  return preprocessing_.Mutable(index);
}
inline ::CoreML::Specification::NeuralNetworkPreprocessing* NeuralNetworkClassifier::add_preprocessing() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.NeuralNetworkClassifier.preprocessing)
  return preprocessing_.Add();
}
inline ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkPreprocessing >*
NeuralNetworkClassifier::mutable_preprocessing() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.NeuralNetworkClassifier.preprocessing)
  return &preprocessing_;
}
inline const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkPreprocessing >&
NeuralNetworkClassifier::preprocessing() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.NeuralNetworkClassifier.preprocessing)
  return preprocessing_;
}

// optional .CoreML.Specification.StringVector stringClassLabels = 100;
inline bool NeuralNetworkClassifier::has_stringclasslabels() const {
  return ClassLabels_case() == kStringClassLabels;
}
inline void NeuralNetworkClassifier::set_has_stringclasslabels() {
  _oneof_case_[0] = kStringClassLabels;
}
inline void NeuralNetworkClassifier::clear_stringclasslabels() {
  if (has_stringclasslabels()) {
    delete ClassLabels_.stringclasslabels_;
    clear_has_ClassLabels();
  }
}
inline  const ::CoreML::Specification::StringVector& NeuralNetworkClassifier::stringclasslabels() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkClassifier.stringClassLabels)
  return has_stringclasslabels()
      ? *ClassLabels_.stringclasslabels_
      : ::CoreML::Specification::StringVector::default_instance();
}
inline ::CoreML::Specification::StringVector* NeuralNetworkClassifier::mutable_stringclasslabels() {
  if (!has_stringclasslabels()) {
    clear_ClassLabels();
    set_has_stringclasslabels();
    ClassLabels_.stringclasslabels_ = new ::CoreML::Specification::StringVector;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkClassifier.stringClassLabels)
  return ClassLabels_.stringclasslabels_;
}
inline ::CoreML::Specification::StringVector* NeuralNetworkClassifier::release_stringclasslabels() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkClassifier.stringClassLabels)
  if (has_stringclasslabels()) {
    clear_has_ClassLabels();
    ::CoreML::Specification::StringVector* temp = ClassLabels_.stringclasslabels_;
    ClassLabels_.stringclasslabels_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkClassifier::set_allocated_stringclasslabels(::CoreML::Specification::StringVector* stringclasslabels) {
  clear_ClassLabels();
  if (stringclasslabels) {
    set_has_stringclasslabels();
    ClassLabels_.stringclasslabels_ = stringclasslabels;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkClassifier.stringClassLabels)
}

// optional .CoreML.Specification.Int64Vector int64ClassLabels = 101;
inline bool NeuralNetworkClassifier::has_int64classlabels() const {
  return ClassLabels_case() == kInt64ClassLabels;
}
inline void NeuralNetworkClassifier::set_has_int64classlabels() {
  _oneof_case_[0] = kInt64ClassLabels;
}
inline void NeuralNetworkClassifier::clear_int64classlabels() {
  if (has_int64classlabels()) {
    delete ClassLabels_.int64classlabels_;
    clear_has_ClassLabels();
  }
}
inline  const ::CoreML::Specification::Int64Vector& NeuralNetworkClassifier::int64classlabels() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkClassifier.int64ClassLabels)
  return has_int64classlabels()
      ? *ClassLabels_.int64classlabels_
      : ::CoreML::Specification::Int64Vector::default_instance();
}
inline ::CoreML::Specification::Int64Vector* NeuralNetworkClassifier::mutable_int64classlabels() {
  if (!has_int64classlabels()) {
    clear_ClassLabels();
    set_has_int64classlabels();
    ClassLabels_.int64classlabels_ = new ::CoreML::Specification::Int64Vector;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkClassifier.int64ClassLabels)
  return ClassLabels_.int64classlabels_;
}
inline ::CoreML::Specification::Int64Vector* NeuralNetworkClassifier::release_int64classlabels() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkClassifier.int64ClassLabels)
  if (has_int64classlabels()) {
    clear_has_ClassLabels();
    ::CoreML::Specification::Int64Vector* temp = ClassLabels_.int64classlabels_;
    ClassLabels_.int64classlabels_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline void NeuralNetworkClassifier::set_allocated_int64classlabels(::CoreML::Specification::Int64Vector* int64classlabels) {
  clear_ClassLabels();
  if (int64classlabels) {
    set_has_int64classlabels();
    ClassLabels_.int64classlabels_ = int64classlabels;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkClassifier.int64ClassLabels)
}

// optional string labelProbabilityLayerName = 200;
inline void NeuralNetworkClassifier::clear_labelprobabilitylayername() {
  labelprobabilitylayername_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline const ::std::string& NeuralNetworkClassifier::labelprobabilitylayername() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkClassifier.labelProbabilityLayerName)
  return labelprobabilitylayername_.GetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline void NeuralNetworkClassifier::set_labelprobabilitylayername(const ::std::string& value) {

  labelprobabilitylayername_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkClassifier.labelProbabilityLayerName)
}
inline void NeuralNetworkClassifier::set_labelprobabilitylayername(const char* value) {

  labelprobabilitylayername_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:CoreML.Specification.NeuralNetworkClassifier.labelProbabilityLayerName)
}
inline void NeuralNetworkClassifier::set_labelprobabilitylayername(const char* value, size_t size) {

  labelprobabilitylayername_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:CoreML.Specification.NeuralNetworkClassifier.labelProbabilityLayerName)
}
inline ::std::string* NeuralNetworkClassifier::mutable_labelprobabilitylayername() {

  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkClassifier.labelProbabilityLayerName)
  return labelprobabilitylayername_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline ::std::string* NeuralNetworkClassifier::release_labelprobabilitylayername() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkClassifier.labelProbabilityLayerName)

  return labelprobabilitylayername_.ReleaseNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline void NeuralNetworkClassifier::set_allocated_labelprobabilitylayername(::std::string* labelprobabilitylayername) {
  if (labelprobabilitylayername != NULL) {

  } else {

  }
  labelprobabilitylayername_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), labelprobabilitylayername);
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkClassifier.labelProbabilityLayerName)
}

inline bool NeuralNetworkClassifier::has_ClassLabels() const {
  return ClassLabels_case() != CLASSLABELS_NOT_SET;
}
inline void NeuralNetworkClassifier::clear_has_ClassLabels() {
  _oneof_case_[0] = CLASSLABELS_NOT_SET;
}
inline NeuralNetworkClassifier::ClassLabelsCase NeuralNetworkClassifier::ClassLabels_case() const {
  return NeuralNetworkClassifier::ClassLabelsCase(_oneof_case_[0]);
}
inline const NeuralNetworkClassifier* NeuralNetworkClassifier::internal_default_instance() {
  return &NeuralNetworkClassifier_default_instance_.get();
}
// -------------------------------------------------------------------

// NeuralNetworkRegressor

// repeated .CoreML.Specification.NeuralNetworkLayer layers = 1;
inline int NeuralNetworkRegressor::layers_size() const {
  return layers_.size();
}
inline void NeuralNetworkRegressor::clear_layers() {
  layers_.Clear();
}
inline const ::CoreML::Specification::NeuralNetworkLayer& NeuralNetworkRegressor::layers(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkRegressor.layers)
  return layers_.Get(index);
}
inline ::CoreML::Specification::NeuralNetworkLayer* NeuralNetworkRegressor::mutable_layers(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkRegressor.layers)
  return layers_.Mutable(index);
}
inline ::CoreML::Specification::NeuralNetworkLayer* NeuralNetworkRegressor::add_layers() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.NeuralNetworkRegressor.layers)
  return layers_.Add();
}
inline ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkLayer >*
NeuralNetworkRegressor::mutable_layers() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.NeuralNetworkRegressor.layers)
  return &layers_;
}
inline const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkLayer >&
NeuralNetworkRegressor::layers() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.NeuralNetworkRegressor.layers)
  return layers_;
}

// repeated .CoreML.Specification.NeuralNetworkPreprocessing preprocessing = 2;
inline int NeuralNetworkRegressor::preprocessing_size() const {
  return preprocessing_.size();
}
inline void NeuralNetworkRegressor::clear_preprocessing() {
  preprocessing_.Clear();
}
inline const ::CoreML::Specification::NeuralNetworkPreprocessing& NeuralNetworkRegressor::preprocessing(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkRegressor.preprocessing)
  return preprocessing_.Get(index);
}
inline ::CoreML::Specification::NeuralNetworkPreprocessing* NeuralNetworkRegressor::mutable_preprocessing(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkRegressor.preprocessing)
  return preprocessing_.Mutable(index);
}
inline ::CoreML::Specification::NeuralNetworkPreprocessing* NeuralNetworkRegressor::add_preprocessing() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.NeuralNetworkRegressor.preprocessing)
  return preprocessing_.Add();
}
inline ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkPreprocessing >*
NeuralNetworkRegressor::mutable_preprocessing() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.NeuralNetworkRegressor.preprocessing)
  return &preprocessing_;
}
inline const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkPreprocessing >&
NeuralNetworkRegressor::preprocessing() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.NeuralNetworkRegressor.preprocessing)
  return preprocessing_;
}

inline const NeuralNetworkRegressor* NeuralNetworkRegressor::internal_default_instance() {
  return &NeuralNetworkRegressor_default_instance_.get();
}
#endif  // !PROTOBUF_INLINE_NOT_IN_HEADERS
// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------


// @@protoc_insertion_point(namespace_scope)

}  // namespace Specification
}  // namespace CoreML

#ifndef SWIG
namespace google {
namespace protobuf {

template <> struct is_proto_enum< ::CoreML::Specification::SamePadding_SamePaddingMode> : ::google::protobuf::internal::true_type {};
template <> struct is_proto_enum< ::CoreML::Specification::PoolingLayerParams_PoolingType> : ::google::protobuf::internal::true_type {};
template <> struct is_proto_enum< ::CoreML::Specification::UnaryFunctionLayerParams_Operation> : ::google::protobuf::internal::true_type {};
template <> struct is_proto_enum< ::CoreML::Specification::FlattenLayerParams_FlattenOrder> : ::google::protobuf::internal::true_type {};
template <> struct is_proto_enum< ::CoreML::Specification::ReshapeLayerParams_ReshapeOrder> : ::google::protobuf::internal::true_type {};
template <> struct is_proto_enum< ::CoreML::Specification::ReduceLayerParams_ReduceOperation> : ::google::protobuf::internal::true_type {};

}  // namespace protobuf
}  // namespace google
#endif  // SWIG

// @@protoc_insertion_point(global_scope)

#endif  // PROTOBUF_NeuralNetwork_2eproto__INCLUDED
