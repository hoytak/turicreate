// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: NeuralNetwork.proto

#define INTERNAL_SUPPRESS_PROTOBUF_FIELD_DEPRECATION
#include "NeuralNetwork.pb.h"

#include <algorithm>

#include <google/protobuf/stubs/common.h>
#include <google/protobuf/stubs/port.h>
#include <google/protobuf/stubs/once.h>
#include <google/protobuf/io/coded_stream.h>
#include <google/protobuf/wire_format_lite_inl.h>
#include <google/protobuf/io/zero_copy_stream_impl_lite.h>
// @@protoc_insertion_point(includes)

namespace CoreML {
namespace Specification {
class NeuralNetworkDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<NeuralNetwork> {
} _NeuralNetwork_default_instance_;
class NeuralNetworkImageScalerDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<NeuralNetworkImageScaler> {
} _NeuralNetworkImageScaler_default_instance_;
class NeuralNetworkMeanImageDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<NeuralNetworkMeanImage> {
} _NeuralNetworkMeanImage_default_instance_;
class NeuralNetworkPreprocessingDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<NeuralNetworkPreprocessing> {
  public:
  const ::CoreML::Specification::NeuralNetworkImageScaler* scaler_;
  const ::CoreML::Specification::NeuralNetworkMeanImage* meanimage_;
} _NeuralNetworkPreprocessing_default_instance_;
class ActivationReLUDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ActivationReLU> {
} _ActivationReLU_default_instance_;
class ActivationLeakyReLUDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ActivationLeakyReLU> {
} _ActivationLeakyReLU_default_instance_;
class ActivationTanhDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ActivationTanh> {
} _ActivationTanh_default_instance_;
class ActivationScaledTanhDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ActivationScaledTanh> {
} _ActivationScaledTanh_default_instance_;
class ActivationSigmoidDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ActivationSigmoid> {
} _ActivationSigmoid_default_instance_;
class ActivationLinearDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ActivationLinear> {
} _ActivationLinear_default_instance_;
class ActivationSigmoidHardDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ActivationSigmoidHard> {
} _ActivationSigmoidHard_default_instance_;
class ActivationPReLUDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ActivationPReLU> {
} _ActivationPReLU_default_instance_;
class ActivationELUDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ActivationELU> {
} _ActivationELU_default_instance_;
class ActivationThresholdedReLUDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ActivationThresholdedReLU> {
} _ActivationThresholdedReLU_default_instance_;
class ActivationSoftsignDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ActivationSoftsign> {
} _ActivationSoftsign_default_instance_;
class ActivationSoftplusDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ActivationSoftplus> {
} _ActivationSoftplus_default_instance_;
class ActivationParametricSoftplusDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ActivationParametricSoftplus> {
} _ActivationParametricSoftplus_default_instance_;
class ActivationParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ActivationParams> {
  public:
  const ::CoreML::Specification::ActivationLinear* linear_;
  const ::CoreML::Specification::ActivationReLU* relu_;
  const ::CoreML::Specification::ActivationLeakyReLU* leakyrelu_;
  const ::CoreML::Specification::ActivationThresholdedReLU* thresholdedrelu_;
  const ::CoreML::Specification::ActivationPReLU* prelu_;
  const ::CoreML::Specification::ActivationTanh* tanh_;
  const ::CoreML::Specification::ActivationScaledTanh* scaledtanh_;
  const ::CoreML::Specification::ActivationSigmoid* sigmoid_;
  const ::CoreML::Specification::ActivationSigmoidHard* sigmoidhard_;
  const ::CoreML::Specification::ActivationELU* elu_;
  const ::CoreML::Specification::ActivationSoftsign* softsign_;
  const ::CoreML::Specification::ActivationSoftplus* softplus_;
  const ::CoreML::Specification::ActivationParametricSoftplus* parametricsoftplus_;
} _ActivationParams_default_instance_;
class TensorDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<Tensor> {
} _Tensor_default_instance_;
class NeuralNetworkLayerDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<NeuralNetworkLayer> {
  public:
  const ::CoreML::Specification::ConvolutionLayerParams* convolution_;
  const ::CoreML::Specification::PoolingLayerParams* pooling_;
  const ::CoreML::Specification::ActivationParams* activation_;
  const ::CoreML::Specification::InnerProductLayerParams* innerproduct_;
  const ::CoreML::Specification::EmbeddingLayerParams* embedding_;
  const ::CoreML::Specification::BatchnormLayerParams* batchnorm_;
  const ::CoreML::Specification::MeanVarianceNormalizeLayerParams* mvn_;
  const ::CoreML::Specification::L2NormalizeLayerParams* l2normalize_;
  const ::CoreML::Specification::SoftmaxLayerParams* softmax_;
  const ::CoreML::Specification::LRNLayerParams* lrn_;
  const ::CoreML::Specification::CropLayerParams* crop_;
  const ::CoreML::Specification::PaddingLayerParams* padding_;
  const ::CoreML::Specification::UpsampleLayerParams* upsample_;
  const ::CoreML::Specification::ResizeBilinearLayerParams* resizebilinear_;
  const ::CoreML::Specification::CropResizeLayerParams* cropresize_;
  const ::CoreML::Specification::UnaryFunctionLayerParams* unary_;
  const ::CoreML::Specification::AddLayerParams* add_;
  const ::CoreML::Specification::MultiplyLayerParams* multiply_;
  const ::CoreML::Specification::AverageLayerParams* average_;
  const ::CoreML::Specification::ScaleLayerParams* scale_;
  const ::CoreML::Specification::BiasLayerParams* bias_;
  const ::CoreML::Specification::MaxLayerParams* max_;
  const ::CoreML::Specification::MinLayerParams* min_;
  const ::CoreML::Specification::DotProductLayerParams* dot_;
  const ::CoreML::Specification::ReduceLayerParams* reduce_;
  const ::CoreML::Specification::LoadConstantLayerParams* loadconstant_;
  const ::CoreML::Specification::ReshapeLayerParams* reshape_;
  const ::CoreML::Specification::FlattenLayerParams* flatten_;
  const ::CoreML::Specification::PermuteLayerParams* permute_;
  const ::CoreML::Specification::ConcatLayerParams* concat_;
  const ::CoreML::Specification::SplitLayerParams* split_;
  const ::CoreML::Specification::SequenceRepeatLayerParams* sequencerepeat_;
  const ::CoreML::Specification::ReorganizeDataLayerParams* reorganizedata_;
  const ::CoreML::Specification::SliceLayerParams* slice_;
  const ::CoreML::Specification::SimpleRecurrentLayerParams* simplerecurrent_;
  const ::CoreML::Specification::GRULayerParams* gru_;
  const ::CoreML::Specification::UniDirectionalLSTMLayerParams* unidirectionallstm_;
  const ::CoreML::Specification::BiDirectionalLSTMLayerParams* bidirectionallstm_;
  const ::CoreML::Specification::CustomLayerParams* custom_;
  const ::CoreML::Specification::CopyLayerParams* copy_;
  const ::CoreML::Specification::BranchLayerParams* branch_;
  const ::CoreML::Specification::LoopLayerParams* loop_;
  const ::CoreML::Specification::LoopBreakLayerParams* loopbreak_;
  const ::CoreML::Specification::LoopContinueLayerParams* loopcontinue_;
  const ::CoreML::Specification::RangeLayerParams* range_;
  const ::CoreML::Specification::ClipLayerParams* clip_;
  const ::CoreML::Specification::CeilLayerParams* ceil_;
  const ::CoreML::Specification::FloorLayerParams* floor_;
  const ::CoreML::Specification::Exp2LayerParams* exp2_;
  const ::CoreML::Specification::SineLayerParams* sine_;
  const ::CoreML::Specification::CosineLayerParams* cosine_;
  const ::CoreML::Specification::ErfActivationLayerParams* erfactivation_;
  const ::CoreML::Specification::GeluActivationLayerParams* geluactivation_;
  const ::CoreML::Specification::EqualLayerParams* equal_;
  const ::CoreML::Specification::NotEqualLayerParams* notequal_;
  const ::CoreML::Specification::LessThanLayerParams* lessthan_;
  const ::CoreML::Specification::GreaterThanLayerParams* greaterthan_;
  const ::CoreML::Specification::LogicalOrLayerParams* logicalor_;
  const ::CoreML::Specification::LogicalXorLayerParams* logicalxor_;
  const ::CoreML::Specification::LogicalNotLayerParams* logicalnot_;
  const ::CoreML::Specification::LogicalAndLayerParams* logicaland_;
  const ::CoreML::Specification::AddBroadcastableLayerParams* addbroadcastable_;
  const ::CoreML::Specification::PowBroadcastableLayerParams* powbroadcastable_;
  const ::CoreML::Specification::DivideBroadcastableLayerParams* dividebroadcastable_;
  const ::CoreML::Specification::MultiplyBroadcastableLayerParams* multiplybroadcastable_;
  const ::CoreML::Specification::SubtractBroadcastableLayerParams* subtractbroadcastable_;
  const ::CoreML::Specification::TileLayerParams* tile_;
  const ::CoreML::Specification::StackNDLayerParams* stacknd_;
  const ::CoreML::Specification::GatherLayerParams* gather_;
  const ::CoreML::Specification::ScatterLayerParams* scatter_;
  const ::CoreML::Specification::SoftmaxNDLayerParams* softmaxnd_;
  const ::CoreML::Specification::SplitNDLayerParams* splitnd_;
  const ::CoreML::Specification::ConcatNDLayerParams* concatnd_;
  const ::CoreML::Specification::TransposeNDLayerParams* transposend_;
  const ::CoreML::Specification::SliceNDLayerParams* slicend_;
  const ::CoreML::Specification::SlidingWindowsLayerParams* slidingwindows_;
  const ::CoreML::Specification::EmbeddingNDLayerParams* embeddingnd_;
  const ::CoreML::Specification::BatchedMatMulParams* batchedmatmul_;
  const ::CoreML::Specification::GetShapeLayerParams* getshape_;
  const ::CoreML::Specification::LoadConstantNDLayerParams* loadconstantnd_;
  const ::CoreML::Specification::FillLikeLayerParams* filllike_;
  const ::CoreML::Specification::FillStaticLayerParams* fillstatic_;
  const ::CoreML::Specification::FillDynamicLayerParams* filldynamic_;
  const ::CoreML::Specification::BroadcastToLikeLayerParams* broadcasttolike_;
  const ::CoreML::Specification::BroadcastToStaticLayerParams* broadcasttostatic_;
  const ::CoreML::Specification::BroadcastToDynamicLayerParams* broadcasttodynamic_;
  const ::CoreML::Specification::SqueezeLayerParams* squeeze_;
  const ::CoreML::Specification::ExpandDimsLayerParams* expanddims_;
  const ::CoreML::Specification::RankPreservingReshapeLayerParams* rankpreservingreshape_;
  const ::CoreML::Specification::LowerTriangularLayerParams* lowertriangular_;
  const ::CoreML::Specification::UpperTriangularLayerParams* uppertriangular_;
  const ::CoreML::Specification::WhereLayerParams* where_;
} _NeuralNetworkLayer_default_instance_;
class BranchLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<BranchLayerParams> {
} _BranchLayerParams_default_instance_;
class LoopLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<LoopLayerParams> {
} _LoopLayerParams_default_instance_;
class LoopBreakLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<LoopBreakLayerParams> {
} _LoopBreakLayerParams_default_instance_;
class LoopContinueLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<LoopContinueLayerParams> {
} _LoopContinueLayerParams_default_instance_;
class CopyLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<CopyLayerParams> {
} _CopyLayerParams_default_instance_;
class GreaterThanLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<GreaterThanLayerParams> {
} _GreaterThanLayerParams_default_instance_;
class LessThanLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<LessThanLayerParams> {
} _LessThanLayerParams_default_instance_;
class EqualLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<EqualLayerParams> {
} _EqualLayerParams_default_instance_;
class NotEqualLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<NotEqualLayerParams> {
} _NotEqualLayerParams_default_instance_;
class LogicalAndLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<LogicalAndLayerParams> {
} _LogicalAndLayerParams_default_instance_;
class LogicalOrLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<LogicalOrLayerParams> {
} _LogicalOrLayerParams_default_instance_;
class LogicalXorLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<LogicalXorLayerParams> {
} _LogicalXorLayerParams_default_instance_;
class LogicalNotLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<LogicalNotLayerParams> {
} _LogicalNotLayerParams_default_instance_;
class BorderAmounts_EdgeSizesDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<BorderAmounts_EdgeSizes> {
} _BorderAmounts_EdgeSizes_default_instance_;
class BorderAmountsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<BorderAmounts> {
} _BorderAmounts_default_instance_;
class ValidPaddingDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ValidPadding> {
} _ValidPadding_default_instance_;
class SamePaddingDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<SamePadding> {
} _SamePadding_default_instance_;
class SamplingModeDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<SamplingMode> {
} _SamplingMode_default_instance_;
class BoxCoordinatesModeDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<BoxCoordinatesMode> {
} _BoxCoordinatesMode_default_instance_;
class WeightParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<WeightParams> {
} _WeightParams_default_instance_;
class QuantizationParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<QuantizationParams> {
  public:
  const ::CoreML::Specification::LinearQuantizationParams* linearquantization_;
  const ::CoreML::Specification::LookUpTableQuantizationParams* lookuptablequantization_;
} _QuantizationParams_default_instance_;
class LinearQuantizationParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<LinearQuantizationParams> {
} _LinearQuantizationParams_default_instance_;
class LookUpTableQuantizationParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<LookUpTableQuantizationParams> {
} _LookUpTableQuantizationParams_default_instance_;
class ConvolutionLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ConvolutionLayerParams> {
  public:
  const ::CoreML::Specification::ValidPadding* valid_;
  const ::CoreML::Specification::SamePadding* same_;
} _ConvolutionLayerParams_default_instance_;
class InnerProductLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<InnerProductLayerParams> {
} _InnerProductLayerParams_default_instance_;
class EmbeddingLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<EmbeddingLayerParams> {
} _EmbeddingLayerParams_default_instance_;
class EmbeddingNDLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<EmbeddingNDLayerParams> {
} _EmbeddingNDLayerParams_default_instance_;
class BatchnormLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<BatchnormLayerParams> {
} _BatchnormLayerParams_default_instance_;
class PoolingLayerParams_ValidCompletePaddingDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<PoolingLayerParams_ValidCompletePadding> {
} _PoolingLayerParams_ValidCompletePadding_default_instance_;
class PoolingLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<PoolingLayerParams> {
  public:
  const ::CoreML::Specification::ValidPadding* valid_;
  const ::CoreML::Specification::SamePadding* same_;
  const ::CoreML::Specification::PoolingLayerParams_ValidCompletePadding* includelastpixel_;
} _PoolingLayerParams_default_instance_;
class PaddingLayerParams_PaddingConstantDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<PaddingLayerParams_PaddingConstant> {
} _PaddingLayerParams_PaddingConstant_default_instance_;
class PaddingLayerParams_PaddingReflectionDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<PaddingLayerParams_PaddingReflection> {
} _PaddingLayerParams_PaddingReflection_default_instance_;
class PaddingLayerParams_PaddingReplicationDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<PaddingLayerParams_PaddingReplication> {
} _PaddingLayerParams_PaddingReplication_default_instance_;
class PaddingLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<PaddingLayerParams> {
  public:
  const ::CoreML::Specification::PaddingLayerParams_PaddingConstant* constant_;
  const ::CoreML::Specification::PaddingLayerParams_PaddingReflection* reflection_;
  const ::CoreML::Specification::PaddingLayerParams_PaddingReplication* replication_;
} _PaddingLayerParams_default_instance_;
class ConcatLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ConcatLayerParams> {
} _ConcatLayerParams_default_instance_;
class LRNLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<LRNLayerParams> {
} _LRNLayerParams_default_instance_;
class SoftmaxLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<SoftmaxLayerParams> {
} _SoftmaxLayerParams_default_instance_;
class SplitLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<SplitLayerParams> {
} _SplitLayerParams_default_instance_;
class AddLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<AddLayerParams> {
} _AddLayerParams_default_instance_;
class MultiplyLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<MultiplyLayerParams> {
} _MultiplyLayerParams_default_instance_;
class UnaryFunctionLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<UnaryFunctionLayerParams> {
} _UnaryFunctionLayerParams_default_instance_;
class UpsampleLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<UpsampleLayerParams> {
} _UpsampleLayerParams_default_instance_;
class ResizeBilinearLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ResizeBilinearLayerParams> {
} _ResizeBilinearLayerParams_default_instance_;
class CropResizeLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<CropResizeLayerParams> {
} _CropResizeLayerParams_default_instance_;
class BiasLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<BiasLayerParams> {
} _BiasLayerParams_default_instance_;
class ScaleLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ScaleLayerParams> {
} _ScaleLayerParams_default_instance_;
class LoadConstantLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<LoadConstantLayerParams> {
} _LoadConstantLayerParams_default_instance_;
class L2NormalizeLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<L2NormalizeLayerParams> {
} _L2NormalizeLayerParams_default_instance_;
class FlattenLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<FlattenLayerParams> {
} _FlattenLayerParams_default_instance_;
class ReshapeLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ReshapeLayerParams> {
} _ReshapeLayerParams_default_instance_;
class PermuteLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<PermuteLayerParams> {
} _PermuteLayerParams_default_instance_;
class ReorganizeDataLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ReorganizeDataLayerParams> {
} _ReorganizeDataLayerParams_default_instance_;
class SliceLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<SliceLayerParams> {
} _SliceLayerParams_default_instance_;
class ReduceLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ReduceLayerParams> {
} _ReduceLayerParams_default_instance_;
class CropLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<CropLayerParams> {
} _CropLayerParams_default_instance_;
class AverageLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<AverageLayerParams> {
} _AverageLayerParams_default_instance_;
class MaxLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<MaxLayerParams> {
} _MaxLayerParams_default_instance_;
class MinLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<MinLayerParams> {
} _MinLayerParams_default_instance_;
class DotProductLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<DotProductLayerParams> {
} _DotProductLayerParams_default_instance_;
class MeanVarianceNormalizeLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<MeanVarianceNormalizeLayerParams> {
} _MeanVarianceNormalizeLayerParams_default_instance_;
class SequenceRepeatLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<SequenceRepeatLayerParams> {
} _SequenceRepeatLayerParams_default_instance_;
class SimpleRecurrentLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<SimpleRecurrentLayerParams> {
} _SimpleRecurrentLayerParams_default_instance_;
class GRULayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<GRULayerParams> {
} _GRULayerParams_default_instance_;
class LSTMParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<LSTMParams> {
} _LSTMParams_default_instance_;
class LSTMWeightParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<LSTMWeightParams> {
} _LSTMWeightParams_default_instance_;
class UniDirectionalLSTMLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<UniDirectionalLSTMLayerParams> {
} _UniDirectionalLSTMLayerParams_default_instance_;
class BiDirectionalLSTMLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<BiDirectionalLSTMLayerParams> {
} _BiDirectionalLSTMLayerParams_default_instance_;
class CustomLayerParams_CustomLayerParamValueDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<CustomLayerParams_CustomLayerParamValue> {
  public:
  double doublevalue_;
  ::google::protobuf::internal::ArenaStringPtr stringvalue_;
  ::google::protobuf::int32 intvalue_;
  ::google::protobuf::int64 longvalue_;
  bool boolvalue_;
} _CustomLayerParams_CustomLayerParamValue_default_instance_;
class CustomLayerParams_ParametersEntryDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<CustomLayerParams::CustomLayerParams_ParametersEntry> {
} _CustomLayerParams_ParametersEntry_default_instance_;
class CustomLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<CustomLayerParams> {
} _CustomLayerParams_default_instance_;
class TransposeNDLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<TransposeNDLayerParams> {
} _TransposeNDLayerParams_default_instance_;
class BatchedMatMulParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<BatchedMatMulParams> {
} _BatchedMatMulParams_default_instance_;
class ConcatNDLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ConcatNDLayerParams> {
} _ConcatNDLayerParams_default_instance_;
class SoftmaxNDLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<SoftmaxNDLayerParams> {
} _SoftmaxNDLayerParams_default_instance_;
class LoadConstantNDLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<LoadConstantNDLayerParams> {
} _LoadConstantNDLayerParams_default_instance_;
class FillLikeLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<FillLikeLayerParams> {
} _FillLikeLayerParams_default_instance_;
class FillStaticLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<FillStaticLayerParams> {
} _FillStaticLayerParams_default_instance_;
class FillDynamicLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<FillDynamicLayerParams> {
} _FillDynamicLayerParams_default_instance_;
class WhereLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<WhereLayerParams> {
} _WhereLayerParams_default_instance_;
class SineLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<SineLayerParams> {
} _SineLayerParams_default_instance_;
class CosineLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<CosineLayerParams> {
} _CosineLayerParams_default_instance_;
class PowBroadcastableLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<PowBroadcastableLayerParams> {
} _PowBroadcastableLayerParams_default_instance_;
class Exp2LayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<Exp2LayerParams> {
} _Exp2LayerParams_default_instance_;
class UpperTriangularLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<UpperTriangularLayerParams> {
} _UpperTriangularLayerParams_default_instance_;
class LowerTriangularLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<LowerTriangularLayerParams> {
} _LowerTriangularLayerParams_default_instance_;
class BroadcastToLikeLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<BroadcastToLikeLayerParams> {
} _BroadcastToLikeLayerParams_default_instance_;
class BroadcastToStaticLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<BroadcastToStaticLayerParams> {
} _BroadcastToStaticLayerParams_default_instance_;
class BroadcastToDynamicLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<BroadcastToDynamicLayerParams> {
} _BroadcastToDynamicLayerParams_default_instance_;
class AddBroadcastableLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<AddBroadcastableLayerParams> {
} _AddBroadcastableLayerParams_default_instance_;
class SubtractBroadcastableLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<SubtractBroadcastableLayerParams> {
} _SubtractBroadcastableLayerParams_default_instance_;
class MultiplyBroadcastableLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<MultiplyBroadcastableLayerParams> {
} _MultiplyBroadcastableLayerParams_default_instance_;
class DivideBroadcastableLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<DivideBroadcastableLayerParams> {
} _DivideBroadcastableLayerParams_default_instance_;
class GatherLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<GatherLayerParams> {
} _GatherLayerParams_default_instance_;
class ScatterLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ScatterLayerParams> {
} _ScatterLayerParams_default_instance_;
class StackNDLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<StackNDLayerParams> {
} _StackNDLayerParams_default_instance_;
class RankPreservingReshapeLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<RankPreservingReshapeLayerParams> {
} _RankPreservingReshapeLayerParams_default_instance_;
class ExpandDimsLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ExpandDimsLayerParams> {
} _ExpandDimsLayerParams_default_instance_;
class SqueezeLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<SqueezeLayerParams> {
} _SqueezeLayerParams_default_instance_;
class SplitNDLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<SplitNDLayerParams> {
} _SplitNDLayerParams_default_instance_;
class CeilLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<CeilLayerParams> {
} _CeilLayerParams_default_instance_;
class FloorLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<FloorLayerParams> {
} _FloorLayerParams_default_instance_;
class ClipLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ClipLayerParams> {
} _ClipLayerParams_default_instance_;
class SliceNDLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<SliceNDLayerParams> {
} _SliceNDLayerParams_default_instance_;
class TileLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<TileLayerParams> {
} _TileLayerParams_default_instance_;
class GetShapeLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<GetShapeLayerParams> {
} _GetShapeLayerParams_default_instance_;
class ErfActivationLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<ErfActivationLayerParams> {
} _ErfActivationLayerParams_default_instance_;
class GeluActivationLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<GeluActivationLayerParams> {
} _GeluActivationLayerParams_default_instance_;
class RangeLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<RangeLayerParams> {
} _RangeLayerParams_default_instance_;
class SlidingWindowsLayerParamsDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<SlidingWindowsLayerParams> {
} _SlidingWindowsLayerParams_default_instance_;
class NeuralNetworkClassifierDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<NeuralNetworkClassifier> {
  public:
  const ::CoreML::Specification::StringVector* stringclasslabels_;
  const ::CoreML::Specification::Int64Vector* int64classlabels_;
} _NeuralNetworkClassifier_default_instance_;
class NeuralNetworkRegressorDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<NeuralNetworkRegressor> {
} _NeuralNetworkRegressor_default_instance_;
class NetworkUpdateParametersDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<NetworkUpdateParameters> {
} _NetworkUpdateParameters_default_instance_;
class LossLayerDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<LossLayer> {
  public:
  const ::CoreML::Specification::CrossEntropyLossLayer* crossentropylosslayer_;
  const ::CoreML::Specification::MSELossLayer* mselosslayer_;
} _LossLayer_default_instance_;
class CrossEntropyLossLayerDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<CrossEntropyLossLayer> {
} _CrossEntropyLossLayer_default_instance_;
class MSELossLayerDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<MSELossLayer> {
} _MSELossLayer_default_instance_;
class OptimizerDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<Optimizer> {
  public:
  const ::CoreML::Specification::SGDOptimizer* sgdoptimizer_;
  const ::CoreML::Specification::AdamOptimizer* adamoptimizer_;
} _Optimizer_default_instance_;
class SGDOptimizerDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<SGDOptimizer> {
} _SGDOptimizer_default_instance_;
class AdamOptimizerDefaultTypeInternal : public ::google::protobuf::internal::ExplicitlyConstructed<AdamOptimizer> {
} _AdamOptimizer_default_instance_;

namespace protobuf_NeuralNetwork_2eproto {

PROTOBUF_CONSTEXPR_VAR ::google::protobuf::internal::ParseTableField
    const TableStruct::entries[] = {
  {0, 0, 0, ::google::protobuf::internal::kInvalidMask, 0, 0},
};

PROTOBUF_CONSTEXPR_VAR ::google::protobuf::internal::AuxillaryParseTableField
    const TableStruct::aux[] = {
  ::google::protobuf::internal::AuxillaryParseTableField(),
};
PROTOBUF_CONSTEXPR_VAR ::google::protobuf::internal::ParseTable const
    TableStruct::schema[] = {
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
  { NULL, NULL, 0, -1, -1, false },
};


void TableStruct::Shutdown() {
  _NeuralNetwork_default_instance_.Shutdown();
  _NeuralNetworkImageScaler_default_instance_.Shutdown();
  _NeuralNetworkMeanImage_default_instance_.Shutdown();
  _NeuralNetworkPreprocessing_default_instance_.Shutdown();
  _ActivationReLU_default_instance_.Shutdown();
  _ActivationLeakyReLU_default_instance_.Shutdown();
  _ActivationTanh_default_instance_.Shutdown();
  _ActivationScaledTanh_default_instance_.Shutdown();
  _ActivationSigmoid_default_instance_.Shutdown();
  _ActivationLinear_default_instance_.Shutdown();
  _ActivationSigmoidHard_default_instance_.Shutdown();
  _ActivationPReLU_default_instance_.Shutdown();
  _ActivationELU_default_instance_.Shutdown();
  _ActivationThresholdedReLU_default_instance_.Shutdown();
  _ActivationSoftsign_default_instance_.Shutdown();
  _ActivationSoftplus_default_instance_.Shutdown();
  _ActivationParametricSoftplus_default_instance_.Shutdown();
  _ActivationParams_default_instance_.Shutdown();
  _Tensor_default_instance_.Shutdown();
  _NeuralNetworkLayer_default_instance_.Shutdown();
  _BranchLayerParams_default_instance_.Shutdown();
  _LoopLayerParams_default_instance_.Shutdown();
  _LoopBreakLayerParams_default_instance_.Shutdown();
  _LoopContinueLayerParams_default_instance_.Shutdown();
  _CopyLayerParams_default_instance_.Shutdown();
  _GreaterThanLayerParams_default_instance_.Shutdown();
  _LessThanLayerParams_default_instance_.Shutdown();
  _EqualLayerParams_default_instance_.Shutdown();
  _NotEqualLayerParams_default_instance_.Shutdown();
  _LogicalAndLayerParams_default_instance_.Shutdown();
  _LogicalOrLayerParams_default_instance_.Shutdown();
  _LogicalXorLayerParams_default_instance_.Shutdown();
  _LogicalNotLayerParams_default_instance_.Shutdown();
  _BorderAmounts_EdgeSizes_default_instance_.Shutdown();
  _BorderAmounts_default_instance_.Shutdown();
  _ValidPadding_default_instance_.Shutdown();
  _SamePadding_default_instance_.Shutdown();
  _SamplingMode_default_instance_.Shutdown();
  _BoxCoordinatesMode_default_instance_.Shutdown();
  _WeightParams_default_instance_.Shutdown();
  _QuantizationParams_default_instance_.Shutdown();
  _LinearQuantizationParams_default_instance_.Shutdown();
  _LookUpTableQuantizationParams_default_instance_.Shutdown();
  _ConvolutionLayerParams_default_instance_.Shutdown();
  _InnerProductLayerParams_default_instance_.Shutdown();
  _EmbeddingLayerParams_default_instance_.Shutdown();
  _EmbeddingNDLayerParams_default_instance_.Shutdown();
  _BatchnormLayerParams_default_instance_.Shutdown();
  _PoolingLayerParams_ValidCompletePadding_default_instance_.Shutdown();
  _PoolingLayerParams_default_instance_.Shutdown();
  _PaddingLayerParams_PaddingConstant_default_instance_.Shutdown();
  _PaddingLayerParams_PaddingReflection_default_instance_.Shutdown();
  _PaddingLayerParams_PaddingReplication_default_instance_.Shutdown();
  _PaddingLayerParams_default_instance_.Shutdown();
  _ConcatLayerParams_default_instance_.Shutdown();
  _LRNLayerParams_default_instance_.Shutdown();
  _SoftmaxLayerParams_default_instance_.Shutdown();
  _SplitLayerParams_default_instance_.Shutdown();
  _AddLayerParams_default_instance_.Shutdown();
  _MultiplyLayerParams_default_instance_.Shutdown();
  _UnaryFunctionLayerParams_default_instance_.Shutdown();
  _UpsampleLayerParams_default_instance_.Shutdown();
  _ResizeBilinearLayerParams_default_instance_.Shutdown();
  _CropResizeLayerParams_default_instance_.Shutdown();
  _BiasLayerParams_default_instance_.Shutdown();
  _ScaleLayerParams_default_instance_.Shutdown();
  _LoadConstantLayerParams_default_instance_.Shutdown();
  _L2NormalizeLayerParams_default_instance_.Shutdown();
  _FlattenLayerParams_default_instance_.Shutdown();
  _ReshapeLayerParams_default_instance_.Shutdown();
  _PermuteLayerParams_default_instance_.Shutdown();
  _ReorganizeDataLayerParams_default_instance_.Shutdown();
  _SliceLayerParams_default_instance_.Shutdown();
  _ReduceLayerParams_default_instance_.Shutdown();
  _CropLayerParams_default_instance_.Shutdown();
  _AverageLayerParams_default_instance_.Shutdown();
  _MaxLayerParams_default_instance_.Shutdown();
  _MinLayerParams_default_instance_.Shutdown();
  _DotProductLayerParams_default_instance_.Shutdown();
  _MeanVarianceNormalizeLayerParams_default_instance_.Shutdown();
  _SequenceRepeatLayerParams_default_instance_.Shutdown();
  _SimpleRecurrentLayerParams_default_instance_.Shutdown();
  _GRULayerParams_default_instance_.Shutdown();
  _LSTMParams_default_instance_.Shutdown();
  _LSTMWeightParams_default_instance_.Shutdown();
  _UniDirectionalLSTMLayerParams_default_instance_.Shutdown();
  _BiDirectionalLSTMLayerParams_default_instance_.Shutdown();
  _CustomLayerParams_CustomLayerParamValue_default_instance_.Shutdown();
  _CustomLayerParams_default_instance_.Shutdown();
  _TransposeNDLayerParams_default_instance_.Shutdown();
  _BatchedMatMulParams_default_instance_.Shutdown();
  _ConcatNDLayerParams_default_instance_.Shutdown();
  _SoftmaxNDLayerParams_default_instance_.Shutdown();
  _LoadConstantNDLayerParams_default_instance_.Shutdown();
  _FillLikeLayerParams_default_instance_.Shutdown();
  _FillStaticLayerParams_default_instance_.Shutdown();
  _FillDynamicLayerParams_default_instance_.Shutdown();
  _WhereLayerParams_default_instance_.Shutdown();
  _SineLayerParams_default_instance_.Shutdown();
  _CosineLayerParams_default_instance_.Shutdown();
  _PowBroadcastableLayerParams_default_instance_.Shutdown();
  _Exp2LayerParams_default_instance_.Shutdown();
  _UpperTriangularLayerParams_default_instance_.Shutdown();
  _LowerTriangularLayerParams_default_instance_.Shutdown();
  _BroadcastToLikeLayerParams_default_instance_.Shutdown();
  _BroadcastToStaticLayerParams_default_instance_.Shutdown();
  _BroadcastToDynamicLayerParams_default_instance_.Shutdown();
  _AddBroadcastableLayerParams_default_instance_.Shutdown();
  _SubtractBroadcastableLayerParams_default_instance_.Shutdown();
  _MultiplyBroadcastableLayerParams_default_instance_.Shutdown();
  _DivideBroadcastableLayerParams_default_instance_.Shutdown();
  _GatherLayerParams_default_instance_.Shutdown();
  _ScatterLayerParams_default_instance_.Shutdown();
  _StackNDLayerParams_default_instance_.Shutdown();
  _RankPreservingReshapeLayerParams_default_instance_.Shutdown();
  _ExpandDimsLayerParams_default_instance_.Shutdown();
  _SqueezeLayerParams_default_instance_.Shutdown();
  _SplitNDLayerParams_default_instance_.Shutdown();
  _CeilLayerParams_default_instance_.Shutdown();
  _FloorLayerParams_default_instance_.Shutdown();
  _ClipLayerParams_default_instance_.Shutdown();
  _SliceNDLayerParams_default_instance_.Shutdown();
  _TileLayerParams_default_instance_.Shutdown();
  _GetShapeLayerParams_default_instance_.Shutdown();
  _ErfActivationLayerParams_default_instance_.Shutdown();
  _GeluActivationLayerParams_default_instance_.Shutdown();
  _RangeLayerParams_default_instance_.Shutdown();
  _SlidingWindowsLayerParams_default_instance_.Shutdown();
  _NeuralNetworkClassifier_default_instance_.Shutdown();
  _NeuralNetworkRegressor_default_instance_.Shutdown();
  _NetworkUpdateParameters_default_instance_.Shutdown();
  _LossLayer_default_instance_.Shutdown();
  _CrossEntropyLossLayer_default_instance_.Shutdown();
  _MSELossLayer_default_instance_.Shutdown();
  _Optimizer_default_instance_.Shutdown();
  _SGDOptimizer_default_instance_.Shutdown();
  _AdamOptimizer_default_instance_.Shutdown();
}

void TableStruct::InitDefaultsImpl() {
  GOOGLE_PROTOBUF_VERIFY_VERSION;

  ::google::protobuf::internal::InitProtobufDefaults();
  ::CoreML::Specification::protobuf_DataStructures_2eproto::InitDefaults();
  ::CoreML::Specification::protobuf_Parameters_2eproto::InitDefaults();
  _NeuralNetwork_default_instance_.DefaultConstruct();
  _NeuralNetworkImageScaler_default_instance_.DefaultConstruct();
  _NeuralNetworkMeanImage_default_instance_.DefaultConstruct();
  _NeuralNetworkPreprocessing_default_instance_.DefaultConstruct();
  _ActivationReLU_default_instance_.DefaultConstruct();
  _ActivationLeakyReLU_default_instance_.DefaultConstruct();
  _ActivationTanh_default_instance_.DefaultConstruct();
  _ActivationScaledTanh_default_instance_.DefaultConstruct();
  _ActivationSigmoid_default_instance_.DefaultConstruct();
  _ActivationLinear_default_instance_.DefaultConstruct();
  _ActivationSigmoidHard_default_instance_.DefaultConstruct();
  _ActivationPReLU_default_instance_.DefaultConstruct();
  _ActivationELU_default_instance_.DefaultConstruct();
  _ActivationThresholdedReLU_default_instance_.DefaultConstruct();
  _ActivationSoftsign_default_instance_.DefaultConstruct();
  _ActivationSoftplus_default_instance_.DefaultConstruct();
  _ActivationParametricSoftplus_default_instance_.DefaultConstruct();
  _ActivationParams_default_instance_.DefaultConstruct();
  _Tensor_default_instance_.DefaultConstruct();
  _NeuralNetworkLayer_default_instance_.DefaultConstruct();
  _BranchLayerParams_default_instance_.DefaultConstruct();
  _LoopLayerParams_default_instance_.DefaultConstruct();
  _LoopBreakLayerParams_default_instance_.DefaultConstruct();
  _LoopContinueLayerParams_default_instance_.DefaultConstruct();
  _CopyLayerParams_default_instance_.DefaultConstruct();
  _GreaterThanLayerParams_default_instance_.DefaultConstruct();
  _LessThanLayerParams_default_instance_.DefaultConstruct();
  _EqualLayerParams_default_instance_.DefaultConstruct();
  _NotEqualLayerParams_default_instance_.DefaultConstruct();
  _LogicalAndLayerParams_default_instance_.DefaultConstruct();
  _LogicalOrLayerParams_default_instance_.DefaultConstruct();
  _LogicalXorLayerParams_default_instance_.DefaultConstruct();
  _LogicalNotLayerParams_default_instance_.DefaultConstruct();
  _BorderAmounts_EdgeSizes_default_instance_.DefaultConstruct();
  _BorderAmounts_default_instance_.DefaultConstruct();
  _ValidPadding_default_instance_.DefaultConstruct();
  _SamePadding_default_instance_.DefaultConstruct();
  _SamplingMode_default_instance_.DefaultConstruct();
  _BoxCoordinatesMode_default_instance_.DefaultConstruct();
  _WeightParams_default_instance_.DefaultConstruct();
  _QuantizationParams_default_instance_.DefaultConstruct();
  _LinearQuantizationParams_default_instance_.DefaultConstruct();
  _LookUpTableQuantizationParams_default_instance_.DefaultConstruct();
  _ConvolutionLayerParams_default_instance_.DefaultConstruct();
  _InnerProductLayerParams_default_instance_.DefaultConstruct();
  _EmbeddingLayerParams_default_instance_.DefaultConstruct();
  _EmbeddingNDLayerParams_default_instance_.DefaultConstruct();
  _BatchnormLayerParams_default_instance_.DefaultConstruct();
  _PoolingLayerParams_ValidCompletePadding_default_instance_.DefaultConstruct();
  _PoolingLayerParams_default_instance_.DefaultConstruct();
  _PaddingLayerParams_PaddingConstant_default_instance_.DefaultConstruct();
  _PaddingLayerParams_PaddingReflection_default_instance_.DefaultConstruct();
  _PaddingLayerParams_PaddingReplication_default_instance_.DefaultConstruct();
  _PaddingLayerParams_default_instance_.DefaultConstruct();
  _ConcatLayerParams_default_instance_.DefaultConstruct();
  _LRNLayerParams_default_instance_.DefaultConstruct();
  _SoftmaxLayerParams_default_instance_.DefaultConstruct();
  _SplitLayerParams_default_instance_.DefaultConstruct();
  _AddLayerParams_default_instance_.DefaultConstruct();
  _MultiplyLayerParams_default_instance_.DefaultConstruct();
  _UnaryFunctionLayerParams_default_instance_.DefaultConstruct();
  _UpsampleLayerParams_default_instance_.DefaultConstruct();
  _ResizeBilinearLayerParams_default_instance_.DefaultConstruct();
  _CropResizeLayerParams_default_instance_.DefaultConstruct();
  _BiasLayerParams_default_instance_.DefaultConstruct();
  _ScaleLayerParams_default_instance_.DefaultConstruct();
  _LoadConstantLayerParams_default_instance_.DefaultConstruct();
  _L2NormalizeLayerParams_default_instance_.DefaultConstruct();
  _FlattenLayerParams_default_instance_.DefaultConstruct();
  _ReshapeLayerParams_default_instance_.DefaultConstruct();
  _PermuteLayerParams_default_instance_.DefaultConstruct();
  _ReorganizeDataLayerParams_default_instance_.DefaultConstruct();
  _SliceLayerParams_default_instance_.DefaultConstruct();
  _ReduceLayerParams_default_instance_.DefaultConstruct();
  _CropLayerParams_default_instance_.DefaultConstruct();
  _AverageLayerParams_default_instance_.DefaultConstruct();
  _MaxLayerParams_default_instance_.DefaultConstruct();
  _MinLayerParams_default_instance_.DefaultConstruct();
  _DotProductLayerParams_default_instance_.DefaultConstruct();
  _MeanVarianceNormalizeLayerParams_default_instance_.DefaultConstruct();
  _SequenceRepeatLayerParams_default_instance_.DefaultConstruct();
  _SimpleRecurrentLayerParams_default_instance_.DefaultConstruct();
  _GRULayerParams_default_instance_.DefaultConstruct();
  _LSTMParams_default_instance_.DefaultConstruct();
  _LSTMWeightParams_default_instance_.DefaultConstruct();
  _UniDirectionalLSTMLayerParams_default_instance_.DefaultConstruct();
  _BiDirectionalLSTMLayerParams_default_instance_.DefaultConstruct();
  _CustomLayerParams_CustomLayerParamValue_default_instance_.DefaultConstruct();
  _CustomLayerParams_ParametersEntry_default_instance_.DefaultConstruct();
  _CustomLayerParams_default_instance_.DefaultConstruct();
  _TransposeNDLayerParams_default_instance_.DefaultConstruct();
  _BatchedMatMulParams_default_instance_.DefaultConstruct();
  _ConcatNDLayerParams_default_instance_.DefaultConstruct();
  _SoftmaxNDLayerParams_default_instance_.DefaultConstruct();
  _LoadConstantNDLayerParams_default_instance_.DefaultConstruct();
  _FillLikeLayerParams_default_instance_.DefaultConstruct();
  _FillStaticLayerParams_default_instance_.DefaultConstruct();
  _FillDynamicLayerParams_default_instance_.DefaultConstruct();
  _WhereLayerParams_default_instance_.DefaultConstruct();
  _SineLayerParams_default_instance_.DefaultConstruct();
  _CosineLayerParams_default_instance_.DefaultConstruct();
  _PowBroadcastableLayerParams_default_instance_.DefaultConstruct();
  _Exp2LayerParams_default_instance_.DefaultConstruct();
  _UpperTriangularLayerParams_default_instance_.DefaultConstruct();
  _LowerTriangularLayerParams_default_instance_.DefaultConstruct();
  _BroadcastToLikeLayerParams_default_instance_.DefaultConstruct();
  _BroadcastToStaticLayerParams_default_instance_.DefaultConstruct();
  _BroadcastToDynamicLayerParams_default_instance_.DefaultConstruct();
  _AddBroadcastableLayerParams_default_instance_.DefaultConstruct();
  _SubtractBroadcastableLayerParams_default_instance_.DefaultConstruct();
  _MultiplyBroadcastableLayerParams_default_instance_.DefaultConstruct();
  _DivideBroadcastableLayerParams_default_instance_.DefaultConstruct();
  _GatherLayerParams_default_instance_.DefaultConstruct();
  _ScatterLayerParams_default_instance_.DefaultConstruct();
  _StackNDLayerParams_default_instance_.DefaultConstruct();
  _RankPreservingReshapeLayerParams_default_instance_.DefaultConstruct();
  _ExpandDimsLayerParams_default_instance_.DefaultConstruct();
  _SqueezeLayerParams_default_instance_.DefaultConstruct();
  _SplitNDLayerParams_default_instance_.DefaultConstruct();
  _CeilLayerParams_default_instance_.DefaultConstruct();
  _FloorLayerParams_default_instance_.DefaultConstruct();
  _ClipLayerParams_default_instance_.DefaultConstruct();
  _SliceNDLayerParams_default_instance_.DefaultConstruct();
  _TileLayerParams_default_instance_.DefaultConstruct();
  _GetShapeLayerParams_default_instance_.DefaultConstruct();
  _ErfActivationLayerParams_default_instance_.DefaultConstruct();
  _GeluActivationLayerParams_default_instance_.DefaultConstruct();
  _RangeLayerParams_default_instance_.DefaultConstruct();
  _SlidingWindowsLayerParams_default_instance_.DefaultConstruct();
  _NeuralNetworkClassifier_default_instance_.DefaultConstruct();
  _NeuralNetworkRegressor_default_instance_.DefaultConstruct();
  _NetworkUpdateParameters_default_instance_.DefaultConstruct();
  _LossLayer_default_instance_.DefaultConstruct();
  _CrossEntropyLossLayer_default_instance_.DefaultConstruct();
  _MSELossLayer_default_instance_.DefaultConstruct();
  _Optimizer_default_instance_.DefaultConstruct();
  _SGDOptimizer_default_instance_.DefaultConstruct();
  _AdamOptimizer_default_instance_.DefaultConstruct();
  _NeuralNetwork_default_instance_.get_mutable()->updateparams_ = const_cast< ::CoreML::Specification::NetworkUpdateParameters*>(
      ::CoreML::Specification::NetworkUpdateParameters::internal_default_instance());
  _ActivationPReLU_default_instance_.get_mutable()->alpha_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _ActivationParametricSoftplus_default_instance_.get_mutable()->alpha_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _ActivationParametricSoftplus_default_instance_.get_mutable()->beta_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _BranchLayerParams_default_instance_.get_mutable()->ifbranch_ = const_cast< ::CoreML::Specification::NeuralNetwork*>(
      ::CoreML::Specification::NeuralNetwork::internal_default_instance());
  _BranchLayerParams_default_instance_.get_mutable()->elsebranch_ = const_cast< ::CoreML::Specification::NeuralNetwork*>(
      ::CoreML::Specification::NeuralNetwork::internal_default_instance());
  _LoopLayerParams_default_instance_.get_mutable()->conditionnetwork_ = const_cast< ::CoreML::Specification::NeuralNetwork*>(
      ::CoreML::Specification::NeuralNetwork::internal_default_instance());
  _LoopLayerParams_default_instance_.get_mutable()->bodynetwork_ = const_cast< ::CoreML::Specification::NeuralNetwork*>(
      ::CoreML::Specification::NeuralNetwork::internal_default_instance());
  _ValidPadding_default_instance_.get_mutable()->paddingamounts_ = const_cast< ::CoreML::Specification::BorderAmounts*>(
      ::CoreML::Specification::BorderAmounts::internal_default_instance());
  _WeightParams_default_instance_.get_mutable()->quantization_ = const_cast< ::CoreML::Specification::QuantizationParams*>(
      ::CoreML::Specification::QuantizationParams::internal_default_instance());
  _ConvolutionLayerParams_default_instance_.get_mutable()->weights_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _ConvolutionLayerParams_default_instance_.get_mutable()->bias_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _InnerProductLayerParams_default_instance_.get_mutable()->weights_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _InnerProductLayerParams_default_instance_.get_mutable()->bias_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _EmbeddingLayerParams_default_instance_.get_mutable()->weights_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _EmbeddingLayerParams_default_instance_.get_mutable()->bias_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _EmbeddingNDLayerParams_default_instance_.get_mutable()->weights_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _EmbeddingNDLayerParams_default_instance_.get_mutable()->bias_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _BatchnormLayerParams_default_instance_.get_mutable()->gamma_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _BatchnormLayerParams_default_instance_.get_mutable()->beta_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _BatchnormLayerParams_default_instance_.get_mutable()->mean_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _BatchnormLayerParams_default_instance_.get_mutable()->variance_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _PaddingLayerParams_default_instance_.get_mutable()->paddingamounts_ = const_cast< ::CoreML::Specification::BorderAmounts*>(
      ::CoreML::Specification::BorderAmounts::internal_default_instance());
  _ResizeBilinearLayerParams_default_instance_.get_mutable()->mode_ = const_cast< ::CoreML::Specification::SamplingMode*>(
      ::CoreML::Specification::SamplingMode::internal_default_instance());
  _CropResizeLayerParams_default_instance_.get_mutable()->mode_ = const_cast< ::CoreML::Specification::SamplingMode*>(
      ::CoreML::Specification::SamplingMode::internal_default_instance());
  _CropResizeLayerParams_default_instance_.get_mutable()->boxindicesmode_ = const_cast< ::CoreML::Specification::BoxCoordinatesMode*>(
      ::CoreML::Specification::BoxCoordinatesMode::internal_default_instance());
  _BiasLayerParams_default_instance_.get_mutable()->bias_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _ScaleLayerParams_default_instance_.get_mutable()->scale_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _ScaleLayerParams_default_instance_.get_mutable()->bias_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _LoadConstantLayerParams_default_instance_.get_mutable()->data_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _CropLayerParams_default_instance_.get_mutable()->cropamounts_ = const_cast< ::CoreML::Specification::BorderAmounts*>(
      ::CoreML::Specification::BorderAmounts::internal_default_instance());
  _SimpleRecurrentLayerParams_default_instance_.get_mutable()->activation_ = const_cast< ::CoreML::Specification::ActivationParams*>(
      ::CoreML::Specification::ActivationParams::internal_default_instance());
  _SimpleRecurrentLayerParams_default_instance_.get_mutable()->weightmatrix_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _SimpleRecurrentLayerParams_default_instance_.get_mutable()->recursionmatrix_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _SimpleRecurrentLayerParams_default_instance_.get_mutable()->biasvector_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _GRULayerParams_default_instance_.get_mutable()->updategateweightmatrix_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _GRULayerParams_default_instance_.get_mutable()->resetgateweightmatrix_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _GRULayerParams_default_instance_.get_mutable()->outputgateweightmatrix_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _GRULayerParams_default_instance_.get_mutable()->updategaterecursionmatrix_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _GRULayerParams_default_instance_.get_mutable()->resetgaterecursionmatrix_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _GRULayerParams_default_instance_.get_mutable()->outputgaterecursionmatrix_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _GRULayerParams_default_instance_.get_mutable()->updategatebiasvector_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _GRULayerParams_default_instance_.get_mutable()->resetgatebiasvector_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _GRULayerParams_default_instance_.get_mutable()->outputgatebiasvector_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _LSTMWeightParams_default_instance_.get_mutable()->inputgateweightmatrix_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _LSTMWeightParams_default_instance_.get_mutable()->forgetgateweightmatrix_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _LSTMWeightParams_default_instance_.get_mutable()->blockinputweightmatrix_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _LSTMWeightParams_default_instance_.get_mutable()->outputgateweightmatrix_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _LSTMWeightParams_default_instance_.get_mutable()->inputgaterecursionmatrix_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _LSTMWeightParams_default_instance_.get_mutable()->forgetgaterecursionmatrix_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _LSTMWeightParams_default_instance_.get_mutable()->blockinputrecursionmatrix_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _LSTMWeightParams_default_instance_.get_mutable()->outputgaterecursionmatrix_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _LSTMWeightParams_default_instance_.get_mutable()->inputgatebiasvector_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _LSTMWeightParams_default_instance_.get_mutable()->forgetgatebiasvector_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _LSTMWeightParams_default_instance_.get_mutable()->blockinputbiasvector_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _LSTMWeightParams_default_instance_.get_mutable()->outputgatebiasvector_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _LSTMWeightParams_default_instance_.get_mutable()->inputgatepeepholevector_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _LSTMWeightParams_default_instance_.get_mutable()->forgetgatepeepholevector_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _LSTMWeightParams_default_instance_.get_mutable()->outputgatepeepholevector_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _UniDirectionalLSTMLayerParams_default_instance_.get_mutable()->params_ = const_cast< ::CoreML::Specification::LSTMParams*>(
      ::CoreML::Specification::LSTMParams::internal_default_instance());
  _UniDirectionalLSTMLayerParams_default_instance_.get_mutable()->weightparams_ = const_cast< ::CoreML::Specification::LSTMWeightParams*>(
      ::CoreML::Specification::LSTMWeightParams::internal_default_instance());
  _BiDirectionalLSTMLayerParams_default_instance_.get_mutable()->params_ = const_cast< ::CoreML::Specification::LSTMParams*>(
      ::CoreML::Specification::LSTMParams::internal_default_instance());
  _CustomLayerParams_ParametersEntry_default_instance_.get_mutable()->set_default_instance(_CustomLayerParams_ParametersEntry_default_instance_.get_mutable());
  _CustomLayerParams_ParametersEntry_default_instance_.get_mutable()->InitAsDefaultInstance();
  _BatchedMatMulParams_default_instance_.get_mutable()->weights_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _BatchedMatMulParams_default_instance_.get_mutable()->bias_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _LoadConstantNDLayerParams_default_instance_.get_mutable()->data_ = const_cast< ::CoreML::Specification::WeightParams*>(
      ::CoreML::Specification::WeightParams::internal_default_instance());
  _NeuralNetworkClassifier_default_instance_.get_mutable()->updateparams_ = const_cast< ::CoreML::Specification::NetworkUpdateParameters*>(
      ::CoreML::Specification::NetworkUpdateParameters::internal_default_instance());
  _NeuralNetworkRegressor_default_instance_.get_mutable()->updateparams_ = const_cast< ::CoreML::Specification::NetworkUpdateParameters*>(
      ::CoreML::Specification::NetworkUpdateParameters::internal_default_instance());
  _NetworkUpdateParameters_default_instance_.get_mutable()->optimizer_ = const_cast< ::CoreML::Specification::Optimizer*>(
      ::CoreML::Specification::Optimizer::internal_default_instance());
  _NetworkUpdateParameters_default_instance_.get_mutable()->epochs_ = const_cast< ::CoreML::Specification::Int64Parameter*>(
      ::CoreML::Specification::Int64Parameter::internal_default_instance());
  _SGDOptimizer_default_instance_.get_mutable()->learningrate_ = const_cast< ::CoreML::Specification::DoubleParameter*>(
      ::CoreML::Specification::DoubleParameter::internal_default_instance());
  _SGDOptimizer_default_instance_.get_mutable()->minibatchsize_ = const_cast< ::CoreML::Specification::Int64Parameter*>(
      ::CoreML::Specification::Int64Parameter::internal_default_instance());
  _SGDOptimizer_default_instance_.get_mutable()->momentum_ = const_cast< ::CoreML::Specification::DoubleParameter*>(
      ::CoreML::Specification::DoubleParameter::internal_default_instance());
  _AdamOptimizer_default_instance_.get_mutable()->learningrate_ = const_cast< ::CoreML::Specification::DoubleParameter*>(
      ::CoreML::Specification::DoubleParameter::internal_default_instance());
  _AdamOptimizer_default_instance_.get_mutable()->minibatchsize_ = const_cast< ::CoreML::Specification::Int64Parameter*>(
      ::CoreML::Specification::Int64Parameter::internal_default_instance());
  _AdamOptimizer_default_instance_.get_mutable()->beta1_ = const_cast< ::CoreML::Specification::DoubleParameter*>(
      ::CoreML::Specification::DoubleParameter::internal_default_instance());
  _AdamOptimizer_default_instance_.get_mutable()->beta2_ = const_cast< ::CoreML::Specification::DoubleParameter*>(
      ::CoreML::Specification::DoubleParameter::internal_default_instance());
  _AdamOptimizer_default_instance_.get_mutable()->eps_ = const_cast< ::CoreML::Specification::DoubleParameter*>(
      ::CoreML::Specification::DoubleParameter::internal_default_instance());
}

void InitDefaults() {
  static GOOGLE_PROTOBUF_DECLARE_ONCE(once);
  ::google::protobuf::GoogleOnceInit(&once, &TableStruct::InitDefaultsImpl);
}
void AddDescriptorsImpl() {
  InitDefaults();
  ::CoreML::Specification::protobuf_DataStructures_2eproto::AddDescriptors();
  ::CoreML::Specification::protobuf_Parameters_2eproto::AddDescriptors();
  ::google::protobuf::internal::OnShutdown(&TableStruct::Shutdown);
}

void AddDescriptors() {
  static GOOGLE_PROTOBUF_DECLARE_ONCE(once);
  ::google::protobuf::GoogleOnceInit(&once, &AddDescriptorsImpl);
}
#ifdef GOOGLE_PROTOBUF_NO_STATIC_INITIALIZER
// Force AddDescriptors() to be called at static initialization time.
struct StaticDescriptorInitializer {
  StaticDescriptorInitializer() {
    AddDescriptors();
  }
} static_descriptor_initializer;
#endif  // GOOGLE_PROTOBUF_NO_STATIC_INITIALIZER

}  // namespace protobuf_NeuralNetwork_2eproto

bool SamePadding_SamePaddingMode_IsValid(int value) {
  switch (value) {
    case 0:
    case 1:
      return true;
    default:
      return false;
  }
}

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const SamePadding_SamePaddingMode SamePadding::BOTTOM_RIGHT_HEAVY;
const SamePadding_SamePaddingMode SamePadding::TOP_LEFT_HEAVY;
const SamePadding_SamePaddingMode SamePadding::SamePaddingMode_MIN;
const SamePadding_SamePaddingMode SamePadding::SamePaddingMode_MAX;
const int SamePadding::SamePaddingMode_ARRAYSIZE;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900
bool SamplingMode_Method_IsValid(int value) {
  switch (value) {
    case 0:
    case 1:
    case 2:
    case 3:
      return true;
    default:
      return false;
  }
}

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const SamplingMode_Method SamplingMode::STRICT_ALIGN_ENDPOINTS_MODE;
const SamplingMode_Method SamplingMode::ALIGN_ENDPOINTS_MODE;
const SamplingMode_Method SamplingMode::UPSAMPLE_MODE;
const SamplingMode_Method SamplingMode::ROI_ALIGN_MODE;
const SamplingMode_Method SamplingMode::Method_MIN;
const SamplingMode_Method SamplingMode::Method_MAX;
const int SamplingMode::Method_ARRAYSIZE;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900
bool BoxCoordinatesMode_Coordinates_IsValid(int value) {
  switch (value) {
    case 0:
    case 1:
    case 2:
    case 3:
      return true;
    default:
      return false;
  }
}

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const BoxCoordinatesMode_Coordinates BoxCoordinatesMode::CORNERS_HEIGHT_FIRST;
const BoxCoordinatesMode_Coordinates BoxCoordinatesMode::CORNERS_WIDTH_FIRST;
const BoxCoordinatesMode_Coordinates BoxCoordinatesMode::CENTER_SIZE_HEIGHT_FIRST;
const BoxCoordinatesMode_Coordinates BoxCoordinatesMode::CENTER_SIZE_WIDTH_FIRST;
const BoxCoordinatesMode_Coordinates BoxCoordinatesMode::Coordinates_MIN;
const BoxCoordinatesMode_Coordinates BoxCoordinatesMode::Coordinates_MAX;
const int BoxCoordinatesMode::Coordinates_ARRAYSIZE;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900
bool PoolingLayerParams_PoolingType_IsValid(int value) {
  switch (value) {
    case 0:
    case 1:
    case 2:
      return true;
    default:
      return false;
  }
}

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const PoolingLayerParams_PoolingType PoolingLayerParams::MAX;
const PoolingLayerParams_PoolingType PoolingLayerParams::AVERAGE;
const PoolingLayerParams_PoolingType PoolingLayerParams::L2;
const PoolingLayerParams_PoolingType PoolingLayerParams::PoolingType_MIN;
const PoolingLayerParams_PoolingType PoolingLayerParams::PoolingType_MAX;
const int PoolingLayerParams::PoolingType_ARRAYSIZE;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900
bool UnaryFunctionLayerParams_Operation_IsValid(int value) {
  switch (value) {
    case 0:
    case 1:
    case 2:
    case 3:
    case 4:
    case 5:
    case 6:
    case 7:
      return true;
    default:
      return false;
  }
}

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const UnaryFunctionLayerParams_Operation UnaryFunctionLayerParams::SQRT;
const UnaryFunctionLayerParams_Operation UnaryFunctionLayerParams::RSQRT;
const UnaryFunctionLayerParams_Operation UnaryFunctionLayerParams::INVERSE;
const UnaryFunctionLayerParams_Operation UnaryFunctionLayerParams::POWER;
const UnaryFunctionLayerParams_Operation UnaryFunctionLayerParams::EXP;
const UnaryFunctionLayerParams_Operation UnaryFunctionLayerParams::LOG;
const UnaryFunctionLayerParams_Operation UnaryFunctionLayerParams::ABS;
const UnaryFunctionLayerParams_Operation UnaryFunctionLayerParams::THRESHOLD;
const UnaryFunctionLayerParams_Operation UnaryFunctionLayerParams::Operation_MIN;
const UnaryFunctionLayerParams_Operation UnaryFunctionLayerParams::Operation_MAX;
const int UnaryFunctionLayerParams::Operation_ARRAYSIZE;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900
bool UpsampleLayerParams_InterpolationMode_IsValid(int value) {
  switch (value) {
    case 0:
    case 1:
      return true;
    default:
      return false;
  }
}

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const UpsampleLayerParams_InterpolationMode UpsampleLayerParams::NN;
const UpsampleLayerParams_InterpolationMode UpsampleLayerParams::BILINEAR;
const UpsampleLayerParams_InterpolationMode UpsampleLayerParams::InterpolationMode_MIN;
const UpsampleLayerParams_InterpolationMode UpsampleLayerParams::InterpolationMode_MAX;
const int UpsampleLayerParams::InterpolationMode_ARRAYSIZE;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900
bool FlattenLayerParams_FlattenOrder_IsValid(int value) {
  switch (value) {
    case 0:
    case 1:
      return true;
    default:
      return false;
  }
}

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const FlattenLayerParams_FlattenOrder FlattenLayerParams::CHANNEL_FIRST;
const FlattenLayerParams_FlattenOrder FlattenLayerParams::CHANNEL_LAST;
const FlattenLayerParams_FlattenOrder FlattenLayerParams::FlattenOrder_MIN;
const FlattenLayerParams_FlattenOrder FlattenLayerParams::FlattenOrder_MAX;
const int FlattenLayerParams::FlattenOrder_ARRAYSIZE;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900
bool ReshapeLayerParams_ReshapeOrder_IsValid(int value) {
  switch (value) {
    case 0:
    case 1:
      return true;
    default:
      return false;
  }
}

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const ReshapeLayerParams_ReshapeOrder ReshapeLayerParams::CHANNEL_FIRST;
const ReshapeLayerParams_ReshapeOrder ReshapeLayerParams::CHANNEL_LAST;
const ReshapeLayerParams_ReshapeOrder ReshapeLayerParams::ReshapeOrder_MIN;
const ReshapeLayerParams_ReshapeOrder ReshapeLayerParams::ReshapeOrder_MAX;
const int ReshapeLayerParams::ReshapeOrder_ARRAYSIZE;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900
bool ReorganizeDataLayerParams_ReorganizationType_IsValid(int value) {
  switch (value) {
    case 0:
    case 1:
      return true;
    default:
      return false;
  }
}

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const ReorganizeDataLayerParams_ReorganizationType ReorganizeDataLayerParams::SPACE_TO_DEPTH;
const ReorganizeDataLayerParams_ReorganizationType ReorganizeDataLayerParams::DEPTH_TO_SPACE;
const ReorganizeDataLayerParams_ReorganizationType ReorganizeDataLayerParams::ReorganizationType_MIN;
const ReorganizeDataLayerParams_ReorganizationType ReorganizeDataLayerParams::ReorganizationType_MAX;
const int ReorganizeDataLayerParams::ReorganizationType_ARRAYSIZE;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900
bool SliceLayerParams_SliceAxis_IsValid(int value) {
  switch (value) {
    case 0:
    case 1:
    case 2:
      return true;
    default:
      return false;
  }
}

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const SliceLayerParams_SliceAxis SliceLayerParams::CHANNEL_AXIS;
const SliceLayerParams_SliceAxis SliceLayerParams::HEIGHT_AXIS;
const SliceLayerParams_SliceAxis SliceLayerParams::WIDTH_AXIS;
const SliceLayerParams_SliceAxis SliceLayerParams::SliceAxis_MIN;
const SliceLayerParams_SliceAxis SliceLayerParams::SliceAxis_MAX;
const int SliceLayerParams::SliceAxis_ARRAYSIZE;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900
bool ReduceLayerParams_ReduceOperation_IsValid(int value) {
  switch (value) {
    case 0:
    case 1:
    case 2:
    case 3:
    case 4:
    case 5:
    case 6:
    case 7:
    case 8:
    case 9:
      return true;
    default:
      return false;
  }
}

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const ReduceLayerParams_ReduceOperation ReduceLayerParams::SUM;
const ReduceLayerParams_ReduceOperation ReduceLayerParams::AVG;
const ReduceLayerParams_ReduceOperation ReduceLayerParams::PROD;
const ReduceLayerParams_ReduceOperation ReduceLayerParams::LOGSUM;
const ReduceLayerParams_ReduceOperation ReduceLayerParams::SUMSQUARE;
const ReduceLayerParams_ReduceOperation ReduceLayerParams::L1;
const ReduceLayerParams_ReduceOperation ReduceLayerParams::L2;
const ReduceLayerParams_ReduceOperation ReduceLayerParams::MAX;
const ReduceLayerParams_ReduceOperation ReduceLayerParams::MIN;
const ReduceLayerParams_ReduceOperation ReduceLayerParams::ARGMAX;
const ReduceLayerParams_ReduceOperation ReduceLayerParams::ReduceOperation_MIN;
const ReduceLayerParams_ReduceOperation ReduceLayerParams::ReduceOperation_MAX;
const int ReduceLayerParams::ReduceOperation_ARRAYSIZE;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900
bool ReduceLayerParams_ReduceAxis_IsValid(int value) {
  switch (value) {
    case 0:
    case 1:
    case 2:
    case 3:
    case 4:
      return true;
    default:
      return false;
  }
}

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const ReduceLayerParams_ReduceAxis ReduceLayerParams::CHW;
const ReduceLayerParams_ReduceAxis ReduceLayerParams::HW;
const ReduceLayerParams_ReduceAxis ReduceLayerParams::C;
const ReduceLayerParams_ReduceAxis ReduceLayerParams::H;
const ReduceLayerParams_ReduceAxis ReduceLayerParams::W;
const ReduceLayerParams_ReduceAxis ReduceLayerParams::ReduceAxis_MIN;
const ReduceLayerParams_ReduceAxis ReduceLayerParams::ReduceAxis_MAX;
const int ReduceLayerParams::ReduceAxis_ARRAYSIZE;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900
bool NeuralNetworkMultiArrayShapeMapping_IsValid(int value) {
  switch (value) {
    case 0:
    case 1:
      return true;
    default:
      return false;
  }
}

bool NeuralNetworkImageShapeMapping_IsValid(int value) {
  switch (value) {
    case 0:
    case 1:
      return true;
    default:
      return false;
  }
}


// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int NeuralNetwork::kLayersFieldNumber;
const int NeuralNetwork::kPreprocessingFieldNumber;
const int NeuralNetwork::kArrayInputShapeMappingFieldNumber;
const int NeuralNetwork::kImageInputShapeMappingFieldNumber;
const int NeuralNetwork::kUpdateParamsFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

NeuralNetwork::NeuralNetwork()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.NeuralNetwork)
}
NeuralNetwork::NeuralNetwork(const NeuralNetwork& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      layers_(from.layers_),
      preprocessing_(from.preprocessing_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_updateparams()) {
    updateparams_ = new ::CoreML::Specification::NetworkUpdateParameters(*from.updateparams_);
  } else {
    updateparams_ = NULL;
  }
  ::memcpy(&arrayinputshapemapping_, &from.arrayinputshapemapping_,
    reinterpret_cast<char*>(&imageinputshapemapping_) -
    reinterpret_cast<char*>(&arrayinputshapemapping_) + sizeof(imageinputshapemapping_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.NeuralNetwork)
}

void NeuralNetwork::SharedCtor() {
  ::memset(&updateparams_, 0, reinterpret_cast<char*>(&imageinputshapemapping_) -
    reinterpret_cast<char*>(&updateparams_) + sizeof(imageinputshapemapping_));
  _cached_size_ = 0;
}

NeuralNetwork::~NeuralNetwork() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.NeuralNetwork)
  SharedDtor();
}

void NeuralNetwork::SharedDtor() {
  if (this != internal_default_instance()) {
    delete updateparams_;
  }
}

void NeuralNetwork::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const NeuralNetwork& NeuralNetwork::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

NeuralNetwork* NeuralNetwork::New(::google::protobuf::Arena* arena) const {
  NeuralNetwork* n = new NeuralNetwork;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void NeuralNetwork::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.NeuralNetwork)
  layers_.Clear();
  preprocessing_.Clear();
  if (GetArenaNoVirtual() == NULL && updateparams_ != NULL) {
    delete updateparams_;
  }
  updateparams_ = NULL;
  ::memset(&arrayinputshapemapping_, 0, reinterpret_cast<char*>(&imageinputshapemapping_) -
    reinterpret_cast<char*>(&arrayinputshapemapping_) + sizeof(imageinputshapemapping_));
}

bool NeuralNetwork::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.NeuralNetwork)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated .CoreML.Specification.NeuralNetworkLayer layers = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
                input, add_layers()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated .CoreML.Specification.NeuralNetworkPreprocessing preprocessing = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(18u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
                input, add_preprocessing()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.NeuralNetworkMultiArrayShapeMapping arrayInputShapeMapping = 5;
      case 5: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(40u)) {
          int value;
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   int, ::google::protobuf::internal::WireFormatLite::TYPE_ENUM>(
                 input, &value)));
          set_arrayinputshapemapping(static_cast< ::CoreML::Specification::NeuralNetworkMultiArrayShapeMapping >(value));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.NeuralNetworkImageShapeMapping imageInputShapeMapping = 6;
      case 6: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(48u)) {
          int value;
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   int, ::google::protobuf::internal::WireFormatLite::TYPE_ENUM>(
                 input, &value)));
          set_imageinputshapemapping(static_cast< ::CoreML::Specification::NeuralNetworkImageShapeMapping >(value));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.NetworkUpdateParameters updateParams = 10;
      case 10: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(82u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_updateparams()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.NeuralNetwork)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.NeuralNetwork)
  return false;
#undef DO_
}

void NeuralNetwork::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.NeuralNetwork)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated .CoreML.Specification.NeuralNetworkLayer layers = 1;
  for (unsigned int i = 0, n = this->layers_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1, this->layers(i), output);
  }

  // repeated .CoreML.Specification.NeuralNetworkPreprocessing preprocessing = 2;
  for (unsigned int i = 0, n = this->preprocessing_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      2, this->preprocessing(i), output);
  }

  // .CoreML.Specification.NeuralNetworkMultiArrayShapeMapping arrayInputShapeMapping = 5;
  if (this->arrayinputshapemapping() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteEnum(
      5, this->arrayinputshapemapping(), output);
  }

  // .CoreML.Specification.NeuralNetworkImageShapeMapping imageInputShapeMapping = 6;
  if (this->imageinputshapemapping() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteEnum(
      6, this->imageinputshapemapping(), output);
  }

  // .CoreML.Specification.NetworkUpdateParameters updateParams = 10;
  if (this->has_updateparams()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      10, *this->updateparams_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.NeuralNetwork)
}

size_t NeuralNetwork::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.NeuralNetwork)
  size_t total_size = 0;

  // repeated .CoreML.Specification.NeuralNetworkLayer layers = 1;
  {
    unsigned int count = this->layers_size();
    total_size += 1UL * count;
    for (unsigned int i = 0; i < count; i++) {
      total_size +=
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          this->layers(i));
    }
  }

  // repeated .CoreML.Specification.NeuralNetworkPreprocessing preprocessing = 2;
  {
    unsigned int count = this->preprocessing_size();
    total_size += 1UL * count;
    for (unsigned int i = 0; i < count; i++) {
      total_size +=
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          this->preprocessing(i));
    }
  }

  // .CoreML.Specification.NetworkUpdateParameters updateParams = 10;
  if (this->has_updateparams()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->updateparams_);
  }

  // .CoreML.Specification.NeuralNetworkMultiArrayShapeMapping arrayInputShapeMapping = 5;
  if (this->arrayinputshapemapping() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::EnumSize(this->arrayinputshapemapping());
  }

  // .CoreML.Specification.NeuralNetworkImageShapeMapping imageInputShapeMapping = 6;
  if (this->imageinputshapemapping() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::EnumSize(this->imageinputshapemapping());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void NeuralNetwork::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const NeuralNetwork*>(&from));
}

void NeuralNetwork::MergeFrom(const NeuralNetwork& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.NeuralNetwork)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  layers_.MergeFrom(from.layers_);
  preprocessing_.MergeFrom(from.preprocessing_);
  if (from.has_updateparams()) {
    mutable_updateparams()->::CoreML::Specification::NetworkUpdateParameters::MergeFrom(from.updateparams());
  }
  if (from.arrayinputshapemapping() != 0) {
    set_arrayinputshapemapping(from.arrayinputshapemapping());
  }
  if (from.imageinputshapemapping() != 0) {
    set_imageinputshapemapping(from.imageinputshapemapping());
  }
}

void NeuralNetwork::CopyFrom(const NeuralNetwork& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.NeuralNetwork)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool NeuralNetwork::IsInitialized() const {
  return true;
}

void NeuralNetwork::Swap(NeuralNetwork* other) {
  if (other == this) return;
  InternalSwap(other);
}
void NeuralNetwork::InternalSwap(NeuralNetwork* other) {
  layers_.InternalSwap(&other->layers_);
  preprocessing_.InternalSwap(&other->preprocessing_);
  std::swap(updateparams_, other->updateparams_);
  std::swap(arrayinputshapemapping_, other->arrayinputshapemapping_);
  std::swap(imageinputshapemapping_, other->imageinputshapemapping_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string NeuralNetwork::GetTypeName() const {
  return "CoreML.Specification.NeuralNetwork";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// NeuralNetwork

// repeated .CoreML.Specification.NeuralNetworkLayer layers = 1;
int NeuralNetwork::layers_size() const {
  return layers_.size();
}
void NeuralNetwork::clear_layers() {
  layers_.Clear();
}
const ::CoreML::Specification::NeuralNetworkLayer& NeuralNetwork::layers(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetwork.layers)
  return layers_.Get(index);
}
::CoreML::Specification::NeuralNetworkLayer* NeuralNetwork::mutable_layers(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetwork.layers)
  return layers_.Mutable(index);
}
::CoreML::Specification::NeuralNetworkLayer* NeuralNetwork::add_layers() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.NeuralNetwork.layers)
  return layers_.Add();
}
::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkLayer >*
NeuralNetwork::mutable_layers() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.NeuralNetwork.layers)
  return &layers_;
}
const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkLayer >&
NeuralNetwork::layers() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.NeuralNetwork.layers)
  return layers_;
}

// repeated .CoreML.Specification.NeuralNetworkPreprocessing preprocessing = 2;
int NeuralNetwork::preprocessing_size() const {
  return preprocessing_.size();
}
void NeuralNetwork::clear_preprocessing() {
  preprocessing_.Clear();
}
const ::CoreML::Specification::NeuralNetworkPreprocessing& NeuralNetwork::preprocessing(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetwork.preprocessing)
  return preprocessing_.Get(index);
}
::CoreML::Specification::NeuralNetworkPreprocessing* NeuralNetwork::mutable_preprocessing(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetwork.preprocessing)
  return preprocessing_.Mutable(index);
}
::CoreML::Specification::NeuralNetworkPreprocessing* NeuralNetwork::add_preprocessing() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.NeuralNetwork.preprocessing)
  return preprocessing_.Add();
}
::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkPreprocessing >*
NeuralNetwork::mutable_preprocessing() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.NeuralNetwork.preprocessing)
  return &preprocessing_;
}
const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkPreprocessing >&
NeuralNetwork::preprocessing() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.NeuralNetwork.preprocessing)
  return preprocessing_;
}

// .CoreML.Specification.NeuralNetworkMultiArrayShapeMapping arrayInputShapeMapping = 5;
void NeuralNetwork::clear_arrayinputshapemapping() {
  arrayinputshapemapping_ = 0;
}
::CoreML::Specification::NeuralNetworkMultiArrayShapeMapping NeuralNetwork::arrayinputshapemapping() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetwork.arrayInputShapeMapping)
  return static_cast< ::CoreML::Specification::NeuralNetworkMultiArrayShapeMapping >(arrayinputshapemapping_);
}
void NeuralNetwork::set_arrayinputshapemapping(::CoreML::Specification::NeuralNetworkMultiArrayShapeMapping value) {
  
  arrayinputshapemapping_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetwork.arrayInputShapeMapping)
}

// .CoreML.Specification.NeuralNetworkImageShapeMapping imageInputShapeMapping = 6;
void NeuralNetwork::clear_imageinputshapemapping() {
  imageinputshapemapping_ = 0;
}
::CoreML::Specification::NeuralNetworkImageShapeMapping NeuralNetwork::imageinputshapemapping() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetwork.imageInputShapeMapping)
  return static_cast< ::CoreML::Specification::NeuralNetworkImageShapeMapping >(imageinputshapemapping_);
}
void NeuralNetwork::set_imageinputshapemapping(::CoreML::Specification::NeuralNetworkImageShapeMapping value) {
  
  imageinputshapemapping_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetwork.imageInputShapeMapping)
}

// .CoreML.Specification.NetworkUpdateParameters updateParams = 10;
bool NeuralNetwork::has_updateparams() const {
  return this != internal_default_instance() && updateparams_ != NULL;
}
void NeuralNetwork::clear_updateparams() {
  if (GetArenaNoVirtual() == NULL && updateparams_ != NULL) delete updateparams_;
  updateparams_ = NULL;
}
const ::CoreML::Specification::NetworkUpdateParameters& NeuralNetwork::updateparams() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetwork.updateParams)
  return updateparams_ != NULL ? *updateparams_
                         : *::CoreML::Specification::NetworkUpdateParameters::internal_default_instance();
}
::CoreML::Specification::NetworkUpdateParameters* NeuralNetwork::mutable_updateparams() {
  
  if (updateparams_ == NULL) {
    updateparams_ = new ::CoreML::Specification::NetworkUpdateParameters;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetwork.updateParams)
  return updateparams_;
}
::CoreML::Specification::NetworkUpdateParameters* NeuralNetwork::release_updateparams() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetwork.updateParams)
  
  ::CoreML::Specification::NetworkUpdateParameters* temp = updateparams_;
  updateparams_ = NULL;
  return temp;
}
void NeuralNetwork::set_allocated_updateparams(::CoreML::Specification::NetworkUpdateParameters* updateparams) {
  delete updateparams_;
  updateparams_ = updateparams;
  if (updateparams) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetwork.updateParams)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int NeuralNetworkImageScaler::kChannelScaleFieldNumber;
const int NeuralNetworkImageScaler::kBlueBiasFieldNumber;
const int NeuralNetworkImageScaler::kGreenBiasFieldNumber;
const int NeuralNetworkImageScaler::kRedBiasFieldNumber;
const int NeuralNetworkImageScaler::kGrayBiasFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

NeuralNetworkImageScaler::NeuralNetworkImageScaler()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.NeuralNetworkImageScaler)
}
NeuralNetworkImageScaler::NeuralNetworkImageScaler(const NeuralNetworkImageScaler& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::memcpy(&graybias_, &from.graybias_,
    reinterpret_cast<char*>(&redbias_) -
    reinterpret_cast<char*>(&graybias_) + sizeof(redbias_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.NeuralNetworkImageScaler)
}

void NeuralNetworkImageScaler::SharedCtor() {
  ::memset(&graybias_, 0, reinterpret_cast<char*>(&redbias_) -
    reinterpret_cast<char*>(&graybias_) + sizeof(redbias_));
  _cached_size_ = 0;
}

NeuralNetworkImageScaler::~NeuralNetworkImageScaler() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.NeuralNetworkImageScaler)
  SharedDtor();
}

void NeuralNetworkImageScaler::SharedDtor() {
}

void NeuralNetworkImageScaler::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const NeuralNetworkImageScaler& NeuralNetworkImageScaler::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

NeuralNetworkImageScaler* NeuralNetworkImageScaler::New(::google::protobuf::Arena* arena) const {
  NeuralNetworkImageScaler* n = new NeuralNetworkImageScaler;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void NeuralNetworkImageScaler::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.NeuralNetworkImageScaler)
  ::memset(&graybias_, 0, reinterpret_cast<char*>(&redbias_) -
    reinterpret_cast<char*>(&graybias_) + sizeof(redbias_));
}

bool NeuralNetworkImageScaler::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.NeuralNetworkImageScaler)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(16383u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // float channelScale = 10;
      case 10: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(85u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &channelscale_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // float blueBias = 20;
      case 20: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(165u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &bluebias_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // float greenBias = 21;
      case 21: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(173u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &greenbias_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // float redBias = 22;
      case 22: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(181u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &redbias_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // float grayBias = 30;
      case 30: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(245u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &graybias_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.NeuralNetworkImageScaler)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.NeuralNetworkImageScaler)
  return false;
#undef DO_
}

void NeuralNetworkImageScaler::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.NeuralNetworkImageScaler)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // float channelScale = 10;
  if (this->channelscale() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(10, this->channelscale(), output);
  }

  // float blueBias = 20;
  if (this->bluebias() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(20, this->bluebias(), output);
  }

  // float greenBias = 21;
  if (this->greenbias() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(21, this->greenbias(), output);
  }

  // float redBias = 22;
  if (this->redbias() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(22, this->redbias(), output);
  }

  // float grayBias = 30;
  if (this->graybias() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(30, this->graybias(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.NeuralNetworkImageScaler)
}

size_t NeuralNetworkImageScaler::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.NeuralNetworkImageScaler)
  size_t total_size = 0;

  // float grayBias = 30;
  if (this->graybias() != 0) {
    total_size += 2 + 4;
  }

  // float channelScale = 10;
  if (this->channelscale() != 0) {
    total_size += 1 + 4;
  }

  // float blueBias = 20;
  if (this->bluebias() != 0) {
    total_size += 2 + 4;
  }

  // float greenBias = 21;
  if (this->greenbias() != 0) {
    total_size += 2 + 4;
  }

  // float redBias = 22;
  if (this->redbias() != 0) {
    total_size += 2 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void NeuralNetworkImageScaler::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const NeuralNetworkImageScaler*>(&from));
}

void NeuralNetworkImageScaler::MergeFrom(const NeuralNetworkImageScaler& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.NeuralNetworkImageScaler)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.graybias() != 0) {
    set_graybias(from.graybias());
  }
  if (from.channelscale() != 0) {
    set_channelscale(from.channelscale());
  }
  if (from.bluebias() != 0) {
    set_bluebias(from.bluebias());
  }
  if (from.greenbias() != 0) {
    set_greenbias(from.greenbias());
  }
  if (from.redbias() != 0) {
    set_redbias(from.redbias());
  }
}

void NeuralNetworkImageScaler::CopyFrom(const NeuralNetworkImageScaler& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.NeuralNetworkImageScaler)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool NeuralNetworkImageScaler::IsInitialized() const {
  return true;
}

void NeuralNetworkImageScaler::Swap(NeuralNetworkImageScaler* other) {
  if (other == this) return;
  InternalSwap(other);
}
void NeuralNetworkImageScaler::InternalSwap(NeuralNetworkImageScaler* other) {
  std::swap(graybias_, other->graybias_);
  std::swap(channelscale_, other->channelscale_);
  std::swap(bluebias_, other->bluebias_);
  std::swap(greenbias_, other->greenbias_);
  std::swap(redbias_, other->redbias_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string NeuralNetworkImageScaler::GetTypeName() const {
  return "CoreML.Specification.NeuralNetworkImageScaler";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// NeuralNetworkImageScaler

// float channelScale = 10;
void NeuralNetworkImageScaler::clear_channelscale() {
  channelscale_ = 0;
}
float NeuralNetworkImageScaler::channelscale() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkImageScaler.channelScale)
  return channelscale_;
}
void NeuralNetworkImageScaler::set_channelscale(float value) {
  
  channelscale_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkImageScaler.channelScale)
}

// float blueBias = 20;
void NeuralNetworkImageScaler::clear_bluebias() {
  bluebias_ = 0;
}
float NeuralNetworkImageScaler::bluebias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkImageScaler.blueBias)
  return bluebias_;
}
void NeuralNetworkImageScaler::set_bluebias(float value) {
  
  bluebias_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkImageScaler.blueBias)
}

// float greenBias = 21;
void NeuralNetworkImageScaler::clear_greenbias() {
  greenbias_ = 0;
}
float NeuralNetworkImageScaler::greenbias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkImageScaler.greenBias)
  return greenbias_;
}
void NeuralNetworkImageScaler::set_greenbias(float value) {
  
  greenbias_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkImageScaler.greenBias)
}

// float redBias = 22;
void NeuralNetworkImageScaler::clear_redbias() {
  redbias_ = 0;
}
float NeuralNetworkImageScaler::redbias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkImageScaler.redBias)
  return redbias_;
}
void NeuralNetworkImageScaler::set_redbias(float value) {
  
  redbias_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkImageScaler.redBias)
}

// float grayBias = 30;
void NeuralNetworkImageScaler::clear_graybias() {
  graybias_ = 0;
}
float NeuralNetworkImageScaler::graybias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkImageScaler.grayBias)
  return graybias_;
}
void NeuralNetworkImageScaler::set_graybias(float value) {
  
  graybias_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkImageScaler.grayBias)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int NeuralNetworkMeanImage::kMeanImageFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

NeuralNetworkMeanImage::NeuralNetworkMeanImage()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.NeuralNetworkMeanImage)
}
NeuralNetworkMeanImage::NeuralNetworkMeanImage(const NeuralNetworkMeanImage& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      meanimage_(from.meanimage_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.NeuralNetworkMeanImage)
}

void NeuralNetworkMeanImage::SharedCtor() {
  _cached_size_ = 0;
}

NeuralNetworkMeanImage::~NeuralNetworkMeanImage() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.NeuralNetworkMeanImage)
  SharedDtor();
}

void NeuralNetworkMeanImage::SharedDtor() {
}

void NeuralNetworkMeanImage::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const NeuralNetworkMeanImage& NeuralNetworkMeanImage::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

NeuralNetworkMeanImage* NeuralNetworkMeanImage::New(::google::protobuf::Arena* arena) const {
  NeuralNetworkMeanImage* n = new NeuralNetworkMeanImage;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void NeuralNetworkMeanImage::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.NeuralNetworkMeanImage)
  meanimage_.Clear();
}

bool NeuralNetworkMeanImage::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.NeuralNetworkMeanImage)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated float meanImage = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, this->mutable_meanimage())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(13u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 1, 10u, input, this->mutable_meanimage())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.NeuralNetworkMeanImage)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.NeuralNetworkMeanImage)
  return false;
#undef DO_
}

void NeuralNetworkMeanImage::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.NeuralNetworkMeanImage)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated float meanImage = 1;
  if (this->meanimage_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(1, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_meanimage_cached_byte_size_);
    ::google::protobuf::internal::WireFormatLite::WriteFloatArray(
      this->meanimage().data(), this->meanimage_size(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.NeuralNetworkMeanImage)
}

size_t NeuralNetworkMeanImage::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.NeuralNetworkMeanImage)
  size_t total_size = 0;

  // repeated float meanImage = 1;
  {
    unsigned int count = this->meanimage_size();
    size_t data_size = 4UL * count;
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _meanimage_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void NeuralNetworkMeanImage::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const NeuralNetworkMeanImage*>(&from));
}

void NeuralNetworkMeanImage::MergeFrom(const NeuralNetworkMeanImage& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.NeuralNetworkMeanImage)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  meanimage_.MergeFrom(from.meanimage_);
}

void NeuralNetworkMeanImage::CopyFrom(const NeuralNetworkMeanImage& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.NeuralNetworkMeanImage)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool NeuralNetworkMeanImage::IsInitialized() const {
  return true;
}

void NeuralNetworkMeanImage::Swap(NeuralNetworkMeanImage* other) {
  if (other == this) return;
  InternalSwap(other);
}
void NeuralNetworkMeanImage::InternalSwap(NeuralNetworkMeanImage* other) {
  meanimage_.InternalSwap(&other->meanimage_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string NeuralNetworkMeanImage::GetTypeName() const {
  return "CoreML.Specification.NeuralNetworkMeanImage";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// NeuralNetworkMeanImage

// repeated float meanImage = 1;
int NeuralNetworkMeanImage::meanimage_size() const {
  return meanimage_.size();
}
void NeuralNetworkMeanImage::clear_meanimage() {
  meanimage_.Clear();
}
float NeuralNetworkMeanImage::meanimage(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkMeanImage.meanImage)
  return meanimage_.Get(index);
}
void NeuralNetworkMeanImage::set_meanimage(int index, float value) {
  meanimage_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkMeanImage.meanImage)
}
void NeuralNetworkMeanImage::add_meanimage(float value) {
  meanimage_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.NeuralNetworkMeanImage.meanImage)
}
const ::google::protobuf::RepeatedField< float >&
NeuralNetworkMeanImage::meanimage() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.NeuralNetworkMeanImage.meanImage)
  return meanimage_;
}
::google::protobuf::RepeatedField< float >*
NeuralNetworkMeanImage::mutable_meanimage() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.NeuralNetworkMeanImage.meanImage)
  return &meanimage_;
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int NeuralNetworkPreprocessing::kFeatureNameFieldNumber;
const int NeuralNetworkPreprocessing::kScalerFieldNumber;
const int NeuralNetworkPreprocessing::kMeanImageFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

NeuralNetworkPreprocessing::NeuralNetworkPreprocessing()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.NeuralNetworkPreprocessing)
}
NeuralNetworkPreprocessing::NeuralNetworkPreprocessing(const NeuralNetworkPreprocessing& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  featurename_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (from.featurename().size() > 0) {
    featurename_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.featurename_);
  }
  clear_has_preprocessor();
  switch (from.preprocessor_case()) {
    case kScaler: {
      mutable_scaler()->::CoreML::Specification::NeuralNetworkImageScaler::MergeFrom(from.scaler());
      break;
    }
    case kMeanImage: {
      mutable_meanimage()->::CoreML::Specification::NeuralNetworkMeanImage::MergeFrom(from.meanimage());
      break;
    }
    case PREPROCESSOR_NOT_SET: {
      break;
    }
  }
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.NeuralNetworkPreprocessing)
}

void NeuralNetworkPreprocessing::SharedCtor() {
  featurename_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  clear_has_preprocessor();
  _cached_size_ = 0;
}

NeuralNetworkPreprocessing::~NeuralNetworkPreprocessing() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.NeuralNetworkPreprocessing)
  SharedDtor();
}

void NeuralNetworkPreprocessing::SharedDtor() {
  featurename_.DestroyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (has_preprocessor()) {
    clear_preprocessor();
  }
}

void NeuralNetworkPreprocessing::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const NeuralNetworkPreprocessing& NeuralNetworkPreprocessing::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

NeuralNetworkPreprocessing* NeuralNetworkPreprocessing::New(::google::protobuf::Arena* arena) const {
  NeuralNetworkPreprocessing* n = new NeuralNetworkPreprocessing;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void NeuralNetworkPreprocessing::clear_preprocessor() {
// @@protoc_insertion_point(one_of_clear_start:CoreML.Specification.NeuralNetworkPreprocessing)
  switch (preprocessor_case()) {
    case kScaler: {
      delete preprocessor_.scaler_;
      break;
    }
    case kMeanImage: {
      delete preprocessor_.meanimage_;
      break;
    }
    case PREPROCESSOR_NOT_SET: {
      break;
    }
  }
  _oneof_case_[0] = PREPROCESSOR_NOT_SET;
}


void NeuralNetworkPreprocessing::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.NeuralNetworkPreprocessing)
  featurename_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  clear_preprocessor();
}

bool NeuralNetworkPreprocessing::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.NeuralNetworkPreprocessing)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // string featureName = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadString(
                input, this->mutable_featurename()));
          DO_(::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
            this->featurename().data(), this->featurename().length(),
            ::google::protobuf::internal::WireFormatLite::PARSE,
            "CoreML.Specification.NeuralNetworkPreprocessing.featureName"));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.NeuralNetworkImageScaler scaler = 10;
      case 10: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(82u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_scaler()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.NeuralNetworkMeanImage meanImage = 11;
      case 11: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(90u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_meanimage()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.NeuralNetworkPreprocessing)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.NeuralNetworkPreprocessing)
  return false;
#undef DO_
}

void NeuralNetworkPreprocessing::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.NeuralNetworkPreprocessing)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // string featureName = 1;
  if (this->featurename().size() > 0) {
    ::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
      this->featurename().data(), this->featurename().length(),
      ::google::protobuf::internal::WireFormatLite::SERIALIZE,
      "CoreML.Specification.NeuralNetworkPreprocessing.featureName");
    ::google::protobuf::internal::WireFormatLite::WriteStringMaybeAliased(
      1, this->featurename(), output);
  }

  // .CoreML.Specification.NeuralNetworkImageScaler scaler = 10;
  if (has_scaler()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      10, *preprocessor_.scaler_, output);
  }

  // .CoreML.Specification.NeuralNetworkMeanImage meanImage = 11;
  if (has_meanimage()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      11, *preprocessor_.meanimage_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.NeuralNetworkPreprocessing)
}

size_t NeuralNetworkPreprocessing::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.NeuralNetworkPreprocessing)
  size_t total_size = 0;

  // string featureName = 1;
  if (this->featurename().size() > 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::StringSize(
        this->featurename());
  }

  switch (preprocessor_case()) {
    // .CoreML.Specification.NeuralNetworkImageScaler scaler = 10;
    case kScaler: {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *preprocessor_.scaler_);
      break;
    }
    // .CoreML.Specification.NeuralNetworkMeanImage meanImage = 11;
    case kMeanImage: {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *preprocessor_.meanimage_);
      break;
    }
    case PREPROCESSOR_NOT_SET: {
      break;
    }
  }
  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void NeuralNetworkPreprocessing::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const NeuralNetworkPreprocessing*>(&from));
}

void NeuralNetworkPreprocessing::MergeFrom(const NeuralNetworkPreprocessing& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.NeuralNetworkPreprocessing)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.featurename().size() > 0) {

    featurename_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.featurename_);
  }
  switch (from.preprocessor_case()) {
    case kScaler: {
      mutable_scaler()->::CoreML::Specification::NeuralNetworkImageScaler::MergeFrom(from.scaler());
      break;
    }
    case kMeanImage: {
      mutable_meanimage()->::CoreML::Specification::NeuralNetworkMeanImage::MergeFrom(from.meanimage());
      break;
    }
    case PREPROCESSOR_NOT_SET: {
      break;
    }
  }
}

void NeuralNetworkPreprocessing::CopyFrom(const NeuralNetworkPreprocessing& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.NeuralNetworkPreprocessing)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool NeuralNetworkPreprocessing::IsInitialized() const {
  return true;
}

void NeuralNetworkPreprocessing::Swap(NeuralNetworkPreprocessing* other) {
  if (other == this) return;
  InternalSwap(other);
}
void NeuralNetworkPreprocessing::InternalSwap(NeuralNetworkPreprocessing* other) {
  featurename_.Swap(&other->featurename_);
  std::swap(preprocessor_, other->preprocessor_);
  std::swap(_oneof_case_[0], other->_oneof_case_[0]);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string NeuralNetworkPreprocessing::GetTypeName() const {
  return "CoreML.Specification.NeuralNetworkPreprocessing";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// NeuralNetworkPreprocessing

// string featureName = 1;
void NeuralNetworkPreprocessing::clear_featurename() {
  featurename_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
const ::std::string& NeuralNetworkPreprocessing::featurename() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkPreprocessing.featureName)
  return featurename_.GetNoArena();
}
void NeuralNetworkPreprocessing::set_featurename(const ::std::string& value) {
  
  featurename_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkPreprocessing.featureName)
}
#if LANG_CXX11
void NeuralNetworkPreprocessing::set_featurename(::std::string&& value) {
  
  featurename_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:CoreML.Specification.NeuralNetworkPreprocessing.featureName)
}
#endif
void NeuralNetworkPreprocessing::set_featurename(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  
  featurename_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:CoreML.Specification.NeuralNetworkPreprocessing.featureName)
}
void NeuralNetworkPreprocessing::set_featurename(const char* value, size_t size) {
  
  featurename_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:CoreML.Specification.NeuralNetworkPreprocessing.featureName)
}
::std::string* NeuralNetworkPreprocessing::mutable_featurename() {
  
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkPreprocessing.featureName)
  return featurename_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
::std::string* NeuralNetworkPreprocessing::release_featurename() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkPreprocessing.featureName)
  
  return featurename_.ReleaseNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
void NeuralNetworkPreprocessing::set_allocated_featurename(::std::string* featurename) {
  if (featurename != NULL) {
    
  } else {
    
  }
  featurename_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), featurename);
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkPreprocessing.featureName)
}

// .CoreML.Specification.NeuralNetworkImageScaler scaler = 10;
bool NeuralNetworkPreprocessing::has_scaler() const {
  return preprocessor_case() == kScaler;
}
void NeuralNetworkPreprocessing::set_has_scaler() {
  _oneof_case_[0] = kScaler;
}
void NeuralNetworkPreprocessing::clear_scaler() {
  if (has_scaler()) {
    delete preprocessor_.scaler_;
    clear_has_preprocessor();
  }
}
 const ::CoreML::Specification::NeuralNetworkImageScaler& NeuralNetworkPreprocessing::scaler() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkPreprocessing.scaler)
  return has_scaler()
      ? *preprocessor_.scaler_
      : ::CoreML::Specification::NeuralNetworkImageScaler::default_instance();
}
::CoreML::Specification::NeuralNetworkImageScaler* NeuralNetworkPreprocessing::mutable_scaler() {
  if (!has_scaler()) {
    clear_preprocessor();
    set_has_scaler();
    preprocessor_.scaler_ = new ::CoreML::Specification::NeuralNetworkImageScaler;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkPreprocessing.scaler)
  return preprocessor_.scaler_;
}
::CoreML::Specification::NeuralNetworkImageScaler* NeuralNetworkPreprocessing::release_scaler() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkPreprocessing.scaler)
  if (has_scaler()) {
    clear_has_preprocessor();
    ::CoreML::Specification::NeuralNetworkImageScaler* temp = preprocessor_.scaler_;
    preprocessor_.scaler_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkPreprocessing::set_allocated_scaler(::CoreML::Specification::NeuralNetworkImageScaler* scaler) {
  clear_preprocessor();
  if (scaler) {
    set_has_scaler();
    preprocessor_.scaler_ = scaler;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkPreprocessing.scaler)
}

// .CoreML.Specification.NeuralNetworkMeanImage meanImage = 11;
bool NeuralNetworkPreprocessing::has_meanimage() const {
  return preprocessor_case() == kMeanImage;
}
void NeuralNetworkPreprocessing::set_has_meanimage() {
  _oneof_case_[0] = kMeanImage;
}
void NeuralNetworkPreprocessing::clear_meanimage() {
  if (has_meanimage()) {
    delete preprocessor_.meanimage_;
    clear_has_preprocessor();
  }
}
 const ::CoreML::Specification::NeuralNetworkMeanImage& NeuralNetworkPreprocessing::meanimage() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkPreprocessing.meanImage)
  return has_meanimage()
      ? *preprocessor_.meanimage_
      : ::CoreML::Specification::NeuralNetworkMeanImage::default_instance();
}
::CoreML::Specification::NeuralNetworkMeanImage* NeuralNetworkPreprocessing::mutable_meanimage() {
  if (!has_meanimage()) {
    clear_preprocessor();
    set_has_meanimage();
    preprocessor_.meanimage_ = new ::CoreML::Specification::NeuralNetworkMeanImage;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkPreprocessing.meanImage)
  return preprocessor_.meanimage_;
}
::CoreML::Specification::NeuralNetworkMeanImage* NeuralNetworkPreprocessing::release_meanimage() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkPreprocessing.meanImage)
  if (has_meanimage()) {
    clear_has_preprocessor();
    ::CoreML::Specification::NeuralNetworkMeanImage* temp = preprocessor_.meanimage_;
    preprocessor_.meanimage_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkPreprocessing::set_allocated_meanimage(::CoreML::Specification::NeuralNetworkMeanImage* meanimage) {
  clear_preprocessor();
  if (meanimage) {
    set_has_meanimage();
    preprocessor_.meanimage_ = meanimage;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkPreprocessing.meanImage)
}

bool NeuralNetworkPreprocessing::has_preprocessor() const {
  return preprocessor_case() != PREPROCESSOR_NOT_SET;
}
void NeuralNetworkPreprocessing::clear_has_preprocessor() {
  _oneof_case_[0] = PREPROCESSOR_NOT_SET;
}
NeuralNetworkPreprocessing::PreprocessorCase NeuralNetworkPreprocessing::preprocessor_case() const {
  return NeuralNetworkPreprocessing::PreprocessorCase(_oneof_case_[0]);
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ActivationReLU::ActivationReLU()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ActivationReLU)
}
ActivationReLU::ActivationReLU(const ActivationReLU& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ActivationReLU)
}

void ActivationReLU::SharedCtor() {
  _cached_size_ = 0;
}

ActivationReLU::~ActivationReLU() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ActivationReLU)
  SharedDtor();
}

void ActivationReLU::SharedDtor() {
}

void ActivationReLU::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ActivationReLU& ActivationReLU::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ActivationReLU* ActivationReLU::New(::google::protobuf::Arena* arena) const {
  ActivationReLU* n = new ActivationReLU;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ActivationReLU::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ActivationReLU)
}

bool ActivationReLU::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ActivationReLU)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ActivationReLU)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ActivationReLU)
  return false;
#undef DO_
}

void ActivationReLU::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ActivationReLU)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ActivationReLU)
}

size_t ActivationReLU::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ActivationReLU)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ActivationReLU::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ActivationReLU*>(&from));
}

void ActivationReLU::MergeFrom(const ActivationReLU& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ActivationReLU)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void ActivationReLU::CopyFrom(const ActivationReLU& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ActivationReLU)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ActivationReLU::IsInitialized() const {
  return true;
}

void ActivationReLU::Swap(ActivationReLU* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ActivationReLU::InternalSwap(ActivationReLU* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ActivationReLU::GetTypeName() const {
  return "CoreML.Specification.ActivationReLU";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ActivationReLU

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ActivationLeakyReLU::kAlphaFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ActivationLeakyReLU::ActivationLeakyReLU()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ActivationLeakyReLU)
}
ActivationLeakyReLU::ActivationLeakyReLU(const ActivationLeakyReLU& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  alpha_ = from.alpha_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ActivationLeakyReLU)
}

void ActivationLeakyReLU::SharedCtor() {
  alpha_ = 0;
  _cached_size_ = 0;
}

ActivationLeakyReLU::~ActivationLeakyReLU() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ActivationLeakyReLU)
  SharedDtor();
}

void ActivationLeakyReLU::SharedDtor() {
}

void ActivationLeakyReLU::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ActivationLeakyReLU& ActivationLeakyReLU::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ActivationLeakyReLU* ActivationLeakyReLU::New(::google::protobuf::Arena* arena) const {
  ActivationLeakyReLU* n = new ActivationLeakyReLU;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ActivationLeakyReLU::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ActivationLeakyReLU)
  alpha_ = 0;
}

bool ActivationLeakyReLU::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ActivationLeakyReLU)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // float alpha = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(13u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &alpha_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ActivationLeakyReLU)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ActivationLeakyReLU)
  return false;
#undef DO_
}

void ActivationLeakyReLU::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ActivationLeakyReLU)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // float alpha = 1;
  if (this->alpha() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(1, this->alpha(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ActivationLeakyReLU)
}

size_t ActivationLeakyReLU::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ActivationLeakyReLU)
  size_t total_size = 0;

  // float alpha = 1;
  if (this->alpha() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ActivationLeakyReLU::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ActivationLeakyReLU*>(&from));
}

void ActivationLeakyReLU::MergeFrom(const ActivationLeakyReLU& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ActivationLeakyReLU)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.alpha() != 0) {
    set_alpha(from.alpha());
  }
}

void ActivationLeakyReLU::CopyFrom(const ActivationLeakyReLU& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ActivationLeakyReLU)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ActivationLeakyReLU::IsInitialized() const {
  return true;
}

void ActivationLeakyReLU::Swap(ActivationLeakyReLU* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ActivationLeakyReLU::InternalSwap(ActivationLeakyReLU* other) {
  std::swap(alpha_, other->alpha_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ActivationLeakyReLU::GetTypeName() const {
  return "CoreML.Specification.ActivationLeakyReLU";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ActivationLeakyReLU

// float alpha = 1;
void ActivationLeakyReLU::clear_alpha() {
  alpha_ = 0;
}
float ActivationLeakyReLU::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationLeakyReLU.alpha)
  return alpha_;
}
void ActivationLeakyReLU::set_alpha(float value) {
  
  alpha_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ActivationLeakyReLU.alpha)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ActivationTanh::ActivationTanh()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ActivationTanh)
}
ActivationTanh::ActivationTanh(const ActivationTanh& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ActivationTanh)
}

void ActivationTanh::SharedCtor() {
  _cached_size_ = 0;
}

ActivationTanh::~ActivationTanh() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ActivationTanh)
  SharedDtor();
}

void ActivationTanh::SharedDtor() {
}

void ActivationTanh::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ActivationTanh& ActivationTanh::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ActivationTanh* ActivationTanh::New(::google::protobuf::Arena* arena) const {
  ActivationTanh* n = new ActivationTanh;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ActivationTanh::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ActivationTanh)
}

bool ActivationTanh::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ActivationTanh)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ActivationTanh)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ActivationTanh)
  return false;
#undef DO_
}

void ActivationTanh::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ActivationTanh)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ActivationTanh)
}

size_t ActivationTanh::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ActivationTanh)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ActivationTanh::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ActivationTanh*>(&from));
}

void ActivationTanh::MergeFrom(const ActivationTanh& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ActivationTanh)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void ActivationTanh::CopyFrom(const ActivationTanh& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ActivationTanh)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ActivationTanh::IsInitialized() const {
  return true;
}

void ActivationTanh::Swap(ActivationTanh* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ActivationTanh::InternalSwap(ActivationTanh* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ActivationTanh::GetTypeName() const {
  return "CoreML.Specification.ActivationTanh";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ActivationTanh

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ActivationScaledTanh::kAlphaFieldNumber;
const int ActivationScaledTanh::kBetaFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ActivationScaledTanh::ActivationScaledTanh()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ActivationScaledTanh)
}
ActivationScaledTanh::ActivationScaledTanh(const ActivationScaledTanh& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::memcpy(&alpha_, &from.alpha_,
    reinterpret_cast<char*>(&beta_) -
    reinterpret_cast<char*>(&alpha_) + sizeof(beta_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ActivationScaledTanh)
}

void ActivationScaledTanh::SharedCtor() {
  ::memset(&alpha_, 0, reinterpret_cast<char*>(&beta_) -
    reinterpret_cast<char*>(&alpha_) + sizeof(beta_));
  _cached_size_ = 0;
}

ActivationScaledTanh::~ActivationScaledTanh() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ActivationScaledTanh)
  SharedDtor();
}

void ActivationScaledTanh::SharedDtor() {
}

void ActivationScaledTanh::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ActivationScaledTanh& ActivationScaledTanh::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ActivationScaledTanh* ActivationScaledTanh::New(::google::protobuf::Arena* arena) const {
  ActivationScaledTanh* n = new ActivationScaledTanh;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ActivationScaledTanh::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ActivationScaledTanh)
  ::memset(&alpha_, 0, reinterpret_cast<char*>(&beta_) -
    reinterpret_cast<char*>(&alpha_) + sizeof(beta_));
}

bool ActivationScaledTanh::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ActivationScaledTanh)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // float alpha = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(13u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &alpha_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // float beta = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(21u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &beta_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ActivationScaledTanh)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ActivationScaledTanh)
  return false;
#undef DO_
}

void ActivationScaledTanh::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ActivationScaledTanh)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // float alpha = 1;
  if (this->alpha() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(1, this->alpha(), output);
  }

  // float beta = 2;
  if (this->beta() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(2, this->beta(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ActivationScaledTanh)
}

size_t ActivationScaledTanh::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ActivationScaledTanh)
  size_t total_size = 0;

  // float alpha = 1;
  if (this->alpha() != 0) {
    total_size += 1 + 4;
  }

  // float beta = 2;
  if (this->beta() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ActivationScaledTanh::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ActivationScaledTanh*>(&from));
}

void ActivationScaledTanh::MergeFrom(const ActivationScaledTanh& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ActivationScaledTanh)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.alpha() != 0) {
    set_alpha(from.alpha());
  }
  if (from.beta() != 0) {
    set_beta(from.beta());
  }
}

void ActivationScaledTanh::CopyFrom(const ActivationScaledTanh& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ActivationScaledTanh)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ActivationScaledTanh::IsInitialized() const {
  return true;
}

void ActivationScaledTanh::Swap(ActivationScaledTanh* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ActivationScaledTanh::InternalSwap(ActivationScaledTanh* other) {
  std::swap(alpha_, other->alpha_);
  std::swap(beta_, other->beta_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ActivationScaledTanh::GetTypeName() const {
  return "CoreML.Specification.ActivationScaledTanh";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ActivationScaledTanh

// float alpha = 1;
void ActivationScaledTanh::clear_alpha() {
  alpha_ = 0;
}
float ActivationScaledTanh::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationScaledTanh.alpha)
  return alpha_;
}
void ActivationScaledTanh::set_alpha(float value) {
  
  alpha_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ActivationScaledTanh.alpha)
}

// float beta = 2;
void ActivationScaledTanh::clear_beta() {
  beta_ = 0;
}
float ActivationScaledTanh::beta() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationScaledTanh.beta)
  return beta_;
}
void ActivationScaledTanh::set_beta(float value) {
  
  beta_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ActivationScaledTanh.beta)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ActivationSigmoid::ActivationSigmoid()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ActivationSigmoid)
}
ActivationSigmoid::ActivationSigmoid(const ActivationSigmoid& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ActivationSigmoid)
}

void ActivationSigmoid::SharedCtor() {
  _cached_size_ = 0;
}

ActivationSigmoid::~ActivationSigmoid() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ActivationSigmoid)
  SharedDtor();
}

void ActivationSigmoid::SharedDtor() {
}

void ActivationSigmoid::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ActivationSigmoid& ActivationSigmoid::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ActivationSigmoid* ActivationSigmoid::New(::google::protobuf::Arena* arena) const {
  ActivationSigmoid* n = new ActivationSigmoid;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ActivationSigmoid::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ActivationSigmoid)
}

bool ActivationSigmoid::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ActivationSigmoid)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ActivationSigmoid)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ActivationSigmoid)
  return false;
#undef DO_
}

void ActivationSigmoid::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ActivationSigmoid)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ActivationSigmoid)
}

size_t ActivationSigmoid::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ActivationSigmoid)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ActivationSigmoid::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ActivationSigmoid*>(&from));
}

void ActivationSigmoid::MergeFrom(const ActivationSigmoid& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ActivationSigmoid)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void ActivationSigmoid::CopyFrom(const ActivationSigmoid& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ActivationSigmoid)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ActivationSigmoid::IsInitialized() const {
  return true;
}

void ActivationSigmoid::Swap(ActivationSigmoid* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ActivationSigmoid::InternalSwap(ActivationSigmoid* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ActivationSigmoid::GetTypeName() const {
  return "CoreML.Specification.ActivationSigmoid";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ActivationSigmoid

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ActivationLinear::kAlphaFieldNumber;
const int ActivationLinear::kBetaFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ActivationLinear::ActivationLinear()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ActivationLinear)
}
ActivationLinear::ActivationLinear(const ActivationLinear& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::memcpy(&alpha_, &from.alpha_,
    reinterpret_cast<char*>(&beta_) -
    reinterpret_cast<char*>(&alpha_) + sizeof(beta_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ActivationLinear)
}

void ActivationLinear::SharedCtor() {
  ::memset(&alpha_, 0, reinterpret_cast<char*>(&beta_) -
    reinterpret_cast<char*>(&alpha_) + sizeof(beta_));
  _cached_size_ = 0;
}

ActivationLinear::~ActivationLinear() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ActivationLinear)
  SharedDtor();
}

void ActivationLinear::SharedDtor() {
}

void ActivationLinear::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ActivationLinear& ActivationLinear::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ActivationLinear* ActivationLinear::New(::google::protobuf::Arena* arena) const {
  ActivationLinear* n = new ActivationLinear;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ActivationLinear::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ActivationLinear)
  ::memset(&alpha_, 0, reinterpret_cast<char*>(&beta_) -
    reinterpret_cast<char*>(&alpha_) + sizeof(beta_));
}

bool ActivationLinear::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ActivationLinear)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // float alpha = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(13u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &alpha_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // float beta = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(21u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &beta_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ActivationLinear)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ActivationLinear)
  return false;
#undef DO_
}

void ActivationLinear::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ActivationLinear)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // float alpha = 1;
  if (this->alpha() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(1, this->alpha(), output);
  }

  // float beta = 2;
  if (this->beta() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(2, this->beta(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ActivationLinear)
}

size_t ActivationLinear::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ActivationLinear)
  size_t total_size = 0;

  // float alpha = 1;
  if (this->alpha() != 0) {
    total_size += 1 + 4;
  }

  // float beta = 2;
  if (this->beta() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ActivationLinear::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ActivationLinear*>(&from));
}

void ActivationLinear::MergeFrom(const ActivationLinear& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ActivationLinear)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.alpha() != 0) {
    set_alpha(from.alpha());
  }
  if (from.beta() != 0) {
    set_beta(from.beta());
  }
}

void ActivationLinear::CopyFrom(const ActivationLinear& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ActivationLinear)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ActivationLinear::IsInitialized() const {
  return true;
}

void ActivationLinear::Swap(ActivationLinear* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ActivationLinear::InternalSwap(ActivationLinear* other) {
  std::swap(alpha_, other->alpha_);
  std::swap(beta_, other->beta_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ActivationLinear::GetTypeName() const {
  return "CoreML.Specification.ActivationLinear";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ActivationLinear

// float alpha = 1;
void ActivationLinear::clear_alpha() {
  alpha_ = 0;
}
float ActivationLinear::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationLinear.alpha)
  return alpha_;
}
void ActivationLinear::set_alpha(float value) {
  
  alpha_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ActivationLinear.alpha)
}

// float beta = 2;
void ActivationLinear::clear_beta() {
  beta_ = 0;
}
float ActivationLinear::beta() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationLinear.beta)
  return beta_;
}
void ActivationLinear::set_beta(float value) {
  
  beta_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ActivationLinear.beta)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ActivationSigmoidHard::kAlphaFieldNumber;
const int ActivationSigmoidHard::kBetaFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ActivationSigmoidHard::ActivationSigmoidHard()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ActivationSigmoidHard)
}
ActivationSigmoidHard::ActivationSigmoidHard(const ActivationSigmoidHard& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::memcpy(&alpha_, &from.alpha_,
    reinterpret_cast<char*>(&beta_) -
    reinterpret_cast<char*>(&alpha_) + sizeof(beta_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ActivationSigmoidHard)
}

void ActivationSigmoidHard::SharedCtor() {
  ::memset(&alpha_, 0, reinterpret_cast<char*>(&beta_) -
    reinterpret_cast<char*>(&alpha_) + sizeof(beta_));
  _cached_size_ = 0;
}

ActivationSigmoidHard::~ActivationSigmoidHard() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ActivationSigmoidHard)
  SharedDtor();
}

void ActivationSigmoidHard::SharedDtor() {
}

void ActivationSigmoidHard::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ActivationSigmoidHard& ActivationSigmoidHard::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ActivationSigmoidHard* ActivationSigmoidHard::New(::google::protobuf::Arena* arena) const {
  ActivationSigmoidHard* n = new ActivationSigmoidHard;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ActivationSigmoidHard::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ActivationSigmoidHard)
  ::memset(&alpha_, 0, reinterpret_cast<char*>(&beta_) -
    reinterpret_cast<char*>(&alpha_) + sizeof(beta_));
}

bool ActivationSigmoidHard::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ActivationSigmoidHard)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // float alpha = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(13u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &alpha_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // float beta = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(21u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &beta_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ActivationSigmoidHard)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ActivationSigmoidHard)
  return false;
#undef DO_
}

void ActivationSigmoidHard::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ActivationSigmoidHard)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // float alpha = 1;
  if (this->alpha() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(1, this->alpha(), output);
  }

  // float beta = 2;
  if (this->beta() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(2, this->beta(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ActivationSigmoidHard)
}

size_t ActivationSigmoidHard::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ActivationSigmoidHard)
  size_t total_size = 0;

  // float alpha = 1;
  if (this->alpha() != 0) {
    total_size += 1 + 4;
  }

  // float beta = 2;
  if (this->beta() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ActivationSigmoidHard::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ActivationSigmoidHard*>(&from));
}

void ActivationSigmoidHard::MergeFrom(const ActivationSigmoidHard& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ActivationSigmoidHard)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.alpha() != 0) {
    set_alpha(from.alpha());
  }
  if (from.beta() != 0) {
    set_beta(from.beta());
  }
}

void ActivationSigmoidHard::CopyFrom(const ActivationSigmoidHard& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ActivationSigmoidHard)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ActivationSigmoidHard::IsInitialized() const {
  return true;
}

void ActivationSigmoidHard::Swap(ActivationSigmoidHard* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ActivationSigmoidHard::InternalSwap(ActivationSigmoidHard* other) {
  std::swap(alpha_, other->alpha_);
  std::swap(beta_, other->beta_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ActivationSigmoidHard::GetTypeName() const {
  return "CoreML.Specification.ActivationSigmoidHard";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ActivationSigmoidHard

// float alpha = 1;
void ActivationSigmoidHard::clear_alpha() {
  alpha_ = 0;
}
float ActivationSigmoidHard::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationSigmoidHard.alpha)
  return alpha_;
}
void ActivationSigmoidHard::set_alpha(float value) {
  
  alpha_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ActivationSigmoidHard.alpha)
}

// float beta = 2;
void ActivationSigmoidHard::clear_beta() {
  beta_ = 0;
}
float ActivationSigmoidHard::beta() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationSigmoidHard.beta)
  return beta_;
}
void ActivationSigmoidHard::set_beta(float value) {
  
  beta_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ActivationSigmoidHard.beta)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ActivationPReLU::kAlphaFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ActivationPReLU::ActivationPReLU()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ActivationPReLU)
}
ActivationPReLU::ActivationPReLU(const ActivationPReLU& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_alpha()) {
    alpha_ = new ::CoreML::Specification::WeightParams(*from.alpha_);
  } else {
    alpha_ = NULL;
  }
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ActivationPReLU)
}

void ActivationPReLU::SharedCtor() {
  alpha_ = NULL;
  _cached_size_ = 0;
}

ActivationPReLU::~ActivationPReLU() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ActivationPReLU)
  SharedDtor();
}

void ActivationPReLU::SharedDtor() {
  if (this != internal_default_instance()) {
    delete alpha_;
  }
}

void ActivationPReLU::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ActivationPReLU& ActivationPReLU::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ActivationPReLU* ActivationPReLU::New(::google::protobuf::Arena* arena) const {
  ActivationPReLU* n = new ActivationPReLU;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ActivationPReLU::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ActivationPReLU)
  if (GetArenaNoVirtual() == NULL && alpha_ != NULL) {
    delete alpha_;
  }
  alpha_ = NULL;
}

bool ActivationPReLU::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ActivationPReLU)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // .CoreML.Specification.WeightParams alpha = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_alpha()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ActivationPReLU)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ActivationPReLU)
  return false;
#undef DO_
}

void ActivationPReLU::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ActivationPReLU)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // .CoreML.Specification.WeightParams alpha = 1;
  if (this->has_alpha()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1, *this->alpha_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ActivationPReLU)
}

size_t ActivationPReLU::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ActivationPReLU)
  size_t total_size = 0;

  // .CoreML.Specification.WeightParams alpha = 1;
  if (this->has_alpha()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->alpha_);
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ActivationPReLU::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ActivationPReLU*>(&from));
}

void ActivationPReLU::MergeFrom(const ActivationPReLU& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ActivationPReLU)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.has_alpha()) {
    mutable_alpha()->::CoreML::Specification::WeightParams::MergeFrom(from.alpha());
  }
}

void ActivationPReLU::CopyFrom(const ActivationPReLU& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ActivationPReLU)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ActivationPReLU::IsInitialized() const {
  return true;
}

void ActivationPReLU::Swap(ActivationPReLU* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ActivationPReLU::InternalSwap(ActivationPReLU* other) {
  std::swap(alpha_, other->alpha_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ActivationPReLU::GetTypeName() const {
  return "CoreML.Specification.ActivationPReLU";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ActivationPReLU

// .CoreML.Specification.WeightParams alpha = 1;
bool ActivationPReLU::has_alpha() const {
  return this != internal_default_instance() && alpha_ != NULL;
}
void ActivationPReLU::clear_alpha() {
  if (GetArenaNoVirtual() == NULL && alpha_ != NULL) delete alpha_;
  alpha_ = NULL;
}
const ::CoreML::Specification::WeightParams& ActivationPReLU::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationPReLU.alpha)
  return alpha_ != NULL ? *alpha_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* ActivationPReLU::mutable_alpha() {
  
  if (alpha_ == NULL) {
    alpha_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationPReLU.alpha)
  return alpha_;
}
::CoreML::Specification::WeightParams* ActivationPReLU::release_alpha() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationPReLU.alpha)
  
  ::CoreML::Specification::WeightParams* temp = alpha_;
  alpha_ = NULL;
  return temp;
}
void ActivationPReLU::set_allocated_alpha(::CoreML::Specification::WeightParams* alpha) {
  delete alpha_;
  alpha_ = alpha;
  if (alpha) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationPReLU.alpha)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ActivationELU::kAlphaFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ActivationELU::ActivationELU()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ActivationELU)
}
ActivationELU::ActivationELU(const ActivationELU& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  alpha_ = from.alpha_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ActivationELU)
}

void ActivationELU::SharedCtor() {
  alpha_ = 0;
  _cached_size_ = 0;
}

ActivationELU::~ActivationELU() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ActivationELU)
  SharedDtor();
}

void ActivationELU::SharedDtor() {
}

void ActivationELU::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ActivationELU& ActivationELU::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ActivationELU* ActivationELU::New(::google::protobuf::Arena* arena) const {
  ActivationELU* n = new ActivationELU;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ActivationELU::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ActivationELU)
  alpha_ = 0;
}

bool ActivationELU::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ActivationELU)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // float alpha = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(13u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &alpha_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ActivationELU)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ActivationELU)
  return false;
#undef DO_
}

void ActivationELU::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ActivationELU)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // float alpha = 1;
  if (this->alpha() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(1, this->alpha(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ActivationELU)
}

size_t ActivationELU::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ActivationELU)
  size_t total_size = 0;

  // float alpha = 1;
  if (this->alpha() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ActivationELU::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ActivationELU*>(&from));
}

void ActivationELU::MergeFrom(const ActivationELU& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ActivationELU)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.alpha() != 0) {
    set_alpha(from.alpha());
  }
}

void ActivationELU::CopyFrom(const ActivationELU& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ActivationELU)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ActivationELU::IsInitialized() const {
  return true;
}

void ActivationELU::Swap(ActivationELU* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ActivationELU::InternalSwap(ActivationELU* other) {
  std::swap(alpha_, other->alpha_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ActivationELU::GetTypeName() const {
  return "CoreML.Specification.ActivationELU";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ActivationELU

// float alpha = 1;
void ActivationELU::clear_alpha() {
  alpha_ = 0;
}
float ActivationELU::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationELU.alpha)
  return alpha_;
}
void ActivationELU::set_alpha(float value) {
  
  alpha_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ActivationELU.alpha)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ActivationThresholdedReLU::kAlphaFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ActivationThresholdedReLU::ActivationThresholdedReLU()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ActivationThresholdedReLU)
}
ActivationThresholdedReLU::ActivationThresholdedReLU(const ActivationThresholdedReLU& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  alpha_ = from.alpha_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ActivationThresholdedReLU)
}

void ActivationThresholdedReLU::SharedCtor() {
  alpha_ = 0;
  _cached_size_ = 0;
}

ActivationThresholdedReLU::~ActivationThresholdedReLU() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ActivationThresholdedReLU)
  SharedDtor();
}

void ActivationThresholdedReLU::SharedDtor() {
}

void ActivationThresholdedReLU::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ActivationThresholdedReLU& ActivationThresholdedReLU::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ActivationThresholdedReLU* ActivationThresholdedReLU::New(::google::protobuf::Arena* arena) const {
  ActivationThresholdedReLU* n = new ActivationThresholdedReLU;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ActivationThresholdedReLU::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ActivationThresholdedReLU)
  alpha_ = 0;
}

bool ActivationThresholdedReLU::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ActivationThresholdedReLU)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // float alpha = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(13u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &alpha_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ActivationThresholdedReLU)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ActivationThresholdedReLU)
  return false;
#undef DO_
}

void ActivationThresholdedReLU::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ActivationThresholdedReLU)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // float alpha = 1;
  if (this->alpha() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(1, this->alpha(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ActivationThresholdedReLU)
}

size_t ActivationThresholdedReLU::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ActivationThresholdedReLU)
  size_t total_size = 0;

  // float alpha = 1;
  if (this->alpha() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ActivationThresholdedReLU::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ActivationThresholdedReLU*>(&from));
}

void ActivationThresholdedReLU::MergeFrom(const ActivationThresholdedReLU& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ActivationThresholdedReLU)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.alpha() != 0) {
    set_alpha(from.alpha());
  }
}

void ActivationThresholdedReLU::CopyFrom(const ActivationThresholdedReLU& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ActivationThresholdedReLU)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ActivationThresholdedReLU::IsInitialized() const {
  return true;
}

void ActivationThresholdedReLU::Swap(ActivationThresholdedReLU* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ActivationThresholdedReLU::InternalSwap(ActivationThresholdedReLU* other) {
  std::swap(alpha_, other->alpha_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ActivationThresholdedReLU::GetTypeName() const {
  return "CoreML.Specification.ActivationThresholdedReLU";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ActivationThresholdedReLU

// float alpha = 1;
void ActivationThresholdedReLU::clear_alpha() {
  alpha_ = 0;
}
float ActivationThresholdedReLU::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationThresholdedReLU.alpha)
  return alpha_;
}
void ActivationThresholdedReLU::set_alpha(float value) {
  
  alpha_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ActivationThresholdedReLU.alpha)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ActivationSoftsign::ActivationSoftsign()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ActivationSoftsign)
}
ActivationSoftsign::ActivationSoftsign(const ActivationSoftsign& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ActivationSoftsign)
}

void ActivationSoftsign::SharedCtor() {
  _cached_size_ = 0;
}

ActivationSoftsign::~ActivationSoftsign() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ActivationSoftsign)
  SharedDtor();
}

void ActivationSoftsign::SharedDtor() {
}

void ActivationSoftsign::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ActivationSoftsign& ActivationSoftsign::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ActivationSoftsign* ActivationSoftsign::New(::google::protobuf::Arena* arena) const {
  ActivationSoftsign* n = new ActivationSoftsign;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ActivationSoftsign::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ActivationSoftsign)
}

bool ActivationSoftsign::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ActivationSoftsign)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ActivationSoftsign)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ActivationSoftsign)
  return false;
#undef DO_
}

void ActivationSoftsign::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ActivationSoftsign)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ActivationSoftsign)
}

size_t ActivationSoftsign::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ActivationSoftsign)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ActivationSoftsign::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ActivationSoftsign*>(&from));
}

void ActivationSoftsign::MergeFrom(const ActivationSoftsign& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ActivationSoftsign)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void ActivationSoftsign::CopyFrom(const ActivationSoftsign& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ActivationSoftsign)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ActivationSoftsign::IsInitialized() const {
  return true;
}

void ActivationSoftsign::Swap(ActivationSoftsign* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ActivationSoftsign::InternalSwap(ActivationSoftsign* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ActivationSoftsign::GetTypeName() const {
  return "CoreML.Specification.ActivationSoftsign";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ActivationSoftsign

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ActivationSoftplus::ActivationSoftplus()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ActivationSoftplus)
}
ActivationSoftplus::ActivationSoftplus(const ActivationSoftplus& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ActivationSoftplus)
}

void ActivationSoftplus::SharedCtor() {
  _cached_size_ = 0;
}

ActivationSoftplus::~ActivationSoftplus() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ActivationSoftplus)
  SharedDtor();
}

void ActivationSoftplus::SharedDtor() {
}

void ActivationSoftplus::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ActivationSoftplus& ActivationSoftplus::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ActivationSoftplus* ActivationSoftplus::New(::google::protobuf::Arena* arena) const {
  ActivationSoftplus* n = new ActivationSoftplus;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ActivationSoftplus::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ActivationSoftplus)
}

bool ActivationSoftplus::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ActivationSoftplus)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ActivationSoftplus)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ActivationSoftplus)
  return false;
#undef DO_
}

void ActivationSoftplus::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ActivationSoftplus)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ActivationSoftplus)
}

size_t ActivationSoftplus::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ActivationSoftplus)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ActivationSoftplus::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ActivationSoftplus*>(&from));
}

void ActivationSoftplus::MergeFrom(const ActivationSoftplus& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ActivationSoftplus)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void ActivationSoftplus::CopyFrom(const ActivationSoftplus& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ActivationSoftplus)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ActivationSoftplus::IsInitialized() const {
  return true;
}

void ActivationSoftplus::Swap(ActivationSoftplus* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ActivationSoftplus::InternalSwap(ActivationSoftplus* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ActivationSoftplus::GetTypeName() const {
  return "CoreML.Specification.ActivationSoftplus";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ActivationSoftplus

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ActivationParametricSoftplus::kAlphaFieldNumber;
const int ActivationParametricSoftplus::kBetaFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ActivationParametricSoftplus::ActivationParametricSoftplus()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ActivationParametricSoftplus)
}
ActivationParametricSoftplus::ActivationParametricSoftplus(const ActivationParametricSoftplus& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_alpha()) {
    alpha_ = new ::CoreML::Specification::WeightParams(*from.alpha_);
  } else {
    alpha_ = NULL;
  }
  if (from.has_beta()) {
    beta_ = new ::CoreML::Specification::WeightParams(*from.beta_);
  } else {
    beta_ = NULL;
  }
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ActivationParametricSoftplus)
}

void ActivationParametricSoftplus::SharedCtor() {
  ::memset(&alpha_, 0, reinterpret_cast<char*>(&beta_) -
    reinterpret_cast<char*>(&alpha_) + sizeof(beta_));
  _cached_size_ = 0;
}

ActivationParametricSoftplus::~ActivationParametricSoftplus() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ActivationParametricSoftplus)
  SharedDtor();
}

void ActivationParametricSoftplus::SharedDtor() {
  if (this != internal_default_instance()) {
    delete alpha_;
  }
  if (this != internal_default_instance()) {
    delete beta_;
  }
}

void ActivationParametricSoftplus::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ActivationParametricSoftplus& ActivationParametricSoftplus::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ActivationParametricSoftplus* ActivationParametricSoftplus::New(::google::protobuf::Arena* arena) const {
  ActivationParametricSoftplus* n = new ActivationParametricSoftplus;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ActivationParametricSoftplus::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ActivationParametricSoftplus)
  if (GetArenaNoVirtual() == NULL && alpha_ != NULL) {
    delete alpha_;
  }
  alpha_ = NULL;
  if (GetArenaNoVirtual() == NULL && beta_ != NULL) {
    delete beta_;
  }
  beta_ = NULL;
}

bool ActivationParametricSoftplus::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ActivationParametricSoftplus)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // .CoreML.Specification.WeightParams alpha = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_alpha()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams beta = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(18u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_beta()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ActivationParametricSoftplus)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ActivationParametricSoftplus)
  return false;
#undef DO_
}

void ActivationParametricSoftplus::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ActivationParametricSoftplus)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // .CoreML.Specification.WeightParams alpha = 1;
  if (this->has_alpha()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1, *this->alpha_, output);
  }

  // .CoreML.Specification.WeightParams beta = 2;
  if (this->has_beta()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      2, *this->beta_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ActivationParametricSoftplus)
}

size_t ActivationParametricSoftplus::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ActivationParametricSoftplus)
  size_t total_size = 0;

  // .CoreML.Specification.WeightParams alpha = 1;
  if (this->has_alpha()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->alpha_);
  }

  // .CoreML.Specification.WeightParams beta = 2;
  if (this->has_beta()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->beta_);
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ActivationParametricSoftplus::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ActivationParametricSoftplus*>(&from));
}

void ActivationParametricSoftplus::MergeFrom(const ActivationParametricSoftplus& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ActivationParametricSoftplus)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.has_alpha()) {
    mutable_alpha()->::CoreML::Specification::WeightParams::MergeFrom(from.alpha());
  }
  if (from.has_beta()) {
    mutable_beta()->::CoreML::Specification::WeightParams::MergeFrom(from.beta());
  }
}

void ActivationParametricSoftplus::CopyFrom(const ActivationParametricSoftplus& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ActivationParametricSoftplus)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ActivationParametricSoftplus::IsInitialized() const {
  return true;
}

void ActivationParametricSoftplus::Swap(ActivationParametricSoftplus* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ActivationParametricSoftplus::InternalSwap(ActivationParametricSoftplus* other) {
  std::swap(alpha_, other->alpha_);
  std::swap(beta_, other->beta_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ActivationParametricSoftplus::GetTypeName() const {
  return "CoreML.Specification.ActivationParametricSoftplus";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ActivationParametricSoftplus

// .CoreML.Specification.WeightParams alpha = 1;
bool ActivationParametricSoftplus::has_alpha() const {
  return this != internal_default_instance() && alpha_ != NULL;
}
void ActivationParametricSoftplus::clear_alpha() {
  if (GetArenaNoVirtual() == NULL && alpha_ != NULL) delete alpha_;
  alpha_ = NULL;
}
const ::CoreML::Specification::WeightParams& ActivationParametricSoftplus::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParametricSoftplus.alpha)
  return alpha_ != NULL ? *alpha_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* ActivationParametricSoftplus::mutable_alpha() {
  
  if (alpha_ == NULL) {
    alpha_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParametricSoftplus.alpha)
  return alpha_;
}
::CoreML::Specification::WeightParams* ActivationParametricSoftplus::release_alpha() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParametricSoftplus.alpha)
  
  ::CoreML::Specification::WeightParams* temp = alpha_;
  alpha_ = NULL;
  return temp;
}
void ActivationParametricSoftplus::set_allocated_alpha(::CoreML::Specification::WeightParams* alpha) {
  delete alpha_;
  alpha_ = alpha;
  if (alpha) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParametricSoftplus.alpha)
}

// .CoreML.Specification.WeightParams beta = 2;
bool ActivationParametricSoftplus::has_beta() const {
  return this != internal_default_instance() && beta_ != NULL;
}
void ActivationParametricSoftplus::clear_beta() {
  if (GetArenaNoVirtual() == NULL && beta_ != NULL) delete beta_;
  beta_ = NULL;
}
const ::CoreML::Specification::WeightParams& ActivationParametricSoftplus::beta() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParametricSoftplus.beta)
  return beta_ != NULL ? *beta_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* ActivationParametricSoftplus::mutable_beta() {
  
  if (beta_ == NULL) {
    beta_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParametricSoftplus.beta)
  return beta_;
}
::CoreML::Specification::WeightParams* ActivationParametricSoftplus::release_beta() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParametricSoftplus.beta)
  
  ::CoreML::Specification::WeightParams* temp = beta_;
  beta_ = NULL;
  return temp;
}
void ActivationParametricSoftplus::set_allocated_beta(::CoreML::Specification::WeightParams* beta) {
  delete beta_;
  beta_ = beta;
  if (beta) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParametricSoftplus.beta)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ActivationParams::kLinearFieldNumber;
const int ActivationParams::kReLUFieldNumber;
const int ActivationParams::kLeakyReLUFieldNumber;
const int ActivationParams::kThresholdedReLUFieldNumber;
const int ActivationParams::kPReLUFieldNumber;
const int ActivationParams::kTanhFieldNumber;
const int ActivationParams::kScaledTanhFieldNumber;
const int ActivationParams::kSigmoidFieldNumber;
const int ActivationParams::kSigmoidHardFieldNumber;
const int ActivationParams::kELUFieldNumber;
const int ActivationParams::kSoftsignFieldNumber;
const int ActivationParams::kSoftplusFieldNumber;
const int ActivationParams::kParametricSoftplusFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ActivationParams::ActivationParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ActivationParams)
}
ActivationParams::ActivationParams(const ActivationParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  clear_has_NonlinearityType();
  switch (from.NonlinearityType_case()) {
    case kLinear: {
      mutable_linear()->::CoreML::Specification::ActivationLinear::MergeFrom(from.linear());
      break;
    }
    case kReLU: {
      mutable_relu()->::CoreML::Specification::ActivationReLU::MergeFrom(from.relu());
      break;
    }
    case kLeakyReLU: {
      mutable_leakyrelu()->::CoreML::Specification::ActivationLeakyReLU::MergeFrom(from.leakyrelu());
      break;
    }
    case kThresholdedReLU: {
      mutable_thresholdedrelu()->::CoreML::Specification::ActivationThresholdedReLU::MergeFrom(from.thresholdedrelu());
      break;
    }
    case kPReLU: {
      mutable_prelu()->::CoreML::Specification::ActivationPReLU::MergeFrom(from.prelu());
      break;
    }
    case kTanh: {
      mutable_tanh()->::CoreML::Specification::ActivationTanh::MergeFrom(from.tanh());
      break;
    }
    case kScaledTanh: {
      mutable_scaledtanh()->::CoreML::Specification::ActivationScaledTanh::MergeFrom(from.scaledtanh());
      break;
    }
    case kSigmoid: {
      mutable_sigmoid()->::CoreML::Specification::ActivationSigmoid::MergeFrom(from.sigmoid());
      break;
    }
    case kSigmoidHard: {
      mutable_sigmoidhard()->::CoreML::Specification::ActivationSigmoidHard::MergeFrom(from.sigmoidhard());
      break;
    }
    case kELU: {
      mutable_elu()->::CoreML::Specification::ActivationELU::MergeFrom(from.elu());
      break;
    }
    case kSoftsign: {
      mutable_softsign()->::CoreML::Specification::ActivationSoftsign::MergeFrom(from.softsign());
      break;
    }
    case kSoftplus: {
      mutable_softplus()->::CoreML::Specification::ActivationSoftplus::MergeFrom(from.softplus());
      break;
    }
    case kParametricSoftplus: {
      mutable_parametricsoftplus()->::CoreML::Specification::ActivationParametricSoftplus::MergeFrom(from.parametricsoftplus());
      break;
    }
    case NONLINEARITYTYPE_NOT_SET: {
      break;
    }
  }
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ActivationParams)
}

void ActivationParams::SharedCtor() {
  clear_has_NonlinearityType();
  _cached_size_ = 0;
}

ActivationParams::~ActivationParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ActivationParams)
  SharedDtor();
}

void ActivationParams::SharedDtor() {
  if (has_NonlinearityType()) {
    clear_NonlinearityType();
  }
}

void ActivationParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ActivationParams& ActivationParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ActivationParams* ActivationParams::New(::google::protobuf::Arena* arena) const {
  ActivationParams* n = new ActivationParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ActivationParams::clear_NonlinearityType() {
// @@protoc_insertion_point(one_of_clear_start:CoreML.Specification.ActivationParams)
  switch (NonlinearityType_case()) {
    case kLinear: {
      delete NonlinearityType_.linear_;
      break;
    }
    case kReLU: {
      delete NonlinearityType_.relu_;
      break;
    }
    case kLeakyReLU: {
      delete NonlinearityType_.leakyrelu_;
      break;
    }
    case kThresholdedReLU: {
      delete NonlinearityType_.thresholdedrelu_;
      break;
    }
    case kPReLU: {
      delete NonlinearityType_.prelu_;
      break;
    }
    case kTanh: {
      delete NonlinearityType_.tanh_;
      break;
    }
    case kScaledTanh: {
      delete NonlinearityType_.scaledtanh_;
      break;
    }
    case kSigmoid: {
      delete NonlinearityType_.sigmoid_;
      break;
    }
    case kSigmoidHard: {
      delete NonlinearityType_.sigmoidhard_;
      break;
    }
    case kELU: {
      delete NonlinearityType_.elu_;
      break;
    }
    case kSoftsign: {
      delete NonlinearityType_.softsign_;
      break;
    }
    case kSoftplus: {
      delete NonlinearityType_.softplus_;
      break;
    }
    case kParametricSoftplus: {
      delete NonlinearityType_.parametricsoftplus_;
      break;
    }
    case NONLINEARITYTYPE_NOT_SET: {
      break;
    }
  }
  _oneof_case_[0] = NONLINEARITYTYPE_NOT_SET;
}


void ActivationParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ActivationParams)
  clear_NonlinearityType();
}

bool ActivationParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ActivationParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(16383u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // .CoreML.Specification.ActivationLinear linear = 5;
      case 5: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(42u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_linear()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ActivationReLU ReLU = 10;
      case 10: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(82u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_relu()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ActivationLeakyReLU leakyReLU = 15;
      case 15: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(122u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_leakyrelu()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ActivationThresholdedReLU thresholdedReLU = 20;
      case 20: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(162u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_thresholdedrelu()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ActivationPReLU PReLU = 25;
      case 25: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(202u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_prelu()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ActivationTanh tanh = 30;
      case 30: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(242u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_tanh()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ActivationScaledTanh scaledTanh = 31;
      case 31: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(250u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_scaledtanh()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ActivationSigmoid sigmoid = 40;
      case 40: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(322u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_sigmoid()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ActivationSigmoidHard sigmoidHard = 41;
      case 41: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(330u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_sigmoidhard()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ActivationELU ELU = 50;
      case 50: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(402u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_elu()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ActivationSoftsign softsign = 60;
      case 60: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(482u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_softsign()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ActivationSoftplus softplus = 70;
      case 70: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(562u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_softplus()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ActivationParametricSoftplus parametricSoftplus = 71;
      case 71: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(570u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_parametricsoftplus()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ActivationParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ActivationParams)
  return false;
#undef DO_
}

void ActivationParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ActivationParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // .CoreML.Specification.ActivationLinear linear = 5;
  if (has_linear()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      5, *NonlinearityType_.linear_, output);
  }

  // .CoreML.Specification.ActivationReLU ReLU = 10;
  if (has_relu()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      10, *NonlinearityType_.relu_, output);
  }

  // .CoreML.Specification.ActivationLeakyReLU leakyReLU = 15;
  if (has_leakyrelu()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      15, *NonlinearityType_.leakyrelu_, output);
  }

  // .CoreML.Specification.ActivationThresholdedReLU thresholdedReLU = 20;
  if (has_thresholdedrelu()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      20, *NonlinearityType_.thresholdedrelu_, output);
  }

  // .CoreML.Specification.ActivationPReLU PReLU = 25;
  if (has_prelu()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      25, *NonlinearityType_.prelu_, output);
  }

  // .CoreML.Specification.ActivationTanh tanh = 30;
  if (has_tanh()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      30, *NonlinearityType_.tanh_, output);
  }

  // .CoreML.Specification.ActivationScaledTanh scaledTanh = 31;
  if (has_scaledtanh()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      31, *NonlinearityType_.scaledtanh_, output);
  }

  // .CoreML.Specification.ActivationSigmoid sigmoid = 40;
  if (has_sigmoid()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      40, *NonlinearityType_.sigmoid_, output);
  }

  // .CoreML.Specification.ActivationSigmoidHard sigmoidHard = 41;
  if (has_sigmoidhard()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      41, *NonlinearityType_.sigmoidhard_, output);
  }

  // .CoreML.Specification.ActivationELU ELU = 50;
  if (has_elu()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      50, *NonlinearityType_.elu_, output);
  }

  // .CoreML.Specification.ActivationSoftsign softsign = 60;
  if (has_softsign()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      60, *NonlinearityType_.softsign_, output);
  }

  // .CoreML.Specification.ActivationSoftplus softplus = 70;
  if (has_softplus()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      70, *NonlinearityType_.softplus_, output);
  }

  // .CoreML.Specification.ActivationParametricSoftplus parametricSoftplus = 71;
  if (has_parametricsoftplus()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      71, *NonlinearityType_.parametricsoftplus_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ActivationParams)
}

size_t ActivationParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ActivationParams)
  size_t total_size = 0;

  switch (NonlinearityType_case()) {
    // .CoreML.Specification.ActivationLinear linear = 5;
    case kLinear: {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *NonlinearityType_.linear_);
      break;
    }
    // .CoreML.Specification.ActivationReLU ReLU = 10;
    case kReLU: {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *NonlinearityType_.relu_);
      break;
    }
    // .CoreML.Specification.ActivationLeakyReLU leakyReLU = 15;
    case kLeakyReLU: {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *NonlinearityType_.leakyrelu_);
      break;
    }
    // .CoreML.Specification.ActivationThresholdedReLU thresholdedReLU = 20;
    case kThresholdedReLU: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *NonlinearityType_.thresholdedrelu_);
      break;
    }
    // .CoreML.Specification.ActivationPReLU PReLU = 25;
    case kPReLU: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *NonlinearityType_.prelu_);
      break;
    }
    // .CoreML.Specification.ActivationTanh tanh = 30;
    case kTanh: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *NonlinearityType_.tanh_);
      break;
    }
    // .CoreML.Specification.ActivationScaledTanh scaledTanh = 31;
    case kScaledTanh: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *NonlinearityType_.scaledtanh_);
      break;
    }
    // .CoreML.Specification.ActivationSigmoid sigmoid = 40;
    case kSigmoid: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *NonlinearityType_.sigmoid_);
      break;
    }
    // .CoreML.Specification.ActivationSigmoidHard sigmoidHard = 41;
    case kSigmoidHard: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *NonlinearityType_.sigmoidhard_);
      break;
    }
    // .CoreML.Specification.ActivationELU ELU = 50;
    case kELU: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *NonlinearityType_.elu_);
      break;
    }
    // .CoreML.Specification.ActivationSoftsign softsign = 60;
    case kSoftsign: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *NonlinearityType_.softsign_);
      break;
    }
    // .CoreML.Specification.ActivationSoftplus softplus = 70;
    case kSoftplus: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *NonlinearityType_.softplus_);
      break;
    }
    // .CoreML.Specification.ActivationParametricSoftplus parametricSoftplus = 71;
    case kParametricSoftplus: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *NonlinearityType_.parametricsoftplus_);
      break;
    }
    case NONLINEARITYTYPE_NOT_SET: {
      break;
    }
  }
  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ActivationParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ActivationParams*>(&from));
}

void ActivationParams::MergeFrom(const ActivationParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ActivationParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  switch (from.NonlinearityType_case()) {
    case kLinear: {
      mutable_linear()->::CoreML::Specification::ActivationLinear::MergeFrom(from.linear());
      break;
    }
    case kReLU: {
      mutable_relu()->::CoreML::Specification::ActivationReLU::MergeFrom(from.relu());
      break;
    }
    case kLeakyReLU: {
      mutable_leakyrelu()->::CoreML::Specification::ActivationLeakyReLU::MergeFrom(from.leakyrelu());
      break;
    }
    case kThresholdedReLU: {
      mutable_thresholdedrelu()->::CoreML::Specification::ActivationThresholdedReLU::MergeFrom(from.thresholdedrelu());
      break;
    }
    case kPReLU: {
      mutable_prelu()->::CoreML::Specification::ActivationPReLU::MergeFrom(from.prelu());
      break;
    }
    case kTanh: {
      mutable_tanh()->::CoreML::Specification::ActivationTanh::MergeFrom(from.tanh());
      break;
    }
    case kScaledTanh: {
      mutable_scaledtanh()->::CoreML::Specification::ActivationScaledTanh::MergeFrom(from.scaledtanh());
      break;
    }
    case kSigmoid: {
      mutable_sigmoid()->::CoreML::Specification::ActivationSigmoid::MergeFrom(from.sigmoid());
      break;
    }
    case kSigmoidHard: {
      mutable_sigmoidhard()->::CoreML::Specification::ActivationSigmoidHard::MergeFrom(from.sigmoidhard());
      break;
    }
    case kELU: {
      mutable_elu()->::CoreML::Specification::ActivationELU::MergeFrom(from.elu());
      break;
    }
    case kSoftsign: {
      mutable_softsign()->::CoreML::Specification::ActivationSoftsign::MergeFrom(from.softsign());
      break;
    }
    case kSoftplus: {
      mutable_softplus()->::CoreML::Specification::ActivationSoftplus::MergeFrom(from.softplus());
      break;
    }
    case kParametricSoftplus: {
      mutable_parametricsoftplus()->::CoreML::Specification::ActivationParametricSoftplus::MergeFrom(from.parametricsoftplus());
      break;
    }
    case NONLINEARITYTYPE_NOT_SET: {
      break;
    }
  }
}

void ActivationParams::CopyFrom(const ActivationParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ActivationParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ActivationParams::IsInitialized() const {
  return true;
}

void ActivationParams::Swap(ActivationParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ActivationParams::InternalSwap(ActivationParams* other) {
  std::swap(NonlinearityType_, other->NonlinearityType_);
  std::swap(_oneof_case_[0], other->_oneof_case_[0]);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ActivationParams::GetTypeName() const {
  return "CoreML.Specification.ActivationParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ActivationParams

// .CoreML.Specification.ActivationLinear linear = 5;
bool ActivationParams::has_linear() const {
  return NonlinearityType_case() == kLinear;
}
void ActivationParams::set_has_linear() {
  _oneof_case_[0] = kLinear;
}
void ActivationParams::clear_linear() {
  if (has_linear()) {
    delete NonlinearityType_.linear_;
    clear_has_NonlinearityType();
  }
}
 const ::CoreML::Specification::ActivationLinear& ActivationParams::linear() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.linear)
  return has_linear()
      ? *NonlinearityType_.linear_
      : ::CoreML::Specification::ActivationLinear::default_instance();
}
::CoreML::Specification::ActivationLinear* ActivationParams::mutable_linear() {
  if (!has_linear()) {
    clear_NonlinearityType();
    set_has_linear();
    NonlinearityType_.linear_ = new ::CoreML::Specification::ActivationLinear;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.linear)
  return NonlinearityType_.linear_;
}
::CoreML::Specification::ActivationLinear* ActivationParams::release_linear() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.linear)
  if (has_linear()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationLinear* temp = NonlinearityType_.linear_;
    NonlinearityType_.linear_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void ActivationParams::set_allocated_linear(::CoreML::Specification::ActivationLinear* linear) {
  clear_NonlinearityType();
  if (linear) {
    set_has_linear();
    NonlinearityType_.linear_ = linear;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParams.linear)
}

// .CoreML.Specification.ActivationReLU ReLU = 10;
bool ActivationParams::has_relu() const {
  return NonlinearityType_case() == kReLU;
}
void ActivationParams::set_has_relu() {
  _oneof_case_[0] = kReLU;
}
void ActivationParams::clear_relu() {
  if (has_relu()) {
    delete NonlinearityType_.relu_;
    clear_has_NonlinearityType();
  }
}
 const ::CoreML::Specification::ActivationReLU& ActivationParams::relu() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.ReLU)
  return has_relu()
      ? *NonlinearityType_.relu_
      : ::CoreML::Specification::ActivationReLU::default_instance();
}
::CoreML::Specification::ActivationReLU* ActivationParams::mutable_relu() {
  if (!has_relu()) {
    clear_NonlinearityType();
    set_has_relu();
    NonlinearityType_.relu_ = new ::CoreML::Specification::ActivationReLU;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.ReLU)
  return NonlinearityType_.relu_;
}
::CoreML::Specification::ActivationReLU* ActivationParams::release_relu() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.ReLU)
  if (has_relu()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationReLU* temp = NonlinearityType_.relu_;
    NonlinearityType_.relu_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void ActivationParams::set_allocated_relu(::CoreML::Specification::ActivationReLU* relu) {
  clear_NonlinearityType();
  if (relu) {
    set_has_relu();
    NonlinearityType_.relu_ = relu;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParams.ReLU)
}

// .CoreML.Specification.ActivationLeakyReLU leakyReLU = 15;
bool ActivationParams::has_leakyrelu() const {
  return NonlinearityType_case() == kLeakyReLU;
}
void ActivationParams::set_has_leakyrelu() {
  _oneof_case_[0] = kLeakyReLU;
}
void ActivationParams::clear_leakyrelu() {
  if (has_leakyrelu()) {
    delete NonlinearityType_.leakyrelu_;
    clear_has_NonlinearityType();
  }
}
 const ::CoreML::Specification::ActivationLeakyReLU& ActivationParams::leakyrelu() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.leakyReLU)
  return has_leakyrelu()
      ? *NonlinearityType_.leakyrelu_
      : ::CoreML::Specification::ActivationLeakyReLU::default_instance();
}
::CoreML::Specification::ActivationLeakyReLU* ActivationParams::mutable_leakyrelu() {
  if (!has_leakyrelu()) {
    clear_NonlinearityType();
    set_has_leakyrelu();
    NonlinearityType_.leakyrelu_ = new ::CoreML::Specification::ActivationLeakyReLU;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.leakyReLU)
  return NonlinearityType_.leakyrelu_;
}
::CoreML::Specification::ActivationLeakyReLU* ActivationParams::release_leakyrelu() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.leakyReLU)
  if (has_leakyrelu()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationLeakyReLU* temp = NonlinearityType_.leakyrelu_;
    NonlinearityType_.leakyrelu_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void ActivationParams::set_allocated_leakyrelu(::CoreML::Specification::ActivationLeakyReLU* leakyrelu) {
  clear_NonlinearityType();
  if (leakyrelu) {
    set_has_leakyrelu();
    NonlinearityType_.leakyrelu_ = leakyrelu;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParams.leakyReLU)
}

// .CoreML.Specification.ActivationThresholdedReLU thresholdedReLU = 20;
bool ActivationParams::has_thresholdedrelu() const {
  return NonlinearityType_case() == kThresholdedReLU;
}
void ActivationParams::set_has_thresholdedrelu() {
  _oneof_case_[0] = kThresholdedReLU;
}
void ActivationParams::clear_thresholdedrelu() {
  if (has_thresholdedrelu()) {
    delete NonlinearityType_.thresholdedrelu_;
    clear_has_NonlinearityType();
  }
}
 const ::CoreML::Specification::ActivationThresholdedReLU& ActivationParams::thresholdedrelu() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.thresholdedReLU)
  return has_thresholdedrelu()
      ? *NonlinearityType_.thresholdedrelu_
      : ::CoreML::Specification::ActivationThresholdedReLU::default_instance();
}
::CoreML::Specification::ActivationThresholdedReLU* ActivationParams::mutable_thresholdedrelu() {
  if (!has_thresholdedrelu()) {
    clear_NonlinearityType();
    set_has_thresholdedrelu();
    NonlinearityType_.thresholdedrelu_ = new ::CoreML::Specification::ActivationThresholdedReLU;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.thresholdedReLU)
  return NonlinearityType_.thresholdedrelu_;
}
::CoreML::Specification::ActivationThresholdedReLU* ActivationParams::release_thresholdedrelu() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.thresholdedReLU)
  if (has_thresholdedrelu()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationThresholdedReLU* temp = NonlinearityType_.thresholdedrelu_;
    NonlinearityType_.thresholdedrelu_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void ActivationParams::set_allocated_thresholdedrelu(::CoreML::Specification::ActivationThresholdedReLU* thresholdedrelu) {
  clear_NonlinearityType();
  if (thresholdedrelu) {
    set_has_thresholdedrelu();
    NonlinearityType_.thresholdedrelu_ = thresholdedrelu;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParams.thresholdedReLU)
}

// .CoreML.Specification.ActivationPReLU PReLU = 25;
bool ActivationParams::has_prelu() const {
  return NonlinearityType_case() == kPReLU;
}
void ActivationParams::set_has_prelu() {
  _oneof_case_[0] = kPReLU;
}
void ActivationParams::clear_prelu() {
  if (has_prelu()) {
    delete NonlinearityType_.prelu_;
    clear_has_NonlinearityType();
  }
}
 const ::CoreML::Specification::ActivationPReLU& ActivationParams::prelu() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.PReLU)
  return has_prelu()
      ? *NonlinearityType_.prelu_
      : ::CoreML::Specification::ActivationPReLU::default_instance();
}
::CoreML::Specification::ActivationPReLU* ActivationParams::mutable_prelu() {
  if (!has_prelu()) {
    clear_NonlinearityType();
    set_has_prelu();
    NonlinearityType_.prelu_ = new ::CoreML::Specification::ActivationPReLU;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.PReLU)
  return NonlinearityType_.prelu_;
}
::CoreML::Specification::ActivationPReLU* ActivationParams::release_prelu() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.PReLU)
  if (has_prelu()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationPReLU* temp = NonlinearityType_.prelu_;
    NonlinearityType_.prelu_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void ActivationParams::set_allocated_prelu(::CoreML::Specification::ActivationPReLU* prelu) {
  clear_NonlinearityType();
  if (prelu) {
    set_has_prelu();
    NonlinearityType_.prelu_ = prelu;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParams.PReLU)
}

// .CoreML.Specification.ActivationTanh tanh = 30;
bool ActivationParams::has_tanh() const {
  return NonlinearityType_case() == kTanh;
}
void ActivationParams::set_has_tanh() {
  _oneof_case_[0] = kTanh;
}
void ActivationParams::clear_tanh() {
  if (has_tanh()) {
    delete NonlinearityType_.tanh_;
    clear_has_NonlinearityType();
  }
}
 const ::CoreML::Specification::ActivationTanh& ActivationParams::tanh() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.tanh)
  return has_tanh()
      ? *NonlinearityType_.tanh_
      : ::CoreML::Specification::ActivationTanh::default_instance();
}
::CoreML::Specification::ActivationTanh* ActivationParams::mutable_tanh() {
  if (!has_tanh()) {
    clear_NonlinearityType();
    set_has_tanh();
    NonlinearityType_.tanh_ = new ::CoreML::Specification::ActivationTanh;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.tanh)
  return NonlinearityType_.tanh_;
}
::CoreML::Specification::ActivationTanh* ActivationParams::release_tanh() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.tanh)
  if (has_tanh()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationTanh* temp = NonlinearityType_.tanh_;
    NonlinearityType_.tanh_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void ActivationParams::set_allocated_tanh(::CoreML::Specification::ActivationTanh* tanh) {
  clear_NonlinearityType();
  if (tanh) {
    set_has_tanh();
    NonlinearityType_.tanh_ = tanh;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParams.tanh)
}

// .CoreML.Specification.ActivationScaledTanh scaledTanh = 31;
bool ActivationParams::has_scaledtanh() const {
  return NonlinearityType_case() == kScaledTanh;
}
void ActivationParams::set_has_scaledtanh() {
  _oneof_case_[0] = kScaledTanh;
}
void ActivationParams::clear_scaledtanh() {
  if (has_scaledtanh()) {
    delete NonlinearityType_.scaledtanh_;
    clear_has_NonlinearityType();
  }
}
 const ::CoreML::Specification::ActivationScaledTanh& ActivationParams::scaledtanh() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.scaledTanh)
  return has_scaledtanh()
      ? *NonlinearityType_.scaledtanh_
      : ::CoreML::Specification::ActivationScaledTanh::default_instance();
}
::CoreML::Specification::ActivationScaledTanh* ActivationParams::mutable_scaledtanh() {
  if (!has_scaledtanh()) {
    clear_NonlinearityType();
    set_has_scaledtanh();
    NonlinearityType_.scaledtanh_ = new ::CoreML::Specification::ActivationScaledTanh;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.scaledTanh)
  return NonlinearityType_.scaledtanh_;
}
::CoreML::Specification::ActivationScaledTanh* ActivationParams::release_scaledtanh() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.scaledTanh)
  if (has_scaledtanh()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationScaledTanh* temp = NonlinearityType_.scaledtanh_;
    NonlinearityType_.scaledtanh_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void ActivationParams::set_allocated_scaledtanh(::CoreML::Specification::ActivationScaledTanh* scaledtanh) {
  clear_NonlinearityType();
  if (scaledtanh) {
    set_has_scaledtanh();
    NonlinearityType_.scaledtanh_ = scaledtanh;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParams.scaledTanh)
}

// .CoreML.Specification.ActivationSigmoid sigmoid = 40;
bool ActivationParams::has_sigmoid() const {
  return NonlinearityType_case() == kSigmoid;
}
void ActivationParams::set_has_sigmoid() {
  _oneof_case_[0] = kSigmoid;
}
void ActivationParams::clear_sigmoid() {
  if (has_sigmoid()) {
    delete NonlinearityType_.sigmoid_;
    clear_has_NonlinearityType();
  }
}
 const ::CoreML::Specification::ActivationSigmoid& ActivationParams::sigmoid() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.sigmoid)
  return has_sigmoid()
      ? *NonlinearityType_.sigmoid_
      : ::CoreML::Specification::ActivationSigmoid::default_instance();
}
::CoreML::Specification::ActivationSigmoid* ActivationParams::mutable_sigmoid() {
  if (!has_sigmoid()) {
    clear_NonlinearityType();
    set_has_sigmoid();
    NonlinearityType_.sigmoid_ = new ::CoreML::Specification::ActivationSigmoid;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.sigmoid)
  return NonlinearityType_.sigmoid_;
}
::CoreML::Specification::ActivationSigmoid* ActivationParams::release_sigmoid() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.sigmoid)
  if (has_sigmoid()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationSigmoid* temp = NonlinearityType_.sigmoid_;
    NonlinearityType_.sigmoid_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void ActivationParams::set_allocated_sigmoid(::CoreML::Specification::ActivationSigmoid* sigmoid) {
  clear_NonlinearityType();
  if (sigmoid) {
    set_has_sigmoid();
    NonlinearityType_.sigmoid_ = sigmoid;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParams.sigmoid)
}

// .CoreML.Specification.ActivationSigmoidHard sigmoidHard = 41;
bool ActivationParams::has_sigmoidhard() const {
  return NonlinearityType_case() == kSigmoidHard;
}
void ActivationParams::set_has_sigmoidhard() {
  _oneof_case_[0] = kSigmoidHard;
}
void ActivationParams::clear_sigmoidhard() {
  if (has_sigmoidhard()) {
    delete NonlinearityType_.sigmoidhard_;
    clear_has_NonlinearityType();
  }
}
 const ::CoreML::Specification::ActivationSigmoidHard& ActivationParams::sigmoidhard() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.sigmoidHard)
  return has_sigmoidhard()
      ? *NonlinearityType_.sigmoidhard_
      : ::CoreML::Specification::ActivationSigmoidHard::default_instance();
}
::CoreML::Specification::ActivationSigmoidHard* ActivationParams::mutable_sigmoidhard() {
  if (!has_sigmoidhard()) {
    clear_NonlinearityType();
    set_has_sigmoidhard();
    NonlinearityType_.sigmoidhard_ = new ::CoreML::Specification::ActivationSigmoidHard;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.sigmoidHard)
  return NonlinearityType_.sigmoidhard_;
}
::CoreML::Specification::ActivationSigmoidHard* ActivationParams::release_sigmoidhard() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.sigmoidHard)
  if (has_sigmoidhard()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationSigmoidHard* temp = NonlinearityType_.sigmoidhard_;
    NonlinearityType_.sigmoidhard_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void ActivationParams::set_allocated_sigmoidhard(::CoreML::Specification::ActivationSigmoidHard* sigmoidhard) {
  clear_NonlinearityType();
  if (sigmoidhard) {
    set_has_sigmoidhard();
    NonlinearityType_.sigmoidhard_ = sigmoidhard;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParams.sigmoidHard)
}

// .CoreML.Specification.ActivationELU ELU = 50;
bool ActivationParams::has_elu() const {
  return NonlinearityType_case() == kELU;
}
void ActivationParams::set_has_elu() {
  _oneof_case_[0] = kELU;
}
void ActivationParams::clear_elu() {
  if (has_elu()) {
    delete NonlinearityType_.elu_;
    clear_has_NonlinearityType();
  }
}
 const ::CoreML::Specification::ActivationELU& ActivationParams::elu() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.ELU)
  return has_elu()
      ? *NonlinearityType_.elu_
      : ::CoreML::Specification::ActivationELU::default_instance();
}
::CoreML::Specification::ActivationELU* ActivationParams::mutable_elu() {
  if (!has_elu()) {
    clear_NonlinearityType();
    set_has_elu();
    NonlinearityType_.elu_ = new ::CoreML::Specification::ActivationELU;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.ELU)
  return NonlinearityType_.elu_;
}
::CoreML::Specification::ActivationELU* ActivationParams::release_elu() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.ELU)
  if (has_elu()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationELU* temp = NonlinearityType_.elu_;
    NonlinearityType_.elu_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void ActivationParams::set_allocated_elu(::CoreML::Specification::ActivationELU* elu) {
  clear_NonlinearityType();
  if (elu) {
    set_has_elu();
    NonlinearityType_.elu_ = elu;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParams.ELU)
}

// .CoreML.Specification.ActivationSoftsign softsign = 60;
bool ActivationParams::has_softsign() const {
  return NonlinearityType_case() == kSoftsign;
}
void ActivationParams::set_has_softsign() {
  _oneof_case_[0] = kSoftsign;
}
void ActivationParams::clear_softsign() {
  if (has_softsign()) {
    delete NonlinearityType_.softsign_;
    clear_has_NonlinearityType();
  }
}
 const ::CoreML::Specification::ActivationSoftsign& ActivationParams::softsign() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.softsign)
  return has_softsign()
      ? *NonlinearityType_.softsign_
      : ::CoreML::Specification::ActivationSoftsign::default_instance();
}
::CoreML::Specification::ActivationSoftsign* ActivationParams::mutable_softsign() {
  if (!has_softsign()) {
    clear_NonlinearityType();
    set_has_softsign();
    NonlinearityType_.softsign_ = new ::CoreML::Specification::ActivationSoftsign;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.softsign)
  return NonlinearityType_.softsign_;
}
::CoreML::Specification::ActivationSoftsign* ActivationParams::release_softsign() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.softsign)
  if (has_softsign()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationSoftsign* temp = NonlinearityType_.softsign_;
    NonlinearityType_.softsign_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void ActivationParams::set_allocated_softsign(::CoreML::Specification::ActivationSoftsign* softsign) {
  clear_NonlinearityType();
  if (softsign) {
    set_has_softsign();
    NonlinearityType_.softsign_ = softsign;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParams.softsign)
}

// .CoreML.Specification.ActivationSoftplus softplus = 70;
bool ActivationParams::has_softplus() const {
  return NonlinearityType_case() == kSoftplus;
}
void ActivationParams::set_has_softplus() {
  _oneof_case_[0] = kSoftplus;
}
void ActivationParams::clear_softplus() {
  if (has_softplus()) {
    delete NonlinearityType_.softplus_;
    clear_has_NonlinearityType();
  }
}
 const ::CoreML::Specification::ActivationSoftplus& ActivationParams::softplus() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.softplus)
  return has_softplus()
      ? *NonlinearityType_.softplus_
      : ::CoreML::Specification::ActivationSoftplus::default_instance();
}
::CoreML::Specification::ActivationSoftplus* ActivationParams::mutable_softplus() {
  if (!has_softplus()) {
    clear_NonlinearityType();
    set_has_softplus();
    NonlinearityType_.softplus_ = new ::CoreML::Specification::ActivationSoftplus;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.softplus)
  return NonlinearityType_.softplus_;
}
::CoreML::Specification::ActivationSoftplus* ActivationParams::release_softplus() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.softplus)
  if (has_softplus()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationSoftplus* temp = NonlinearityType_.softplus_;
    NonlinearityType_.softplus_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void ActivationParams::set_allocated_softplus(::CoreML::Specification::ActivationSoftplus* softplus) {
  clear_NonlinearityType();
  if (softplus) {
    set_has_softplus();
    NonlinearityType_.softplus_ = softplus;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParams.softplus)
}

// .CoreML.Specification.ActivationParametricSoftplus parametricSoftplus = 71;
bool ActivationParams::has_parametricsoftplus() const {
  return NonlinearityType_case() == kParametricSoftplus;
}
void ActivationParams::set_has_parametricsoftplus() {
  _oneof_case_[0] = kParametricSoftplus;
}
void ActivationParams::clear_parametricsoftplus() {
  if (has_parametricsoftplus()) {
    delete NonlinearityType_.parametricsoftplus_;
    clear_has_NonlinearityType();
  }
}
 const ::CoreML::Specification::ActivationParametricSoftplus& ActivationParams::parametricsoftplus() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ActivationParams.parametricSoftplus)
  return has_parametricsoftplus()
      ? *NonlinearityType_.parametricsoftplus_
      : ::CoreML::Specification::ActivationParametricSoftplus::default_instance();
}
::CoreML::Specification::ActivationParametricSoftplus* ActivationParams::mutable_parametricsoftplus() {
  if (!has_parametricsoftplus()) {
    clear_NonlinearityType();
    set_has_parametricsoftplus();
    NonlinearityType_.parametricsoftplus_ = new ::CoreML::Specification::ActivationParametricSoftplus;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ActivationParams.parametricSoftplus)
  return NonlinearityType_.parametricsoftplus_;
}
::CoreML::Specification::ActivationParametricSoftplus* ActivationParams::release_parametricsoftplus() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ActivationParams.parametricSoftplus)
  if (has_parametricsoftplus()) {
    clear_has_NonlinearityType();
    ::CoreML::Specification::ActivationParametricSoftplus* temp = NonlinearityType_.parametricsoftplus_;
    NonlinearityType_.parametricsoftplus_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void ActivationParams::set_allocated_parametricsoftplus(::CoreML::Specification::ActivationParametricSoftplus* parametricsoftplus) {
  clear_NonlinearityType();
  if (parametricsoftplus) {
    set_has_parametricsoftplus();
    NonlinearityType_.parametricsoftplus_ = parametricsoftplus;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ActivationParams.parametricSoftplus)
}

bool ActivationParams::has_NonlinearityType() const {
  return NonlinearityType_case() != NONLINEARITYTYPE_NOT_SET;
}
void ActivationParams::clear_has_NonlinearityType() {
  _oneof_case_[0] = NONLINEARITYTYPE_NOT_SET;
}
ActivationParams::NonlinearityTypeCase ActivationParams::NonlinearityType_case() const {
  return ActivationParams::NonlinearityTypeCase(_oneof_case_[0]);
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int Tensor::kRankFieldNumber;
const int Tensor::kDimValueFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

Tensor::Tensor()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.Tensor)
}
Tensor::Tensor(const Tensor& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      dimvalue_(from.dimvalue_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  rank_ = from.rank_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.Tensor)
}

void Tensor::SharedCtor() {
  rank_ = 0u;
  _cached_size_ = 0;
}

Tensor::~Tensor() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.Tensor)
  SharedDtor();
}

void Tensor::SharedDtor() {
}

void Tensor::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const Tensor& Tensor::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

Tensor* Tensor::New(::google::protobuf::Arena* arena) const {
  Tensor* n = new Tensor;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void Tensor::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.Tensor)
  dimvalue_.Clear();
  rank_ = 0u;
}

bool Tensor::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.Tensor)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // uint32 rank = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint32, ::google::protobuf::internal::WireFormatLite::TYPE_UINT32>(
                 input, &rank_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated int64 dimValue = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(18u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 input, this->mutable_dimvalue())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(16u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 1, 18u, input, this->mutable_dimvalue())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.Tensor)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.Tensor)
  return false;
#undef DO_
}

void Tensor::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.Tensor)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // uint32 rank = 1;
  if (this->rank() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt32(1, this->rank(), output);
  }

  // repeated int64 dimValue = 2;
  if (this->dimvalue_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(2, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_dimvalue_cached_byte_size_);
  }
  for (int i = 0, n = this->dimvalue_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteInt64NoTag(
      this->dimvalue(i), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.Tensor)
}

size_t Tensor::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.Tensor)
  size_t total_size = 0;

  // repeated int64 dimValue = 2;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      Int64Size(this->dimvalue_);
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _dimvalue_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // uint32 rank = 1;
  if (this->rank() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt32Size(
        this->rank());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void Tensor::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const Tensor*>(&from));
}

void Tensor::MergeFrom(const Tensor& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.Tensor)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  dimvalue_.MergeFrom(from.dimvalue_);
  if (from.rank() != 0) {
    set_rank(from.rank());
  }
}

void Tensor::CopyFrom(const Tensor& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.Tensor)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool Tensor::IsInitialized() const {
  return true;
}

void Tensor::Swap(Tensor* other) {
  if (other == this) return;
  InternalSwap(other);
}
void Tensor::InternalSwap(Tensor* other) {
  dimvalue_.InternalSwap(&other->dimvalue_);
  std::swap(rank_, other->rank_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string Tensor::GetTypeName() const {
  return "CoreML.Specification.Tensor";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// Tensor

// uint32 rank = 1;
void Tensor::clear_rank() {
  rank_ = 0u;
}
::google::protobuf::uint32 Tensor::rank() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.Tensor.rank)
  return rank_;
}
void Tensor::set_rank(::google::protobuf::uint32 value) {
  
  rank_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.Tensor.rank)
}

// repeated int64 dimValue = 2;
int Tensor::dimvalue_size() const {
  return dimvalue_.size();
}
void Tensor::clear_dimvalue() {
  dimvalue_.Clear();
}
::google::protobuf::int64 Tensor::dimvalue(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.Tensor.dimValue)
  return dimvalue_.Get(index);
}
void Tensor::set_dimvalue(int index, ::google::protobuf::int64 value) {
  dimvalue_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.Tensor.dimValue)
}
void Tensor::add_dimvalue(::google::protobuf::int64 value) {
  dimvalue_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.Tensor.dimValue)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
Tensor::dimvalue() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.Tensor.dimValue)
  return dimvalue_;
}
::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
Tensor::mutable_dimvalue() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.Tensor.dimValue)
  return &dimvalue_;
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int NeuralNetworkLayer::kNameFieldNumber;
const int NeuralNetworkLayer::kInputFieldNumber;
const int NeuralNetworkLayer::kOutputFieldNumber;
const int NeuralNetworkLayer::kInputTensorFieldNumber;
const int NeuralNetworkLayer::kOutputTensorFieldNumber;
const int NeuralNetworkLayer::kIsUpdatableFieldNumber;
const int NeuralNetworkLayer::kConvolutionFieldNumber;
const int NeuralNetworkLayer::kPoolingFieldNumber;
const int NeuralNetworkLayer::kActivationFieldNumber;
const int NeuralNetworkLayer::kInnerProductFieldNumber;
const int NeuralNetworkLayer::kEmbeddingFieldNumber;
const int NeuralNetworkLayer::kBatchnormFieldNumber;
const int NeuralNetworkLayer::kMvnFieldNumber;
const int NeuralNetworkLayer::kL2NormalizeFieldNumber;
const int NeuralNetworkLayer::kSoftmaxFieldNumber;
const int NeuralNetworkLayer::kLrnFieldNumber;
const int NeuralNetworkLayer::kCropFieldNumber;
const int NeuralNetworkLayer::kPaddingFieldNumber;
const int NeuralNetworkLayer::kUpsampleFieldNumber;
const int NeuralNetworkLayer::kResizeBilinearFieldNumber;
const int NeuralNetworkLayer::kCropResizeFieldNumber;
const int NeuralNetworkLayer::kUnaryFieldNumber;
const int NeuralNetworkLayer::kAddFieldNumber;
const int NeuralNetworkLayer::kMultiplyFieldNumber;
const int NeuralNetworkLayer::kAverageFieldNumber;
const int NeuralNetworkLayer::kScaleFieldNumber;
const int NeuralNetworkLayer::kBiasFieldNumber;
const int NeuralNetworkLayer::kMaxFieldNumber;
const int NeuralNetworkLayer::kMinFieldNumber;
const int NeuralNetworkLayer::kDotFieldNumber;
const int NeuralNetworkLayer::kReduceFieldNumber;
const int NeuralNetworkLayer::kLoadConstantFieldNumber;
const int NeuralNetworkLayer::kReshapeFieldNumber;
const int NeuralNetworkLayer::kFlattenFieldNumber;
const int NeuralNetworkLayer::kPermuteFieldNumber;
const int NeuralNetworkLayer::kConcatFieldNumber;
const int NeuralNetworkLayer::kSplitFieldNumber;
const int NeuralNetworkLayer::kSequenceRepeatFieldNumber;
const int NeuralNetworkLayer::kReorganizeDataFieldNumber;
const int NeuralNetworkLayer::kSliceFieldNumber;
const int NeuralNetworkLayer::kSimpleRecurrentFieldNumber;
const int NeuralNetworkLayer::kGruFieldNumber;
const int NeuralNetworkLayer::kUniDirectionalLSTMFieldNumber;
const int NeuralNetworkLayer::kBiDirectionalLSTMFieldNumber;
const int NeuralNetworkLayer::kCustomFieldNumber;
const int NeuralNetworkLayer::kCopyFieldNumber;
const int NeuralNetworkLayer::kBranchFieldNumber;
const int NeuralNetworkLayer::kLoopFieldNumber;
const int NeuralNetworkLayer::kLoopBreakFieldNumber;
const int NeuralNetworkLayer::kLoopContinueFieldNumber;
const int NeuralNetworkLayer::kRangeFieldNumber;
const int NeuralNetworkLayer::kClipFieldNumber;
const int NeuralNetworkLayer::kCeilFieldNumber;
const int NeuralNetworkLayer::kFloorFieldNumber;
const int NeuralNetworkLayer::kExp2FieldNumber;
const int NeuralNetworkLayer::kSineFieldNumber;
const int NeuralNetworkLayer::kCosineFieldNumber;
const int NeuralNetworkLayer::kErfActivationFieldNumber;
const int NeuralNetworkLayer::kGeluActivationFieldNumber;
const int NeuralNetworkLayer::kEqualFieldNumber;
const int NeuralNetworkLayer::kNotEqualFieldNumber;
const int NeuralNetworkLayer::kLessThanFieldNumber;
const int NeuralNetworkLayer::kGreaterThanFieldNumber;
const int NeuralNetworkLayer::kLogicalOrFieldNumber;
const int NeuralNetworkLayer::kLogicalXorFieldNumber;
const int NeuralNetworkLayer::kLogicalNotFieldNumber;
const int NeuralNetworkLayer::kLogicalAndFieldNumber;
const int NeuralNetworkLayer::kAddBroadcastableFieldNumber;
const int NeuralNetworkLayer::kPowBroadcastableFieldNumber;
const int NeuralNetworkLayer::kDivideBroadcastableFieldNumber;
const int NeuralNetworkLayer::kMultiplyBroadcastableFieldNumber;
const int NeuralNetworkLayer::kSubtractBroadcastableFieldNumber;
const int NeuralNetworkLayer::kTileFieldNumber;
const int NeuralNetworkLayer::kStackNDFieldNumber;
const int NeuralNetworkLayer::kGatherFieldNumber;
const int NeuralNetworkLayer::kScatterFieldNumber;
const int NeuralNetworkLayer::kSoftmaxNDFieldNumber;
const int NeuralNetworkLayer::kSplitNDFieldNumber;
const int NeuralNetworkLayer::kConcatNDFieldNumber;
const int NeuralNetworkLayer::kTransposeNDFieldNumber;
const int NeuralNetworkLayer::kSliceNDFieldNumber;
const int NeuralNetworkLayer::kSlidingWindowsFieldNumber;
const int NeuralNetworkLayer::kEmbeddingNDFieldNumber;
const int NeuralNetworkLayer::kBatchedMatmulFieldNumber;
const int NeuralNetworkLayer::kGetShapeFieldNumber;
const int NeuralNetworkLayer::kLoadConstantNDFieldNumber;
const int NeuralNetworkLayer::kFillLikeFieldNumber;
const int NeuralNetworkLayer::kFillStaticFieldNumber;
const int NeuralNetworkLayer::kFillDynamicFieldNumber;
const int NeuralNetworkLayer::kBroadcastToLikeFieldNumber;
const int NeuralNetworkLayer::kBroadcastToStaticFieldNumber;
const int NeuralNetworkLayer::kBroadcastToDynamicFieldNumber;
const int NeuralNetworkLayer::kSqueezeFieldNumber;
const int NeuralNetworkLayer::kExpandDimsFieldNumber;
const int NeuralNetworkLayer::kRankPreservingReshapeFieldNumber;
const int NeuralNetworkLayer::kLowerTriangularFieldNumber;
const int NeuralNetworkLayer::kUpperTriangularFieldNumber;
const int NeuralNetworkLayer::kWhereFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

NeuralNetworkLayer::NeuralNetworkLayer()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.NeuralNetworkLayer)
}
NeuralNetworkLayer::NeuralNetworkLayer(const NeuralNetworkLayer& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      input_(from.input_),
      output_(from.output_),
      inputtensor_(from.inputtensor_),
      outputtensor_(from.outputtensor_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  name_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (from.name().size() > 0) {
    name_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.name_);
  }
  isupdatable_ = from.isupdatable_;
  clear_has_layer();
  switch (from.layer_case()) {
    case kConvolution: {
      mutable_convolution()->::CoreML::Specification::ConvolutionLayerParams::MergeFrom(from.convolution());
      break;
    }
    case kPooling: {
      mutable_pooling()->::CoreML::Specification::PoolingLayerParams::MergeFrom(from.pooling());
      break;
    }
    case kActivation: {
      mutable_activation()->::CoreML::Specification::ActivationParams::MergeFrom(from.activation());
      break;
    }
    case kInnerProduct: {
      mutable_innerproduct()->::CoreML::Specification::InnerProductLayerParams::MergeFrom(from.innerproduct());
      break;
    }
    case kEmbedding: {
      mutable_embedding()->::CoreML::Specification::EmbeddingLayerParams::MergeFrom(from.embedding());
      break;
    }
    case kBatchnorm: {
      mutable_batchnorm()->::CoreML::Specification::BatchnormLayerParams::MergeFrom(from.batchnorm());
      break;
    }
    case kMvn: {
      mutable_mvn()->::CoreML::Specification::MeanVarianceNormalizeLayerParams::MergeFrom(from.mvn());
      break;
    }
    case kL2Normalize: {
      mutable_l2normalize()->::CoreML::Specification::L2NormalizeLayerParams::MergeFrom(from.l2normalize());
      break;
    }
    case kSoftmax: {
      mutable_softmax()->::CoreML::Specification::SoftmaxLayerParams::MergeFrom(from.softmax());
      break;
    }
    case kLrn: {
      mutable_lrn()->::CoreML::Specification::LRNLayerParams::MergeFrom(from.lrn());
      break;
    }
    case kCrop: {
      mutable_crop()->::CoreML::Specification::CropLayerParams::MergeFrom(from.crop());
      break;
    }
    case kPadding: {
      mutable_padding()->::CoreML::Specification::PaddingLayerParams::MergeFrom(from.padding());
      break;
    }
    case kUpsample: {
      mutable_upsample()->::CoreML::Specification::UpsampleLayerParams::MergeFrom(from.upsample());
      break;
    }
    case kResizeBilinear: {
      mutable_resizebilinear()->::CoreML::Specification::ResizeBilinearLayerParams::MergeFrom(from.resizebilinear());
      break;
    }
    case kCropResize: {
      mutable_cropresize()->::CoreML::Specification::CropResizeLayerParams::MergeFrom(from.cropresize());
      break;
    }
    case kUnary: {
      mutable_unary()->::CoreML::Specification::UnaryFunctionLayerParams::MergeFrom(from.unary());
      break;
    }
    case kAdd: {
      mutable_add()->::CoreML::Specification::AddLayerParams::MergeFrom(from.add());
      break;
    }
    case kMultiply: {
      mutable_multiply()->::CoreML::Specification::MultiplyLayerParams::MergeFrom(from.multiply());
      break;
    }
    case kAverage: {
      mutable_average()->::CoreML::Specification::AverageLayerParams::MergeFrom(from.average());
      break;
    }
    case kScale: {
      mutable_scale()->::CoreML::Specification::ScaleLayerParams::MergeFrom(from.scale());
      break;
    }
    case kBias: {
      mutable_bias()->::CoreML::Specification::BiasLayerParams::MergeFrom(from.bias());
      break;
    }
    case kMax: {
      mutable_max()->::CoreML::Specification::MaxLayerParams::MergeFrom(from.max());
      break;
    }
    case kMin: {
      mutable_min()->::CoreML::Specification::MinLayerParams::MergeFrom(from.min());
      break;
    }
    case kDot: {
      mutable_dot()->::CoreML::Specification::DotProductLayerParams::MergeFrom(from.dot());
      break;
    }
    case kReduce: {
      mutable_reduce()->::CoreML::Specification::ReduceLayerParams::MergeFrom(from.reduce());
      break;
    }
    case kLoadConstant: {
      mutable_loadconstant()->::CoreML::Specification::LoadConstantLayerParams::MergeFrom(from.loadconstant());
      break;
    }
    case kReshape: {
      mutable_reshape()->::CoreML::Specification::ReshapeLayerParams::MergeFrom(from.reshape());
      break;
    }
    case kFlatten: {
      mutable_flatten()->::CoreML::Specification::FlattenLayerParams::MergeFrom(from.flatten());
      break;
    }
    case kPermute: {
      mutable_permute()->::CoreML::Specification::PermuteLayerParams::MergeFrom(from.permute());
      break;
    }
    case kConcat: {
      mutable_concat()->::CoreML::Specification::ConcatLayerParams::MergeFrom(from.concat());
      break;
    }
    case kSplit: {
      mutable_split()->::CoreML::Specification::SplitLayerParams::MergeFrom(from.split());
      break;
    }
    case kSequenceRepeat: {
      mutable_sequencerepeat()->::CoreML::Specification::SequenceRepeatLayerParams::MergeFrom(from.sequencerepeat());
      break;
    }
    case kReorganizeData: {
      mutable_reorganizedata()->::CoreML::Specification::ReorganizeDataLayerParams::MergeFrom(from.reorganizedata());
      break;
    }
    case kSlice: {
      mutable_slice()->::CoreML::Specification::SliceLayerParams::MergeFrom(from.slice());
      break;
    }
    case kSimpleRecurrent: {
      mutable_simplerecurrent()->::CoreML::Specification::SimpleRecurrentLayerParams::MergeFrom(from.simplerecurrent());
      break;
    }
    case kGru: {
      mutable_gru()->::CoreML::Specification::GRULayerParams::MergeFrom(from.gru());
      break;
    }
    case kUniDirectionalLSTM: {
      mutable_unidirectionallstm()->::CoreML::Specification::UniDirectionalLSTMLayerParams::MergeFrom(from.unidirectionallstm());
      break;
    }
    case kBiDirectionalLSTM: {
      mutable_bidirectionallstm()->::CoreML::Specification::BiDirectionalLSTMLayerParams::MergeFrom(from.bidirectionallstm());
      break;
    }
    case kCustom: {
      mutable_custom()->::CoreML::Specification::CustomLayerParams::MergeFrom(from.custom());
      break;
    }
    case kCopy: {
      mutable_copy()->::CoreML::Specification::CopyLayerParams::MergeFrom(from.copy());
      break;
    }
    case kBranch: {
      mutable_branch()->::CoreML::Specification::BranchLayerParams::MergeFrom(from.branch());
      break;
    }
    case kLoop: {
      mutable_loop()->::CoreML::Specification::LoopLayerParams::MergeFrom(from.loop());
      break;
    }
    case kLoopBreak: {
      mutable_loopbreak()->::CoreML::Specification::LoopBreakLayerParams::MergeFrom(from.loopbreak());
      break;
    }
    case kLoopContinue: {
      mutable_loopcontinue()->::CoreML::Specification::LoopContinueLayerParams::MergeFrom(from.loopcontinue());
      break;
    }
    case kRange: {
      mutable_range()->::CoreML::Specification::RangeLayerParams::MergeFrom(from.range());
      break;
    }
    case kClip: {
      mutable_clip()->::CoreML::Specification::ClipLayerParams::MergeFrom(from.clip());
      break;
    }
    case kCeil: {
      mutable_ceil()->::CoreML::Specification::CeilLayerParams::MergeFrom(from.ceil());
      break;
    }
    case kFloor: {
      mutable_floor()->::CoreML::Specification::FloorLayerParams::MergeFrom(from.floor());
      break;
    }
    case kExp2: {
      mutable_exp2()->::CoreML::Specification::Exp2LayerParams::MergeFrom(from.exp2());
      break;
    }
    case kSine: {
      mutable_sine()->::CoreML::Specification::SineLayerParams::MergeFrom(from.sine());
      break;
    }
    case kCosine: {
      mutable_cosine()->::CoreML::Specification::CosineLayerParams::MergeFrom(from.cosine());
      break;
    }
    case kErfActivation: {
      mutable_erfactivation()->::CoreML::Specification::ErfActivationLayerParams::MergeFrom(from.erfactivation());
      break;
    }
    case kGeluActivation: {
      mutable_geluactivation()->::CoreML::Specification::GeluActivationLayerParams::MergeFrom(from.geluactivation());
      break;
    }
    case kEqual: {
      mutable_equal()->::CoreML::Specification::EqualLayerParams::MergeFrom(from.equal());
      break;
    }
    case kNotEqual: {
      mutable_notequal()->::CoreML::Specification::NotEqualLayerParams::MergeFrom(from.notequal());
      break;
    }
    case kLessThan: {
      mutable_lessthan()->::CoreML::Specification::LessThanLayerParams::MergeFrom(from.lessthan());
      break;
    }
    case kGreaterThan: {
      mutable_greaterthan()->::CoreML::Specification::GreaterThanLayerParams::MergeFrom(from.greaterthan());
      break;
    }
    case kLogicalOr: {
      mutable_logicalor()->::CoreML::Specification::LogicalOrLayerParams::MergeFrom(from.logicalor());
      break;
    }
    case kLogicalXor: {
      mutable_logicalxor()->::CoreML::Specification::LogicalXorLayerParams::MergeFrom(from.logicalxor());
      break;
    }
    case kLogicalNot: {
      mutable_logicalnot()->::CoreML::Specification::LogicalNotLayerParams::MergeFrom(from.logicalnot());
      break;
    }
    case kLogicalAnd: {
      mutable_logicaland()->::CoreML::Specification::LogicalAndLayerParams::MergeFrom(from.logicaland());
      break;
    }
    case kAddBroadcastable: {
      mutable_addbroadcastable()->::CoreML::Specification::AddBroadcastableLayerParams::MergeFrom(from.addbroadcastable());
      break;
    }
    case kPowBroadcastable: {
      mutable_powbroadcastable()->::CoreML::Specification::PowBroadcastableLayerParams::MergeFrom(from.powbroadcastable());
      break;
    }
    case kDivideBroadcastable: {
      mutable_dividebroadcastable()->::CoreML::Specification::DivideBroadcastableLayerParams::MergeFrom(from.dividebroadcastable());
      break;
    }
    case kMultiplyBroadcastable: {
      mutable_multiplybroadcastable()->::CoreML::Specification::MultiplyBroadcastableLayerParams::MergeFrom(from.multiplybroadcastable());
      break;
    }
    case kSubtractBroadcastable: {
      mutable_subtractbroadcastable()->::CoreML::Specification::SubtractBroadcastableLayerParams::MergeFrom(from.subtractbroadcastable());
      break;
    }
    case kTile: {
      mutable_tile()->::CoreML::Specification::TileLayerParams::MergeFrom(from.tile());
      break;
    }
    case kStackND: {
      mutable_stacknd()->::CoreML::Specification::StackNDLayerParams::MergeFrom(from.stacknd());
      break;
    }
    case kGather: {
      mutable_gather()->::CoreML::Specification::GatherLayerParams::MergeFrom(from.gather());
      break;
    }
    case kScatter: {
      mutable_scatter()->::CoreML::Specification::ScatterLayerParams::MergeFrom(from.scatter());
      break;
    }
    case kSoftmaxND: {
      mutable_softmaxnd()->::CoreML::Specification::SoftmaxNDLayerParams::MergeFrom(from.softmaxnd());
      break;
    }
    case kSplitND: {
      mutable_splitnd()->::CoreML::Specification::SplitNDLayerParams::MergeFrom(from.splitnd());
      break;
    }
    case kConcatND: {
      mutable_concatnd()->::CoreML::Specification::ConcatNDLayerParams::MergeFrom(from.concatnd());
      break;
    }
    case kTransposeND: {
      mutable_transposend()->::CoreML::Specification::TransposeNDLayerParams::MergeFrom(from.transposend());
      break;
    }
    case kSliceND: {
      mutable_slicend()->::CoreML::Specification::SliceNDLayerParams::MergeFrom(from.slicend());
      break;
    }
    case kSlidingWindows: {
      mutable_slidingwindows()->::CoreML::Specification::SlidingWindowsLayerParams::MergeFrom(from.slidingwindows());
      break;
    }
    case kEmbeddingND: {
      mutable_embeddingnd()->::CoreML::Specification::EmbeddingNDLayerParams::MergeFrom(from.embeddingnd());
      break;
    }
    case kBatchedMatmul: {
      mutable_batchedmatmul()->::CoreML::Specification::BatchedMatMulParams::MergeFrom(from.batchedmatmul());
      break;
    }
    case kGetShape: {
      mutable_getshape()->::CoreML::Specification::GetShapeLayerParams::MergeFrom(from.getshape());
      break;
    }
    case kLoadConstantND: {
      mutable_loadconstantnd()->::CoreML::Specification::LoadConstantNDLayerParams::MergeFrom(from.loadconstantnd());
      break;
    }
    case kFillLike: {
      mutable_filllike()->::CoreML::Specification::FillLikeLayerParams::MergeFrom(from.filllike());
      break;
    }
    case kFillStatic: {
      mutable_fillstatic()->::CoreML::Specification::FillStaticLayerParams::MergeFrom(from.fillstatic());
      break;
    }
    case kFillDynamic: {
      mutable_filldynamic()->::CoreML::Specification::FillDynamicLayerParams::MergeFrom(from.filldynamic());
      break;
    }
    case kBroadcastToLike: {
      mutable_broadcasttolike()->::CoreML::Specification::BroadcastToLikeLayerParams::MergeFrom(from.broadcasttolike());
      break;
    }
    case kBroadcastToStatic: {
      mutable_broadcasttostatic()->::CoreML::Specification::BroadcastToStaticLayerParams::MergeFrom(from.broadcasttostatic());
      break;
    }
    case kBroadcastToDynamic: {
      mutable_broadcasttodynamic()->::CoreML::Specification::BroadcastToDynamicLayerParams::MergeFrom(from.broadcasttodynamic());
      break;
    }
    case kSqueeze: {
      mutable_squeeze()->::CoreML::Specification::SqueezeLayerParams::MergeFrom(from.squeeze());
      break;
    }
    case kExpandDims: {
      mutable_expanddims()->::CoreML::Specification::ExpandDimsLayerParams::MergeFrom(from.expanddims());
      break;
    }
    case kRankPreservingReshape: {
      mutable_rankpreservingreshape()->::CoreML::Specification::RankPreservingReshapeLayerParams::MergeFrom(from.rankpreservingreshape());
      break;
    }
    case kLowerTriangular: {
      mutable_lowertriangular()->::CoreML::Specification::LowerTriangularLayerParams::MergeFrom(from.lowertriangular());
      break;
    }
    case kUpperTriangular: {
      mutable_uppertriangular()->::CoreML::Specification::UpperTriangularLayerParams::MergeFrom(from.uppertriangular());
      break;
    }
    case kWhere: {
      mutable_where()->::CoreML::Specification::WhereLayerParams::MergeFrom(from.where());
      break;
    }
    case LAYER_NOT_SET: {
      break;
    }
  }
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.NeuralNetworkLayer)
}

void NeuralNetworkLayer::SharedCtor() {
  name_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  isupdatable_ = false;
  clear_has_layer();
  _cached_size_ = 0;
}

NeuralNetworkLayer::~NeuralNetworkLayer() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.NeuralNetworkLayer)
  SharedDtor();
}

void NeuralNetworkLayer::SharedDtor() {
  name_.DestroyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (has_layer()) {
    clear_layer();
  }
}

void NeuralNetworkLayer::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const NeuralNetworkLayer& NeuralNetworkLayer::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

NeuralNetworkLayer* NeuralNetworkLayer::New(::google::protobuf::Arena* arena) const {
  NeuralNetworkLayer* n = new NeuralNetworkLayer;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void NeuralNetworkLayer::clear_layer() {
// @@protoc_insertion_point(one_of_clear_start:CoreML.Specification.NeuralNetworkLayer)
  switch (layer_case()) {
    case kConvolution: {
      delete layer_.convolution_;
      break;
    }
    case kPooling: {
      delete layer_.pooling_;
      break;
    }
    case kActivation: {
      delete layer_.activation_;
      break;
    }
    case kInnerProduct: {
      delete layer_.innerproduct_;
      break;
    }
    case kEmbedding: {
      delete layer_.embedding_;
      break;
    }
    case kBatchnorm: {
      delete layer_.batchnorm_;
      break;
    }
    case kMvn: {
      delete layer_.mvn_;
      break;
    }
    case kL2Normalize: {
      delete layer_.l2normalize_;
      break;
    }
    case kSoftmax: {
      delete layer_.softmax_;
      break;
    }
    case kLrn: {
      delete layer_.lrn_;
      break;
    }
    case kCrop: {
      delete layer_.crop_;
      break;
    }
    case kPadding: {
      delete layer_.padding_;
      break;
    }
    case kUpsample: {
      delete layer_.upsample_;
      break;
    }
    case kResizeBilinear: {
      delete layer_.resizebilinear_;
      break;
    }
    case kCropResize: {
      delete layer_.cropresize_;
      break;
    }
    case kUnary: {
      delete layer_.unary_;
      break;
    }
    case kAdd: {
      delete layer_.add_;
      break;
    }
    case kMultiply: {
      delete layer_.multiply_;
      break;
    }
    case kAverage: {
      delete layer_.average_;
      break;
    }
    case kScale: {
      delete layer_.scale_;
      break;
    }
    case kBias: {
      delete layer_.bias_;
      break;
    }
    case kMax: {
      delete layer_.max_;
      break;
    }
    case kMin: {
      delete layer_.min_;
      break;
    }
    case kDot: {
      delete layer_.dot_;
      break;
    }
    case kReduce: {
      delete layer_.reduce_;
      break;
    }
    case kLoadConstant: {
      delete layer_.loadconstant_;
      break;
    }
    case kReshape: {
      delete layer_.reshape_;
      break;
    }
    case kFlatten: {
      delete layer_.flatten_;
      break;
    }
    case kPermute: {
      delete layer_.permute_;
      break;
    }
    case kConcat: {
      delete layer_.concat_;
      break;
    }
    case kSplit: {
      delete layer_.split_;
      break;
    }
    case kSequenceRepeat: {
      delete layer_.sequencerepeat_;
      break;
    }
    case kReorganizeData: {
      delete layer_.reorganizedata_;
      break;
    }
    case kSlice: {
      delete layer_.slice_;
      break;
    }
    case kSimpleRecurrent: {
      delete layer_.simplerecurrent_;
      break;
    }
    case kGru: {
      delete layer_.gru_;
      break;
    }
    case kUniDirectionalLSTM: {
      delete layer_.unidirectionallstm_;
      break;
    }
    case kBiDirectionalLSTM: {
      delete layer_.bidirectionallstm_;
      break;
    }
    case kCustom: {
      delete layer_.custom_;
      break;
    }
    case kCopy: {
      delete layer_.copy_;
      break;
    }
    case kBranch: {
      delete layer_.branch_;
      break;
    }
    case kLoop: {
      delete layer_.loop_;
      break;
    }
    case kLoopBreak: {
      delete layer_.loopbreak_;
      break;
    }
    case kLoopContinue: {
      delete layer_.loopcontinue_;
      break;
    }
    case kRange: {
      delete layer_.range_;
      break;
    }
    case kClip: {
      delete layer_.clip_;
      break;
    }
    case kCeil: {
      delete layer_.ceil_;
      break;
    }
    case kFloor: {
      delete layer_.floor_;
      break;
    }
    case kExp2: {
      delete layer_.exp2_;
      break;
    }
    case kSine: {
      delete layer_.sine_;
      break;
    }
    case kCosine: {
      delete layer_.cosine_;
      break;
    }
    case kErfActivation: {
      delete layer_.erfactivation_;
      break;
    }
    case kGeluActivation: {
      delete layer_.geluactivation_;
      break;
    }
    case kEqual: {
      delete layer_.equal_;
      break;
    }
    case kNotEqual: {
      delete layer_.notequal_;
      break;
    }
    case kLessThan: {
      delete layer_.lessthan_;
      break;
    }
    case kGreaterThan: {
      delete layer_.greaterthan_;
      break;
    }
    case kLogicalOr: {
      delete layer_.logicalor_;
      break;
    }
    case kLogicalXor: {
      delete layer_.logicalxor_;
      break;
    }
    case kLogicalNot: {
      delete layer_.logicalnot_;
      break;
    }
    case kLogicalAnd: {
      delete layer_.logicaland_;
      break;
    }
    case kAddBroadcastable: {
      delete layer_.addbroadcastable_;
      break;
    }
    case kPowBroadcastable: {
      delete layer_.powbroadcastable_;
      break;
    }
    case kDivideBroadcastable: {
      delete layer_.dividebroadcastable_;
      break;
    }
    case kMultiplyBroadcastable: {
      delete layer_.multiplybroadcastable_;
      break;
    }
    case kSubtractBroadcastable: {
      delete layer_.subtractbroadcastable_;
      break;
    }
    case kTile: {
      delete layer_.tile_;
      break;
    }
    case kStackND: {
      delete layer_.stacknd_;
      break;
    }
    case kGather: {
      delete layer_.gather_;
      break;
    }
    case kScatter: {
      delete layer_.scatter_;
      break;
    }
    case kSoftmaxND: {
      delete layer_.softmaxnd_;
      break;
    }
    case kSplitND: {
      delete layer_.splitnd_;
      break;
    }
    case kConcatND: {
      delete layer_.concatnd_;
      break;
    }
    case kTransposeND: {
      delete layer_.transposend_;
      break;
    }
    case kSliceND: {
      delete layer_.slicend_;
      break;
    }
    case kSlidingWindows: {
      delete layer_.slidingwindows_;
      break;
    }
    case kEmbeddingND: {
      delete layer_.embeddingnd_;
      break;
    }
    case kBatchedMatmul: {
      delete layer_.batchedmatmul_;
      break;
    }
    case kGetShape: {
      delete layer_.getshape_;
      break;
    }
    case kLoadConstantND: {
      delete layer_.loadconstantnd_;
      break;
    }
    case kFillLike: {
      delete layer_.filllike_;
      break;
    }
    case kFillStatic: {
      delete layer_.fillstatic_;
      break;
    }
    case kFillDynamic: {
      delete layer_.filldynamic_;
      break;
    }
    case kBroadcastToLike: {
      delete layer_.broadcasttolike_;
      break;
    }
    case kBroadcastToStatic: {
      delete layer_.broadcasttostatic_;
      break;
    }
    case kBroadcastToDynamic: {
      delete layer_.broadcasttodynamic_;
      break;
    }
    case kSqueeze: {
      delete layer_.squeeze_;
      break;
    }
    case kExpandDims: {
      delete layer_.expanddims_;
      break;
    }
    case kRankPreservingReshape: {
      delete layer_.rankpreservingreshape_;
      break;
    }
    case kLowerTriangular: {
      delete layer_.lowertriangular_;
      break;
    }
    case kUpperTriangular: {
      delete layer_.uppertriangular_;
      break;
    }
    case kWhere: {
      delete layer_.where_;
      break;
    }
    case LAYER_NOT_SET: {
      break;
    }
  }
  _oneof_case_[0] = LAYER_NOT_SET;
}


void NeuralNetworkLayer::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.NeuralNetworkLayer)
  input_.Clear();
  output_.Clear();
  inputtensor_.Clear();
  outputtensor_.Clear();
  name_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  isupdatable_ = false;
  clear_layer();
}

bool NeuralNetworkLayer::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.NeuralNetworkLayer)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(16383u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // string name = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadString(
                input, this->mutable_name()));
          DO_(::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
            this->name().data(), this->name().length(),
            ::google::protobuf::internal::WireFormatLite::PARSE,
            "CoreML.Specification.NeuralNetworkLayer.name"));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated string input = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(18u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadString(
                input, this->add_input()));
          DO_(::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
            this->input(this->input_size() - 1).data(),
            this->input(this->input_size() - 1).length(),
            ::google::protobuf::internal::WireFormatLite::PARSE,
            "CoreML.Specification.NeuralNetworkLayer.input"));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated string output = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(26u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadString(
                input, this->add_output()));
          DO_(::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
            this->output(this->output_size() - 1).data(),
            this->output(this->output_size() - 1).length(),
            ::google::protobuf::internal::WireFormatLite::PARSE,
            "CoreML.Specification.NeuralNetworkLayer.output"));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated .CoreML.Specification.Tensor inputTensor = 4;
      case 4: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(34u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
                input, add_inputtensor()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated .CoreML.Specification.Tensor outputTensor = 5;
      case 5: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(42u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
                input, add_outputtensor()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool isUpdatable = 10;
      case 10: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(80u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &isupdatable_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ConvolutionLayerParams convolution = 100;
      case 100: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(802u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_convolution()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.PoolingLayerParams pooling = 120;
      case 120: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(962u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_pooling()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ActivationParams activation = 130;
      case 130: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(1042u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_activation()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.InnerProductLayerParams innerProduct = 140;
      case 140: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(1122u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_innerproduct()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.EmbeddingLayerParams embedding = 150;
      case 150: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(1202u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_embedding()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.BatchnormLayerParams batchnorm = 160;
      case 160: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(1282u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_batchnorm()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.MeanVarianceNormalizeLayerParams mvn = 165;
      case 165: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(1322u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_mvn()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.L2NormalizeLayerParams l2normalize = 170;
      case 170: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(1362u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_l2normalize()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.SoftmaxLayerParams softmax = 175;
      case 175: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(1402u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_softmax()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.LRNLayerParams lrn = 180;
      case 180: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(1442u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_lrn()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.CropLayerParams crop = 190;
      case 190: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(1522u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_crop()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.PaddingLayerParams padding = 200;
      case 200: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(1602u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_padding()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.UpsampleLayerParams upsample = 210;
      case 210: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(1682u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_upsample()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ResizeBilinearLayerParams resizeBilinear = 211;
      case 211: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(1690u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_resizebilinear()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.CropResizeLayerParams cropResize = 212;
      case 212: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(1698u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_cropresize()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.UnaryFunctionLayerParams unary = 220;
      case 220: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(1762u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_unary()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.AddLayerParams add = 230;
      case 230: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(1842u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_add()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.MultiplyLayerParams multiply = 231;
      case 231: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(1850u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_multiply()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.AverageLayerParams average = 240;
      case 240: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(1922u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_average()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ScaleLayerParams scale = 245;
      case 245: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(1962u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_scale()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.BiasLayerParams bias = 250;
      case 250: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(2002u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_bias()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.MaxLayerParams max = 260;
      case 260: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(2082u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_max()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.MinLayerParams min = 261;
      case 261: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(2090u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_min()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.DotProductLayerParams dot = 270;
      case 270: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(2162u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_dot()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ReduceLayerParams reduce = 280;
      case 280: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(2242u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_reduce()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.LoadConstantLayerParams loadConstant = 290;
      case 290: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(2322u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_loadconstant()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ReshapeLayerParams reshape = 300;
      case 300: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(2402u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_reshape()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.FlattenLayerParams flatten = 301;
      case 301: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(2410u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_flatten()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.PermuteLayerParams permute = 310;
      case 310: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(2482u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_permute()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ConcatLayerParams concat = 320;
      case 320: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(2562u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_concat()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.SplitLayerParams split = 330;
      case 330: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(2642u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_split()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.SequenceRepeatLayerParams sequenceRepeat = 340;
      case 340: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(2722u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_sequencerepeat()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ReorganizeDataLayerParams reorganizeData = 345;
      case 345: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(2762u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_reorganizedata()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.SliceLayerParams slice = 350;
      case 350: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(2802u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_slice()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.SimpleRecurrentLayerParams simpleRecurrent = 400;
      case 400: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(3202u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_simplerecurrent()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.GRULayerParams gru = 410;
      case 410: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(3282u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_gru()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.UniDirectionalLSTMLayerParams uniDirectionalLSTM = 420;
      case 420: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(3362u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_unidirectionallstm()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.BiDirectionalLSTMLayerParams biDirectionalLSTM = 430;
      case 430: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(3442u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_bidirectionallstm()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.CustomLayerParams custom = 500;
      case 500: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(4002u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_custom()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.CopyLayerParams copy = 600;
      case 600: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(4802u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_copy()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.BranchLayerParams branch = 605;
      case 605: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(4842u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_branch()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.LoopLayerParams loop = 615;
      case 615: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(4922u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_loop()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.LoopBreakLayerParams loopBreak = 620;
      case 620: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(4962u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_loopbreak()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.LoopContinueLayerParams loopContinue = 625;
      case 625: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(5002u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_loopcontinue()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.RangeLayerParams range = 635;
      case 635: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(5082u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_range()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ClipLayerParams clip = 660;
      case 660: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(5282u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_clip()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.CeilLayerParams ceil = 665;
      case 665: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(5322u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_ceil()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.FloorLayerParams floor = 670;
      case 670: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(5362u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_floor()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.Exp2LayerParams exp2 = 700;
      case 700: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(5602u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_exp2()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.SineLayerParams sine = 710;
      case 710: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(5682u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_sine()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.CosineLayerParams cosine = 715;
      case 715: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(5722u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_cosine()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ErfActivationLayerParams erfActivation = 790;
      case 790: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(6322u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_erfactivation()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.GeluActivationLayerParams geluActivation = 795;
      case 795: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(6362u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_geluactivation()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.EqualLayerParams equal = 815;
      case 815: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(6522u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_equal()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.NotEqualLayerParams notEqual = 820;
      case 820: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(6562u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_notequal()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.LessThanLayerParams lessThan = 825;
      case 825: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(6602u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_lessthan()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.GreaterThanLayerParams greaterThan = 830;
      case 830: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(6642u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_greaterthan()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.LogicalOrLayerParams logicalOr = 840;
      case 840: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(6722u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_logicalor()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.LogicalXorLayerParams logicalXor = 845;
      case 845: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(6762u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_logicalxor()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.LogicalNotLayerParams logicalNot = 850;
      case 850: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(6802u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_logicalnot()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.LogicalAndLayerParams logicalAnd = 855;
      case 855: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(6842u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_logicaland()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.AddBroadcastableLayerParams addBroadcastable = 880;
      case 880: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(7042u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_addbroadcastable()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.PowBroadcastableLayerParams powBroadcastable = 885;
      case 885: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(7082u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_powbroadcastable()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.DivideBroadcastableLayerParams divideBroadcastable = 890;
      case 890: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(7122u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_dividebroadcastable()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.MultiplyBroadcastableLayerParams multiplyBroadcastable = 900;
      case 900: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(7202u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_multiplybroadcastable()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.SubtractBroadcastableLayerParams subtractBroadcastable = 905;
      case 905: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(7242u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_subtractbroadcastable()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.TileLayerParams tile = 920;
      case 920: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(7362u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_tile()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.StackNDLayerParams stackND = 925;
      case 925: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(7402u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_stacknd()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.GatherLayerParams gather = 930;
      case 930: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(7442u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_gather()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ScatterLayerParams scatter = 935;
      case 935: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(7482u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_scatter()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.SoftmaxNDLayerParams softmaxND = 950;
      case 950: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(7602u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_softmaxnd()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.SplitNDLayerParams splitND = 975;
      case 975: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(7802u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_splitnd()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ConcatNDLayerParams concatND = 980;
      case 980: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(7842u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_concatnd()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.TransposeNDLayerParams transposeND = 985;
      case 985: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(7882u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_transposend()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.SliceNDLayerParams sliceND = 995;
      case 995: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(7962u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_slicend()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.SlidingWindowsLayerParams slidingWindows = 1005;
      case 1005: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8042u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_slidingwindows()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.EmbeddingNDLayerParams embeddingND = 1040;
      case 1040: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8322u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_embeddingnd()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.BatchedMatMulParams batchedMatmul = 1045;
      case 1045: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8362u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_batchedmatmul()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.GetShapeLayerParams getShape = 1065;
      case 1065: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8522u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_getshape()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.LoadConstantNDLayerParams loadConstantND = 1070;
      case 1070: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8562u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_loadconstantnd()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.FillLikeLayerParams fillLike = 1080;
      case 1080: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8642u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_filllike()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.FillStaticLayerParams fillStatic = 1085;
      case 1085: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8682u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_fillstatic()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.FillDynamicLayerParams fillDynamic = 1090;
      case 1090: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8722u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_filldynamic()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.BroadcastToLikeLayerParams broadcastToLike = 1100;
      case 1100: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8802u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_broadcasttolike()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.BroadcastToStaticLayerParams broadcastToStatic = 1105;
      case 1105: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8842u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_broadcasttostatic()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.BroadcastToDynamicLayerParams broadcastToDynamic = 1110;
      case 1110: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8882u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_broadcasttodynamic()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.SqueezeLayerParams squeeze = 1120;
      case 1120: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8962u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_squeeze()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ExpandDimsLayerParams expandDims = 1125;
      case 1125: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(9002u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_expanddims()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.RankPreservingReshapeLayerParams rankPreservingReshape = 1150;
      case 1150: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(9202u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_rankpreservingreshape()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.LowerTriangularLayerParams lowerTriangular = 1320;
      case 1320: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10562u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_lowertriangular()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.UpperTriangularLayerParams upperTriangular = 1325;
      case 1325: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10602u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_uppertriangular()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WhereLayerParams where = 1330;
      case 1330: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10642u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_where()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.NeuralNetworkLayer)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.NeuralNetworkLayer)
  return false;
#undef DO_
}

void NeuralNetworkLayer::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.NeuralNetworkLayer)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // string name = 1;
  if (this->name().size() > 0) {
    ::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
      this->name().data(), this->name().length(),
      ::google::protobuf::internal::WireFormatLite::SERIALIZE,
      "CoreML.Specification.NeuralNetworkLayer.name");
    ::google::protobuf::internal::WireFormatLite::WriteStringMaybeAliased(
      1, this->name(), output);
  }

  // repeated string input = 2;
  for (int i = 0, n = this->input_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
      this->input(i).data(), this->input(i).length(),
      ::google::protobuf::internal::WireFormatLite::SERIALIZE,
      "CoreML.Specification.NeuralNetworkLayer.input");
    ::google::protobuf::internal::WireFormatLite::WriteString(
      2, this->input(i), output);
  }

  // repeated string output = 3;
  for (int i = 0, n = this->output_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
      this->output(i).data(), this->output(i).length(),
      ::google::protobuf::internal::WireFormatLite::SERIALIZE,
      "CoreML.Specification.NeuralNetworkLayer.output");
    ::google::protobuf::internal::WireFormatLite::WriteString(
      3, this->output(i), output);
  }

  // repeated .CoreML.Specification.Tensor inputTensor = 4;
  for (unsigned int i = 0, n = this->inputtensor_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      4, this->inputtensor(i), output);
  }

  // repeated .CoreML.Specification.Tensor outputTensor = 5;
  for (unsigned int i = 0, n = this->outputtensor_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      5, this->outputtensor(i), output);
  }

  // bool isUpdatable = 10;
  if (this->isupdatable() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(10, this->isupdatable(), output);
  }

  // .CoreML.Specification.ConvolutionLayerParams convolution = 100;
  if (has_convolution()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      100, *layer_.convolution_, output);
  }

  // .CoreML.Specification.PoolingLayerParams pooling = 120;
  if (has_pooling()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      120, *layer_.pooling_, output);
  }

  // .CoreML.Specification.ActivationParams activation = 130;
  if (has_activation()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      130, *layer_.activation_, output);
  }

  // .CoreML.Specification.InnerProductLayerParams innerProduct = 140;
  if (has_innerproduct()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      140, *layer_.innerproduct_, output);
  }

  // .CoreML.Specification.EmbeddingLayerParams embedding = 150;
  if (has_embedding()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      150, *layer_.embedding_, output);
  }

  // .CoreML.Specification.BatchnormLayerParams batchnorm = 160;
  if (has_batchnorm()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      160, *layer_.batchnorm_, output);
  }

  // .CoreML.Specification.MeanVarianceNormalizeLayerParams mvn = 165;
  if (has_mvn()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      165, *layer_.mvn_, output);
  }

  // .CoreML.Specification.L2NormalizeLayerParams l2normalize = 170;
  if (has_l2normalize()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      170, *layer_.l2normalize_, output);
  }

  // .CoreML.Specification.SoftmaxLayerParams softmax = 175;
  if (has_softmax()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      175, *layer_.softmax_, output);
  }

  // .CoreML.Specification.LRNLayerParams lrn = 180;
  if (has_lrn()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      180, *layer_.lrn_, output);
  }

  // .CoreML.Specification.CropLayerParams crop = 190;
  if (has_crop()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      190, *layer_.crop_, output);
  }

  // .CoreML.Specification.PaddingLayerParams padding = 200;
  if (has_padding()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      200, *layer_.padding_, output);
  }

  // .CoreML.Specification.UpsampleLayerParams upsample = 210;
  if (has_upsample()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      210, *layer_.upsample_, output);
  }

  // .CoreML.Specification.ResizeBilinearLayerParams resizeBilinear = 211;
  if (has_resizebilinear()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      211, *layer_.resizebilinear_, output);
  }

  // .CoreML.Specification.CropResizeLayerParams cropResize = 212;
  if (has_cropresize()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      212, *layer_.cropresize_, output);
  }

  // .CoreML.Specification.UnaryFunctionLayerParams unary = 220;
  if (has_unary()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      220, *layer_.unary_, output);
  }

  // .CoreML.Specification.AddLayerParams add = 230;
  if (has_add()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      230, *layer_.add_, output);
  }

  // .CoreML.Specification.MultiplyLayerParams multiply = 231;
  if (has_multiply()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      231, *layer_.multiply_, output);
  }

  // .CoreML.Specification.AverageLayerParams average = 240;
  if (has_average()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      240, *layer_.average_, output);
  }

  // .CoreML.Specification.ScaleLayerParams scale = 245;
  if (has_scale()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      245, *layer_.scale_, output);
  }

  // .CoreML.Specification.BiasLayerParams bias = 250;
  if (has_bias()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      250, *layer_.bias_, output);
  }

  // .CoreML.Specification.MaxLayerParams max = 260;
  if (has_max()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      260, *layer_.max_, output);
  }

  // .CoreML.Specification.MinLayerParams min = 261;
  if (has_min()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      261, *layer_.min_, output);
  }

  // .CoreML.Specification.DotProductLayerParams dot = 270;
  if (has_dot()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      270, *layer_.dot_, output);
  }

  // .CoreML.Specification.ReduceLayerParams reduce = 280;
  if (has_reduce()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      280, *layer_.reduce_, output);
  }

  // .CoreML.Specification.LoadConstantLayerParams loadConstant = 290;
  if (has_loadconstant()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      290, *layer_.loadconstant_, output);
  }

  // .CoreML.Specification.ReshapeLayerParams reshape = 300;
  if (has_reshape()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      300, *layer_.reshape_, output);
  }

  // .CoreML.Specification.FlattenLayerParams flatten = 301;
  if (has_flatten()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      301, *layer_.flatten_, output);
  }

  // .CoreML.Specification.PermuteLayerParams permute = 310;
  if (has_permute()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      310, *layer_.permute_, output);
  }

  // .CoreML.Specification.ConcatLayerParams concat = 320;
  if (has_concat()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      320, *layer_.concat_, output);
  }

  // .CoreML.Specification.SplitLayerParams split = 330;
  if (has_split()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      330, *layer_.split_, output);
  }

  // .CoreML.Specification.SequenceRepeatLayerParams sequenceRepeat = 340;
  if (has_sequencerepeat()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      340, *layer_.sequencerepeat_, output);
  }

  // .CoreML.Specification.ReorganizeDataLayerParams reorganizeData = 345;
  if (has_reorganizedata()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      345, *layer_.reorganizedata_, output);
  }

  // .CoreML.Specification.SliceLayerParams slice = 350;
  if (has_slice()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      350, *layer_.slice_, output);
  }

  // .CoreML.Specification.SimpleRecurrentLayerParams simpleRecurrent = 400;
  if (has_simplerecurrent()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      400, *layer_.simplerecurrent_, output);
  }

  // .CoreML.Specification.GRULayerParams gru = 410;
  if (has_gru()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      410, *layer_.gru_, output);
  }

  // .CoreML.Specification.UniDirectionalLSTMLayerParams uniDirectionalLSTM = 420;
  if (has_unidirectionallstm()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      420, *layer_.unidirectionallstm_, output);
  }

  // .CoreML.Specification.BiDirectionalLSTMLayerParams biDirectionalLSTM = 430;
  if (has_bidirectionallstm()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      430, *layer_.bidirectionallstm_, output);
  }

  // .CoreML.Specification.CustomLayerParams custom = 500;
  if (has_custom()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      500, *layer_.custom_, output);
  }

  // .CoreML.Specification.CopyLayerParams copy = 600;
  if (has_copy()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      600, *layer_.copy_, output);
  }

  // .CoreML.Specification.BranchLayerParams branch = 605;
  if (has_branch()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      605, *layer_.branch_, output);
  }

  // .CoreML.Specification.LoopLayerParams loop = 615;
  if (has_loop()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      615, *layer_.loop_, output);
  }

  // .CoreML.Specification.LoopBreakLayerParams loopBreak = 620;
  if (has_loopbreak()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      620, *layer_.loopbreak_, output);
  }

  // .CoreML.Specification.LoopContinueLayerParams loopContinue = 625;
  if (has_loopcontinue()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      625, *layer_.loopcontinue_, output);
  }

  // .CoreML.Specification.RangeLayerParams range = 635;
  if (has_range()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      635, *layer_.range_, output);
  }

  // .CoreML.Specification.ClipLayerParams clip = 660;
  if (has_clip()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      660, *layer_.clip_, output);
  }

  // .CoreML.Specification.CeilLayerParams ceil = 665;
  if (has_ceil()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      665, *layer_.ceil_, output);
  }

  // .CoreML.Specification.FloorLayerParams floor = 670;
  if (has_floor()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      670, *layer_.floor_, output);
  }

  // .CoreML.Specification.Exp2LayerParams exp2 = 700;
  if (has_exp2()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      700, *layer_.exp2_, output);
  }

  // .CoreML.Specification.SineLayerParams sine = 710;
  if (has_sine()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      710, *layer_.sine_, output);
  }

  // .CoreML.Specification.CosineLayerParams cosine = 715;
  if (has_cosine()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      715, *layer_.cosine_, output);
  }

  // .CoreML.Specification.ErfActivationLayerParams erfActivation = 790;
  if (has_erfactivation()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      790, *layer_.erfactivation_, output);
  }

  // .CoreML.Specification.GeluActivationLayerParams geluActivation = 795;
  if (has_geluactivation()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      795, *layer_.geluactivation_, output);
  }

  // .CoreML.Specification.EqualLayerParams equal = 815;
  if (has_equal()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      815, *layer_.equal_, output);
  }

  // .CoreML.Specification.NotEqualLayerParams notEqual = 820;
  if (has_notequal()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      820, *layer_.notequal_, output);
  }

  // .CoreML.Specification.LessThanLayerParams lessThan = 825;
  if (has_lessthan()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      825, *layer_.lessthan_, output);
  }

  // .CoreML.Specification.GreaterThanLayerParams greaterThan = 830;
  if (has_greaterthan()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      830, *layer_.greaterthan_, output);
  }

  // .CoreML.Specification.LogicalOrLayerParams logicalOr = 840;
  if (has_logicalor()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      840, *layer_.logicalor_, output);
  }

  // .CoreML.Specification.LogicalXorLayerParams logicalXor = 845;
  if (has_logicalxor()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      845, *layer_.logicalxor_, output);
  }

  // .CoreML.Specification.LogicalNotLayerParams logicalNot = 850;
  if (has_logicalnot()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      850, *layer_.logicalnot_, output);
  }

  // .CoreML.Specification.LogicalAndLayerParams logicalAnd = 855;
  if (has_logicaland()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      855, *layer_.logicaland_, output);
  }

  // .CoreML.Specification.AddBroadcastableLayerParams addBroadcastable = 880;
  if (has_addbroadcastable()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      880, *layer_.addbroadcastable_, output);
  }

  // .CoreML.Specification.PowBroadcastableLayerParams powBroadcastable = 885;
  if (has_powbroadcastable()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      885, *layer_.powbroadcastable_, output);
  }

  // .CoreML.Specification.DivideBroadcastableLayerParams divideBroadcastable = 890;
  if (has_dividebroadcastable()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      890, *layer_.dividebroadcastable_, output);
  }

  // .CoreML.Specification.MultiplyBroadcastableLayerParams multiplyBroadcastable = 900;
  if (has_multiplybroadcastable()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      900, *layer_.multiplybroadcastable_, output);
  }

  // .CoreML.Specification.SubtractBroadcastableLayerParams subtractBroadcastable = 905;
  if (has_subtractbroadcastable()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      905, *layer_.subtractbroadcastable_, output);
  }

  // .CoreML.Specification.TileLayerParams tile = 920;
  if (has_tile()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      920, *layer_.tile_, output);
  }

  // .CoreML.Specification.StackNDLayerParams stackND = 925;
  if (has_stacknd()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      925, *layer_.stacknd_, output);
  }

  // .CoreML.Specification.GatherLayerParams gather = 930;
  if (has_gather()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      930, *layer_.gather_, output);
  }

  // .CoreML.Specification.ScatterLayerParams scatter = 935;
  if (has_scatter()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      935, *layer_.scatter_, output);
  }

  // .CoreML.Specification.SoftmaxNDLayerParams softmaxND = 950;
  if (has_softmaxnd()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      950, *layer_.softmaxnd_, output);
  }

  // .CoreML.Specification.SplitNDLayerParams splitND = 975;
  if (has_splitnd()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      975, *layer_.splitnd_, output);
  }

  // .CoreML.Specification.ConcatNDLayerParams concatND = 980;
  if (has_concatnd()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      980, *layer_.concatnd_, output);
  }

  // .CoreML.Specification.TransposeNDLayerParams transposeND = 985;
  if (has_transposend()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      985, *layer_.transposend_, output);
  }

  // .CoreML.Specification.SliceNDLayerParams sliceND = 995;
  if (has_slicend()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      995, *layer_.slicend_, output);
  }

  // .CoreML.Specification.SlidingWindowsLayerParams slidingWindows = 1005;
  if (has_slidingwindows()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1005, *layer_.slidingwindows_, output);
  }

  // .CoreML.Specification.EmbeddingNDLayerParams embeddingND = 1040;
  if (has_embeddingnd()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1040, *layer_.embeddingnd_, output);
  }

  // .CoreML.Specification.BatchedMatMulParams batchedMatmul = 1045;
  if (has_batchedmatmul()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1045, *layer_.batchedmatmul_, output);
  }

  // .CoreML.Specification.GetShapeLayerParams getShape = 1065;
  if (has_getshape()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1065, *layer_.getshape_, output);
  }

  // .CoreML.Specification.LoadConstantNDLayerParams loadConstantND = 1070;
  if (has_loadconstantnd()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1070, *layer_.loadconstantnd_, output);
  }

  // .CoreML.Specification.FillLikeLayerParams fillLike = 1080;
  if (has_filllike()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1080, *layer_.filllike_, output);
  }

  // .CoreML.Specification.FillStaticLayerParams fillStatic = 1085;
  if (has_fillstatic()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1085, *layer_.fillstatic_, output);
  }

  // .CoreML.Specification.FillDynamicLayerParams fillDynamic = 1090;
  if (has_filldynamic()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1090, *layer_.filldynamic_, output);
  }

  // .CoreML.Specification.BroadcastToLikeLayerParams broadcastToLike = 1100;
  if (has_broadcasttolike()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1100, *layer_.broadcasttolike_, output);
  }

  // .CoreML.Specification.BroadcastToStaticLayerParams broadcastToStatic = 1105;
  if (has_broadcasttostatic()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1105, *layer_.broadcasttostatic_, output);
  }

  // .CoreML.Specification.BroadcastToDynamicLayerParams broadcastToDynamic = 1110;
  if (has_broadcasttodynamic()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1110, *layer_.broadcasttodynamic_, output);
  }

  // .CoreML.Specification.SqueezeLayerParams squeeze = 1120;
  if (has_squeeze()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1120, *layer_.squeeze_, output);
  }

  // .CoreML.Specification.ExpandDimsLayerParams expandDims = 1125;
  if (has_expanddims()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1125, *layer_.expanddims_, output);
  }

  // .CoreML.Specification.RankPreservingReshapeLayerParams rankPreservingReshape = 1150;
  if (has_rankpreservingreshape()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1150, *layer_.rankpreservingreshape_, output);
  }

  // .CoreML.Specification.LowerTriangularLayerParams lowerTriangular = 1320;
  if (has_lowertriangular()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1320, *layer_.lowertriangular_, output);
  }

  // .CoreML.Specification.UpperTriangularLayerParams upperTriangular = 1325;
  if (has_uppertriangular()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1325, *layer_.uppertriangular_, output);
  }

  // .CoreML.Specification.WhereLayerParams where = 1330;
  if (has_where()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1330, *layer_.where_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.NeuralNetworkLayer)
}

size_t NeuralNetworkLayer::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.NeuralNetworkLayer)
  size_t total_size = 0;

  // repeated string input = 2;
  total_size += 1 *
      ::google::protobuf::internal::FromIntSize(this->input_size());
  for (int i = 0, n = this->input_size(); i < n; i++) {
    total_size += ::google::protobuf::internal::WireFormatLite::StringSize(
      this->input(i));
  }

  // repeated string output = 3;
  total_size += 1 *
      ::google::protobuf::internal::FromIntSize(this->output_size());
  for (int i = 0, n = this->output_size(); i < n; i++) {
    total_size += ::google::protobuf::internal::WireFormatLite::StringSize(
      this->output(i));
  }

  // repeated .CoreML.Specification.Tensor inputTensor = 4;
  {
    unsigned int count = this->inputtensor_size();
    total_size += 1UL * count;
    for (unsigned int i = 0; i < count; i++) {
      total_size +=
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          this->inputtensor(i));
    }
  }

  // repeated .CoreML.Specification.Tensor outputTensor = 5;
  {
    unsigned int count = this->outputtensor_size();
    total_size += 1UL * count;
    for (unsigned int i = 0; i < count; i++) {
      total_size +=
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          this->outputtensor(i));
    }
  }

  // string name = 1;
  if (this->name().size() > 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::StringSize(
        this->name());
  }

  // bool isUpdatable = 10;
  if (this->isupdatable() != 0) {
    total_size += 1 + 1;
  }

  switch (layer_case()) {
    // .CoreML.Specification.ConvolutionLayerParams convolution = 100;
    case kConvolution: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.convolution_);
      break;
    }
    // .CoreML.Specification.PoolingLayerParams pooling = 120;
    case kPooling: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.pooling_);
      break;
    }
    // .CoreML.Specification.ActivationParams activation = 130;
    case kActivation: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.activation_);
      break;
    }
    // .CoreML.Specification.InnerProductLayerParams innerProduct = 140;
    case kInnerProduct: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.innerproduct_);
      break;
    }
    // .CoreML.Specification.EmbeddingLayerParams embedding = 150;
    case kEmbedding: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.embedding_);
      break;
    }
    // .CoreML.Specification.BatchnormLayerParams batchnorm = 160;
    case kBatchnorm: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.batchnorm_);
      break;
    }
    // .CoreML.Specification.MeanVarianceNormalizeLayerParams mvn = 165;
    case kMvn: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.mvn_);
      break;
    }
    // .CoreML.Specification.L2NormalizeLayerParams l2normalize = 170;
    case kL2Normalize: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.l2normalize_);
      break;
    }
    // .CoreML.Specification.SoftmaxLayerParams softmax = 175;
    case kSoftmax: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.softmax_);
      break;
    }
    // .CoreML.Specification.LRNLayerParams lrn = 180;
    case kLrn: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.lrn_);
      break;
    }
    // .CoreML.Specification.CropLayerParams crop = 190;
    case kCrop: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.crop_);
      break;
    }
    // .CoreML.Specification.PaddingLayerParams padding = 200;
    case kPadding: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.padding_);
      break;
    }
    // .CoreML.Specification.UpsampleLayerParams upsample = 210;
    case kUpsample: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.upsample_);
      break;
    }
    // .CoreML.Specification.ResizeBilinearLayerParams resizeBilinear = 211;
    case kResizeBilinear: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.resizebilinear_);
      break;
    }
    // .CoreML.Specification.CropResizeLayerParams cropResize = 212;
    case kCropResize: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.cropresize_);
      break;
    }
    // .CoreML.Specification.UnaryFunctionLayerParams unary = 220;
    case kUnary: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.unary_);
      break;
    }
    // .CoreML.Specification.AddLayerParams add = 230;
    case kAdd: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.add_);
      break;
    }
    // .CoreML.Specification.MultiplyLayerParams multiply = 231;
    case kMultiply: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.multiply_);
      break;
    }
    // .CoreML.Specification.AverageLayerParams average = 240;
    case kAverage: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.average_);
      break;
    }
    // .CoreML.Specification.ScaleLayerParams scale = 245;
    case kScale: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.scale_);
      break;
    }
    // .CoreML.Specification.BiasLayerParams bias = 250;
    case kBias: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.bias_);
      break;
    }
    // .CoreML.Specification.MaxLayerParams max = 260;
    case kMax: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.max_);
      break;
    }
    // .CoreML.Specification.MinLayerParams min = 261;
    case kMin: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.min_);
      break;
    }
    // .CoreML.Specification.DotProductLayerParams dot = 270;
    case kDot: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.dot_);
      break;
    }
    // .CoreML.Specification.ReduceLayerParams reduce = 280;
    case kReduce: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.reduce_);
      break;
    }
    // .CoreML.Specification.LoadConstantLayerParams loadConstant = 290;
    case kLoadConstant: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.loadconstant_);
      break;
    }
    // .CoreML.Specification.ReshapeLayerParams reshape = 300;
    case kReshape: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.reshape_);
      break;
    }
    // .CoreML.Specification.FlattenLayerParams flatten = 301;
    case kFlatten: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.flatten_);
      break;
    }
    // .CoreML.Specification.PermuteLayerParams permute = 310;
    case kPermute: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.permute_);
      break;
    }
    // .CoreML.Specification.ConcatLayerParams concat = 320;
    case kConcat: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.concat_);
      break;
    }
    // .CoreML.Specification.SplitLayerParams split = 330;
    case kSplit: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.split_);
      break;
    }
    // .CoreML.Specification.SequenceRepeatLayerParams sequenceRepeat = 340;
    case kSequenceRepeat: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.sequencerepeat_);
      break;
    }
    // .CoreML.Specification.ReorganizeDataLayerParams reorganizeData = 345;
    case kReorganizeData: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.reorganizedata_);
      break;
    }
    // .CoreML.Specification.SliceLayerParams slice = 350;
    case kSlice: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.slice_);
      break;
    }
    // .CoreML.Specification.SimpleRecurrentLayerParams simpleRecurrent = 400;
    case kSimpleRecurrent: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.simplerecurrent_);
      break;
    }
    // .CoreML.Specification.GRULayerParams gru = 410;
    case kGru: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.gru_);
      break;
    }
    // .CoreML.Specification.UniDirectionalLSTMLayerParams uniDirectionalLSTM = 420;
    case kUniDirectionalLSTM: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.unidirectionallstm_);
      break;
    }
    // .CoreML.Specification.BiDirectionalLSTMLayerParams biDirectionalLSTM = 430;
    case kBiDirectionalLSTM: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.bidirectionallstm_);
      break;
    }
    // .CoreML.Specification.CustomLayerParams custom = 500;
    case kCustom: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.custom_);
      break;
    }
    // .CoreML.Specification.CopyLayerParams copy = 600;
    case kCopy: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.copy_);
      break;
    }
    // .CoreML.Specification.BranchLayerParams branch = 605;
    case kBranch: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.branch_);
      break;
    }
    // .CoreML.Specification.LoopLayerParams loop = 615;
    case kLoop: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.loop_);
      break;
    }
    // .CoreML.Specification.LoopBreakLayerParams loopBreak = 620;
    case kLoopBreak: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.loopbreak_);
      break;
    }
    // .CoreML.Specification.LoopContinueLayerParams loopContinue = 625;
    case kLoopContinue: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.loopcontinue_);
      break;
    }
    // .CoreML.Specification.RangeLayerParams range = 635;
    case kRange: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.range_);
      break;
    }
    // .CoreML.Specification.ClipLayerParams clip = 660;
    case kClip: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.clip_);
      break;
    }
    // .CoreML.Specification.CeilLayerParams ceil = 665;
    case kCeil: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.ceil_);
      break;
    }
    // .CoreML.Specification.FloorLayerParams floor = 670;
    case kFloor: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.floor_);
      break;
    }
    // .CoreML.Specification.Exp2LayerParams exp2 = 700;
    case kExp2: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.exp2_);
      break;
    }
    // .CoreML.Specification.SineLayerParams sine = 710;
    case kSine: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.sine_);
      break;
    }
    // .CoreML.Specification.CosineLayerParams cosine = 715;
    case kCosine: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.cosine_);
      break;
    }
    // .CoreML.Specification.ErfActivationLayerParams erfActivation = 790;
    case kErfActivation: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.erfactivation_);
      break;
    }
    // .CoreML.Specification.GeluActivationLayerParams geluActivation = 795;
    case kGeluActivation: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.geluactivation_);
      break;
    }
    // .CoreML.Specification.EqualLayerParams equal = 815;
    case kEqual: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.equal_);
      break;
    }
    // .CoreML.Specification.NotEqualLayerParams notEqual = 820;
    case kNotEqual: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.notequal_);
      break;
    }
    // .CoreML.Specification.LessThanLayerParams lessThan = 825;
    case kLessThan: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.lessthan_);
      break;
    }
    // .CoreML.Specification.GreaterThanLayerParams greaterThan = 830;
    case kGreaterThan: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.greaterthan_);
      break;
    }
    // .CoreML.Specification.LogicalOrLayerParams logicalOr = 840;
    case kLogicalOr: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.logicalor_);
      break;
    }
    // .CoreML.Specification.LogicalXorLayerParams logicalXor = 845;
    case kLogicalXor: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.logicalxor_);
      break;
    }
    // .CoreML.Specification.LogicalNotLayerParams logicalNot = 850;
    case kLogicalNot: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.logicalnot_);
      break;
    }
    // .CoreML.Specification.LogicalAndLayerParams logicalAnd = 855;
    case kLogicalAnd: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.logicaland_);
      break;
    }
    // .CoreML.Specification.AddBroadcastableLayerParams addBroadcastable = 880;
    case kAddBroadcastable: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.addbroadcastable_);
      break;
    }
    // .CoreML.Specification.PowBroadcastableLayerParams powBroadcastable = 885;
    case kPowBroadcastable: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.powbroadcastable_);
      break;
    }
    // .CoreML.Specification.DivideBroadcastableLayerParams divideBroadcastable = 890;
    case kDivideBroadcastable: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.dividebroadcastable_);
      break;
    }
    // .CoreML.Specification.MultiplyBroadcastableLayerParams multiplyBroadcastable = 900;
    case kMultiplyBroadcastable: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.multiplybroadcastable_);
      break;
    }
    // .CoreML.Specification.SubtractBroadcastableLayerParams subtractBroadcastable = 905;
    case kSubtractBroadcastable: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.subtractbroadcastable_);
      break;
    }
    // .CoreML.Specification.TileLayerParams tile = 920;
    case kTile: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.tile_);
      break;
    }
    // .CoreML.Specification.StackNDLayerParams stackND = 925;
    case kStackND: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.stacknd_);
      break;
    }
    // .CoreML.Specification.GatherLayerParams gather = 930;
    case kGather: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.gather_);
      break;
    }
    // .CoreML.Specification.ScatterLayerParams scatter = 935;
    case kScatter: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.scatter_);
      break;
    }
    // .CoreML.Specification.SoftmaxNDLayerParams softmaxND = 950;
    case kSoftmaxND: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.softmaxnd_);
      break;
    }
    // .CoreML.Specification.SplitNDLayerParams splitND = 975;
    case kSplitND: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.splitnd_);
      break;
    }
    // .CoreML.Specification.ConcatNDLayerParams concatND = 980;
    case kConcatND: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.concatnd_);
      break;
    }
    // .CoreML.Specification.TransposeNDLayerParams transposeND = 985;
    case kTransposeND: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.transposend_);
      break;
    }
    // .CoreML.Specification.SliceNDLayerParams sliceND = 995;
    case kSliceND: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.slicend_);
      break;
    }
    // .CoreML.Specification.SlidingWindowsLayerParams slidingWindows = 1005;
    case kSlidingWindows: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.slidingwindows_);
      break;
    }
    // .CoreML.Specification.EmbeddingNDLayerParams embeddingND = 1040;
    case kEmbeddingND: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.embeddingnd_);
      break;
    }
    // .CoreML.Specification.BatchedMatMulParams batchedMatmul = 1045;
    case kBatchedMatmul: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.batchedmatmul_);
      break;
    }
    // .CoreML.Specification.GetShapeLayerParams getShape = 1065;
    case kGetShape: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.getshape_);
      break;
    }
    // .CoreML.Specification.LoadConstantNDLayerParams loadConstantND = 1070;
    case kLoadConstantND: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.loadconstantnd_);
      break;
    }
    // .CoreML.Specification.FillLikeLayerParams fillLike = 1080;
    case kFillLike: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.filllike_);
      break;
    }
    // .CoreML.Specification.FillStaticLayerParams fillStatic = 1085;
    case kFillStatic: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.fillstatic_);
      break;
    }
    // .CoreML.Specification.FillDynamicLayerParams fillDynamic = 1090;
    case kFillDynamic: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.filldynamic_);
      break;
    }
    // .CoreML.Specification.BroadcastToLikeLayerParams broadcastToLike = 1100;
    case kBroadcastToLike: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.broadcasttolike_);
      break;
    }
    // .CoreML.Specification.BroadcastToStaticLayerParams broadcastToStatic = 1105;
    case kBroadcastToStatic: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.broadcasttostatic_);
      break;
    }
    // .CoreML.Specification.BroadcastToDynamicLayerParams broadcastToDynamic = 1110;
    case kBroadcastToDynamic: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.broadcasttodynamic_);
      break;
    }
    // .CoreML.Specification.SqueezeLayerParams squeeze = 1120;
    case kSqueeze: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.squeeze_);
      break;
    }
    // .CoreML.Specification.ExpandDimsLayerParams expandDims = 1125;
    case kExpandDims: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.expanddims_);
      break;
    }
    // .CoreML.Specification.RankPreservingReshapeLayerParams rankPreservingReshape = 1150;
    case kRankPreservingReshape: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.rankpreservingreshape_);
      break;
    }
    // .CoreML.Specification.LowerTriangularLayerParams lowerTriangular = 1320;
    case kLowerTriangular: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.lowertriangular_);
      break;
    }
    // .CoreML.Specification.UpperTriangularLayerParams upperTriangular = 1325;
    case kUpperTriangular: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.uppertriangular_);
      break;
    }
    // .CoreML.Specification.WhereLayerParams where = 1330;
    case kWhere: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *layer_.where_);
      break;
    }
    case LAYER_NOT_SET: {
      break;
    }
  }
  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void NeuralNetworkLayer::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const NeuralNetworkLayer*>(&from));
}

void NeuralNetworkLayer::MergeFrom(const NeuralNetworkLayer& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.NeuralNetworkLayer)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  input_.MergeFrom(from.input_);
  output_.MergeFrom(from.output_);
  inputtensor_.MergeFrom(from.inputtensor_);
  outputtensor_.MergeFrom(from.outputtensor_);
  if (from.name().size() > 0) {

    name_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.name_);
  }
  if (from.isupdatable() != 0) {
    set_isupdatable(from.isupdatable());
  }
  switch (from.layer_case()) {
    case kConvolution: {
      mutable_convolution()->::CoreML::Specification::ConvolutionLayerParams::MergeFrom(from.convolution());
      break;
    }
    case kPooling: {
      mutable_pooling()->::CoreML::Specification::PoolingLayerParams::MergeFrom(from.pooling());
      break;
    }
    case kActivation: {
      mutable_activation()->::CoreML::Specification::ActivationParams::MergeFrom(from.activation());
      break;
    }
    case kInnerProduct: {
      mutable_innerproduct()->::CoreML::Specification::InnerProductLayerParams::MergeFrom(from.innerproduct());
      break;
    }
    case kEmbedding: {
      mutable_embedding()->::CoreML::Specification::EmbeddingLayerParams::MergeFrom(from.embedding());
      break;
    }
    case kBatchnorm: {
      mutable_batchnorm()->::CoreML::Specification::BatchnormLayerParams::MergeFrom(from.batchnorm());
      break;
    }
    case kMvn: {
      mutable_mvn()->::CoreML::Specification::MeanVarianceNormalizeLayerParams::MergeFrom(from.mvn());
      break;
    }
    case kL2Normalize: {
      mutable_l2normalize()->::CoreML::Specification::L2NormalizeLayerParams::MergeFrom(from.l2normalize());
      break;
    }
    case kSoftmax: {
      mutable_softmax()->::CoreML::Specification::SoftmaxLayerParams::MergeFrom(from.softmax());
      break;
    }
    case kLrn: {
      mutable_lrn()->::CoreML::Specification::LRNLayerParams::MergeFrom(from.lrn());
      break;
    }
    case kCrop: {
      mutable_crop()->::CoreML::Specification::CropLayerParams::MergeFrom(from.crop());
      break;
    }
    case kPadding: {
      mutable_padding()->::CoreML::Specification::PaddingLayerParams::MergeFrom(from.padding());
      break;
    }
    case kUpsample: {
      mutable_upsample()->::CoreML::Specification::UpsampleLayerParams::MergeFrom(from.upsample());
      break;
    }
    case kResizeBilinear: {
      mutable_resizebilinear()->::CoreML::Specification::ResizeBilinearLayerParams::MergeFrom(from.resizebilinear());
      break;
    }
    case kCropResize: {
      mutable_cropresize()->::CoreML::Specification::CropResizeLayerParams::MergeFrom(from.cropresize());
      break;
    }
    case kUnary: {
      mutable_unary()->::CoreML::Specification::UnaryFunctionLayerParams::MergeFrom(from.unary());
      break;
    }
    case kAdd: {
      mutable_add()->::CoreML::Specification::AddLayerParams::MergeFrom(from.add());
      break;
    }
    case kMultiply: {
      mutable_multiply()->::CoreML::Specification::MultiplyLayerParams::MergeFrom(from.multiply());
      break;
    }
    case kAverage: {
      mutable_average()->::CoreML::Specification::AverageLayerParams::MergeFrom(from.average());
      break;
    }
    case kScale: {
      mutable_scale()->::CoreML::Specification::ScaleLayerParams::MergeFrom(from.scale());
      break;
    }
    case kBias: {
      mutable_bias()->::CoreML::Specification::BiasLayerParams::MergeFrom(from.bias());
      break;
    }
    case kMax: {
      mutable_max()->::CoreML::Specification::MaxLayerParams::MergeFrom(from.max());
      break;
    }
    case kMin: {
      mutable_min()->::CoreML::Specification::MinLayerParams::MergeFrom(from.min());
      break;
    }
    case kDot: {
      mutable_dot()->::CoreML::Specification::DotProductLayerParams::MergeFrom(from.dot());
      break;
    }
    case kReduce: {
      mutable_reduce()->::CoreML::Specification::ReduceLayerParams::MergeFrom(from.reduce());
      break;
    }
    case kLoadConstant: {
      mutable_loadconstant()->::CoreML::Specification::LoadConstantLayerParams::MergeFrom(from.loadconstant());
      break;
    }
    case kReshape: {
      mutable_reshape()->::CoreML::Specification::ReshapeLayerParams::MergeFrom(from.reshape());
      break;
    }
    case kFlatten: {
      mutable_flatten()->::CoreML::Specification::FlattenLayerParams::MergeFrom(from.flatten());
      break;
    }
    case kPermute: {
      mutable_permute()->::CoreML::Specification::PermuteLayerParams::MergeFrom(from.permute());
      break;
    }
    case kConcat: {
      mutable_concat()->::CoreML::Specification::ConcatLayerParams::MergeFrom(from.concat());
      break;
    }
    case kSplit: {
      mutable_split()->::CoreML::Specification::SplitLayerParams::MergeFrom(from.split());
      break;
    }
    case kSequenceRepeat: {
      mutable_sequencerepeat()->::CoreML::Specification::SequenceRepeatLayerParams::MergeFrom(from.sequencerepeat());
      break;
    }
    case kReorganizeData: {
      mutable_reorganizedata()->::CoreML::Specification::ReorganizeDataLayerParams::MergeFrom(from.reorganizedata());
      break;
    }
    case kSlice: {
      mutable_slice()->::CoreML::Specification::SliceLayerParams::MergeFrom(from.slice());
      break;
    }
    case kSimpleRecurrent: {
      mutable_simplerecurrent()->::CoreML::Specification::SimpleRecurrentLayerParams::MergeFrom(from.simplerecurrent());
      break;
    }
    case kGru: {
      mutable_gru()->::CoreML::Specification::GRULayerParams::MergeFrom(from.gru());
      break;
    }
    case kUniDirectionalLSTM: {
      mutable_unidirectionallstm()->::CoreML::Specification::UniDirectionalLSTMLayerParams::MergeFrom(from.unidirectionallstm());
      break;
    }
    case kBiDirectionalLSTM: {
      mutable_bidirectionallstm()->::CoreML::Specification::BiDirectionalLSTMLayerParams::MergeFrom(from.bidirectionallstm());
      break;
    }
    case kCustom: {
      mutable_custom()->::CoreML::Specification::CustomLayerParams::MergeFrom(from.custom());
      break;
    }
    case kCopy: {
      mutable_copy()->::CoreML::Specification::CopyLayerParams::MergeFrom(from.copy());
      break;
    }
    case kBranch: {
      mutable_branch()->::CoreML::Specification::BranchLayerParams::MergeFrom(from.branch());
      break;
    }
    case kLoop: {
      mutable_loop()->::CoreML::Specification::LoopLayerParams::MergeFrom(from.loop());
      break;
    }
    case kLoopBreak: {
      mutable_loopbreak()->::CoreML::Specification::LoopBreakLayerParams::MergeFrom(from.loopbreak());
      break;
    }
    case kLoopContinue: {
      mutable_loopcontinue()->::CoreML::Specification::LoopContinueLayerParams::MergeFrom(from.loopcontinue());
      break;
    }
    case kRange: {
      mutable_range()->::CoreML::Specification::RangeLayerParams::MergeFrom(from.range());
      break;
    }
    case kClip: {
      mutable_clip()->::CoreML::Specification::ClipLayerParams::MergeFrom(from.clip());
      break;
    }
    case kCeil: {
      mutable_ceil()->::CoreML::Specification::CeilLayerParams::MergeFrom(from.ceil());
      break;
    }
    case kFloor: {
      mutable_floor()->::CoreML::Specification::FloorLayerParams::MergeFrom(from.floor());
      break;
    }
    case kExp2: {
      mutable_exp2()->::CoreML::Specification::Exp2LayerParams::MergeFrom(from.exp2());
      break;
    }
    case kSine: {
      mutable_sine()->::CoreML::Specification::SineLayerParams::MergeFrom(from.sine());
      break;
    }
    case kCosine: {
      mutable_cosine()->::CoreML::Specification::CosineLayerParams::MergeFrom(from.cosine());
      break;
    }
    case kErfActivation: {
      mutable_erfactivation()->::CoreML::Specification::ErfActivationLayerParams::MergeFrom(from.erfactivation());
      break;
    }
    case kGeluActivation: {
      mutable_geluactivation()->::CoreML::Specification::GeluActivationLayerParams::MergeFrom(from.geluactivation());
      break;
    }
    case kEqual: {
      mutable_equal()->::CoreML::Specification::EqualLayerParams::MergeFrom(from.equal());
      break;
    }
    case kNotEqual: {
      mutable_notequal()->::CoreML::Specification::NotEqualLayerParams::MergeFrom(from.notequal());
      break;
    }
    case kLessThan: {
      mutable_lessthan()->::CoreML::Specification::LessThanLayerParams::MergeFrom(from.lessthan());
      break;
    }
    case kGreaterThan: {
      mutable_greaterthan()->::CoreML::Specification::GreaterThanLayerParams::MergeFrom(from.greaterthan());
      break;
    }
    case kLogicalOr: {
      mutable_logicalor()->::CoreML::Specification::LogicalOrLayerParams::MergeFrom(from.logicalor());
      break;
    }
    case kLogicalXor: {
      mutable_logicalxor()->::CoreML::Specification::LogicalXorLayerParams::MergeFrom(from.logicalxor());
      break;
    }
    case kLogicalNot: {
      mutable_logicalnot()->::CoreML::Specification::LogicalNotLayerParams::MergeFrom(from.logicalnot());
      break;
    }
    case kLogicalAnd: {
      mutable_logicaland()->::CoreML::Specification::LogicalAndLayerParams::MergeFrom(from.logicaland());
      break;
    }
    case kAddBroadcastable: {
      mutable_addbroadcastable()->::CoreML::Specification::AddBroadcastableLayerParams::MergeFrom(from.addbroadcastable());
      break;
    }
    case kPowBroadcastable: {
      mutable_powbroadcastable()->::CoreML::Specification::PowBroadcastableLayerParams::MergeFrom(from.powbroadcastable());
      break;
    }
    case kDivideBroadcastable: {
      mutable_dividebroadcastable()->::CoreML::Specification::DivideBroadcastableLayerParams::MergeFrom(from.dividebroadcastable());
      break;
    }
    case kMultiplyBroadcastable: {
      mutable_multiplybroadcastable()->::CoreML::Specification::MultiplyBroadcastableLayerParams::MergeFrom(from.multiplybroadcastable());
      break;
    }
    case kSubtractBroadcastable: {
      mutable_subtractbroadcastable()->::CoreML::Specification::SubtractBroadcastableLayerParams::MergeFrom(from.subtractbroadcastable());
      break;
    }
    case kTile: {
      mutable_tile()->::CoreML::Specification::TileLayerParams::MergeFrom(from.tile());
      break;
    }
    case kStackND: {
      mutable_stacknd()->::CoreML::Specification::StackNDLayerParams::MergeFrom(from.stacknd());
      break;
    }
    case kGather: {
      mutable_gather()->::CoreML::Specification::GatherLayerParams::MergeFrom(from.gather());
      break;
    }
    case kScatter: {
      mutable_scatter()->::CoreML::Specification::ScatterLayerParams::MergeFrom(from.scatter());
      break;
    }
    case kSoftmaxND: {
      mutable_softmaxnd()->::CoreML::Specification::SoftmaxNDLayerParams::MergeFrom(from.softmaxnd());
      break;
    }
    case kSplitND: {
      mutable_splitnd()->::CoreML::Specification::SplitNDLayerParams::MergeFrom(from.splitnd());
      break;
    }
    case kConcatND: {
      mutable_concatnd()->::CoreML::Specification::ConcatNDLayerParams::MergeFrom(from.concatnd());
      break;
    }
    case kTransposeND: {
      mutable_transposend()->::CoreML::Specification::TransposeNDLayerParams::MergeFrom(from.transposend());
      break;
    }
    case kSliceND: {
      mutable_slicend()->::CoreML::Specification::SliceNDLayerParams::MergeFrom(from.slicend());
      break;
    }
    case kSlidingWindows: {
      mutable_slidingwindows()->::CoreML::Specification::SlidingWindowsLayerParams::MergeFrom(from.slidingwindows());
      break;
    }
    case kEmbeddingND: {
      mutable_embeddingnd()->::CoreML::Specification::EmbeddingNDLayerParams::MergeFrom(from.embeddingnd());
      break;
    }
    case kBatchedMatmul: {
      mutable_batchedmatmul()->::CoreML::Specification::BatchedMatMulParams::MergeFrom(from.batchedmatmul());
      break;
    }
    case kGetShape: {
      mutable_getshape()->::CoreML::Specification::GetShapeLayerParams::MergeFrom(from.getshape());
      break;
    }
    case kLoadConstantND: {
      mutable_loadconstantnd()->::CoreML::Specification::LoadConstantNDLayerParams::MergeFrom(from.loadconstantnd());
      break;
    }
    case kFillLike: {
      mutable_filllike()->::CoreML::Specification::FillLikeLayerParams::MergeFrom(from.filllike());
      break;
    }
    case kFillStatic: {
      mutable_fillstatic()->::CoreML::Specification::FillStaticLayerParams::MergeFrom(from.fillstatic());
      break;
    }
    case kFillDynamic: {
      mutable_filldynamic()->::CoreML::Specification::FillDynamicLayerParams::MergeFrom(from.filldynamic());
      break;
    }
    case kBroadcastToLike: {
      mutable_broadcasttolike()->::CoreML::Specification::BroadcastToLikeLayerParams::MergeFrom(from.broadcasttolike());
      break;
    }
    case kBroadcastToStatic: {
      mutable_broadcasttostatic()->::CoreML::Specification::BroadcastToStaticLayerParams::MergeFrom(from.broadcasttostatic());
      break;
    }
    case kBroadcastToDynamic: {
      mutable_broadcasttodynamic()->::CoreML::Specification::BroadcastToDynamicLayerParams::MergeFrom(from.broadcasttodynamic());
      break;
    }
    case kSqueeze: {
      mutable_squeeze()->::CoreML::Specification::SqueezeLayerParams::MergeFrom(from.squeeze());
      break;
    }
    case kExpandDims: {
      mutable_expanddims()->::CoreML::Specification::ExpandDimsLayerParams::MergeFrom(from.expanddims());
      break;
    }
    case kRankPreservingReshape: {
      mutable_rankpreservingreshape()->::CoreML::Specification::RankPreservingReshapeLayerParams::MergeFrom(from.rankpreservingreshape());
      break;
    }
    case kLowerTriangular: {
      mutable_lowertriangular()->::CoreML::Specification::LowerTriangularLayerParams::MergeFrom(from.lowertriangular());
      break;
    }
    case kUpperTriangular: {
      mutable_uppertriangular()->::CoreML::Specification::UpperTriangularLayerParams::MergeFrom(from.uppertriangular());
      break;
    }
    case kWhere: {
      mutable_where()->::CoreML::Specification::WhereLayerParams::MergeFrom(from.where());
      break;
    }
    case LAYER_NOT_SET: {
      break;
    }
  }
}

void NeuralNetworkLayer::CopyFrom(const NeuralNetworkLayer& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.NeuralNetworkLayer)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool NeuralNetworkLayer::IsInitialized() const {
  return true;
}

void NeuralNetworkLayer::Swap(NeuralNetworkLayer* other) {
  if (other == this) return;
  InternalSwap(other);
}
void NeuralNetworkLayer::InternalSwap(NeuralNetworkLayer* other) {
  input_.InternalSwap(&other->input_);
  output_.InternalSwap(&other->output_);
  inputtensor_.InternalSwap(&other->inputtensor_);
  outputtensor_.InternalSwap(&other->outputtensor_);
  name_.Swap(&other->name_);
  std::swap(isupdatable_, other->isupdatable_);
  std::swap(layer_, other->layer_);
  std::swap(_oneof_case_[0], other->_oneof_case_[0]);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string NeuralNetworkLayer::GetTypeName() const {
  return "CoreML.Specification.NeuralNetworkLayer";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// NeuralNetworkLayer

// string name = 1;
void NeuralNetworkLayer::clear_name() {
  name_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
const ::std::string& NeuralNetworkLayer::name() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.name)
  return name_.GetNoArena();
}
void NeuralNetworkLayer::set_name(const ::std::string& value) {
  
  name_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkLayer.name)
}
#if LANG_CXX11
void NeuralNetworkLayer::set_name(::std::string&& value) {
  
  name_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:CoreML.Specification.NeuralNetworkLayer.name)
}
#endif
void NeuralNetworkLayer::set_name(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  
  name_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:CoreML.Specification.NeuralNetworkLayer.name)
}
void NeuralNetworkLayer::set_name(const char* value, size_t size) {
  
  name_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:CoreML.Specification.NeuralNetworkLayer.name)
}
::std::string* NeuralNetworkLayer::mutable_name() {
  
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.name)
  return name_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
::std::string* NeuralNetworkLayer::release_name() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.name)
  
  return name_.ReleaseNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
void NeuralNetworkLayer::set_allocated_name(::std::string* name) {
  if (name != NULL) {
    
  } else {
    
  }
  name_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), name);
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.name)
}

// repeated string input = 2;
int NeuralNetworkLayer::input_size() const {
  return input_.size();
}
void NeuralNetworkLayer::clear_input() {
  input_.Clear();
}
const ::std::string& NeuralNetworkLayer::input(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.input)
  return input_.Get(index);
}
::std::string* NeuralNetworkLayer::mutable_input(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.input)
  return input_.Mutable(index);
}
void NeuralNetworkLayer::set_input(int index, const ::std::string& value) {
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkLayer.input)
  input_.Mutable(index)->assign(value);
}
#if LANG_CXX11
void NeuralNetworkLayer::set_input(int index, ::std::string&& value) {
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkLayer.input)
  input_.Mutable(index)->assign(std::move(value));
}
#endif
void NeuralNetworkLayer::set_input(int index, const char* value) {
  GOOGLE_DCHECK(value != NULL);
  input_.Mutable(index)->assign(value);
  // @@protoc_insertion_point(field_set_char:CoreML.Specification.NeuralNetworkLayer.input)
}
void NeuralNetworkLayer::set_input(int index, const char* value, size_t size) {
  input_.Mutable(index)->assign(
    reinterpret_cast<const char*>(value), size);
  // @@protoc_insertion_point(field_set_pointer:CoreML.Specification.NeuralNetworkLayer.input)
}
::std::string* NeuralNetworkLayer::add_input() {
  // @@protoc_insertion_point(field_add_mutable:CoreML.Specification.NeuralNetworkLayer.input)
  return input_.Add();
}
void NeuralNetworkLayer::add_input(const ::std::string& value) {
  input_.Add()->assign(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.NeuralNetworkLayer.input)
}
#if LANG_CXX11
void NeuralNetworkLayer::add_input(::std::string&& value) {
  input_.Add(std::move(value));
  // @@protoc_insertion_point(field_add:CoreML.Specification.NeuralNetworkLayer.input)
}
#endif
void NeuralNetworkLayer::add_input(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  input_.Add()->assign(value);
  // @@protoc_insertion_point(field_add_char:CoreML.Specification.NeuralNetworkLayer.input)
}
void NeuralNetworkLayer::add_input(const char* value, size_t size) {
  input_.Add()->assign(reinterpret_cast<const char*>(value), size);
  // @@protoc_insertion_point(field_add_pointer:CoreML.Specification.NeuralNetworkLayer.input)
}
const ::google::protobuf::RepeatedPtrField< ::std::string>&
NeuralNetworkLayer::input() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.NeuralNetworkLayer.input)
  return input_;
}
::google::protobuf::RepeatedPtrField< ::std::string>*
NeuralNetworkLayer::mutable_input() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.NeuralNetworkLayer.input)
  return &input_;
}

// repeated string output = 3;
int NeuralNetworkLayer::output_size() const {
  return output_.size();
}
void NeuralNetworkLayer::clear_output() {
  output_.Clear();
}
const ::std::string& NeuralNetworkLayer::output(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.output)
  return output_.Get(index);
}
::std::string* NeuralNetworkLayer::mutable_output(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.output)
  return output_.Mutable(index);
}
void NeuralNetworkLayer::set_output(int index, const ::std::string& value) {
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkLayer.output)
  output_.Mutable(index)->assign(value);
}
#if LANG_CXX11
void NeuralNetworkLayer::set_output(int index, ::std::string&& value) {
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkLayer.output)
  output_.Mutable(index)->assign(std::move(value));
}
#endif
void NeuralNetworkLayer::set_output(int index, const char* value) {
  GOOGLE_DCHECK(value != NULL);
  output_.Mutable(index)->assign(value);
  // @@protoc_insertion_point(field_set_char:CoreML.Specification.NeuralNetworkLayer.output)
}
void NeuralNetworkLayer::set_output(int index, const char* value, size_t size) {
  output_.Mutable(index)->assign(
    reinterpret_cast<const char*>(value), size);
  // @@protoc_insertion_point(field_set_pointer:CoreML.Specification.NeuralNetworkLayer.output)
}
::std::string* NeuralNetworkLayer::add_output() {
  // @@protoc_insertion_point(field_add_mutable:CoreML.Specification.NeuralNetworkLayer.output)
  return output_.Add();
}
void NeuralNetworkLayer::add_output(const ::std::string& value) {
  output_.Add()->assign(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.NeuralNetworkLayer.output)
}
#if LANG_CXX11
void NeuralNetworkLayer::add_output(::std::string&& value) {
  output_.Add(std::move(value));
  // @@protoc_insertion_point(field_add:CoreML.Specification.NeuralNetworkLayer.output)
}
#endif
void NeuralNetworkLayer::add_output(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  output_.Add()->assign(value);
  // @@protoc_insertion_point(field_add_char:CoreML.Specification.NeuralNetworkLayer.output)
}
void NeuralNetworkLayer::add_output(const char* value, size_t size) {
  output_.Add()->assign(reinterpret_cast<const char*>(value), size);
  // @@protoc_insertion_point(field_add_pointer:CoreML.Specification.NeuralNetworkLayer.output)
}
const ::google::protobuf::RepeatedPtrField< ::std::string>&
NeuralNetworkLayer::output() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.NeuralNetworkLayer.output)
  return output_;
}
::google::protobuf::RepeatedPtrField< ::std::string>*
NeuralNetworkLayer::mutable_output() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.NeuralNetworkLayer.output)
  return &output_;
}

// repeated .CoreML.Specification.Tensor inputTensor = 4;
int NeuralNetworkLayer::inputtensor_size() const {
  return inputtensor_.size();
}
void NeuralNetworkLayer::clear_inputtensor() {
  inputtensor_.Clear();
}
const ::CoreML::Specification::Tensor& NeuralNetworkLayer::inputtensor(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.inputTensor)
  return inputtensor_.Get(index);
}
::CoreML::Specification::Tensor* NeuralNetworkLayer::mutable_inputtensor(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.inputTensor)
  return inputtensor_.Mutable(index);
}
::CoreML::Specification::Tensor* NeuralNetworkLayer::add_inputtensor() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.NeuralNetworkLayer.inputTensor)
  return inputtensor_.Add();
}
::google::protobuf::RepeatedPtrField< ::CoreML::Specification::Tensor >*
NeuralNetworkLayer::mutable_inputtensor() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.NeuralNetworkLayer.inputTensor)
  return &inputtensor_;
}
const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::Tensor >&
NeuralNetworkLayer::inputtensor() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.NeuralNetworkLayer.inputTensor)
  return inputtensor_;
}

// repeated .CoreML.Specification.Tensor outputTensor = 5;
int NeuralNetworkLayer::outputtensor_size() const {
  return outputtensor_.size();
}
void NeuralNetworkLayer::clear_outputtensor() {
  outputtensor_.Clear();
}
const ::CoreML::Specification::Tensor& NeuralNetworkLayer::outputtensor(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.outputTensor)
  return outputtensor_.Get(index);
}
::CoreML::Specification::Tensor* NeuralNetworkLayer::mutable_outputtensor(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.outputTensor)
  return outputtensor_.Mutable(index);
}
::CoreML::Specification::Tensor* NeuralNetworkLayer::add_outputtensor() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.NeuralNetworkLayer.outputTensor)
  return outputtensor_.Add();
}
::google::protobuf::RepeatedPtrField< ::CoreML::Specification::Tensor >*
NeuralNetworkLayer::mutable_outputtensor() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.NeuralNetworkLayer.outputTensor)
  return &outputtensor_;
}
const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::Tensor >&
NeuralNetworkLayer::outputtensor() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.NeuralNetworkLayer.outputTensor)
  return outputtensor_;
}

// bool isUpdatable = 10;
void NeuralNetworkLayer::clear_isupdatable() {
  isupdatable_ = false;
}
bool NeuralNetworkLayer::isupdatable() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.isUpdatable)
  return isupdatable_;
}
void NeuralNetworkLayer::set_isupdatable(bool value) {
  
  isupdatable_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkLayer.isUpdatable)
}

// .CoreML.Specification.ConvolutionLayerParams convolution = 100;
bool NeuralNetworkLayer::has_convolution() const {
  return layer_case() == kConvolution;
}
void NeuralNetworkLayer::set_has_convolution() {
  _oneof_case_[0] = kConvolution;
}
void NeuralNetworkLayer::clear_convolution() {
  if (has_convolution()) {
    delete layer_.convolution_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::ConvolutionLayerParams& NeuralNetworkLayer::convolution() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.convolution)
  return has_convolution()
      ? *layer_.convolution_
      : ::CoreML::Specification::ConvolutionLayerParams::default_instance();
}
::CoreML::Specification::ConvolutionLayerParams* NeuralNetworkLayer::mutable_convolution() {
  if (!has_convolution()) {
    clear_layer();
    set_has_convolution();
    layer_.convolution_ = new ::CoreML::Specification::ConvolutionLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.convolution)
  return layer_.convolution_;
}
::CoreML::Specification::ConvolutionLayerParams* NeuralNetworkLayer::release_convolution() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.convolution)
  if (has_convolution()) {
    clear_has_layer();
    ::CoreML::Specification::ConvolutionLayerParams* temp = layer_.convolution_;
    layer_.convolution_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_convolution(::CoreML::Specification::ConvolutionLayerParams* convolution) {
  clear_layer();
  if (convolution) {
    set_has_convolution();
    layer_.convolution_ = convolution;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.convolution)
}

// .CoreML.Specification.PoolingLayerParams pooling = 120;
bool NeuralNetworkLayer::has_pooling() const {
  return layer_case() == kPooling;
}
void NeuralNetworkLayer::set_has_pooling() {
  _oneof_case_[0] = kPooling;
}
void NeuralNetworkLayer::clear_pooling() {
  if (has_pooling()) {
    delete layer_.pooling_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::PoolingLayerParams& NeuralNetworkLayer::pooling() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.pooling)
  return has_pooling()
      ? *layer_.pooling_
      : ::CoreML::Specification::PoolingLayerParams::default_instance();
}
::CoreML::Specification::PoolingLayerParams* NeuralNetworkLayer::mutable_pooling() {
  if (!has_pooling()) {
    clear_layer();
    set_has_pooling();
    layer_.pooling_ = new ::CoreML::Specification::PoolingLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.pooling)
  return layer_.pooling_;
}
::CoreML::Specification::PoolingLayerParams* NeuralNetworkLayer::release_pooling() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.pooling)
  if (has_pooling()) {
    clear_has_layer();
    ::CoreML::Specification::PoolingLayerParams* temp = layer_.pooling_;
    layer_.pooling_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_pooling(::CoreML::Specification::PoolingLayerParams* pooling) {
  clear_layer();
  if (pooling) {
    set_has_pooling();
    layer_.pooling_ = pooling;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.pooling)
}

// .CoreML.Specification.ActivationParams activation = 130;
bool NeuralNetworkLayer::has_activation() const {
  return layer_case() == kActivation;
}
void NeuralNetworkLayer::set_has_activation() {
  _oneof_case_[0] = kActivation;
}
void NeuralNetworkLayer::clear_activation() {
  if (has_activation()) {
    delete layer_.activation_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::ActivationParams& NeuralNetworkLayer::activation() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.activation)
  return has_activation()
      ? *layer_.activation_
      : ::CoreML::Specification::ActivationParams::default_instance();
}
::CoreML::Specification::ActivationParams* NeuralNetworkLayer::mutable_activation() {
  if (!has_activation()) {
    clear_layer();
    set_has_activation();
    layer_.activation_ = new ::CoreML::Specification::ActivationParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.activation)
  return layer_.activation_;
}
::CoreML::Specification::ActivationParams* NeuralNetworkLayer::release_activation() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.activation)
  if (has_activation()) {
    clear_has_layer();
    ::CoreML::Specification::ActivationParams* temp = layer_.activation_;
    layer_.activation_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_activation(::CoreML::Specification::ActivationParams* activation) {
  clear_layer();
  if (activation) {
    set_has_activation();
    layer_.activation_ = activation;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.activation)
}

// .CoreML.Specification.InnerProductLayerParams innerProduct = 140;
bool NeuralNetworkLayer::has_innerproduct() const {
  return layer_case() == kInnerProduct;
}
void NeuralNetworkLayer::set_has_innerproduct() {
  _oneof_case_[0] = kInnerProduct;
}
void NeuralNetworkLayer::clear_innerproduct() {
  if (has_innerproduct()) {
    delete layer_.innerproduct_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::InnerProductLayerParams& NeuralNetworkLayer::innerproduct() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.innerProduct)
  return has_innerproduct()
      ? *layer_.innerproduct_
      : ::CoreML::Specification::InnerProductLayerParams::default_instance();
}
::CoreML::Specification::InnerProductLayerParams* NeuralNetworkLayer::mutable_innerproduct() {
  if (!has_innerproduct()) {
    clear_layer();
    set_has_innerproduct();
    layer_.innerproduct_ = new ::CoreML::Specification::InnerProductLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.innerProduct)
  return layer_.innerproduct_;
}
::CoreML::Specification::InnerProductLayerParams* NeuralNetworkLayer::release_innerproduct() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.innerProduct)
  if (has_innerproduct()) {
    clear_has_layer();
    ::CoreML::Specification::InnerProductLayerParams* temp = layer_.innerproduct_;
    layer_.innerproduct_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_innerproduct(::CoreML::Specification::InnerProductLayerParams* innerproduct) {
  clear_layer();
  if (innerproduct) {
    set_has_innerproduct();
    layer_.innerproduct_ = innerproduct;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.innerProduct)
}

// .CoreML.Specification.EmbeddingLayerParams embedding = 150;
bool NeuralNetworkLayer::has_embedding() const {
  return layer_case() == kEmbedding;
}
void NeuralNetworkLayer::set_has_embedding() {
  _oneof_case_[0] = kEmbedding;
}
void NeuralNetworkLayer::clear_embedding() {
  if (has_embedding()) {
    delete layer_.embedding_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::EmbeddingLayerParams& NeuralNetworkLayer::embedding() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.embedding)
  return has_embedding()
      ? *layer_.embedding_
      : ::CoreML::Specification::EmbeddingLayerParams::default_instance();
}
::CoreML::Specification::EmbeddingLayerParams* NeuralNetworkLayer::mutable_embedding() {
  if (!has_embedding()) {
    clear_layer();
    set_has_embedding();
    layer_.embedding_ = new ::CoreML::Specification::EmbeddingLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.embedding)
  return layer_.embedding_;
}
::CoreML::Specification::EmbeddingLayerParams* NeuralNetworkLayer::release_embedding() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.embedding)
  if (has_embedding()) {
    clear_has_layer();
    ::CoreML::Specification::EmbeddingLayerParams* temp = layer_.embedding_;
    layer_.embedding_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_embedding(::CoreML::Specification::EmbeddingLayerParams* embedding) {
  clear_layer();
  if (embedding) {
    set_has_embedding();
    layer_.embedding_ = embedding;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.embedding)
}

// .CoreML.Specification.BatchnormLayerParams batchnorm = 160;
bool NeuralNetworkLayer::has_batchnorm() const {
  return layer_case() == kBatchnorm;
}
void NeuralNetworkLayer::set_has_batchnorm() {
  _oneof_case_[0] = kBatchnorm;
}
void NeuralNetworkLayer::clear_batchnorm() {
  if (has_batchnorm()) {
    delete layer_.batchnorm_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::BatchnormLayerParams& NeuralNetworkLayer::batchnorm() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.batchnorm)
  return has_batchnorm()
      ? *layer_.batchnorm_
      : ::CoreML::Specification::BatchnormLayerParams::default_instance();
}
::CoreML::Specification::BatchnormLayerParams* NeuralNetworkLayer::mutable_batchnorm() {
  if (!has_batchnorm()) {
    clear_layer();
    set_has_batchnorm();
    layer_.batchnorm_ = new ::CoreML::Specification::BatchnormLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.batchnorm)
  return layer_.batchnorm_;
}
::CoreML::Specification::BatchnormLayerParams* NeuralNetworkLayer::release_batchnorm() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.batchnorm)
  if (has_batchnorm()) {
    clear_has_layer();
    ::CoreML::Specification::BatchnormLayerParams* temp = layer_.batchnorm_;
    layer_.batchnorm_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_batchnorm(::CoreML::Specification::BatchnormLayerParams* batchnorm) {
  clear_layer();
  if (batchnorm) {
    set_has_batchnorm();
    layer_.batchnorm_ = batchnorm;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.batchnorm)
}

// .CoreML.Specification.MeanVarianceNormalizeLayerParams mvn = 165;
bool NeuralNetworkLayer::has_mvn() const {
  return layer_case() == kMvn;
}
void NeuralNetworkLayer::set_has_mvn() {
  _oneof_case_[0] = kMvn;
}
void NeuralNetworkLayer::clear_mvn() {
  if (has_mvn()) {
    delete layer_.mvn_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::MeanVarianceNormalizeLayerParams& NeuralNetworkLayer::mvn() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.mvn)
  return has_mvn()
      ? *layer_.mvn_
      : ::CoreML::Specification::MeanVarianceNormalizeLayerParams::default_instance();
}
::CoreML::Specification::MeanVarianceNormalizeLayerParams* NeuralNetworkLayer::mutable_mvn() {
  if (!has_mvn()) {
    clear_layer();
    set_has_mvn();
    layer_.mvn_ = new ::CoreML::Specification::MeanVarianceNormalizeLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.mvn)
  return layer_.mvn_;
}
::CoreML::Specification::MeanVarianceNormalizeLayerParams* NeuralNetworkLayer::release_mvn() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.mvn)
  if (has_mvn()) {
    clear_has_layer();
    ::CoreML::Specification::MeanVarianceNormalizeLayerParams* temp = layer_.mvn_;
    layer_.mvn_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_mvn(::CoreML::Specification::MeanVarianceNormalizeLayerParams* mvn) {
  clear_layer();
  if (mvn) {
    set_has_mvn();
    layer_.mvn_ = mvn;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.mvn)
}

// .CoreML.Specification.L2NormalizeLayerParams l2normalize = 170;
bool NeuralNetworkLayer::has_l2normalize() const {
  return layer_case() == kL2Normalize;
}
void NeuralNetworkLayer::set_has_l2normalize() {
  _oneof_case_[0] = kL2Normalize;
}
void NeuralNetworkLayer::clear_l2normalize() {
  if (has_l2normalize()) {
    delete layer_.l2normalize_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::L2NormalizeLayerParams& NeuralNetworkLayer::l2normalize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.l2normalize)
  return has_l2normalize()
      ? *layer_.l2normalize_
      : ::CoreML::Specification::L2NormalizeLayerParams::default_instance();
}
::CoreML::Specification::L2NormalizeLayerParams* NeuralNetworkLayer::mutable_l2normalize() {
  if (!has_l2normalize()) {
    clear_layer();
    set_has_l2normalize();
    layer_.l2normalize_ = new ::CoreML::Specification::L2NormalizeLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.l2normalize)
  return layer_.l2normalize_;
}
::CoreML::Specification::L2NormalizeLayerParams* NeuralNetworkLayer::release_l2normalize() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.l2normalize)
  if (has_l2normalize()) {
    clear_has_layer();
    ::CoreML::Specification::L2NormalizeLayerParams* temp = layer_.l2normalize_;
    layer_.l2normalize_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_l2normalize(::CoreML::Specification::L2NormalizeLayerParams* l2normalize) {
  clear_layer();
  if (l2normalize) {
    set_has_l2normalize();
    layer_.l2normalize_ = l2normalize;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.l2normalize)
}

// .CoreML.Specification.SoftmaxLayerParams softmax = 175;
bool NeuralNetworkLayer::has_softmax() const {
  return layer_case() == kSoftmax;
}
void NeuralNetworkLayer::set_has_softmax() {
  _oneof_case_[0] = kSoftmax;
}
void NeuralNetworkLayer::clear_softmax() {
  if (has_softmax()) {
    delete layer_.softmax_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::SoftmaxLayerParams& NeuralNetworkLayer::softmax() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.softmax)
  return has_softmax()
      ? *layer_.softmax_
      : ::CoreML::Specification::SoftmaxLayerParams::default_instance();
}
::CoreML::Specification::SoftmaxLayerParams* NeuralNetworkLayer::mutable_softmax() {
  if (!has_softmax()) {
    clear_layer();
    set_has_softmax();
    layer_.softmax_ = new ::CoreML::Specification::SoftmaxLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.softmax)
  return layer_.softmax_;
}
::CoreML::Specification::SoftmaxLayerParams* NeuralNetworkLayer::release_softmax() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.softmax)
  if (has_softmax()) {
    clear_has_layer();
    ::CoreML::Specification::SoftmaxLayerParams* temp = layer_.softmax_;
    layer_.softmax_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_softmax(::CoreML::Specification::SoftmaxLayerParams* softmax) {
  clear_layer();
  if (softmax) {
    set_has_softmax();
    layer_.softmax_ = softmax;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.softmax)
}

// .CoreML.Specification.LRNLayerParams lrn = 180;
bool NeuralNetworkLayer::has_lrn() const {
  return layer_case() == kLrn;
}
void NeuralNetworkLayer::set_has_lrn() {
  _oneof_case_[0] = kLrn;
}
void NeuralNetworkLayer::clear_lrn() {
  if (has_lrn()) {
    delete layer_.lrn_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::LRNLayerParams& NeuralNetworkLayer::lrn() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.lrn)
  return has_lrn()
      ? *layer_.lrn_
      : ::CoreML::Specification::LRNLayerParams::default_instance();
}
::CoreML::Specification::LRNLayerParams* NeuralNetworkLayer::mutable_lrn() {
  if (!has_lrn()) {
    clear_layer();
    set_has_lrn();
    layer_.lrn_ = new ::CoreML::Specification::LRNLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.lrn)
  return layer_.lrn_;
}
::CoreML::Specification::LRNLayerParams* NeuralNetworkLayer::release_lrn() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.lrn)
  if (has_lrn()) {
    clear_has_layer();
    ::CoreML::Specification::LRNLayerParams* temp = layer_.lrn_;
    layer_.lrn_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_lrn(::CoreML::Specification::LRNLayerParams* lrn) {
  clear_layer();
  if (lrn) {
    set_has_lrn();
    layer_.lrn_ = lrn;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.lrn)
}

// .CoreML.Specification.CropLayerParams crop = 190;
bool NeuralNetworkLayer::has_crop() const {
  return layer_case() == kCrop;
}
void NeuralNetworkLayer::set_has_crop() {
  _oneof_case_[0] = kCrop;
}
void NeuralNetworkLayer::clear_crop() {
  if (has_crop()) {
    delete layer_.crop_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::CropLayerParams& NeuralNetworkLayer::crop() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.crop)
  return has_crop()
      ? *layer_.crop_
      : ::CoreML::Specification::CropLayerParams::default_instance();
}
::CoreML::Specification::CropLayerParams* NeuralNetworkLayer::mutable_crop() {
  if (!has_crop()) {
    clear_layer();
    set_has_crop();
    layer_.crop_ = new ::CoreML::Specification::CropLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.crop)
  return layer_.crop_;
}
::CoreML::Specification::CropLayerParams* NeuralNetworkLayer::release_crop() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.crop)
  if (has_crop()) {
    clear_has_layer();
    ::CoreML::Specification::CropLayerParams* temp = layer_.crop_;
    layer_.crop_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_crop(::CoreML::Specification::CropLayerParams* crop) {
  clear_layer();
  if (crop) {
    set_has_crop();
    layer_.crop_ = crop;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.crop)
}

// .CoreML.Specification.PaddingLayerParams padding = 200;
bool NeuralNetworkLayer::has_padding() const {
  return layer_case() == kPadding;
}
void NeuralNetworkLayer::set_has_padding() {
  _oneof_case_[0] = kPadding;
}
void NeuralNetworkLayer::clear_padding() {
  if (has_padding()) {
    delete layer_.padding_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::PaddingLayerParams& NeuralNetworkLayer::padding() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.padding)
  return has_padding()
      ? *layer_.padding_
      : ::CoreML::Specification::PaddingLayerParams::default_instance();
}
::CoreML::Specification::PaddingLayerParams* NeuralNetworkLayer::mutable_padding() {
  if (!has_padding()) {
    clear_layer();
    set_has_padding();
    layer_.padding_ = new ::CoreML::Specification::PaddingLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.padding)
  return layer_.padding_;
}
::CoreML::Specification::PaddingLayerParams* NeuralNetworkLayer::release_padding() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.padding)
  if (has_padding()) {
    clear_has_layer();
    ::CoreML::Specification::PaddingLayerParams* temp = layer_.padding_;
    layer_.padding_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_padding(::CoreML::Specification::PaddingLayerParams* padding) {
  clear_layer();
  if (padding) {
    set_has_padding();
    layer_.padding_ = padding;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.padding)
}

// .CoreML.Specification.UpsampleLayerParams upsample = 210;
bool NeuralNetworkLayer::has_upsample() const {
  return layer_case() == kUpsample;
}
void NeuralNetworkLayer::set_has_upsample() {
  _oneof_case_[0] = kUpsample;
}
void NeuralNetworkLayer::clear_upsample() {
  if (has_upsample()) {
    delete layer_.upsample_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::UpsampleLayerParams& NeuralNetworkLayer::upsample() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.upsample)
  return has_upsample()
      ? *layer_.upsample_
      : ::CoreML::Specification::UpsampleLayerParams::default_instance();
}
::CoreML::Specification::UpsampleLayerParams* NeuralNetworkLayer::mutable_upsample() {
  if (!has_upsample()) {
    clear_layer();
    set_has_upsample();
    layer_.upsample_ = new ::CoreML::Specification::UpsampleLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.upsample)
  return layer_.upsample_;
}
::CoreML::Specification::UpsampleLayerParams* NeuralNetworkLayer::release_upsample() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.upsample)
  if (has_upsample()) {
    clear_has_layer();
    ::CoreML::Specification::UpsampleLayerParams* temp = layer_.upsample_;
    layer_.upsample_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_upsample(::CoreML::Specification::UpsampleLayerParams* upsample) {
  clear_layer();
  if (upsample) {
    set_has_upsample();
    layer_.upsample_ = upsample;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.upsample)
}

// .CoreML.Specification.ResizeBilinearLayerParams resizeBilinear = 211;
bool NeuralNetworkLayer::has_resizebilinear() const {
  return layer_case() == kResizeBilinear;
}
void NeuralNetworkLayer::set_has_resizebilinear() {
  _oneof_case_[0] = kResizeBilinear;
}
void NeuralNetworkLayer::clear_resizebilinear() {
  if (has_resizebilinear()) {
    delete layer_.resizebilinear_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::ResizeBilinearLayerParams& NeuralNetworkLayer::resizebilinear() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.resizeBilinear)
  return has_resizebilinear()
      ? *layer_.resizebilinear_
      : ::CoreML::Specification::ResizeBilinearLayerParams::default_instance();
}
::CoreML::Specification::ResizeBilinearLayerParams* NeuralNetworkLayer::mutable_resizebilinear() {
  if (!has_resizebilinear()) {
    clear_layer();
    set_has_resizebilinear();
    layer_.resizebilinear_ = new ::CoreML::Specification::ResizeBilinearLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.resizeBilinear)
  return layer_.resizebilinear_;
}
::CoreML::Specification::ResizeBilinearLayerParams* NeuralNetworkLayer::release_resizebilinear() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.resizeBilinear)
  if (has_resizebilinear()) {
    clear_has_layer();
    ::CoreML::Specification::ResizeBilinearLayerParams* temp = layer_.resizebilinear_;
    layer_.resizebilinear_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_resizebilinear(::CoreML::Specification::ResizeBilinearLayerParams* resizebilinear) {
  clear_layer();
  if (resizebilinear) {
    set_has_resizebilinear();
    layer_.resizebilinear_ = resizebilinear;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.resizeBilinear)
}

// .CoreML.Specification.CropResizeLayerParams cropResize = 212;
bool NeuralNetworkLayer::has_cropresize() const {
  return layer_case() == kCropResize;
}
void NeuralNetworkLayer::set_has_cropresize() {
  _oneof_case_[0] = kCropResize;
}
void NeuralNetworkLayer::clear_cropresize() {
  if (has_cropresize()) {
    delete layer_.cropresize_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::CropResizeLayerParams& NeuralNetworkLayer::cropresize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.cropResize)
  return has_cropresize()
      ? *layer_.cropresize_
      : ::CoreML::Specification::CropResizeLayerParams::default_instance();
}
::CoreML::Specification::CropResizeLayerParams* NeuralNetworkLayer::mutable_cropresize() {
  if (!has_cropresize()) {
    clear_layer();
    set_has_cropresize();
    layer_.cropresize_ = new ::CoreML::Specification::CropResizeLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.cropResize)
  return layer_.cropresize_;
}
::CoreML::Specification::CropResizeLayerParams* NeuralNetworkLayer::release_cropresize() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.cropResize)
  if (has_cropresize()) {
    clear_has_layer();
    ::CoreML::Specification::CropResizeLayerParams* temp = layer_.cropresize_;
    layer_.cropresize_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_cropresize(::CoreML::Specification::CropResizeLayerParams* cropresize) {
  clear_layer();
  if (cropresize) {
    set_has_cropresize();
    layer_.cropresize_ = cropresize;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.cropResize)
}

// .CoreML.Specification.UnaryFunctionLayerParams unary = 220;
bool NeuralNetworkLayer::has_unary() const {
  return layer_case() == kUnary;
}
void NeuralNetworkLayer::set_has_unary() {
  _oneof_case_[0] = kUnary;
}
void NeuralNetworkLayer::clear_unary() {
  if (has_unary()) {
    delete layer_.unary_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::UnaryFunctionLayerParams& NeuralNetworkLayer::unary() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.unary)
  return has_unary()
      ? *layer_.unary_
      : ::CoreML::Specification::UnaryFunctionLayerParams::default_instance();
}
::CoreML::Specification::UnaryFunctionLayerParams* NeuralNetworkLayer::mutable_unary() {
  if (!has_unary()) {
    clear_layer();
    set_has_unary();
    layer_.unary_ = new ::CoreML::Specification::UnaryFunctionLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.unary)
  return layer_.unary_;
}
::CoreML::Specification::UnaryFunctionLayerParams* NeuralNetworkLayer::release_unary() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.unary)
  if (has_unary()) {
    clear_has_layer();
    ::CoreML::Specification::UnaryFunctionLayerParams* temp = layer_.unary_;
    layer_.unary_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_unary(::CoreML::Specification::UnaryFunctionLayerParams* unary) {
  clear_layer();
  if (unary) {
    set_has_unary();
    layer_.unary_ = unary;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.unary)
}

// .CoreML.Specification.AddLayerParams add = 230;
bool NeuralNetworkLayer::has_add() const {
  return layer_case() == kAdd;
}
void NeuralNetworkLayer::set_has_add() {
  _oneof_case_[0] = kAdd;
}
void NeuralNetworkLayer::clear_add() {
  if (has_add()) {
    delete layer_.add_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::AddLayerParams& NeuralNetworkLayer::add() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.add)
  return has_add()
      ? *layer_.add_
      : ::CoreML::Specification::AddLayerParams::default_instance();
}
::CoreML::Specification::AddLayerParams* NeuralNetworkLayer::mutable_add() {
  if (!has_add()) {
    clear_layer();
    set_has_add();
    layer_.add_ = new ::CoreML::Specification::AddLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.add)
  return layer_.add_;
}
::CoreML::Specification::AddLayerParams* NeuralNetworkLayer::release_add() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.add)
  if (has_add()) {
    clear_has_layer();
    ::CoreML::Specification::AddLayerParams* temp = layer_.add_;
    layer_.add_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_add(::CoreML::Specification::AddLayerParams* add) {
  clear_layer();
  if (add) {
    set_has_add();
    layer_.add_ = add;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.add)
}

// .CoreML.Specification.MultiplyLayerParams multiply = 231;
bool NeuralNetworkLayer::has_multiply() const {
  return layer_case() == kMultiply;
}
void NeuralNetworkLayer::set_has_multiply() {
  _oneof_case_[0] = kMultiply;
}
void NeuralNetworkLayer::clear_multiply() {
  if (has_multiply()) {
    delete layer_.multiply_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::MultiplyLayerParams& NeuralNetworkLayer::multiply() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.multiply)
  return has_multiply()
      ? *layer_.multiply_
      : ::CoreML::Specification::MultiplyLayerParams::default_instance();
}
::CoreML::Specification::MultiplyLayerParams* NeuralNetworkLayer::mutable_multiply() {
  if (!has_multiply()) {
    clear_layer();
    set_has_multiply();
    layer_.multiply_ = new ::CoreML::Specification::MultiplyLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.multiply)
  return layer_.multiply_;
}
::CoreML::Specification::MultiplyLayerParams* NeuralNetworkLayer::release_multiply() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.multiply)
  if (has_multiply()) {
    clear_has_layer();
    ::CoreML::Specification::MultiplyLayerParams* temp = layer_.multiply_;
    layer_.multiply_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_multiply(::CoreML::Specification::MultiplyLayerParams* multiply) {
  clear_layer();
  if (multiply) {
    set_has_multiply();
    layer_.multiply_ = multiply;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.multiply)
}

// .CoreML.Specification.AverageLayerParams average = 240;
bool NeuralNetworkLayer::has_average() const {
  return layer_case() == kAverage;
}
void NeuralNetworkLayer::set_has_average() {
  _oneof_case_[0] = kAverage;
}
void NeuralNetworkLayer::clear_average() {
  if (has_average()) {
    delete layer_.average_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::AverageLayerParams& NeuralNetworkLayer::average() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.average)
  return has_average()
      ? *layer_.average_
      : ::CoreML::Specification::AverageLayerParams::default_instance();
}
::CoreML::Specification::AverageLayerParams* NeuralNetworkLayer::mutable_average() {
  if (!has_average()) {
    clear_layer();
    set_has_average();
    layer_.average_ = new ::CoreML::Specification::AverageLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.average)
  return layer_.average_;
}
::CoreML::Specification::AverageLayerParams* NeuralNetworkLayer::release_average() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.average)
  if (has_average()) {
    clear_has_layer();
    ::CoreML::Specification::AverageLayerParams* temp = layer_.average_;
    layer_.average_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_average(::CoreML::Specification::AverageLayerParams* average) {
  clear_layer();
  if (average) {
    set_has_average();
    layer_.average_ = average;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.average)
}

// .CoreML.Specification.ScaleLayerParams scale = 245;
bool NeuralNetworkLayer::has_scale() const {
  return layer_case() == kScale;
}
void NeuralNetworkLayer::set_has_scale() {
  _oneof_case_[0] = kScale;
}
void NeuralNetworkLayer::clear_scale() {
  if (has_scale()) {
    delete layer_.scale_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::ScaleLayerParams& NeuralNetworkLayer::scale() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.scale)
  return has_scale()
      ? *layer_.scale_
      : ::CoreML::Specification::ScaleLayerParams::default_instance();
}
::CoreML::Specification::ScaleLayerParams* NeuralNetworkLayer::mutable_scale() {
  if (!has_scale()) {
    clear_layer();
    set_has_scale();
    layer_.scale_ = new ::CoreML::Specification::ScaleLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.scale)
  return layer_.scale_;
}
::CoreML::Specification::ScaleLayerParams* NeuralNetworkLayer::release_scale() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.scale)
  if (has_scale()) {
    clear_has_layer();
    ::CoreML::Specification::ScaleLayerParams* temp = layer_.scale_;
    layer_.scale_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_scale(::CoreML::Specification::ScaleLayerParams* scale) {
  clear_layer();
  if (scale) {
    set_has_scale();
    layer_.scale_ = scale;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.scale)
}

// .CoreML.Specification.BiasLayerParams bias = 250;
bool NeuralNetworkLayer::has_bias() const {
  return layer_case() == kBias;
}
void NeuralNetworkLayer::set_has_bias() {
  _oneof_case_[0] = kBias;
}
void NeuralNetworkLayer::clear_bias() {
  if (has_bias()) {
    delete layer_.bias_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::BiasLayerParams& NeuralNetworkLayer::bias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.bias)
  return has_bias()
      ? *layer_.bias_
      : ::CoreML::Specification::BiasLayerParams::default_instance();
}
::CoreML::Specification::BiasLayerParams* NeuralNetworkLayer::mutable_bias() {
  if (!has_bias()) {
    clear_layer();
    set_has_bias();
    layer_.bias_ = new ::CoreML::Specification::BiasLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.bias)
  return layer_.bias_;
}
::CoreML::Specification::BiasLayerParams* NeuralNetworkLayer::release_bias() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.bias)
  if (has_bias()) {
    clear_has_layer();
    ::CoreML::Specification::BiasLayerParams* temp = layer_.bias_;
    layer_.bias_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_bias(::CoreML::Specification::BiasLayerParams* bias) {
  clear_layer();
  if (bias) {
    set_has_bias();
    layer_.bias_ = bias;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.bias)
}

// .CoreML.Specification.MaxLayerParams max = 260;
bool NeuralNetworkLayer::has_max() const {
  return layer_case() == kMax;
}
void NeuralNetworkLayer::set_has_max() {
  _oneof_case_[0] = kMax;
}
void NeuralNetworkLayer::clear_max() {
  if (has_max()) {
    delete layer_.max_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::MaxLayerParams& NeuralNetworkLayer::max() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.max)
  return has_max()
      ? *layer_.max_
      : ::CoreML::Specification::MaxLayerParams::default_instance();
}
::CoreML::Specification::MaxLayerParams* NeuralNetworkLayer::mutable_max() {
  if (!has_max()) {
    clear_layer();
    set_has_max();
    layer_.max_ = new ::CoreML::Specification::MaxLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.max)
  return layer_.max_;
}
::CoreML::Specification::MaxLayerParams* NeuralNetworkLayer::release_max() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.max)
  if (has_max()) {
    clear_has_layer();
    ::CoreML::Specification::MaxLayerParams* temp = layer_.max_;
    layer_.max_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_max(::CoreML::Specification::MaxLayerParams* max) {
  clear_layer();
  if (max) {
    set_has_max();
    layer_.max_ = max;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.max)
}

// .CoreML.Specification.MinLayerParams min = 261;
bool NeuralNetworkLayer::has_min() const {
  return layer_case() == kMin;
}
void NeuralNetworkLayer::set_has_min() {
  _oneof_case_[0] = kMin;
}
void NeuralNetworkLayer::clear_min() {
  if (has_min()) {
    delete layer_.min_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::MinLayerParams& NeuralNetworkLayer::min() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.min)
  return has_min()
      ? *layer_.min_
      : ::CoreML::Specification::MinLayerParams::default_instance();
}
::CoreML::Specification::MinLayerParams* NeuralNetworkLayer::mutable_min() {
  if (!has_min()) {
    clear_layer();
    set_has_min();
    layer_.min_ = new ::CoreML::Specification::MinLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.min)
  return layer_.min_;
}
::CoreML::Specification::MinLayerParams* NeuralNetworkLayer::release_min() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.min)
  if (has_min()) {
    clear_has_layer();
    ::CoreML::Specification::MinLayerParams* temp = layer_.min_;
    layer_.min_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_min(::CoreML::Specification::MinLayerParams* min) {
  clear_layer();
  if (min) {
    set_has_min();
    layer_.min_ = min;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.min)
}

// .CoreML.Specification.DotProductLayerParams dot = 270;
bool NeuralNetworkLayer::has_dot() const {
  return layer_case() == kDot;
}
void NeuralNetworkLayer::set_has_dot() {
  _oneof_case_[0] = kDot;
}
void NeuralNetworkLayer::clear_dot() {
  if (has_dot()) {
    delete layer_.dot_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::DotProductLayerParams& NeuralNetworkLayer::dot() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.dot)
  return has_dot()
      ? *layer_.dot_
      : ::CoreML::Specification::DotProductLayerParams::default_instance();
}
::CoreML::Specification::DotProductLayerParams* NeuralNetworkLayer::mutable_dot() {
  if (!has_dot()) {
    clear_layer();
    set_has_dot();
    layer_.dot_ = new ::CoreML::Specification::DotProductLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.dot)
  return layer_.dot_;
}
::CoreML::Specification::DotProductLayerParams* NeuralNetworkLayer::release_dot() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.dot)
  if (has_dot()) {
    clear_has_layer();
    ::CoreML::Specification::DotProductLayerParams* temp = layer_.dot_;
    layer_.dot_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_dot(::CoreML::Specification::DotProductLayerParams* dot) {
  clear_layer();
  if (dot) {
    set_has_dot();
    layer_.dot_ = dot;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.dot)
}

// .CoreML.Specification.ReduceLayerParams reduce = 280;
bool NeuralNetworkLayer::has_reduce() const {
  return layer_case() == kReduce;
}
void NeuralNetworkLayer::set_has_reduce() {
  _oneof_case_[0] = kReduce;
}
void NeuralNetworkLayer::clear_reduce() {
  if (has_reduce()) {
    delete layer_.reduce_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::ReduceLayerParams& NeuralNetworkLayer::reduce() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.reduce)
  return has_reduce()
      ? *layer_.reduce_
      : ::CoreML::Specification::ReduceLayerParams::default_instance();
}
::CoreML::Specification::ReduceLayerParams* NeuralNetworkLayer::mutable_reduce() {
  if (!has_reduce()) {
    clear_layer();
    set_has_reduce();
    layer_.reduce_ = new ::CoreML::Specification::ReduceLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.reduce)
  return layer_.reduce_;
}
::CoreML::Specification::ReduceLayerParams* NeuralNetworkLayer::release_reduce() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.reduce)
  if (has_reduce()) {
    clear_has_layer();
    ::CoreML::Specification::ReduceLayerParams* temp = layer_.reduce_;
    layer_.reduce_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_reduce(::CoreML::Specification::ReduceLayerParams* reduce) {
  clear_layer();
  if (reduce) {
    set_has_reduce();
    layer_.reduce_ = reduce;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.reduce)
}

// .CoreML.Specification.LoadConstantLayerParams loadConstant = 290;
bool NeuralNetworkLayer::has_loadconstant() const {
  return layer_case() == kLoadConstant;
}
void NeuralNetworkLayer::set_has_loadconstant() {
  _oneof_case_[0] = kLoadConstant;
}
void NeuralNetworkLayer::clear_loadconstant() {
  if (has_loadconstant()) {
    delete layer_.loadconstant_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::LoadConstantLayerParams& NeuralNetworkLayer::loadconstant() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.loadConstant)
  return has_loadconstant()
      ? *layer_.loadconstant_
      : ::CoreML::Specification::LoadConstantLayerParams::default_instance();
}
::CoreML::Specification::LoadConstantLayerParams* NeuralNetworkLayer::mutable_loadconstant() {
  if (!has_loadconstant()) {
    clear_layer();
    set_has_loadconstant();
    layer_.loadconstant_ = new ::CoreML::Specification::LoadConstantLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.loadConstant)
  return layer_.loadconstant_;
}
::CoreML::Specification::LoadConstantLayerParams* NeuralNetworkLayer::release_loadconstant() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.loadConstant)
  if (has_loadconstant()) {
    clear_has_layer();
    ::CoreML::Specification::LoadConstantLayerParams* temp = layer_.loadconstant_;
    layer_.loadconstant_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_loadconstant(::CoreML::Specification::LoadConstantLayerParams* loadconstant) {
  clear_layer();
  if (loadconstant) {
    set_has_loadconstant();
    layer_.loadconstant_ = loadconstant;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.loadConstant)
}

// .CoreML.Specification.ReshapeLayerParams reshape = 300;
bool NeuralNetworkLayer::has_reshape() const {
  return layer_case() == kReshape;
}
void NeuralNetworkLayer::set_has_reshape() {
  _oneof_case_[0] = kReshape;
}
void NeuralNetworkLayer::clear_reshape() {
  if (has_reshape()) {
    delete layer_.reshape_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::ReshapeLayerParams& NeuralNetworkLayer::reshape() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.reshape)
  return has_reshape()
      ? *layer_.reshape_
      : ::CoreML::Specification::ReshapeLayerParams::default_instance();
}
::CoreML::Specification::ReshapeLayerParams* NeuralNetworkLayer::mutable_reshape() {
  if (!has_reshape()) {
    clear_layer();
    set_has_reshape();
    layer_.reshape_ = new ::CoreML::Specification::ReshapeLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.reshape)
  return layer_.reshape_;
}
::CoreML::Specification::ReshapeLayerParams* NeuralNetworkLayer::release_reshape() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.reshape)
  if (has_reshape()) {
    clear_has_layer();
    ::CoreML::Specification::ReshapeLayerParams* temp = layer_.reshape_;
    layer_.reshape_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_reshape(::CoreML::Specification::ReshapeLayerParams* reshape) {
  clear_layer();
  if (reshape) {
    set_has_reshape();
    layer_.reshape_ = reshape;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.reshape)
}

// .CoreML.Specification.FlattenLayerParams flatten = 301;
bool NeuralNetworkLayer::has_flatten() const {
  return layer_case() == kFlatten;
}
void NeuralNetworkLayer::set_has_flatten() {
  _oneof_case_[0] = kFlatten;
}
void NeuralNetworkLayer::clear_flatten() {
  if (has_flatten()) {
    delete layer_.flatten_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::FlattenLayerParams& NeuralNetworkLayer::flatten() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.flatten)
  return has_flatten()
      ? *layer_.flatten_
      : ::CoreML::Specification::FlattenLayerParams::default_instance();
}
::CoreML::Specification::FlattenLayerParams* NeuralNetworkLayer::mutable_flatten() {
  if (!has_flatten()) {
    clear_layer();
    set_has_flatten();
    layer_.flatten_ = new ::CoreML::Specification::FlattenLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.flatten)
  return layer_.flatten_;
}
::CoreML::Specification::FlattenLayerParams* NeuralNetworkLayer::release_flatten() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.flatten)
  if (has_flatten()) {
    clear_has_layer();
    ::CoreML::Specification::FlattenLayerParams* temp = layer_.flatten_;
    layer_.flatten_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_flatten(::CoreML::Specification::FlattenLayerParams* flatten) {
  clear_layer();
  if (flatten) {
    set_has_flatten();
    layer_.flatten_ = flatten;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.flatten)
}

// .CoreML.Specification.PermuteLayerParams permute = 310;
bool NeuralNetworkLayer::has_permute() const {
  return layer_case() == kPermute;
}
void NeuralNetworkLayer::set_has_permute() {
  _oneof_case_[0] = kPermute;
}
void NeuralNetworkLayer::clear_permute() {
  if (has_permute()) {
    delete layer_.permute_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::PermuteLayerParams& NeuralNetworkLayer::permute() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.permute)
  return has_permute()
      ? *layer_.permute_
      : ::CoreML::Specification::PermuteLayerParams::default_instance();
}
::CoreML::Specification::PermuteLayerParams* NeuralNetworkLayer::mutable_permute() {
  if (!has_permute()) {
    clear_layer();
    set_has_permute();
    layer_.permute_ = new ::CoreML::Specification::PermuteLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.permute)
  return layer_.permute_;
}
::CoreML::Specification::PermuteLayerParams* NeuralNetworkLayer::release_permute() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.permute)
  if (has_permute()) {
    clear_has_layer();
    ::CoreML::Specification::PermuteLayerParams* temp = layer_.permute_;
    layer_.permute_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_permute(::CoreML::Specification::PermuteLayerParams* permute) {
  clear_layer();
  if (permute) {
    set_has_permute();
    layer_.permute_ = permute;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.permute)
}

// .CoreML.Specification.ConcatLayerParams concat = 320;
bool NeuralNetworkLayer::has_concat() const {
  return layer_case() == kConcat;
}
void NeuralNetworkLayer::set_has_concat() {
  _oneof_case_[0] = kConcat;
}
void NeuralNetworkLayer::clear_concat() {
  if (has_concat()) {
    delete layer_.concat_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::ConcatLayerParams& NeuralNetworkLayer::concat() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.concat)
  return has_concat()
      ? *layer_.concat_
      : ::CoreML::Specification::ConcatLayerParams::default_instance();
}
::CoreML::Specification::ConcatLayerParams* NeuralNetworkLayer::mutable_concat() {
  if (!has_concat()) {
    clear_layer();
    set_has_concat();
    layer_.concat_ = new ::CoreML::Specification::ConcatLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.concat)
  return layer_.concat_;
}
::CoreML::Specification::ConcatLayerParams* NeuralNetworkLayer::release_concat() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.concat)
  if (has_concat()) {
    clear_has_layer();
    ::CoreML::Specification::ConcatLayerParams* temp = layer_.concat_;
    layer_.concat_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_concat(::CoreML::Specification::ConcatLayerParams* concat) {
  clear_layer();
  if (concat) {
    set_has_concat();
    layer_.concat_ = concat;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.concat)
}

// .CoreML.Specification.SplitLayerParams split = 330;
bool NeuralNetworkLayer::has_split() const {
  return layer_case() == kSplit;
}
void NeuralNetworkLayer::set_has_split() {
  _oneof_case_[0] = kSplit;
}
void NeuralNetworkLayer::clear_split() {
  if (has_split()) {
    delete layer_.split_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::SplitLayerParams& NeuralNetworkLayer::split() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.split)
  return has_split()
      ? *layer_.split_
      : ::CoreML::Specification::SplitLayerParams::default_instance();
}
::CoreML::Specification::SplitLayerParams* NeuralNetworkLayer::mutable_split() {
  if (!has_split()) {
    clear_layer();
    set_has_split();
    layer_.split_ = new ::CoreML::Specification::SplitLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.split)
  return layer_.split_;
}
::CoreML::Specification::SplitLayerParams* NeuralNetworkLayer::release_split() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.split)
  if (has_split()) {
    clear_has_layer();
    ::CoreML::Specification::SplitLayerParams* temp = layer_.split_;
    layer_.split_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_split(::CoreML::Specification::SplitLayerParams* split) {
  clear_layer();
  if (split) {
    set_has_split();
    layer_.split_ = split;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.split)
}

// .CoreML.Specification.SequenceRepeatLayerParams sequenceRepeat = 340;
bool NeuralNetworkLayer::has_sequencerepeat() const {
  return layer_case() == kSequenceRepeat;
}
void NeuralNetworkLayer::set_has_sequencerepeat() {
  _oneof_case_[0] = kSequenceRepeat;
}
void NeuralNetworkLayer::clear_sequencerepeat() {
  if (has_sequencerepeat()) {
    delete layer_.sequencerepeat_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::SequenceRepeatLayerParams& NeuralNetworkLayer::sequencerepeat() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.sequenceRepeat)
  return has_sequencerepeat()
      ? *layer_.sequencerepeat_
      : ::CoreML::Specification::SequenceRepeatLayerParams::default_instance();
}
::CoreML::Specification::SequenceRepeatLayerParams* NeuralNetworkLayer::mutable_sequencerepeat() {
  if (!has_sequencerepeat()) {
    clear_layer();
    set_has_sequencerepeat();
    layer_.sequencerepeat_ = new ::CoreML::Specification::SequenceRepeatLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.sequenceRepeat)
  return layer_.sequencerepeat_;
}
::CoreML::Specification::SequenceRepeatLayerParams* NeuralNetworkLayer::release_sequencerepeat() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.sequenceRepeat)
  if (has_sequencerepeat()) {
    clear_has_layer();
    ::CoreML::Specification::SequenceRepeatLayerParams* temp = layer_.sequencerepeat_;
    layer_.sequencerepeat_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_sequencerepeat(::CoreML::Specification::SequenceRepeatLayerParams* sequencerepeat) {
  clear_layer();
  if (sequencerepeat) {
    set_has_sequencerepeat();
    layer_.sequencerepeat_ = sequencerepeat;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.sequenceRepeat)
}

// .CoreML.Specification.ReorganizeDataLayerParams reorganizeData = 345;
bool NeuralNetworkLayer::has_reorganizedata() const {
  return layer_case() == kReorganizeData;
}
void NeuralNetworkLayer::set_has_reorganizedata() {
  _oneof_case_[0] = kReorganizeData;
}
void NeuralNetworkLayer::clear_reorganizedata() {
  if (has_reorganizedata()) {
    delete layer_.reorganizedata_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::ReorganizeDataLayerParams& NeuralNetworkLayer::reorganizedata() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.reorganizeData)
  return has_reorganizedata()
      ? *layer_.reorganizedata_
      : ::CoreML::Specification::ReorganizeDataLayerParams::default_instance();
}
::CoreML::Specification::ReorganizeDataLayerParams* NeuralNetworkLayer::mutable_reorganizedata() {
  if (!has_reorganizedata()) {
    clear_layer();
    set_has_reorganizedata();
    layer_.reorganizedata_ = new ::CoreML::Specification::ReorganizeDataLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.reorganizeData)
  return layer_.reorganizedata_;
}
::CoreML::Specification::ReorganizeDataLayerParams* NeuralNetworkLayer::release_reorganizedata() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.reorganizeData)
  if (has_reorganizedata()) {
    clear_has_layer();
    ::CoreML::Specification::ReorganizeDataLayerParams* temp = layer_.reorganizedata_;
    layer_.reorganizedata_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_reorganizedata(::CoreML::Specification::ReorganizeDataLayerParams* reorganizedata) {
  clear_layer();
  if (reorganizedata) {
    set_has_reorganizedata();
    layer_.reorganizedata_ = reorganizedata;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.reorganizeData)
}

// .CoreML.Specification.SliceLayerParams slice = 350;
bool NeuralNetworkLayer::has_slice() const {
  return layer_case() == kSlice;
}
void NeuralNetworkLayer::set_has_slice() {
  _oneof_case_[0] = kSlice;
}
void NeuralNetworkLayer::clear_slice() {
  if (has_slice()) {
    delete layer_.slice_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::SliceLayerParams& NeuralNetworkLayer::slice() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.slice)
  return has_slice()
      ? *layer_.slice_
      : ::CoreML::Specification::SliceLayerParams::default_instance();
}
::CoreML::Specification::SliceLayerParams* NeuralNetworkLayer::mutable_slice() {
  if (!has_slice()) {
    clear_layer();
    set_has_slice();
    layer_.slice_ = new ::CoreML::Specification::SliceLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.slice)
  return layer_.slice_;
}
::CoreML::Specification::SliceLayerParams* NeuralNetworkLayer::release_slice() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.slice)
  if (has_slice()) {
    clear_has_layer();
    ::CoreML::Specification::SliceLayerParams* temp = layer_.slice_;
    layer_.slice_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_slice(::CoreML::Specification::SliceLayerParams* slice) {
  clear_layer();
  if (slice) {
    set_has_slice();
    layer_.slice_ = slice;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.slice)
}

// .CoreML.Specification.SimpleRecurrentLayerParams simpleRecurrent = 400;
bool NeuralNetworkLayer::has_simplerecurrent() const {
  return layer_case() == kSimpleRecurrent;
}
void NeuralNetworkLayer::set_has_simplerecurrent() {
  _oneof_case_[0] = kSimpleRecurrent;
}
void NeuralNetworkLayer::clear_simplerecurrent() {
  if (has_simplerecurrent()) {
    delete layer_.simplerecurrent_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::SimpleRecurrentLayerParams& NeuralNetworkLayer::simplerecurrent() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.simpleRecurrent)
  return has_simplerecurrent()
      ? *layer_.simplerecurrent_
      : ::CoreML::Specification::SimpleRecurrentLayerParams::default_instance();
}
::CoreML::Specification::SimpleRecurrentLayerParams* NeuralNetworkLayer::mutable_simplerecurrent() {
  if (!has_simplerecurrent()) {
    clear_layer();
    set_has_simplerecurrent();
    layer_.simplerecurrent_ = new ::CoreML::Specification::SimpleRecurrentLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.simpleRecurrent)
  return layer_.simplerecurrent_;
}
::CoreML::Specification::SimpleRecurrentLayerParams* NeuralNetworkLayer::release_simplerecurrent() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.simpleRecurrent)
  if (has_simplerecurrent()) {
    clear_has_layer();
    ::CoreML::Specification::SimpleRecurrentLayerParams* temp = layer_.simplerecurrent_;
    layer_.simplerecurrent_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_simplerecurrent(::CoreML::Specification::SimpleRecurrentLayerParams* simplerecurrent) {
  clear_layer();
  if (simplerecurrent) {
    set_has_simplerecurrent();
    layer_.simplerecurrent_ = simplerecurrent;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.simpleRecurrent)
}

// .CoreML.Specification.GRULayerParams gru = 410;
bool NeuralNetworkLayer::has_gru() const {
  return layer_case() == kGru;
}
void NeuralNetworkLayer::set_has_gru() {
  _oneof_case_[0] = kGru;
}
void NeuralNetworkLayer::clear_gru() {
  if (has_gru()) {
    delete layer_.gru_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::GRULayerParams& NeuralNetworkLayer::gru() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.gru)
  return has_gru()
      ? *layer_.gru_
      : ::CoreML::Specification::GRULayerParams::default_instance();
}
::CoreML::Specification::GRULayerParams* NeuralNetworkLayer::mutable_gru() {
  if (!has_gru()) {
    clear_layer();
    set_has_gru();
    layer_.gru_ = new ::CoreML::Specification::GRULayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.gru)
  return layer_.gru_;
}
::CoreML::Specification::GRULayerParams* NeuralNetworkLayer::release_gru() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.gru)
  if (has_gru()) {
    clear_has_layer();
    ::CoreML::Specification::GRULayerParams* temp = layer_.gru_;
    layer_.gru_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_gru(::CoreML::Specification::GRULayerParams* gru) {
  clear_layer();
  if (gru) {
    set_has_gru();
    layer_.gru_ = gru;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.gru)
}

// .CoreML.Specification.UniDirectionalLSTMLayerParams uniDirectionalLSTM = 420;
bool NeuralNetworkLayer::has_unidirectionallstm() const {
  return layer_case() == kUniDirectionalLSTM;
}
void NeuralNetworkLayer::set_has_unidirectionallstm() {
  _oneof_case_[0] = kUniDirectionalLSTM;
}
void NeuralNetworkLayer::clear_unidirectionallstm() {
  if (has_unidirectionallstm()) {
    delete layer_.unidirectionallstm_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::UniDirectionalLSTMLayerParams& NeuralNetworkLayer::unidirectionallstm() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.uniDirectionalLSTM)
  return has_unidirectionallstm()
      ? *layer_.unidirectionallstm_
      : ::CoreML::Specification::UniDirectionalLSTMLayerParams::default_instance();
}
::CoreML::Specification::UniDirectionalLSTMLayerParams* NeuralNetworkLayer::mutable_unidirectionallstm() {
  if (!has_unidirectionallstm()) {
    clear_layer();
    set_has_unidirectionallstm();
    layer_.unidirectionallstm_ = new ::CoreML::Specification::UniDirectionalLSTMLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.uniDirectionalLSTM)
  return layer_.unidirectionallstm_;
}
::CoreML::Specification::UniDirectionalLSTMLayerParams* NeuralNetworkLayer::release_unidirectionallstm() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.uniDirectionalLSTM)
  if (has_unidirectionallstm()) {
    clear_has_layer();
    ::CoreML::Specification::UniDirectionalLSTMLayerParams* temp = layer_.unidirectionallstm_;
    layer_.unidirectionallstm_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_unidirectionallstm(::CoreML::Specification::UniDirectionalLSTMLayerParams* unidirectionallstm) {
  clear_layer();
  if (unidirectionallstm) {
    set_has_unidirectionallstm();
    layer_.unidirectionallstm_ = unidirectionallstm;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.uniDirectionalLSTM)
}

// .CoreML.Specification.BiDirectionalLSTMLayerParams biDirectionalLSTM = 430;
bool NeuralNetworkLayer::has_bidirectionallstm() const {
  return layer_case() == kBiDirectionalLSTM;
}
void NeuralNetworkLayer::set_has_bidirectionallstm() {
  _oneof_case_[0] = kBiDirectionalLSTM;
}
void NeuralNetworkLayer::clear_bidirectionallstm() {
  if (has_bidirectionallstm()) {
    delete layer_.bidirectionallstm_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::BiDirectionalLSTMLayerParams& NeuralNetworkLayer::bidirectionallstm() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.biDirectionalLSTM)
  return has_bidirectionallstm()
      ? *layer_.bidirectionallstm_
      : ::CoreML::Specification::BiDirectionalLSTMLayerParams::default_instance();
}
::CoreML::Specification::BiDirectionalLSTMLayerParams* NeuralNetworkLayer::mutable_bidirectionallstm() {
  if (!has_bidirectionallstm()) {
    clear_layer();
    set_has_bidirectionallstm();
    layer_.bidirectionallstm_ = new ::CoreML::Specification::BiDirectionalLSTMLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.biDirectionalLSTM)
  return layer_.bidirectionallstm_;
}
::CoreML::Specification::BiDirectionalLSTMLayerParams* NeuralNetworkLayer::release_bidirectionallstm() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.biDirectionalLSTM)
  if (has_bidirectionallstm()) {
    clear_has_layer();
    ::CoreML::Specification::BiDirectionalLSTMLayerParams* temp = layer_.bidirectionallstm_;
    layer_.bidirectionallstm_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_bidirectionallstm(::CoreML::Specification::BiDirectionalLSTMLayerParams* bidirectionallstm) {
  clear_layer();
  if (bidirectionallstm) {
    set_has_bidirectionallstm();
    layer_.bidirectionallstm_ = bidirectionallstm;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.biDirectionalLSTM)
}

// .CoreML.Specification.CustomLayerParams custom = 500;
bool NeuralNetworkLayer::has_custom() const {
  return layer_case() == kCustom;
}
void NeuralNetworkLayer::set_has_custom() {
  _oneof_case_[0] = kCustom;
}
void NeuralNetworkLayer::clear_custom() {
  if (has_custom()) {
    delete layer_.custom_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::CustomLayerParams& NeuralNetworkLayer::custom() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.custom)
  return has_custom()
      ? *layer_.custom_
      : ::CoreML::Specification::CustomLayerParams::default_instance();
}
::CoreML::Specification::CustomLayerParams* NeuralNetworkLayer::mutable_custom() {
  if (!has_custom()) {
    clear_layer();
    set_has_custom();
    layer_.custom_ = new ::CoreML::Specification::CustomLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.custom)
  return layer_.custom_;
}
::CoreML::Specification::CustomLayerParams* NeuralNetworkLayer::release_custom() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.custom)
  if (has_custom()) {
    clear_has_layer();
    ::CoreML::Specification::CustomLayerParams* temp = layer_.custom_;
    layer_.custom_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_custom(::CoreML::Specification::CustomLayerParams* custom) {
  clear_layer();
  if (custom) {
    set_has_custom();
    layer_.custom_ = custom;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.custom)
}

// .CoreML.Specification.CopyLayerParams copy = 600;
bool NeuralNetworkLayer::has_copy() const {
  return layer_case() == kCopy;
}
void NeuralNetworkLayer::set_has_copy() {
  _oneof_case_[0] = kCopy;
}
void NeuralNetworkLayer::clear_copy() {
  if (has_copy()) {
    delete layer_.copy_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::CopyLayerParams& NeuralNetworkLayer::copy() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.copy)
  return has_copy()
      ? *layer_.copy_
      : ::CoreML::Specification::CopyLayerParams::default_instance();
}
::CoreML::Specification::CopyLayerParams* NeuralNetworkLayer::mutable_copy() {
  if (!has_copy()) {
    clear_layer();
    set_has_copy();
    layer_.copy_ = new ::CoreML::Specification::CopyLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.copy)
  return layer_.copy_;
}
::CoreML::Specification::CopyLayerParams* NeuralNetworkLayer::release_copy() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.copy)
  if (has_copy()) {
    clear_has_layer();
    ::CoreML::Specification::CopyLayerParams* temp = layer_.copy_;
    layer_.copy_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_copy(::CoreML::Specification::CopyLayerParams* copy) {
  clear_layer();
  if (copy) {
    set_has_copy();
    layer_.copy_ = copy;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.copy)
}

// .CoreML.Specification.BranchLayerParams branch = 605;
bool NeuralNetworkLayer::has_branch() const {
  return layer_case() == kBranch;
}
void NeuralNetworkLayer::set_has_branch() {
  _oneof_case_[0] = kBranch;
}
void NeuralNetworkLayer::clear_branch() {
  if (has_branch()) {
    delete layer_.branch_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::BranchLayerParams& NeuralNetworkLayer::branch() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.branch)
  return has_branch()
      ? *layer_.branch_
      : ::CoreML::Specification::BranchLayerParams::default_instance();
}
::CoreML::Specification::BranchLayerParams* NeuralNetworkLayer::mutable_branch() {
  if (!has_branch()) {
    clear_layer();
    set_has_branch();
    layer_.branch_ = new ::CoreML::Specification::BranchLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.branch)
  return layer_.branch_;
}
::CoreML::Specification::BranchLayerParams* NeuralNetworkLayer::release_branch() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.branch)
  if (has_branch()) {
    clear_has_layer();
    ::CoreML::Specification::BranchLayerParams* temp = layer_.branch_;
    layer_.branch_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_branch(::CoreML::Specification::BranchLayerParams* branch) {
  clear_layer();
  if (branch) {
    set_has_branch();
    layer_.branch_ = branch;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.branch)
}

// .CoreML.Specification.LoopLayerParams loop = 615;
bool NeuralNetworkLayer::has_loop() const {
  return layer_case() == kLoop;
}
void NeuralNetworkLayer::set_has_loop() {
  _oneof_case_[0] = kLoop;
}
void NeuralNetworkLayer::clear_loop() {
  if (has_loop()) {
    delete layer_.loop_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::LoopLayerParams& NeuralNetworkLayer::loop() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.loop)
  return has_loop()
      ? *layer_.loop_
      : ::CoreML::Specification::LoopLayerParams::default_instance();
}
::CoreML::Specification::LoopLayerParams* NeuralNetworkLayer::mutable_loop() {
  if (!has_loop()) {
    clear_layer();
    set_has_loop();
    layer_.loop_ = new ::CoreML::Specification::LoopLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.loop)
  return layer_.loop_;
}
::CoreML::Specification::LoopLayerParams* NeuralNetworkLayer::release_loop() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.loop)
  if (has_loop()) {
    clear_has_layer();
    ::CoreML::Specification::LoopLayerParams* temp = layer_.loop_;
    layer_.loop_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_loop(::CoreML::Specification::LoopLayerParams* loop) {
  clear_layer();
  if (loop) {
    set_has_loop();
    layer_.loop_ = loop;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.loop)
}

// .CoreML.Specification.LoopBreakLayerParams loopBreak = 620;
bool NeuralNetworkLayer::has_loopbreak() const {
  return layer_case() == kLoopBreak;
}
void NeuralNetworkLayer::set_has_loopbreak() {
  _oneof_case_[0] = kLoopBreak;
}
void NeuralNetworkLayer::clear_loopbreak() {
  if (has_loopbreak()) {
    delete layer_.loopbreak_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::LoopBreakLayerParams& NeuralNetworkLayer::loopbreak() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.loopBreak)
  return has_loopbreak()
      ? *layer_.loopbreak_
      : ::CoreML::Specification::LoopBreakLayerParams::default_instance();
}
::CoreML::Specification::LoopBreakLayerParams* NeuralNetworkLayer::mutable_loopbreak() {
  if (!has_loopbreak()) {
    clear_layer();
    set_has_loopbreak();
    layer_.loopbreak_ = new ::CoreML::Specification::LoopBreakLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.loopBreak)
  return layer_.loopbreak_;
}
::CoreML::Specification::LoopBreakLayerParams* NeuralNetworkLayer::release_loopbreak() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.loopBreak)
  if (has_loopbreak()) {
    clear_has_layer();
    ::CoreML::Specification::LoopBreakLayerParams* temp = layer_.loopbreak_;
    layer_.loopbreak_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_loopbreak(::CoreML::Specification::LoopBreakLayerParams* loopbreak) {
  clear_layer();
  if (loopbreak) {
    set_has_loopbreak();
    layer_.loopbreak_ = loopbreak;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.loopBreak)
}

// .CoreML.Specification.LoopContinueLayerParams loopContinue = 625;
bool NeuralNetworkLayer::has_loopcontinue() const {
  return layer_case() == kLoopContinue;
}
void NeuralNetworkLayer::set_has_loopcontinue() {
  _oneof_case_[0] = kLoopContinue;
}
void NeuralNetworkLayer::clear_loopcontinue() {
  if (has_loopcontinue()) {
    delete layer_.loopcontinue_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::LoopContinueLayerParams& NeuralNetworkLayer::loopcontinue() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.loopContinue)
  return has_loopcontinue()
      ? *layer_.loopcontinue_
      : ::CoreML::Specification::LoopContinueLayerParams::default_instance();
}
::CoreML::Specification::LoopContinueLayerParams* NeuralNetworkLayer::mutable_loopcontinue() {
  if (!has_loopcontinue()) {
    clear_layer();
    set_has_loopcontinue();
    layer_.loopcontinue_ = new ::CoreML::Specification::LoopContinueLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.loopContinue)
  return layer_.loopcontinue_;
}
::CoreML::Specification::LoopContinueLayerParams* NeuralNetworkLayer::release_loopcontinue() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.loopContinue)
  if (has_loopcontinue()) {
    clear_has_layer();
    ::CoreML::Specification::LoopContinueLayerParams* temp = layer_.loopcontinue_;
    layer_.loopcontinue_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_loopcontinue(::CoreML::Specification::LoopContinueLayerParams* loopcontinue) {
  clear_layer();
  if (loopcontinue) {
    set_has_loopcontinue();
    layer_.loopcontinue_ = loopcontinue;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.loopContinue)
}

// .CoreML.Specification.RangeLayerParams range = 635;
bool NeuralNetworkLayer::has_range() const {
  return layer_case() == kRange;
}
void NeuralNetworkLayer::set_has_range() {
  _oneof_case_[0] = kRange;
}
void NeuralNetworkLayer::clear_range() {
  if (has_range()) {
    delete layer_.range_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::RangeLayerParams& NeuralNetworkLayer::range() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.range)
  return has_range()
      ? *layer_.range_
      : ::CoreML::Specification::RangeLayerParams::default_instance();
}
::CoreML::Specification::RangeLayerParams* NeuralNetworkLayer::mutable_range() {
  if (!has_range()) {
    clear_layer();
    set_has_range();
    layer_.range_ = new ::CoreML::Specification::RangeLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.range)
  return layer_.range_;
}
::CoreML::Specification::RangeLayerParams* NeuralNetworkLayer::release_range() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.range)
  if (has_range()) {
    clear_has_layer();
    ::CoreML::Specification::RangeLayerParams* temp = layer_.range_;
    layer_.range_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_range(::CoreML::Specification::RangeLayerParams* range) {
  clear_layer();
  if (range) {
    set_has_range();
    layer_.range_ = range;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.range)
}

// .CoreML.Specification.ClipLayerParams clip = 660;
bool NeuralNetworkLayer::has_clip() const {
  return layer_case() == kClip;
}
void NeuralNetworkLayer::set_has_clip() {
  _oneof_case_[0] = kClip;
}
void NeuralNetworkLayer::clear_clip() {
  if (has_clip()) {
    delete layer_.clip_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::ClipLayerParams& NeuralNetworkLayer::clip() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.clip)
  return has_clip()
      ? *layer_.clip_
      : ::CoreML::Specification::ClipLayerParams::default_instance();
}
::CoreML::Specification::ClipLayerParams* NeuralNetworkLayer::mutable_clip() {
  if (!has_clip()) {
    clear_layer();
    set_has_clip();
    layer_.clip_ = new ::CoreML::Specification::ClipLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.clip)
  return layer_.clip_;
}
::CoreML::Specification::ClipLayerParams* NeuralNetworkLayer::release_clip() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.clip)
  if (has_clip()) {
    clear_has_layer();
    ::CoreML::Specification::ClipLayerParams* temp = layer_.clip_;
    layer_.clip_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_clip(::CoreML::Specification::ClipLayerParams* clip) {
  clear_layer();
  if (clip) {
    set_has_clip();
    layer_.clip_ = clip;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.clip)
}

// .CoreML.Specification.CeilLayerParams ceil = 665;
bool NeuralNetworkLayer::has_ceil() const {
  return layer_case() == kCeil;
}
void NeuralNetworkLayer::set_has_ceil() {
  _oneof_case_[0] = kCeil;
}
void NeuralNetworkLayer::clear_ceil() {
  if (has_ceil()) {
    delete layer_.ceil_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::CeilLayerParams& NeuralNetworkLayer::ceil() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.ceil)
  return has_ceil()
      ? *layer_.ceil_
      : ::CoreML::Specification::CeilLayerParams::default_instance();
}
::CoreML::Specification::CeilLayerParams* NeuralNetworkLayer::mutable_ceil() {
  if (!has_ceil()) {
    clear_layer();
    set_has_ceil();
    layer_.ceil_ = new ::CoreML::Specification::CeilLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.ceil)
  return layer_.ceil_;
}
::CoreML::Specification::CeilLayerParams* NeuralNetworkLayer::release_ceil() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.ceil)
  if (has_ceil()) {
    clear_has_layer();
    ::CoreML::Specification::CeilLayerParams* temp = layer_.ceil_;
    layer_.ceil_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_ceil(::CoreML::Specification::CeilLayerParams* ceil) {
  clear_layer();
  if (ceil) {
    set_has_ceil();
    layer_.ceil_ = ceil;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.ceil)
}

// .CoreML.Specification.FloorLayerParams floor = 670;
bool NeuralNetworkLayer::has_floor() const {
  return layer_case() == kFloor;
}
void NeuralNetworkLayer::set_has_floor() {
  _oneof_case_[0] = kFloor;
}
void NeuralNetworkLayer::clear_floor() {
  if (has_floor()) {
    delete layer_.floor_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::FloorLayerParams& NeuralNetworkLayer::floor() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.floor)
  return has_floor()
      ? *layer_.floor_
      : ::CoreML::Specification::FloorLayerParams::default_instance();
}
::CoreML::Specification::FloorLayerParams* NeuralNetworkLayer::mutable_floor() {
  if (!has_floor()) {
    clear_layer();
    set_has_floor();
    layer_.floor_ = new ::CoreML::Specification::FloorLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.floor)
  return layer_.floor_;
}
::CoreML::Specification::FloorLayerParams* NeuralNetworkLayer::release_floor() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.floor)
  if (has_floor()) {
    clear_has_layer();
    ::CoreML::Specification::FloorLayerParams* temp = layer_.floor_;
    layer_.floor_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_floor(::CoreML::Specification::FloorLayerParams* floor) {
  clear_layer();
  if (floor) {
    set_has_floor();
    layer_.floor_ = floor;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.floor)
}

// .CoreML.Specification.Exp2LayerParams exp2 = 700;
bool NeuralNetworkLayer::has_exp2() const {
  return layer_case() == kExp2;
}
void NeuralNetworkLayer::set_has_exp2() {
  _oneof_case_[0] = kExp2;
}
void NeuralNetworkLayer::clear_exp2() {
  if (has_exp2()) {
    delete layer_.exp2_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::Exp2LayerParams& NeuralNetworkLayer::exp2() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.exp2)
  return has_exp2()
      ? *layer_.exp2_
      : ::CoreML::Specification::Exp2LayerParams::default_instance();
}
::CoreML::Specification::Exp2LayerParams* NeuralNetworkLayer::mutable_exp2() {
  if (!has_exp2()) {
    clear_layer();
    set_has_exp2();
    layer_.exp2_ = new ::CoreML::Specification::Exp2LayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.exp2)
  return layer_.exp2_;
}
::CoreML::Specification::Exp2LayerParams* NeuralNetworkLayer::release_exp2() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.exp2)
  if (has_exp2()) {
    clear_has_layer();
    ::CoreML::Specification::Exp2LayerParams* temp = layer_.exp2_;
    layer_.exp2_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_exp2(::CoreML::Specification::Exp2LayerParams* exp2) {
  clear_layer();
  if (exp2) {
    set_has_exp2();
    layer_.exp2_ = exp2;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.exp2)
}

// .CoreML.Specification.SineLayerParams sine = 710;
bool NeuralNetworkLayer::has_sine() const {
  return layer_case() == kSine;
}
void NeuralNetworkLayer::set_has_sine() {
  _oneof_case_[0] = kSine;
}
void NeuralNetworkLayer::clear_sine() {
  if (has_sine()) {
    delete layer_.sine_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::SineLayerParams& NeuralNetworkLayer::sine() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.sine)
  return has_sine()
      ? *layer_.sine_
      : ::CoreML::Specification::SineLayerParams::default_instance();
}
::CoreML::Specification::SineLayerParams* NeuralNetworkLayer::mutable_sine() {
  if (!has_sine()) {
    clear_layer();
    set_has_sine();
    layer_.sine_ = new ::CoreML::Specification::SineLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.sine)
  return layer_.sine_;
}
::CoreML::Specification::SineLayerParams* NeuralNetworkLayer::release_sine() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.sine)
  if (has_sine()) {
    clear_has_layer();
    ::CoreML::Specification::SineLayerParams* temp = layer_.sine_;
    layer_.sine_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_sine(::CoreML::Specification::SineLayerParams* sine) {
  clear_layer();
  if (sine) {
    set_has_sine();
    layer_.sine_ = sine;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.sine)
}

// .CoreML.Specification.CosineLayerParams cosine = 715;
bool NeuralNetworkLayer::has_cosine() const {
  return layer_case() == kCosine;
}
void NeuralNetworkLayer::set_has_cosine() {
  _oneof_case_[0] = kCosine;
}
void NeuralNetworkLayer::clear_cosine() {
  if (has_cosine()) {
    delete layer_.cosine_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::CosineLayerParams& NeuralNetworkLayer::cosine() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.cosine)
  return has_cosine()
      ? *layer_.cosine_
      : ::CoreML::Specification::CosineLayerParams::default_instance();
}
::CoreML::Specification::CosineLayerParams* NeuralNetworkLayer::mutable_cosine() {
  if (!has_cosine()) {
    clear_layer();
    set_has_cosine();
    layer_.cosine_ = new ::CoreML::Specification::CosineLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.cosine)
  return layer_.cosine_;
}
::CoreML::Specification::CosineLayerParams* NeuralNetworkLayer::release_cosine() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.cosine)
  if (has_cosine()) {
    clear_has_layer();
    ::CoreML::Specification::CosineLayerParams* temp = layer_.cosine_;
    layer_.cosine_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_cosine(::CoreML::Specification::CosineLayerParams* cosine) {
  clear_layer();
  if (cosine) {
    set_has_cosine();
    layer_.cosine_ = cosine;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.cosine)
}

// .CoreML.Specification.ErfActivationLayerParams erfActivation = 790;
bool NeuralNetworkLayer::has_erfactivation() const {
  return layer_case() == kErfActivation;
}
void NeuralNetworkLayer::set_has_erfactivation() {
  _oneof_case_[0] = kErfActivation;
}
void NeuralNetworkLayer::clear_erfactivation() {
  if (has_erfactivation()) {
    delete layer_.erfactivation_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::ErfActivationLayerParams& NeuralNetworkLayer::erfactivation() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.erfActivation)
  return has_erfactivation()
      ? *layer_.erfactivation_
      : ::CoreML::Specification::ErfActivationLayerParams::default_instance();
}
::CoreML::Specification::ErfActivationLayerParams* NeuralNetworkLayer::mutable_erfactivation() {
  if (!has_erfactivation()) {
    clear_layer();
    set_has_erfactivation();
    layer_.erfactivation_ = new ::CoreML::Specification::ErfActivationLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.erfActivation)
  return layer_.erfactivation_;
}
::CoreML::Specification::ErfActivationLayerParams* NeuralNetworkLayer::release_erfactivation() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.erfActivation)
  if (has_erfactivation()) {
    clear_has_layer();
    ::CoreML::Specification::ErfActivationLayerParams* temp = layer_.erfactivation_;
    layer_.erfactivation_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_erfactivation(::CoreML::Specification::ErfActivationLayerParams* erfactivation) {
  clear_layer();
  if (erfactivation) {
    set_has_erfactivation();
    layer_.erfactivation_ = erfactivation;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.erfActivation)
}

// .CoreML.Specification.GeluActivationLayerParams geluActivation = 795;
bool NeuralNetworkLayer::has_geluactivation() const {
  return layer_case() == kGeluActivation;
}
void NeuralNetworkLayer::set_has_geluactivation() {
  _oneof_case_[0] = kGeluActivation;
}
void NeuralNetworkLayer::clear_geluactivation() {
  if (has_geluactivation()) {
    delete layer_.geluactivation_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::GeluActivationLayerParams& NeuralNetworkLayer::geluactivation() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.geluActivation)
  return has_geluactivation()
      ? *layer_.geluactivation_
      : ::CoreML::Specification::GeluActivationLayerParams::default_instance();
}
::CoreML::Specification::GeluActivationLayerParams* NeuralNetworkLayer::mutable_geluactivation() {
  if (!has_geluactivation()) {
    clear_layer();
    set_has_geluactivation();
    layer_.geluactivation_ = new ::CoreML::Specification::GeluActivationLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.geluActivation)
  return layer_.geluactivation_;
}
::CoreML::Specification::GeluActivationLayerParams* NeuralNetworkLayer::release_geluactivation() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.geluActivation)
  if (has_geluactivation()) {
    clear_has_layer();
    ::CoreML::Specification::GeluActivationLayerParams* temp = layer_.geluactivation_;
    layer_.geluactivation_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_geluactivation(::CoreML::Specification::GeluActivationLayerParams* geluactivation) {
  clear_layer();
  if (geluactivation) {
    set_has_geluactivation();
    layer_.geluactivation_ = geluactivation;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.geluActivation)
}

// .CoreML.Specification.EqualLayerParams equal = 815;
bool NeuralNetworkLayer::has_equal() const {
  return layer_case() == kEqual;
}
void NeuralNetworkLayer::set_has_equal() {
  _oneof_case_[0] = kEqual;
}
void NeuralNetworkLayer::clear_equal() {
  if (has_equal()) {
    delete layer_.equal_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::EqualLayerParams& NeuralNetworkLayer::equal() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.equal)
  return has_equal()
      ? *layer_.equal_
      : ::CoreML::Specification::EqualLayerParams::default_instance();
}
::CoreML::Specification::EqualLayerParams* NeuralNetworkLayer::mutable_equal() {
  if (!has_equal()) {
    clear_layer();
    set_has_equal();
    layer_.equal_ = new ::CoreML::Specification::EqualLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.equal)
  return layer_.equal_;
}
::CoreML::Specification::EqualLayerParams* NeuralNetworkLayer::release_equal() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.equal)
  if (has_equal()) {
    clear_has_layer();
    ::CoreML::Specification::EqualLayerParams* temp = layer_.equal_;
    layer_.equal_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_equal(::CoreML::Specification::EqualLayerParams* equal) {
  clear_layer();
  if (equal) {
    set_has_equal();
    layer_.equal_ = equal;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.equal)
}

// .CoreML.Specification.NotEqualLayerParams notEqual = 820;
bool NeuralNetworkLayer::has_notequal() const {
  return layer_case() == kNotEqual;
}
void NeuralNetworkLayer::set_has_notequal() {
  _oneof_case_[0] = kNotEqual;
}
void NeuralNetworkLayer::clear_notequal() {
  if (has_notequal()) {
    delete layer_.notequal_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::NotEqualLayerParams& NeuralNetworkLayer::notequal() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.notEqual)
  return has_notequal()
      ? *layer_.notequal_
      : ::CoreML::Specification::NotEqualLayerParams::default_instance();
}
::CoreML::Specification::NotEqualLayerParams* NeuralNetworkLayer::mutable_notequal() {
  if (!has_notequal()) {
    clear_layer();
    set_has_notequal();
    layer_.notequal_ = new ::CoreML::Specification::NotEqualLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.notEqual)
  return layer_.notequal_;
}
::CoreML::Specification::NotEqualLayerParams* NeuralNetworkLayer::release_notequal() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.notEqual)
  if (has_notequal()) {
    clear_has_layer();
    ::CoreML::Specification::NotEqualLayerParams* temp = layer_.notequal_;
    layer_.notequal_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_notequal(::CoreML::Specification::NotEqualLayerParams* notequal) {
  clear_layer();
  if (notequal) {
    set_has_notequal();
    layer_.notequal_ = notequal;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.notEqual)
}

// .CoreML.Specification.LessThanLayerParams lessThan = 825;
bool NeuralNetworkLayer::has_lessthan() const {
  return layer_case() == kLessThan;
}
void NeuralNetworkLayer::set_has_lessthan() {
  _oneof_case_[0] = kLessThan;
}
void NeuralNetworkLayer::clear_lessthan() {
  if (has_lessthan()) {
    delete layer_.lessthan_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::LessThanLayerParams& NeuralNetworkLayer::lessthan() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.lessThan)
  return has_lessthan()
      ? *layer_.lessthan_
      : ::CoreML::Specification::LessThanLayerParams::default_instance();
}
::CoreML::Specification::LessThanLayerParams* NeuralNetworkLayer::mutable_lessthan() {
  if (!has_lessthan()) {
    clear_layer();
    set_has_lessthan();
    layer_.lessthan_ = new ::CoreML::Specification::LessThanLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.lessThan)
  return layer_.lessthan_;
}
::CoreML::Specification::LessThanLayerParams* NeuralNetworkLayer::release_lessthan() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.lessThan)
  if (has_lessthan()) {
    clear_has_layer();
    ::CoreML::Specification::LessThanLayerParams* temp = layer_.lessthan_;
    layer_.lessthan_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_lessthan(::CoreML::Specification::LessThanLayerParams* lessthan) {
  clear_layer();
  if (lessthan) {
    set_has_lessthan();
    layer_.lessthan_ = lessthan;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.lessThan)
}

// .CoreML.Specification.GreaterThanLayerParams greaterThan = 830;
bool NeuralNetworkLayer::has_greaterthan() const {
  return layer_case() == kGreaterThan;
}
void NeuralNetworkLayer::set_has_greaterthan() {
  _oneof_case_[0] = kGreaterThan;
}
void NeuralNetworkLayer::clear_greaterthan() {
  if (has_greaterthan()) {
    delete layer_.greaterthan_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::GreaterThanLayerParams& NeuralNetworkLayer::greaterthan() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.greaterThan)
  return has_greaterthan()
      ? *layer_.greaterthan_
      : ::CoreML::Specification::GreaterThanLayerParams::default_instance();
}
::CoreML::Specification::GreaterThanLayerParams* NeuralNetworkLayer::mutable_greaterthan() {
  if (!has_greaterthan()) {
    clear_layer();
    set_has_greaterthan();
    layer_.greaterthan_ = new ::CoreML::Specification::GreaterThanLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.greaterThan)
  return layer_.greaterthan_;
}
::CoreML::Specification::GreaterThanLayerParams* NeuralNetworkLayer::release_greaterthan() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.greaterThan)
  if (has_greaterthan()) {
    clear_has_layer();
    ::CoreML::Specification::GreaterThanLayerParams* temp = layer_.greaterthan_;
    layer_.greaterthan_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_greaterthan(::CoreML::Specification::GreaterThanLayerParams* greaterthan) {
  clear_layer();
  if (greaterthan) {
    set_has_greaterthan();
    layer_.greaterthan_ = greaterthan;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.greaterThan)
}

// .CoreML.Specification.LogicalOrLayerParams logicalOr = 840;
bool NeuralNetworkLayer::has_logicalor() const {
  return layer_case() == kLogicalOr;
}
void NeuralNetworkLayer::set_has_logicalor() {
  _oneof_case_[0] = kLogicalOr;
}
void NeuralNetworkLayer::clear_logicalor() {
  if (has_logicalor()) {
    delete layer_.logicalor_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::LogicalOrLayerParams& NeuralNetworkLayer::logicalor() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.logicalOr)
  return has_logicalor()
      ? *layer_.logicalor_
      : ::CoreML::Specification::LogicalOrLayerParams::default_instance();
}
::CoreML::Specification::LogicalOrLayerParams* NeuralNetworkLayer::mutable_logicalor() {
  if (!has_logicalor()) {
    clear_layer();
    set_has_logicalor();
    layer_.logicalor_ = new ::CoreML::Specification::LogicalOrLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.logicalOr)
  return layer_.logicalor_;
}
::CoreML::Specification::LogicalOrLayerParams* NeuralNetworkLayer::release_logicalor() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.logicalOr)
  if (has_logicalor()) {
    clear_has_layer();
    ::CoreML::Specification::LogicalOrLayerParams* temp = layer_.logicalor_;
    layer_.logicalor_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_logicalor(::CoreML::Specification::LogicalOrLayerParams* logicalor) {
  clear_layer();
  if (logicalor) {
    set_has_logicalor();
    layer_.logicalor_ = logicalor;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.logicalOr)
}

// .CoreML.Specification.LogicalXorLayerParams logicalXor = 845;
bool NeuralNetworkLayer::has_logicalxor() const {
  return layer_case() == kLogicalXor;
}
void NeuralNetworkLayer::set_has_logicalxor() {
  _oneof_case_[0] = kLogicalXor;
}
void NeuralNetworkLayer::clear_logicalxor() {
  if (has_logicalxor()) {
    delete layer_.logicalxor_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::LogicalXorLayerParams& NeuralNetworkLayer::logicalxor() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.logicalXor)
  return has_logicalxor()
      ? *layer_.logicalxor_
      : ::CoreML::Specification::LogicalXorLayerParams::default_instance();
}
::CoreML::Specification::LogicalXorLayerParams* NeuralNetworkLayer::mutable_logicalxor() {
  if (!has_logicalxor()) {
    clear_layer();
    set_has_logicalxor();
    layer_.logicalxor_ = new ::CoreML::Specification::LogicalXorLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.logicalXor)
  return layer_.logicalxor_;
}
::CoreML::Specification::LogicalXorLayerParams* NeuralNetworkLayer::release_logicalxor() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.logicalXor)
  if (has_logicalxor()) {
    clear_has_layer();
    ::CoreML::Specification::LogicalXorLayerParams* temp = layer_.logicalxor_;
    layer_.logicalxor_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_logicalxor(::CoreML::Specification::LogicalXorLayerParams* logicalxor) {
  clear_layer();
  if (logicalxor) {
    set_has_logicalxor();
    layer_.logicalxor_ = logicalxor;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.logicalXor)
}

// .CoreML.Specification.LogicalNotLayerParams logicalNot = 850;
bool NeuralNetworkLayer::has_logicalnot() const {
  return layer_case() == kLogicalNot;
}
void NeuralNetworkLayer::set_has_logicalnot() {
  _oneof_case_[0] = kLogicalNot;
}
void NeuralNetworkLayer::clear_logicalnot() {
  if (has_logicalnot()) {
    delete layer_.logicalnot_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::LogicalNotLayerParams& NeuralNetworkLayer::logicalnot() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.logicalNot)
  return has_logicalnot()
      ? *layer_.logicalnot_
      : ::CoreML::Specification::LogicalNotLayerParams::default_instance();
}
::CoreML::Specification::LogicalNotLayerParams* NeuralNetworkLayer::mutable_logicalnot() {
  if (!has_logicalnot()) {
    clear_layer();
    set_has_logicalnot();
    layer_.logicalnot_ = new ::CoreML::Specification::LogicalNotLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.logicalNot)
  return layer_.logicalnot_;
}
::CoreML::Specification::LogicalNotLayerParams* NeuralNetworkLayer::release_logicalnot() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.logicalNot)
  if (has_logicalnot()) {
    clear_has_layer();
    ::CoreML::Specification::LogicalNotLayerParams* temp = layer_.logicalnot_;
    layer_.logicalnot_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_logicalnot(::CoreML::Specification::LogicalNotLayerParams* logicalnot) {
  clear_layer();
  if (logicalnot) {
    set_has_logicalnot();
    layer_.logicalnot_ = logicalnot;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.logicalNot)
}

// .CoreML.Specification.LogicalAndLayerParams logicalAnd = 855;
bool NeuralNetworkLayer::has_logicaland() const {
  return layer_case() == kLogicalAnd;
}
void NeuralNetworkLayer::set_has_logicaland() {
  _oneof_case_[0] = kLogicalAnd;
}
void NeuralNetworkLayer::clear_logicaland() {
  if (has_logicaland()) {
    delete layer_.logicaland_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::LogicalAndLayerParams& NeuralNetworkLayer::logicaland() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.logicalAnd)
  return has_logicaland()
      ? *layer_.logicaland_
      : ::CoreML::Specification::LogicalAndLayerParams::default_instance();
}
::CoreML::Specification::LogicalAndLayerParams* NeuralNetworkLayer::mutable_logicaland() {
  if (!has_logicaland()) {
    clear_layer();
    set_has_logicaland();
    layer_.logicaland_ = new ::CoreML::Specification::LogicalAndLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.logicalAnd)
  return layer_.logicaland_;
}
::CoreML::Specification::LogicalAndLayerParams* NeuralNetworkLayer::release_logicaland() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.logicalAnd)
  if (has_logicaland()) {
    clear_has_layer();
    ::CoreML::Specification::LogicalAndLayerParams* temp = layer_.logicaland_;
    layer_.logicaland_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_logicaland(::CoreML::Specification::LogicalAndLayerParams* logicaland) {
  clear_layer();
  if (logicaland) {
    set_has_logicaland();
    layer_.logicaland_ = logicaland;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.logicalAnd)
}

// .CoreML.Specification.AddBroadcastableLayerParams addBroadcastable = 880;
bool NeuralNetworkLayer::has_addbroadcastable() const {
  return layer_case() == kAddBroadcastable;
}
void NeuralNetworkLayer::set_has_addbroadcastable() {
  _oneof_case_[0] = kAddBroadcastable;
}
void NeuralNetworkLayer::clear_addbroadcastable() {
  if (has_addbroadcastable()) {
    delete layer_.addbroadcastable_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::AddBroadcastableLayerParams& NeuralNetworkLayer::addbroadcastable() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.addBroadcastable)
  return has_addbroadcastable()
      ? *layer_.addbroadcastable_
      : ::CoreML::Specification::AddBroadcastableLayerParams::default_instance();
}
::CoreML::Specification::AddBroadcastableLayerParams* NeuralNetworkLayer::mutable_addbroadcastable() {
  if (!has_addbroadcastable()) {
    clear_layer();
    set_has_addbroadcastable();
    layer_.addbroadcastable_ = new ::CoreML::Specification::AddBroadcastableLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.addBroadcastable)
  return layer_.addbroadcastable_;
}
::CoreML::Specification::AddBroadcastableLayerParams* NeuralNetworkLayer::release_addbroadcastable() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.addBroadcastable)
  if (has_addbroadcastable()) {
    clear_has_layer();
    ::CoreML::Specification::AddBroadcastableLayerParams* temp = layer_.addbroadcastable_;
    layer_.addbroadcastable_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_addbroadcastable(::CoreML::Specification::AddBroadcastableLayerParams* addbroadcastable) {
  clear_layer();
  if (addbroadcastable) {
    set_has_addbroadcastable();
    layer_.addbroadcastable_ = addbroadcastable;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.addBroadcastable)
}

// .CoreML.Specification.PowBroadcastableLayerParams powBroadcastable = 885;
bool NeuralNetworkLayer::has_powbroadcastable() const {
  return layer_case() == kPowBroadcastable;
}
void NeuralNetworkLayer::set_has_powbroadcastable() {
  _oneof_case_[0] = kPowBroadcastable;
}
void NeuralNetworkLayer::clear_powbroadcastable() {
  if (has_powbroadcastable()) {
    delete layer_.powbroadcastable_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::PowBroadcastableLayerParams& NeuralNetworkLayer::powbroadcastable() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.powBroadcastable)
  return has_powbroadcastable()
      ? *layer_.powbroadcastable_
      : ::CoreML::Specification::PowBroadcastableLayerParams::default_instance();
}
::CoreML::Specification::PowBroadcastableLayerParams* NeuralNetworkLayer::mutable_powbroadcastable() {
  if (!has_powbroadcastable()) {
    clear_layer();
    set_has_powbroadcastable();
    layer_.powbroadcastable_ = new ::CoreML::Specification::PowBroadcastableLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.powBroadcastable)
  return layer_.powbroadcastable_;
}
::CoreML::Specification::PowBroadcastableLayerParams* NeuralNetworkLayer::release_powbroadcastable() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.powBroadcastable)
  if (has_powbroadcastable()) {
    clear_has_layer();
    ::CoreML::Specification::PowBroadcastableLayerParams* temp = layer_.powbroadcastable_;
    layer_.powbroadcastable_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_powbroadcastable(::CoreML::Specification::PowBroadcastableLayerParams* powbroadcastable) {
  clear_layer();
  if (powbroadcastable) {
    set_has_powbroadcastable();
    layer_.powbroadcastable_ = powbroadcastable;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.powBroadcastable)
}

// .CoreML.Specification.DivideBroadcastableLayerParams divideBroadcastable = 890;
bool NeuralNetworkLayer::has_dividebroadcastable() const {
  return layer_case() == kDivideBroadcastable;
}
void NeuralNetworkLayer::set_has_dividebroadcastable() {
  _oneof_case_[0] = kDivideBroadcastable;
}
void NeuralNetworkLayer::clear_dividebroadcastable() {
  if (has_dividebroadcastable()) {
    delete layer_.dividebroadcastable_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::DivideBroadcastableLayerParams& NeuralNetworkLayer::dividebroadcastable() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.divideBroadcastable)
  return has_dividebroadcastable()
      ? *layer_.dividebroadcastable_
      : ::CoreML::Specification::DivideBroadcastableLayerParams::default_instance();
}
::CoreML::Specification::DivideBroadcastableLayerParams* NeuralNetworkLayer::mutable_dividebroadcastable() {
  if (!has_dividebroadcastable()) {
    clear_layer();
    set_has_dividebroadcastable();
    layer_.dividebroadcastable_ = new ::CoreML::Specification::DivideBroadcastableLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.divideBroadcastable)
  return layer_.dividebroadcastable_;
}
::CoreML::Specification::DivideBroadcastableLayerParams* NeuralNetworkLayer::release_dividebroadcastable() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.divideBroadcastable)
  if (has_dividebroadcastable()) {
    clear_has_layer();
    ::CoreML::Specification::DivideBroadcastableLayerParams* temp = layer_.dividebroadcastable_;
    layer_.dividebroadcastable_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_dividebroadcastable(::CoreML::Specification::DivideBroadcastableLayerParams* dividebroadcastable) {
  clear_layer();
  if (dividebroadcastable) {
    set_has_dividebroadcastable();
    layer_.dividebroadcastable_ = dividebroadcastable;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.divideBroadcastable)
}

// .CoreML.Specification.MultiplyBroadcastableLayerParams multiplyBroadcastable = 900;
bool NeuralNetworkLayer::has_multiplybroadcastable() const {
  return layer_case() == kMultiplyBroadcastable;
}
void NeuralNetworkLayer::set_has_multiplybroadcastable() {
  _oneof_case_[0] = kMultiplyBroadcastable;
}
void NeuralNetworkLayer::clear_multiplybroadcastable() {
  if (has_multiplybroadcastable()) {
    delete layer_.multiplybroadcastable_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::MultiplyBroadcastableLayerParams& NeuralNetworkLayer::multiplybroadcastable() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.multiplyBroadcastable)
  return has_multiplybroadcastable()
      ? *layer_.multiplybroadcastable_
      : ::CoreML::Specification::MultiplyBroadcastableLayerParams::default_instance();
}
::CoreML::Specification::MultiplyBroadcastableLayerParams* NeuralNetworkLayer::mutable_multiplybroadcastable() {
  if (!has_multiplybroadcastable()) {
    clear_layer();
    set_has_multiplybroadcastable();
    layer_.multiplybroadcastable_ = new ::CoreML::Specification::MultiplyBroadcastableLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.multiplyBroadcastable)
  return layer_.multiplybroadcastable_;
}
::CoreML::Specification::MultiplyBroadcastableLayerParams* NeuralNetworkLayer::release_multiplybroadcastable() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.multiplyBroadcastable)
  if (has_multiplybroadcastable()) {
    clear_has_layer();
    ::CoreML::Specification::MultiplyBroadcastableLayerParams* temp = layer_.multiplybroadcastable_;
    layer_.multiplybroadcastable_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_multiplybroadcastable(::CoreML::Specification::MultiplyBroadcastableLayerParams* multiplybroadcastable) {
  clear_layer();
  if (multiplybroadcastable) {
    set_has_multiplybroadcastable();
    layer_.multiplybroadcastable_ = multiplybroadcastable;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.multiplyBroadcastable)
}

// .CoreML.Specification.SubtractBroadcastableLayerParams subtractBroadcastable = 905;
bool NeuralNetworkLayer::has_subtractbroadcastable() const {
  return layer_case() == kSubtractBroadcastable;
}
void NeuralNetworkLayer::set_has_subtractbroadcastable() {
  _oneof_case_[0] = kSubtractBroadcastable;
}
void NeuralNetworkLayer::clear_subtractbroadcastable() {
  if (has_subtractbroadcastable()) {
    delete layer_.subtractbroadcastable_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::SubtractBroadcastableLayerParams& NeuralNetworkLayer::subtractbroadcastable() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.subtractBroadcastable)
  return has_subtractbroadcastable()
      ? *layer_.subtractbroadcastable_
      : ::CoreML::Specification::SubtractBroadcastableLayerParams::default_instance();
}
::CoreML::Specification::SubtractBroadcastableLayerParams* NeuralNetworkLayer::mutable_subtractbroadcastable() {
  if (!has_subtractbroadcastable()) {
    clear_layer();
    set_has_subtractbroadcastable();
    layer_.subtractbroadcastable_ = new ::CoreML::Specification::SubtractBroadcastableLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.subtractBroadcastable)
  return layer_.subtractbroadcastable_;
}
::CoreML::Specification::SubtractBroadcastableLayerParams* NeuralNetworkLayer::release_subtractbroadcastable() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.subtractBroadcastable)
  if (has_subtractbroadcastable()) {
    clear_has_layer();
    ::CoreML::Specification::SubtractBroadcastableLayerParams* temp = layer_.subtractbroadcastable_;
    layer_.subtractbroadcastable_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_subtractbroadcastable(::CoreML::Specification::SubtractBroadcastableLayerParams* subtractbroadcastable) {
  clear_layer();
  if (subtractbroadcastable) {
    set_has_subtractbroadcastable();
    layer_.subtractbroadcastable_ = subtractbroadcastable;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.subtractBroadcastable)
}

// .CoreML.Specification.TileLayerParams tile = 920;
bool NeuralNetworkLayer::has_tile() const {
  return layer_case() == kTile;
}
void NeuralNetworkLayer::set_has_tile() {
  _oneof_case_[0] = kTile;
}
void NeuralNetworkLayer::clear_tile() {
  if (has_tile()) {
    delete layer_.tile_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::TileLayerParams& NeuralNetworkLayer::tile() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.tile)
  return has_tile()
      ? *layer_.tile_
      : ::CoreML::Specification::TileLayerParams::default_instance();
}
::CoreML::Specification::TileLayerParams* NeuralNetworkLayer::mutable_tile() {
  if (!has_tile()) {
    clear_layer();
    set_has_tile();
    layer_.tile_ = new ::CoreML::Specification::TileLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.tile)
  return layer_.tile_;
}
::CoreML::Specification::TileLayerParams* NeuralNetworkLayer::release_tile() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.tile)
  if (has_tile()) {
    clear_has_layer();
    ::CoreML::Specification::TileLayerParams* temp = layer_.tile_;
    layer_.tile_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_tile(::CoreML::Specification::TileLayerParams* tile) {
  clear_layer();
  if (tile) {
    set_has_tile();
    layer_.tile_ = tile;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.tile)
}

// .CoreML.Specification.StackNDLayerParams stackND = 925;
bool NeuralNetworkLayer::has_stacknd() const {
  return layer_case() == kStackND;
}
void NeuralNetworkLayer::set_has_stacknd() {
  _oneof_case_[0] = kStackND;
}
void NeuralNetworkLayer::clear_stacknd() {
  if (has_stacknd()) {
    delete layer_.stacknd_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::StackNDLayerParams& NeuralNetworkLayer::stacknd() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.stackND)
  return has_stacknd()
      ? *layer_.stacknd_
      : ::CoreML::Specification::StackNDLayerParams::default_instance();
}
::CoreML::Specification::StackNDLayerParams* NeuralNetworkLayer::mutable_stacknd() {
  if (!has_stacknd()) {
    clear_layer();
    set_has_stacknd();
    layer_.stacknd_ = new ::CoreML::Specification::StackNDLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.stackND)
  return layer_.stacknd_;
}
::CoreML::Specification::StackNDLayerParams* NeuralNetworkLayer::release_stacknd() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.stackND)
  if (has_stacknd()) {
    clear_has_layer();
    ::CoreML::Specification::StackNDLayerParams* temp = layer_.stacknd_;
    layer_.stacknd_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_stacknd(::CoreML::Specification::StackNDLayerParams* stacknd) {
  clear_layer();
  if (stacknd) {
    set_has_stacknd();
    layer_.stacknd_ = stacknd;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.stackND)
}

// .CoreML.Specification.GatherLayerParams gather = 930;
bool NeuralNetworkLayer::has_gather() const {
  return layer_case() == kGather;
}
void NeuralNetworkLayer::set_has_gather() {
  _oneof_case_[0] = kGather;
}
void NeuralNetworkLayer::clear_gather() {
  if (has_gather()) {
    delete layer_.gather_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::GatherLayerParams& NeuralNetworkLayer::gather() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.gather)
  return has_gather()
      ? *layer_.gather_
      : ::CoreML::Specification::GatherLayerParams::default_instance();
}
::CoreML::Specification::GatherLayerParams* NeuralNetworkLayer::mutable_gather() {
  if (!has_gather()) {
    clear_layer();
    set_has_gather();
    layer_.gather_ = new ::CoreML::Specification::GatherLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.gather)
  return layer_.gather_;
}
::CoreML::Specification::GatherLayerParams* NeuralNetworkLayer::release_gather() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.gather)
  if (has_gather()) {
    clear_has_layer();
    ::CoreML::Specification::GatherLayerParams* temp = layer_.gather_;
    layer_.gather_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_gather(::CoreML::Specification::GatherLayerParams* gather) {
  clear_layer();
  if (gather) {
    set_has_gather();
    layer_.gather_ = gather;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.gather)
}

// .CoreML.Specification.ScatterLayerParams scatter = 935;
bool NeuralNetworkLayer::has_scatter() const {
  return layer_case() == kScatter;
}
void NeuralNetworkLayer::set_has_scatter() {
  _oneof_case_[0] = kScatter;
}
void NeuralNetworkLayer::clear_scatter() {
  if (has_scatter()) {
    delete layer_.scatter_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::ScatterLayerParams& NeuralNetworkLayer::scatter() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.scatter)
  return has_scatter()
      ? *layer_.scatter_
      : ::CoreML::Specification::ScatterLayerParams::default_instance();
}
::CoreML::Specification::ScatterLayerParams* NeuralNetworkLayer::mutable_scatter() {
  if (!has_scatter()) {
    clear_layer();
    set_has_scatter();
    layer_.scatter_ = new ::CoreML::Specification::ScatterLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.scatter)
  return layer_.scatter_;
}
::CoreML::Specification::ScatterLayerParams* NeuralNetworkLayer::release_scatter() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.scatter)
  if (has_scatter()) {
    clear_has_layer();
    ::CoreML::Specification::ScatterLayerParams* temp = layer_.scatter_;
    layer_.scatter_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_scatter(::CoreML::Specification::ScatterLayerParams* scatter) {
  clear_layer();
  if (scatter) {
    set_has_scatter();
    layer_.scatter_ = scatter;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.scatter)
}

// .CoreML.Specification.SoftmaxNDLayerParams softmaxND = 950;
bool NeuralNetworkLayer::has_softmaxnd() const {
  return layer_case() == kSoftmaxND;
}
void NeuralNetworkLayer::set_has_softmaxnd() {
  _oneof_case_[0] = kSoftmaxND;
}
void NeuralNetworkLayer::clear_softmaxnd() {
  if (has_softmaxnd()) {
    delete layer_.softmaxnd_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::SoftmaxNDLayerParams& NeuralNetworkLayer::softmaxnd() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.softmaxND)
  return has_softmaxnd()
      ? *layer_.softmaxnd_
      : ::CoreML::Specification::SoftmaxNDLayerParams::default_instance();
}
::CoreML::Specification::SoftmaxNDLayerParams* NeuralNetworkLayer::mutable_softmaxnd() {
  if (!has_softmaxnd()) {
    clear_layer();
    set_has_softmaxnd();
    layer_.softmaxnd_ = new ::CoreML::Specification::SoftmaxNDLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.softmaxND)
  return layer_.softmaxnd_;
}
::CoreML::Specification::SoftmaxNDLayerParams* NeuralNetworkLayer::release_softmaxnd() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.softmaxND)
  if (has_softmaxnd()) {
    clear_has_layer();
    ::CoreML::Specification::SoftmaxNDLayerParams* temp = layer_.softmaxnd_;
    layer_.softmaxnd_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_softmaxnd(::CoreML::Specification::SoftmaxNDLayerParams* softmaxnd) {
  clear_layer();
  if (softmaxnd) {
    set_has_softmaxnd();
    layer_.softmaxnd_ = softmaxnd;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.softmaxND)
}

// .CoreML.Specification.SplitNDLayerParams splitND = 975;
bool NeuralNetworkLayer::has_splitnd() const {
  return layer_case() == kSplitND;
}
void NeuralNetworkLayer::set_has_splitnd() {
  _oneof_case_[0] = kSplitND;
}
void NeuralNetworkLayer::clear_splitnd() {
  if (has_splitnd()) {
    delete layer_.splitnd_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::SplitNDLayerParams& NeuralNetworkLayer::splitnd() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.splitND)
  return has_splitnd()
      ? *layer_.splitnd_
      : ::CoreML::Specification::SplitNDLayerParams::default_instance();
}
::CoreML::Specification::SplitNDLayerParams* NeuralNetworkLayer::mutable_splitnd() {
  if (!has_splitnd()) {
    clear_layer();
    set_has_splitnd();
    layer_.splitnd_ = new ::CoreML::Specification::SplitNDLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.splitND)
  return layer_.splitnd_;
}
::CoreML::Specification::SplitNDLayerParams* NeuralNetworkLayer::release_splitnd() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.splitND)
  if (has_splitnd()) {
    clear_has_layer();
    ::CoreML::Specification::SplitNDLayerParams* temp = layer_.splitnd_;
    layer_.splitnd_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_splitnd(::CoreML::Specification::SplitNDLayerParams* splitnd) {
  clear_layer();
  if (splitnd) {
    set_has_splitnd();
    layer_.splitnd_ = splitnd;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.splitND)
}

// .CoreML.Specification.ConcatNDLayerParams concatND = 980;
bool NeuralNetworkLayer::has_concatnd() const {
  return layer_case() == kConcatND;
}
void NeuralNetworkLayer::set_has_concatnd() {
  _oneof_case_[0] = kConcatND;
}
void NeuralNetworkLayer::clear_concatnd() {
  if (has_concatnd()) {
    delete layer_.concatnd_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::ConcatNDLayerParams& NeuralNetworkLayer::concatnd() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.concatND)
  return has_concatnd()
      ? *layer_.concatnd_
      : ::CoreML::Specification::ConcatNDLayerParams::default_instance();
}
::CoreML::Specification::ConcatNDLayerParams* NeuralNetworkLayer::mutable_concatnd() {
  if (!has_concatnd()) {
    clear_layer();
    set_has_concatnd();
    layer_.concatnd_ = new ::CoreML::Specification::ConcatNDLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.concatND)
  return layer_.concatnd_;
}
::CoreML::Specification::ConcatNDLayerParams* NeuralNetworkLayer::release_concatnd() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.concatND)
  if (has_concatnd()) {
    clear_has_layer();
    ::CoreML::Specification::ConcatNDLayerParams* temp = layer_.concatnd_;
    layer_.concatnd_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_concatnd(::CoreML::Specification::ConcatNDLayerParams* concatnd) {
  clear_layer();
  if (concatnd) {
    set_has_concatnd();
    layer_.concatnd_ = concatnd;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.concatND)
}

// .CoreML.Specification.TransposeNDLayerParams transposeND = 985;
bool NeuralNetworkLayer::has_transposend() const {
  return layer_case() == kTransposeND;
}
void NeuralNetworkLayer::set_has_transposend() {
  _oneof_case_[0] = kTransposeND;
}
void NeuralNetworkLayer::clear_transposend() {
  if (has_transposend()) {
    delete layer_.transposend_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::TransposeNDLayerParams& NeuralNetworkLayer::transposend() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.transposeND)
  return has_transposend()
      ? *layer_.transposend_
      : ::CoreML::Specification::TransposeNDLayerParams::default_instance();
}
::CoreML::Specification::TransposeNDLayerParams* NeuralNetworkLayer::mutable_transposend() {
  if (!has_transposend()) {
    clear_layer();
    set_has_transposend();
    layer_.transposend_ = new ::CoreML::Specification::TransposeNDLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.transposeND)
  return layer_.transposend_;
}
::CoreML::Specification::TransposeNDLayerParams* NeuralNetworkLayer::release_transposend() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.transposeND)
  if (has_transposend()) {
    clear_has_layer();
    ::CoreML::Specification::TransposeNDLayerParams* temp = layer_.transposend_;
    layer_.transposend_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_transposend(::CoreML::Specification::TransposeNDLayerParams* transposend) {
  clear_layer();
  if (transposend) {
    set_has_transposend();
    layer_.transposend_ = transposend;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.transposeND)
}

// .CoreML.Specification.SliceNDLayerParams sliceND = 995;
bool NeuralNetworkLayer::has_slicend() const {
  return layer_case() == kSliceND;
}
void NeuralNetworkLayer::set_has_slicend() {
  _oneof_case_[0] = kSliceND;
}
void NeuralNetworkLayer::clear_slicend() {
  if (has_slicend()) {
    delete layer_.slicend_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::SliceNDLayerParams& NeuralNetworkLayer::slicend() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.sliceND)
  return has_slicend()
      ? *layer_.slicend_
      : ::CoreML::Specification::SliceNDLayerParams::default_instance();
}
::CoreML::Specification::SliceNDLayerParams* NeuralNetworkLayer::mutable_slicend() {
  if (!has_slicend()) {
    clear_layer();
    set_has_slicend();
    layer_.slicend_ = new ::CoreML::Specification::SliceNDLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.sliceND)
  return layer_.slicend_;
}
::CoreML::Specification::SliceNDLayerParams* NeuralNetworkLayer::release_slicend() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.sliceND)
  if (has_slicend()) {
    clear_has_layer();
    ::CoreML::Specification::SliceNDLayerParams* temp = layer_.slicend_;
    layer_.slicend_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_slicend(::CoreML::Specification::SliceNDLayerParams* slicend) {
  clear_layer();
  if (slicend) {
    set_has_slicend();
    layer_.slicend_ = slicend;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.sliceND)
}

// .CoreML.Specification.SlidingWindowsLayerParams slidingWindows = 1005;
bool NeuralNetworkLayer::has_slidingwindows() const {
  return layer_case() == kSlidingWindows;
}
void NeuralNetworkLayer::set_has_slidingwindows() {
  _oneof_case_[0] = kSlidingWindows;
}
void NeuralNetworkLayer::clear_slidingwindows() {
  if (has_slidingwindows()) {
    delete layer_.slidingwindows_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::SlidingWindowsLayerParams& NeuralNetworkLayer::slidingwindows() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.slidingWindows)
  return has_slidingwindows()
      ? *layer_.slidingwindows_
      : ::CoreML::Specification::SlidingWindowsLayerParams::default_instance();
}
::CoreML::Specification::SlidingWindowsLayerParams* NeuralNetworkLayer::mutable_slidingwindows() {
  if (!has_slidingwindows()) {
    clear_layer();
    set_has_slidingwindows();
    layer_.slidingwindows_ = new ::CoreML::Specification::SlidingWindowsLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.slidingWindows)
  return layer_.slidingwindows_;
}
::CoreML::Specification::SlidingWindowsLayerParams* NeuralNetworkLayer::release_slidingwindows() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.slidingWindows)
  if (has_slidingwindows()) {
    clear_has_layer();
    ::CoreML::Specification::SlidingWindowsLayerParams* temp = layer_.slidingwindows_;
    layer_.slidingwindows_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_slidingwindows(::CoreML::Specification::SlidingWindowsLayerParams* slidingwindows) {
  clear_layer();
  if (slidingwindows) {
    set_has_slidingwindows();
    layer_.slidingwindows_ = slidingwindows;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.slidingWindows)
}

// .CoreML.Specification.EmbeddingNDLayerParams embeddingND = 1040;
bool NeuralNetworkLayer::has_embeddingnd() const {
  return layer_case() == kEmbeddingND;
}
void NeuralNetworkLayer::set_has_embeddingnd() {
  _oneof_case_[0] = kEmbeddingND;
}
void NeuralNetworkLayer::clear_embeddingnd() {
  if (has_embeddingnd()) {
    delete layer_.embeddingnd_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::EmbeddingNDLayerParams& NeuralNetworkLayer::embeddingnd() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.embeddingND)
  return has_embeddingnd()
      ? *layer_.embeddingnd_
      : ::CoreML::Specification::EmbeddingNDLayerParams::default_instance();
}
::CoreML::Specification::EmbeddingNDLayerParams* NeuralNetworkLayer::mutable_embeddingnd() {
  if (!has_embeddingnd()) {
    clear_layer();
    set_has_embeddingnd();
    layer_.embeddingnd_ = new ::CoreML::Specification::EmbeddingNDLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.embeddingND)
  return layer_.embeddingnd_;
}
::CoreML::Specification::EmbeddingNDLayerParams* NeuralNetworkLayer::release_embeddingnd() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.embeddingND)
  if (has_embeddingnd()) {
    clear_has_layer();
    ::CoreML::Specification::EmbeddingNDLayerParams* temp = layer_.embeddingnd_;
    layer_.embeddingnd_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_embeddingnd(::CoreML::Specification::EmbeddingNDLayerParams* embeddingnd) {
  clear_layer();
  if (embeddingnd) {
    set_has_embeddingnd();
    layer_.embeddingnd_ = embeddingnd;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.embeddingND)
}

// .CoreML.Specification.BatchedMatMulParams batchedMatmul = 1045;
bool NeuralNetworkLayer::has_batchedmatmul() const {
  return layer_case() == kBatchedMatmul;
}
void NeuralNetworkLayer::set_has_batchedmatmul() {
  _oneof_case_[0] = kBatchedMatmul;
}
void NeuralNetworkLayer::clear_batchedmatmul() {
  if (has_batchedmatmul()) {
    delete layer_.batchedmatmul_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::BatchedMatMulParams& NeuralNetworkLayer::batchedmatmul() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.batchedMatmul)
  return has_batchedmatmul()
      ? *layer_.batchedmatmul_
      : ::CoreML::Specification::BatchedMatMulParams::default_instance();
}
::CoreML::Specification::BatchedMatMulParams* NeuralNetworkLayer::mutable_batchedmatmul() {
  if (!has_batchedmatmul()) {
    clear_layer();
    set_has_batchedmatmul();
    layer_.batchedmatmul_ = new ::CoreML::Specification::BatchedMatMulParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.batchedMatmul)
  return layer_.batchedmatmul_;
}
::CoreML::Specification::BatchedMatMulParams* NeuralNetworkLayer::release_batchedmatmul() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.batchedMatmul)
  if (has_batchedmatmul()) {
    clear_has_layer();
    ::CoreML::Specification::BatchedMatMulParams* temp = layer_.batchedmatmul_;
    layer_.batchedmatmul_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_batchedmatmul(::CoreML::Specification::BatchedMatMulParams* batchedmatmul) {
  clear_layer();
  if (batchedmatmul) {
    set_has_batchedmatmul();
    layer_.batchedmatmul_ = batchedmatmul;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.batchedMatmul)
}

// .CoreML.Specification.GetShapeLayerParams getShape = 1065;
bool NeuralNetworkLayer::has_getshape() const {
  return layer_case() == kGetShape;
}
void NeuralNetworkLayer::set_has_getshape() {
  _oneof_case_[0] = kGetShape;
}
void NeuralNetworkLayer::clear_getshape() {
  if (has_getshape()) {
    delete layer_.getshape_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::GetShapeLayerParams& NeuralNetworkLayer::getshape() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.getShape)
  return has_getshape()
      ? *layer_.getshape_
      : ::CoreML::Specification::GetShapeLayerParams::default_instance();
}
::CoreML::Specification::GetShapeLayerParams* NeuralNetworkLayer::mutable_getshape() {
  if (!has_getshape()) {
    clear_layer();
    set_has_getshape();
    layer_.getshape_ = new ::CoreML::Specification::GetShapeLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.getShape)
  return layer_.getshape_;
}
::CoreML::Specification::GetShapeLayerParams* NeuralNetworkLayer::release_getshape() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.getShape)
  if (has_getshape()) {
    clear_has_layer();
    ::CoreML::Specification::GetShapeLayerParams* temp = layer_.getshape_;
    layer_.getshape_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_getshape(::CoreML::Specification::GetShapeLayerParams* getshape) {
  clear_layer();
  if (getshape) {
    set_has_getshape();
    layer_.getshape_ = getshape;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.getShape)
}

// .CoreML.Specification.LoadConstantNDLayerParams loadConstantND = 1070;
bool NeuralNetworkLayer::has_loadconstantnd() const {
  return layer_case() == kLoadConstantND;
}
void NeuralNetworkLayer::set_has_loadconstantnd() {
  _oneof_case_[0] = kLoadConstantND;
}
void NeuralNetworkLayer::clear_loadconstantnd() {
  if (has_loadconstantnd()) {
    delete layer_.loadconstantnd_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::LoadConstantNDLayerParams& NeuralNetworkLayer::loadconstantnd() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.loadConstantND)
  return has_loadconstantnd()
      ? *layer_.loadconstantnd_
      : ::CoreML::Specification::LoadConstantNDLayerParams::default_instance();
}
::CoreML::Specification::LoadConstantNDLayerParams* NeuralNetworkLayer::mutable_loadconstantnd() {
  if (!has_loadconstantnd()) {
    clear_layer();
    set_has_loadconstantnd();
    layer_.loadconstantnd_ = new ::CoreML::Specification::LoadConstantNDLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.loadConstantND)
  return layer_.loadconstantnd_;
}
::CoreML::Specification::LoadConstantNDLayerParams* NeuralNetworkLayer::release_loadconstantnd() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.loadConstantND)
  if (has_loadconstantnd()) {
    clear_has_layer();
    ::CoreML::Specification::LoadConstantNDLayerParams* temp = layer_.loadconstantnd_;
    layer_.loadconstantnd_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_loadconstantnd(::CoreML::Specification::LoadConstantNDLayerParams* loadconstantnd) {
  clear_layer();
  if (loadconstantnd) {
    set_has_loadconstantnd();
    layer_.loadconstantnd_ = loadconstantnd;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.loadConstantND)
}

// .CoreML.Specification.FillLikeLayerParams fillLike = 1080;
bool NeuralNetworkLayer::has_filllike() const {
  return layer_case() == kFillLike;
}
void NeuralNetworkLayer::set_has_filllike() {
  _oneof_case_[0] = kFillLike;
}
void NeuralNetworkLayer::clear_filllike() {
  if (has_filllike()) {
    delete layer_.filllike_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::FillLikeLayerParams& NeuralNetworkLayer::filllike() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.fillLike)
  return has_filllike()
      ? *layer_.filllike_
      : ::CoreML::Specification::FillLikeLayerParams::default_instance();
}
::CoreML::Specification::FillLikeLayerParams* NeuralNetworkLayer::mutable_filllike() {
  if (!has_filllike()) {
    clear_layer();
    set_has_filllike();
    layer_.filllike_ = new ::CoreML::Specification::FillLikeLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.fillLike)
  return layer_.filllike_;
}
::CoreML::Specification::FillLikeLayerParams* NeuralNetworkLayer::release_filllike() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.fillLike)
  if (has_filllike()) {
    clear_has_layer();
    ::CoreML::Specification::FillLikeLayerParams* temp = layer_.filllike_;
    layer_.filllike_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_filllike(::CoreML::Specification::FillLikeLayerParams* filllike) {
  clear_layer();
  if (filllike) {
    set_has_filllike();
    layer_.filllike_ = filllike;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.fillLike)
}

// .CoreML.Specification.FillStaticLayerParams fillStatic = 1085;
bool NeuralNetworkLayer::has_fillstatic() const {
  return layer_case() == kFillStatic;
}
void NeuralNetworkLayer::set_has_fillstatic() {
  _oneof_case_[0] = kFillStatic;
}
void NeuralNetworkLayer::clear_fillstatic() {
  if (has_fillstatic()) {
    delete layer_.fillstatic_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::FillStaticLayerParams& NeuralNetworkLayer::fillstatic() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.fillStatic)
  return has_fillstatic()
      ? *layer_.fillstatic_
      : ::CoreML::Specification::FillStaticLayerParams::default_instance();
}
::CoreML::Specification::FillStaticLayerParams* NeuralNetworkLayer::mutable_fillstatic() {
  if (!has_fillstatic()) {
    clear_layer();
    set_has_fillstatic();
    layer_.fillstatic_ = new ::CoreML::Specification::FillStaticLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.fillStatic)
  return layer_.fillstatic_;
}
::CoreML::Specification::FillStaticLayerParams* NeuralNetworkLayer::release_fillstatic() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.fillStatic)
  if (has_fillstatic()) {
    clear_has_layer();
    ::CoreML::Specification::FillStaticLayerParams* temp = layer_.fillstatic_;
    layer_.fillstatic_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_fillstatic(::CoreML::Specification::FillStaticLayerParams* fillstatic) {
  clear_layer();
  if (fillstatic) {
    set_has_fillstatic();
    layer_.fillstatic_ = fillstatic;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.fillStatic)
}

// .CoreML.Specification.FillDynamicLayerParams fillDynamic = 1090;
bool NeuralNetworkLayer::has_filldynamic() const {
  return layer_case() == kFillDynamic;
}
void NeuralNetworkLayer::set_has_filldynamic() {
  _oneof_case_[0] = kFillDynamic;
}
void NeuralNetworkLayer::clear_filldynamic() {
  if (has_filldynamic()) {
    delete layer_.filldynamic_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::FillDynamicLayerParams& NeuralNetworkLayer::filldynamic() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.fillDynamic)
  return has_filldynamic()
      ? *layer_.filldynamic_
      : ::CoreML::Specification::FillDynamicLayerParams::default_instance();
}
::CoreML::Specification::FillDynamicLayerParams* NeuralNetworkLayer::mutable_filldynamic() {
  if (!has_filldynamic()) {
    clear_layer();
    set_has_filldynamic();
    layer_.filldynamic_ = new ::CoreML::Specification::FillDynamicLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.fillDynamic)
  return layer_.filldynamic_;
}
::CoreML::Specification::FillDynamicLayerParams* NeuralNetworkLayer::release_filldynamic() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.fillDynamic)
  if (has_filldynamic()) {
    clear_has_layer();
    ::CoreML::Specification::FillDynamicLayerParams* temp = layer_.filldynamic_;
    layer_.filldynamic_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_filldynamic(::CoreML::Specification::FillDynamicLayerParams* filldynamic) {
  clear_layer();
  if (filldynamic) {
    set_has_filldynamic();
    layer_.filldynamic_ = filldynamic;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.fillDynamic)
}

// .CoreML.Specification.BroadcastToLikeLayerParams broadcastToLike = 1100;
bool NeuralNetworkLayer::has_broadcasttolike() const {
  return layer_case() == kBroadcastToLike;
}
void NeuralNetworkLayer::set_has_broadcasttolike() {
  _oneof_case_[0] = kBroadcastToLike;
}
void NeuralNetworkLayer::clear_broadcasttolike() {
  if (has_broadcasttolike()) {
    delete layer_.broadcasttolike_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::BroadcastToLikeLayerParams& NeuralNetworkLayer::broadcasttolike() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.broadcastToLike)
  return has_broadcasttolike()
      ? *layer_.broadcasttolike_
      : ::CoreML::Specification::BroadcastToLikeLayerParams::default_instance();
}
::CoreML::Specification::BroadcastToLikeLayerParams* NeuralNetworkLayer::mutable_broadcasttolike() {
  if (!has_broadcasttolike()) {
    clear_layer();
    set_has_broadcasttolike();
    layer_.broadcasttolike_ = new ::CoreML::Specification::BroadcastToLikeLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.broadcastToLike)
  return layer_.broadcasttolike_;
}
::CoreML::Specification::BroadcastToLikeLayerParams* NeuralNetworkLayer::release_broadcasttolike() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.broadcastToLike)
  if (has_broadcasttolike()) {
    clear_has_layer();
    ::CoreML::Specification::BroadcastToLikeLayerParams* temp = layer_.broadcasttolike_;
    layer_.broadcasttolike_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_broadcasttolike(::CoreML::Specification::BroadcastToLikeLayerParams* broadcasttolike) {
  clear_layer();
  if (broadcasttolike) {
    set_has_broadcasttolike();
    layer_.broadcasttolike_ = broadcasttolike;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.broadcastToLike)
}

// .CoreML.Specification.BroadcastToStaticLayerParams broadcastToStatic = 1105;
bool NeuralNetworkLayer::has_broadcasttostatic() const {
  return layer_case() == kBroadcastToStatic;
}
void NeuralNetworkLayer::set_has_broadcasttostatic() {
  _oneof_case_[0] = kBroadcastToStatic;
}
void NeuralNetworkLayer::clear_broadcasttostatic() {
  if (has_broadcasttostatic()) {
    delete layer_.broadcasttostatic_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::BroadcastToStaticLayerParams& NeuralNetworkLayer::broadcasttostatic() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.broadcastToStatic)
  return has_broadcasttostatic()
      ? *layer_.broadcasttostatic_
      : ::CoreML::Specification::BroadcastToStaticLayerParams::default_instance();
}
::CoreML::Specification::BroadcastToStaticLayerParams* NeuralNetworkLayer::mutable_broadcasttostatic() {
  if (!has_broadcasttostatic()) {
    clear_layer();
    set_has_broadcasttostatic();
    layer_.broadcasttostatic_ = new ::CoreML::Specification::BroadcastToStaticLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.broadcastToStatic)
  return layer_.broadcasttostatic_;
}
::CoreML::Specification::BroadcastToStaticLayerParams* NeuralNetworkLayer::release_broadcasttostatic() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.broadcastToStatic)
  if (has_broadcasttostatic()) {
    clear_has_layer();
    ::CoreML::Specification::BroadcastToStaticLayerParams* temp = layer_.broadcasttostatic_;
    layer_.broadcasttostatic_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_broadcasttostatic(::CoreML::Specification::BroadcastToStaticLayerParams* broadcasttostatic) {
  clear_layer();
  if (broadcasttostatic) {
    set_has_broadcasttostatic();
    layer_.broadcasttostatic_ = broadcasttostatic;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.broadcastToStatic)
}

// .CoreML.Specification.BroadcastToDynamicLayerParams broadcastToDynamic = 1110;
bool NeuralNetworkLayer::has_broadcasttodynamic() const {
  return layer_case() == kBroadcastToDynamic;
}
void NeuralNetworkLayer::set_has_broadcasttodynamic() {
  _oneof_case_[0] = kBroadcastToDynamic;
}
void NeuralNetworkLayer::clear_broadcasttodynamic() {
  if (has_broadcasttodynamic()) {
    delete layer_.broadcasttodynamic_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::BroadcastToDynamicLayerParams& NeuralNetworkLayer::broadcasttodynamic() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.broadcastToDynamic)
  return has_broadcasttodynamic()
      ? *layer_.broadcasttodynamic_
      : ::CoreML::Specification::BroadcastToDynamicLayerParams::default_instance();
}
::CoreML::Specification::BroadcastToDynamicLayerParams* NeuralNetworkLayer::mutable_broadcasttodynamic() {
  if (!has_broadcasttodynamic()) {
    clear_layer();
    set_has_broadcasttodynamic();
    layer_.broadcasttodynamic_ = new ::CoreML::Specification::BroadcastToDynamicLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.broadcastToDynamic)
  return layer_.broadcasttodynamic_;
}
::CoreML::Specification::BroadcastToDynamicLayerParams* NeuralNetworkLayer::release_broadcasttodynamic() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.broadcastToDynamic)
  if (has_broadcasttodynamic()) {
    clear_has_layer();
    ::CoreML::Specification::BroadcastToDynamicLayerParams* temp = layer_.broadcasttodynamic_;
    layer_.broadcasttodynamic_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_broadcasttodynamic(::CoreML::Specification::BroadcastToDynamicLayerParams* broadcasttodynamic) {
  clear_layer();
  if (broadcasttodynamic) {
    set_has_broadcasttodynamic();
    layer_.broadcasttodynamic_ = broadcasttodynamic;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.broadcastToDynamic)
}

// .CoreML.Specification.SqueezeLayerParams squeeze = 1120;
bool NeuralNetworkLayer::has_squeeze() const {
  return layer_case() == kSqueeze;
}
void NeuralNetworkLayer::set_has_squeeze() {
  _oneof_case_[0] = kSqueeze;
}
void NeuralNetworkLayer::clear_squeeze() {
  if (has_squeeze()) {
    delete layer_.squeeze_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::SqueezeLayerParams& NeuralNetworkLayer::squeeze() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.squeeze)
  return has_squeeze()
      ? *layer_.squeeze_
      : ::CoreML::Specification::SqueezeLayerParams::default_instance();
}
::CoreML::Specification::SqueezeLayerParams* NeuralNetworkLayer::mutable_squeeze() {
  if (!has_squeeze()) {
    clear_layer();
    set_has_squeeze();
    layer_.squeeze_ = new ::CoreML::Specification::SqueezeLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.squeeze)
  return layer_.squeeze_;
}
::CoreML::Specification::SqueezeLayerParams* NeuralNetworkLayer::release_squeeze() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.squeeze)
  if (has_squeeze()) {
    clear_has_layer();
    ::CoreML::Specification::SqueezeLayerParams* temp = layer_.squeeze_;
    layer_.squeeze_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_squeeze(::CoreML::Specification::SqueezeLayerParams* squeeze) {
  clear_layer();
  if (squeeze) {
    set_has_squeeze();
    layer_.squeeze_ = squeeze;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.squeeze)
}

// .CoreML.Specification.ExpandDimsLayerParams expandDims = 1125;
bool NeuralNetworkLayer::has_expanddims() const {
  return layer_case() == kExpandDims;
}
void NeuralNetworkLayer::set_has_expanddims() {
  _oneof_case_[0] = kExpandDims;
}
void NeuralNetworkLayer::clear_expanddims() {
  if (has_expanddims()) {
    delete layer_.expanddims_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::ExpandDimsLayerParams& NeuralNetworkLayer::expanddims() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.expandDims)
  return has_expanddims()
      ? *layer_.expanddims_
      : ::CoreML::Specification::ExpandDimsLayerParams::default_instance();
}
::CoreML::Specification::ExpandDimsLayerParams* NeuralNetworkLayer::mutable_expanddims() {
  if (!has_expanddims()) {
    clear_layer();
    set_has_expanddims();
    layer_.expanddims_ = new ::CoreML::Specification::ExpandDimsLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.expandDims)
  return layer_.expanddims_;
}
::CoreML::Specification::ExpandDimsLayerParams* NeuralNetworkLayer::release_expanddims() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.expandDims)
  if (has_expanddims()) {
    clear_has_layer();
    ::CoreML::Specification::ExpandDimsLayerParams* temp = layer_.expanddims_;
    layer_.expanddims_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_expanddims(::CoreML::Specification::ExpandDimsLayerParams* expanddims) {
  clear_layer();
  if (expanddims) {
    set_has_expanddims();
    layer_.expanddims_ = expanddims;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.expandDims)
}

// .CoreML.Specification.RankPreservingReshapeLayerParams rankPreservingReshape = 1150;
bool NeuralNetworkLayer::has_rankpreservingreshape() const {
  return layer_case() == kRankPreservingReshape;
}
void NeuralNetworkLayer::set_has_rankpreservingreshape() {
  _oneof_case_[0] = kRankPreservingReshape;
}
void NeuralNetworkLayer::clear_rankpreservingreshape() {
  if (has_rankpreservingreshape()) {
    delete layer_.rankpreservingreshape_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::RankPreservingReshapeLayerParams& NeuralNetworkLayer::rankpreservingreshape() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.rankPreservingReshape)
  return has_rankpreservingreshape()
      ? *layer_.rankpreservingreshape_
      : ::CoreML::Specification::RankPreservingReshapeLayerParams::default_instance();
}
::CoreML::Specification::RankPreservingReshapeLayerParams* NeuralNetworkLayer::mutable_rankpreservingreshape() {
  if (!has_rankpreservingreshape()) {
    clear_layer();
    set_has_rankpreservingreshape();
    layer_.rankpreservingreshape_ = new ::CoreML::Specification::RankPreservingReshapeLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.rankPreservingReshape)
  return layer_.rankpreservingreshape_;
}
::CoreML::Specification::RankPreservingReshapeLayerParams* NeuralNetworkLayer::release_rankpreservingreshape() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.rankPreservingReshape)
  if (has_rankpreservingreshape()) {
    clear_has_layer();
    ::CoreML::Specification::RankPreservingReshapeLayerParams* temp = layer_.rankpreservingreshape_;
    layer_.rankpreservingreshape_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_rankpreservingreshape(::CoreML::Specification::RankPreservingReshapeLayerParams* rankpreservingreshape) {
  clear_layer();
  if (rankpreservingreshape) {
    set_has_rankpreservingreshape();
    layer_.rankpreservingreshape_ = rankpreservingreshape;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.rankPreservingReshape)
}

// .CoreML.Specification.LowerTriangularLayerParams lowerTriangular = 1320;
bool NeuralNetworkLayer::has_lowertriangular() const {
  return layer_case() == kLowerTriangular;
}
void NeuralNetworkLayer::set_has_lowertriangular() {
  _oneof_case_[0] = kLowerTriangular;
}
void NeuralNetworkLayer::clear_lowertriangular() {
  if (has_lowertriangular()) {
    delete layer_.lowertriangular_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::LowerTriangularLayerParams& NeuralNetworkLayer::lowertriangular() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.lowerTriangular)
  return has_lowertriangular()
      ? *layer_.lowertriangular_
      : ::CoreML::Specification::LowerTriangularLayerParams::default_instance();
}
::CoreML::Specification::LowerTriangularLayerParams* NeuralNetworkLayer::mutable_lowertriangular() {
  if (!has_lowertriangular()) {
    clear_layer();
    set_has_lowertriangular();
    layer_.lowertriangular_ = new ::CoreML::Specification::LowerTriangularLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.lowerTriangular)
  return layer_.lowertriangular_;
}
::CoreML::Specification::LowerTriangularLayerParams* NeuralNetworkLayer::release_lowertriangular() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.lowerTriangular)
  if (has_lowertriangular()) {
    clear_has_layer();
    ::CoreML::Specification::LowerTriangularLayerParams* temp = layer_.lowertriangular_;
    layer_.lowertriangular_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_lowertriangular(::CoreML::Specification::LowerTriangularLayerParams* lowertriangular) {
  clear_layer();
  if (lowertriangular) {
    set_has_lowertriangular();
    layer_.lowertriangular_ = lowertriangular;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.lowerTriangular)
}

// .CoreML.Specification.UpperTriangularLayerParams upperTriangular = 1325;
bool NeuralNetworkLayer::has_uppertriangular() const {
  return layer_case() == kUpperTriangular;
}
void NeuralNetworkLayer::set_has_uppertriangular() {
  _oneof_case_[0] = kUpperTriangular;
}
void NeuralNetworkLayer::clear_uppertriangular() {
  if (has_uppertriangular()) {
    delete layer_.uppertriangular_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::UpperTriangularLayerParams& NeuralNetworkLayer::uppertriangular() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.upperTriangular)
  return has_uppertriangular()
      ? *layer_.uppertriangular_
      : ::CoreML::Specification::UpperTriangularLayerParams::default_instance();
}
::CoreML::Specification::UpperTriangularLayerParams* NeuralNetworkLayer::mutable_uppertriangular() {
  if (!has_uppertriangular()) {
    clear_layer();
    set_has_uppertriangular();
    layer_.uppertriangular_ = new ::CoreML::Specification::UpperTriangularLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.upperTriangular)
  return layer_.uppertriangular_;
}
::CoreML::Specification::UpperTriangularLayerParams* NeuralNetworkLayer::release_uppertriangular() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.upperTriangular)
  if (has_uppertriangular()) {
    clear_has_layer();
    ::CoreML::Specification::UpperTriangularLayerParams* temp = layer_.uppertriangular_;
    layer_.uppertriangular_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_uppertriangular(::CoreML::Specification::UpperTriangularLayerParams* uppertriangular) {
  clear_layer();
  if (uppertriangular) {
    set_has_uppertriangular();
    layer_.uppertriangular_ = uppertriangular;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.upperTriangular)
}

// .CoreML.Specification.WhereLayerParams where = 1330;
bool NeuralNetworkLayer::has_where() const {
  return layer_case() == kWhere;
}
void NeuralNetworkLayer::set_has_where() {
  _oneof_case_[0] = kWhere;
}
void NeuralNetworkLayer::clear_where() {
  if (has_where()) {
    delete layer_.where_;
    clear_has_layer();
  }
}
 const ::CoreML::Specification::WhereLayerParams& NeuralNetworkLayer::where() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkLayer.where)
  return has_where()
      ? *layer_.where_
      : ::CoreML::Specification::WhereLayerParams::default_instance();
}
::CoreML::Specification::WhereLayerParams* NeuralNetworkLayer::mutable_where() {
  if (!has_where()) {
    clear_layer();
    set_has_where();
    layer_.where_ = new ::CoreML::Specification::WhereLayerParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkLayer.where)
  return layer_.where_;
}
::CoreML::Specification::WhereLayerParams* NeuralNetworkLayer::release_where() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkLayer.where)
  if (has_where()) {
    clear_has_layer();
    ::CoreML::Specification::WhereLayerParams* temp = layer_.where_;
    layer_.where_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkLayer::set_allocated_where(::CoreML::Specification::WhereLayerParams* where) {
  clear_layer();
  if (where) {
    set_has_where();
    layer_.where_ = where;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkLayer.where)
}

bool NeuralNetworkLayer::has_layer() const {
  return layer_case() != LAYER_NOT_SET;
}
void NeuralNetworkLayer::clear_has_layer() {
  _oneof_case_[0] = LAYER_NOT_SET;
}
NeuralNetworkLayer::LayerCase NeuralNetworkLayer::layer_case() const {
  return NeuralNetworkLayer::LayerCase(_oneof_case_[0]);
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int BranchLayerParams::kIfBranchFieldNumber;
const int BranchLayerParams::kElseBranchFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

BranchLayerParams::BranchLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.BranchLayerParams)
}
BranchLayerParams::BranchLayerParams(const BranchLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_ifbranch()) {
    ifbranch_ = new ::CoreML::Specification::NeuralNetwork(*from.ifbranch_);
  } else {
    ifbranch_ = NULL;
  }
  if (from.has_elsebranch()) {
    elsebranch_ = new ::CoreML::Specification::NeuralNetwork(*from.elsebranch_);
  } else {
    elsebranch_ = NULL;
  }
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.BranchLayerParams)
}

void BranchLayerParams::SharedCtor() {
  ::memset(&ifbranch_, 0, reinterpret_cast<char*>(&elsebranch_) -
    reinterpret_cast<char*>(&ifbranch_) + sizeof(elsebranch_));
  _cached_size_ = 0;
}

BranchLayerParams::~BranchLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.BranchLayerParams)
  SharedDtor();
}

void BranchLayerParams::SharedDtor() {
  if (this != internal_default_instance()) {
    delete ifbranch_;
  }
  if (this != internal_default_instance()) {
    delete elsebranch_;
  }
}

void BranchLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const BranchLayerParams& BranchLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

BranchLayerParams* BranchLayerParams::New(::google::protobuf::Arena* arena) const {
  BranchLayerParams* n = new BranchLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void BranchLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.BranchLayerParams)
  if (GetArenaNoVirtual() == NULL && ifbranch_ != NULL) {
    delete ifbranch_;
  }
  ifbranch_ = NULL;
  if (GetArenaNoVirtual() == NULL && elsebranch_ != NULL) {
    delete elsebranch_;
  }
  elsebranch_ = NULL;
}

bool BranchLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.BranchLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // .CoreML.Specification.NeuralNetwork ifBranch = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_ifbranch()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.NeuralNetwork elseBranch = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(18u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_elsebranch()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.BranchLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.BranchLayerParams)
  return false;
#undef DO_
}

void BranchLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.BranchLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // .CoreML.Specification.NeuralNetwork ifBranch = 1;
  if (this->has_ifbranch()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1, *this->ifbranch_, output);
  }

  // .CoreML.Specification.NeuralNetwork elseBranch = 2;
  if (this->has_elsebranch()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      2, *this->elsebranch_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.BranchLayerParams)
}

size_t BranchLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.BranchLayerParams)
  size_t total_size = 0;

  // .CoreML.Specification.NeuralNetwork ifBranch = 1;
  if (this->has_ifbranch()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->ifbranch_);
  }

  // .CoreML.Specification.NeuralNetwork elseBranch = 2;
  if (this->has_elsebranch()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->elsebranch_);
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void BranchLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const BranchLayerParams*>(&from));
}

void BranchLayerParams::MergeFrom(const BranchLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.BranchLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.has_ifbranch()) {
    mutable_ifbranch()->::CoreML::Specification::NeuralNetwork::MergeFrom(from.ifbranch());
  }
  if (from.has_elsebranch()) {
    mutable_elsebranch()->::CoreML::Specification::NeuralNetwork::MergeFrom(from.elsebranch());
  }
}

void BranchLayerParams::CopyFrom(const BranchLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.BranchLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool BranchLayerParams::IsInitialized() const {
  return true;
}

void BranchLayerParams::Swap(BranchLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void BranchLayerParams::InternalSwap(BranchLayerParams* other) {
  std::swap(ifbranch_, other->ifbranch_);
  std::swap(elsebranch_, other->elsebranch_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string BranchLayerParams::GetTypeName() const {
  return "CoreML.Specification.BranchLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// BranchLayerParams

// .CoreML.Specification.NeuralNetwork ifBranch = 1;
bool BranchLayerParams::has_ifbranch() const {
  return this != internal_default_instance() && ifbranch_ != NULL;
}
void BranchLayerParams::clear_ifbranch() {
  if (GetArenaNoVirtual() == NULL && ifbranch_ != NULL) delete ifbranch_;
  ifbranch_ = NULL;
}
const ::CoreML::Specification::NeuralNetwork& BranchLayerParams::ifbranch() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BranchLayerParams.ifBranch)
  return ifbranch_ != NULL ? *ifbranch_
                         : *::CoreML::Specification::NeuralNetwork::internal_default_instance();
}
::CoreML::Specification::NeuralNetwork* BranchLayerParams::mutable_ifbranch() {
  
  if (ifbranch_ == NULL) {
    ifbranch_ = new ::CoreML::Specification::NeuralNetwork;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BranchLayerParams.ifBranch)
  return ifbranch_;
}
::CoreML::Specification::NeuralNetwork* BranchLayerParams::release_ifbranch() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.BranchLayerParams.ifBranch)
  
  ::CoreML::Specification::NeuralNetwork* temp = ifbranch_;
  ifbranch_ = NULL;
  return temp;
}
void BranchLayerParams::set_allocated_ifbranch(::CoreML::Specification::NeuralNetwork* ifbranch) {
  delete ifbranch_;
  ifbranch_ = ifbranch;
  if (ifbranch) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.BranchLayerParams.ifBranch)
}

// .CoreML.Specification.NeuralNetwork elseBranch = 2;
bool BranchLayerParams::has_elsebranch() const {
  return this != internal_default_instance() && elsebranch_ != NULL;
}
void BranchLayerParams::clear_elsebranch() {
  if (GetArenaNoVirtual() == NULL && elsebranch_ != NULL) delete elsebranch_;
  elsebranch_ = NULL;
}
const ::CoreML::Specification::NeuralNetwork& BranchLayerParams::elsebranch() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BranchLayerParams.elseBranch)
  return elsebranch_ != NULL ? *elsebranch_
                         : *::CoreML::Specification::NeuralNetwork::internal_default_instance();
}
::CoreML::Specification::NeuralNetwork* BranchLayerParams::mutable_elsebranch() {
  
  if (elsebranch_ == NULL) {
    elsebranch_ = new ::CoreML::Specification::NeuralNetwork;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BranchLayerParams.elseBranch)
  return elsebranch_;
}
::CoreML::Specification::NeuralNetwork* BranchLayerParams::release_elsebranch() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.BranchLayerParams.elseBranch)
  
  ::CoreML::Specification::NeuralNetwork* temp = elsebranch_;
  elsebranch_ = NULL;
  return temp;
}
void BranchLayerParams::set_allocated_elsebranch(::CoreML::Specification::NeuralNetwork* elsebranch) {
  delete elsebranch_;
  elsebranch_ = elsebranch;
  if (elsebranch) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.BranchLayerParams.elseBranch)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int LoopLayerParams::kMaxLoopIterationsFieldNumber;
const int LoopLayerParams::kConditionVarFieldNumber;
const int LoopLayerParams::kConditionNetworkFieldNumber;
const int LoopLayerParams::kBodyNetworkFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

LoopLayerParams::LoopLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.LoopLayerParams)
}
LoopLayerParams::LoopLayerParams(const LoopLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  conditionvar_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (from.conditionvar().size() > 0) {
    conditionvar_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.conditionvar_);
  }
  if (from.has_conditionnetwork()) {
    conditionnetwork_ = new ::CoreML::Specification::NeuralNetwork(*from.conditionnetwork_);
  } else {
    conditionnetwork_ = NULL;
  }
  if (from.has_bodynetwork()) {
    bodynetwork_ = new ::CoreML::Specification::NeuralNetwork(*from.bodynetwork_);
  } else {
    bodynetwork_ = NULL;
  }
  maxloopiterations_ = from.maxloopiterations_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.LoopLayerParams)
}

void LoopLayerParams::SharedCtor() {
  conditionvar_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  ::memset(&conditionnetwork_, 0, reinterpret_cast<char*>(&maxloopiterations_) -
    reinterpret_cast<char*>(&conditionnetwork_) + sizeof(maxloopiterations_));
  _cached_size_ = 0;
}

LoopLayerParams::~LoopLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.LoopLayerParams)
  SharedDtor();
}

void LoopLayerParams::SharedDtor() {
  conditionvar_.DestroyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (this != internal_default_instance()) {
    delete conditionnetwork_;
  }
  if (this != internal_default_instance()) {
    delete bodynetwork_;
  }
}

void LoopLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const LoopLayerParams& LoopLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

LoopLayerParams* LoopLayerParams::New(::google::protobuf::Arena* arena) const {
  LoopLayerParams* n = new LoopLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void LoopLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.LoopLayerParams)
  conditionvar_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (GetArenaNoVirtual() == NULL && conditionnetwork_ != NULL) {
    delete conditionnetwork_;
  }
  conditionnetwork_ = NULL;
  if (GetArenaNoVirtual() == NULL && bodynetwork_ != NULL) {
    delete bodynetwork_;
  }
  bodynetwork_ = NULL;
  maxloopiterations_ = GOOGLE_ULONGLONG(0);
}

bool LoopLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.LoopLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // uint64 maxLoopIterations = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &maxloopiterations_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // string conditionVar = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(18u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadString(
                input, this->mutable_conditionvar()));
          DO_(::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
            this->conditionvar().data(), this->conditionvar().length(),
            ::google::protobuf::internal::WireFormatLite::PARSE,
            "CoreML.Specification.LoopLayerParams.conditionVar"));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.NeuralNetwork conditionNetwork = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(26u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_conditionnetwork()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.NeuralNetwork bodyNetwork = 4;
      case 4: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(34u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_bodynetwork()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.LoopLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.LoopLayerParams)
  return false;
#undef DO_
}

void LoopLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.LoopLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // uint64 maxLoopIterations = 1;
  if (this->maxloopiterations() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(1, this->maxloopiterations(), output);
  }

  // string conditionVar = 2;
  if (this->conditionvar().size() > 0) {
    ::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
      this->conditionvar().data(), this->conditionvar().length(),
      ::google::protobuf::internal::WireFormatLite::SERIALIZE,
      "CoreML.Specification.LoopLayerParams.conditionVar");
    ::google::protobuf::internal::WireFormatLite::WriteStringMaybeAliased(
      2, this->conditionvar(), output);
  }

  // .CoreML.Specification.NeuralNetwork conditionNetwork = 3;
  if (this->has_conditionnetwork()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      3, *this->conditionnetwork_, output);
  }

  // .CoreML.Specification.NeuralNetwork bodyNetwork = 4;
  if (this->has_bodynetwork()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      4, *this->bodynetwork_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.LoopLayerParams)
}

size_t LoopLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.LoopLayerParams)
  size_t total_size = 0;

  // string conditionVar = 2;
  if (this->conditionvar().size() > 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::StringSize(
        this->conditionvar());
  }

  // .CoreML.Specification.NeuralNetwork conditionNetwork = 3;
  if (this->has_conditionnetwork()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->conditionnetwork_);
  }

  // .CoreML.Specification.NeuralNetwork bodyNetwork = 4;
  if (this->has_bodynetwork()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->bodynetwork_);
  }

  // uint64 maxLoopIterations = 1;
  if (this->maxloopiterations() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->maxloopiterations());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void LoopLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const LoopLayerParams*>(&from));
}

void LoopLayerParams::MergeFrom(const LoopLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.LoopLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.conditionvar().size() > 0) {

    conditionvar_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.conditionvar_);
  }
  if (from.has_conditionnetwork()) {
    mutable_conditionnetwork()->::CoreML::Specification::NeuralNetwork::MergeFrom(from.conditionnetwork());
  }
  if (from.has_bodynetwork()) {
    mutable_bodynetwork()->::CoreML::Specification::NeuralNetwork::MergeFrom(from.bodynetwork());
  }
  if (from.maxloopiterations() != 0) {
    set_maxloopiterations(from.maxloopiterations());
  }
}

void LoopLayerParams::CopyFrom(const LoopLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.LoopLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool LoopLayerParams::IsInitialized() const {
  return true;
}

void LoopLayerParams::Swap(LoopLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void LoopLayerParams::InternalSwap(LoopLayerParams* other) {
  conditionvar_.Swap(&other->conditionvar_);
  std::swap(conditionnetwork_, other->conditionnetwork_);
  std::swap(bodynetwork_, other->bodynetwork_);
  std::swap(maxloopiterations_, other->maxloopiterations_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string LoopLayerParams::GetTypeName() const {
  return "CoreML.Specification.LoopLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// LoopLayerParams

// uint64 maxLoopIterations = 1;
void LoopLayerParams::clear_maxloopiterations() {
  maxloopiterations_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 LoopLayerParams::maxloopiterations() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LoopLayerParams.maxLoopIterations)
  return maxloopiterations_;
}
void LoopLayerParams::set_maxloopiterations(::google::protobuf::uint64 value) {
  
  maxloopiterations_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.LoopLayerParams.maxLoopIterations)
}

// string conditionVar = 2;
void LoopLayerParams::clear_conditionvar() {
  conditionvar_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
const ::std::string& LoopLayerParams::conditionvar() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LoopLayerParams.conditionVar)
  return conditionvar_.GetNoArena();
}
void LoopLayerParams::set_conditionvar(const ::std::string& value) {
  
  conditionvar_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.LoopLayerParams.conditionVar)
}
#if LANG_CXX11
void LoopLayerParams::set_conditionvar(::std::string&& value) {
  
  conditionvar_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:CoreML.Specification.LoopLayerParams.conditionVar)
}
#endif
void LoopLayerParams::set_conditionvar(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  
  conditionvar_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:CoreML.Specification.LoopLayerParams.conditionVar)
}
void LoopLayerParams::set_conditionvar(const char* value, size_t size) {
  
  conditionvar_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:CoreML.Specification.LoopLayerParams.conditionVar)
}
::std::string* LoopLayerParams::mutable_conditionvar() {
  
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LoopLayerParams.conditionVar)
  return conditionvar_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
::std::string* LoopLayerParams::release_conditionvar() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LoopLayerParams.conditionVar)
  
  return conditionvar_.ReleaseNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
void LoopLayerParams::set_allocated_conditionvar(::std::string* conditionvar) {
  if (conditionvar != NULL) {
    
  } else {
    
  }
  conditionvar_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), conditionvar);
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LoopLayerParams.conditionVar)
}

// .CoreML.Specification.NeuralNetwork conditionNetwork = 3;
bool LoopLayerParams::has_conditionnetwork() const {
  return this != internal_default_instance() && conditionnetwork_ != NULL;
}
void LoopLayerParams::clear_conditionnetwork() {
  if (GetArenaNoVirtual() == NULL && conditionnetwork_ != NULL) delete conditionnetwork_;
  conditionnetwork_ = NULL;
}
const ::CoreML::Specification::NeuralNetwork& LoopLayerParams::conditionnetwork() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LoopLayerParams.conditionNetwork)
  return conditionnetwork_ != NULL ? *conditionnetwork_
                         : *::CoreML::Specification::NeuralNetwork::internal_default_instance();
}
::CoreML::Specification::NeuralNetwork* LoopLayerParams::mutable_conditionnetwork() {
  
  if (conditionnetwork_ == NULL) {
    conditionnetwork_ = new ::CoreML::Specification::NeuralNetwork;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LoopLayerParams.conditionNetwork)
  return conditionnetwork_;
}
::CoreML::Specification::NeuralNetwork* LoopLayerParams::release_conditionnetwork() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LoopLayerParams.conditionNetwork)
  
  ::CoreML::Specification::NeuralNetwork* temp = conditionnetwork_;
  conditionnetwork_ = NULL;
  return temp;
}
void LoopLayerParams::set_allocated_conditionnetwork(::CoreML::Specification::NeuralNetwork* conditionnetwork) {
  delete conditionnetwork_;
  conditionnetwork_ = conditionnetwork;
  if (conditionnetwork) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LoopLayerParams.conditionNetwork)
}

// .CoreML.Specification.NeuralNetwork bodyNetwork = 4;
bool LoopLayerParams::has_bodynetwork() const {
  return this != internal_default_instance() && bodynetwork_ != NULL;
}
void LoopLayerParams::clear_bodynetwork() {
  if (GetArenaNoVirtual() == NULL && bodynetwork_ != NULL) delete bodynetwork_;
  bodynetwork_ = NULL;
}
const ::CoreML::Specification::NeuralNetwork& LoopLayerParams::bodynetwork() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LoopLayerParams.bodyNetwork)
  return bodynetwork_ != NULL ? *bodynetwork_
                         : *::CoreML::Specification::NeuralNetwork::internal_default_instance();
}
::CoreML::Specification::NeuralNetwork* LoopLayerParams::mutable_bodynetwork() {
  
  if (bodynetwork_ == NULL) {
    bodynetwork_ = new ::CoreML::Specification::NeuralNetwork;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LoopLayerParams.bodyNetwork)
  return bodynetwork_;
}
::CoreML::Specification::NeuralNetwork* LoopLayerParams::release_bodynetwork() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LoopLayerParams.bodyNetwork)
  
  ::CoreML::Specification::NeuralNetwork* temp = bodynetwork_;
  bodynetwork_ = NULL;
  return temp;
}
void LoopLayerParams::set_allocated_bodynetwork(::CoreML::Specification::NeuralNetwork* bodynetwork) {
  delete bodynetwork_;
  bodynetwork_ = bodynetwork;
  if (bodynetwork) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LoopLayerParams.bodyNetwork)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

LoopBreakLayerParams::LoopBreakLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.LoopBreakLayerParams)
}
LoopBreakLayerParams::LoopBreakLayerParams(const LoopBreakLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.LoopBreakLayerParams)
}

void LoopBreakLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

LoopBreakLayerParams::~LoopBreakLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.LoopBreakLayerParams)
  SharedDtor();
}

void LoopBreakLayerParams::SharedDtor() {
}

void LoopBreakLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const LoopBreakLayerParams& LoopBreakLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

LoopBreakLayerParams* LoopBreakLayerParams::New(::google::protobuf::Arena* arena) const {
  LoopBreakLayerParams* n = new LoopBreakLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void LoopBreakLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.LoopBreakLayerParams)
}

bool LoopBreakLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.LoopBreakLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.LoopBreakLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.LoopBreakLayerParams)
  return false;
#undef DO_
}

void LoopBreakLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.LoopBreakLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.LoopBreakLayerParams)
}

size_t LoopBreakLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.LoopBreakLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void LoopBreakLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const LoopBreakLayerParams*>(&from));
}

void LoopBreakLayerParams::MergeFrom(const LoopBreakLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.LoopBreakLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void LoopBreakLayerParams::CopyFrom(const LoopBreakLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.LoopBreakLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool LoopBreakLayerParams::IsInitialized() const {
  return true;
}

void LoopBreakLayerParams::Swap(LoopBreakLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void LoopBreakLayerParams::InternalSwap(LoopBreakLayerParams* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string LoopBreakLayerParams::GetTypeName() const {
  return "CoreML.Specification.LoopBreakLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// LoopBreakLayerParams

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

LoopContinueLayerParams::LoopContinueLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.LoopContinueLayerParams)
}
LoopContinueLayerParams::LoopContinueLayerParams(const LoopContinueLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.LoopContinueLayerParams)
}

void LoopContinueLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

LoopContinueLayerParams::~LoopContinueLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.LoopContinueLayerParams)
  SharedDtor();
}

void LoopContinueLayerParams::SharedDtor() {
}

void LoopContinueLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const LoopContinueLayerParams& LoopContinueLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

LoopContinueLayerParams* LoopContinueLayerParams::New(::google::protobuf::Arena* arena) const {
  LoopContinueLayerParams* n = new LoopContinueLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void LoopContinueLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.LoopContinueLayerParams)
}

bool LoopContinueLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.LoopContinueLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.LoopContinueLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.LoopContinueLayerParams)
  return false;
#undef DO_
}

void LoopContinueLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.LoopContinueLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.LoopContinueLayerParams)
}

size_t LoopContinueLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.LoopContinueLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void LoopContinueLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const LoopContinueLayerParams*>(&from));
}

void LoopContinueLayerParams::MergeFrom(const LoopContinueLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.LoopContinueLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void LoopContinueLayerParams::CopyFrom(const LoopContinueLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.LoopContinueLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool LoopContinueLayerParams::IsInitialized() const {
  return true;
}

void LoopContinueLayerParams::Swap(LoopContinueLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void LoopContinueLayerParams::InternalSwap(LoopContinueLayerParams* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string LoopContinueLayerParams::GetTypeName() const {
  return "CoreML.Specification.LoopContinueLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// LoopContinueLayerParams

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

CopyLayerParams::CopyLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.CopyLayerParams)
}
CopyLayerParams::CopyLayerParams(const CopyLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.CopyLayerParams)
}

void CopyLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

CopyLayerParams::~CopyLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.CopyLayerParams)
  SharedDtor();
}

void CopyLayerParams::SharedDtor() {
}

void CopyLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const CopyLayerParams& CopyLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

CopyLayerParams* CopyLayerParams::New(::google::protobuf::Arena* arena) const {
  CopyLayerParams* n = new CopyLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void CopyLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.CopyLayerParams)
}

bool CopyLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.CopyLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.CopyLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.CopyLayerParams)
  return false;
#undef DO_
}

void CopyLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.CopyLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.CopyLayerParams)
}

size_t CopyLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.CopyLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void CopyLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const CopyLayerParams*>(&from));
}

void CopyLayerParams::MergeFrom(const CopyLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.CopyLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void CopyLayerParams::CopyFrom(const CopyLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.CopyLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool CopyLayerParams::IsInitialized() const {
  return true;
}

void CopyLayerParams::Swap(CopyLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void CopyLayerParams::InternalSwap(CopyLayerParams* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string CopyLayerParams::GetTypeName() const {
  return "CoreML.Specification.CopyLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// CopyLayerParams

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int GreaterThanLayerParams::kUseNonStrictInequalityFieldNumber;
const int GreaterThanLayerParams::kAlphaFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

GreaterThanLayerParams::GreaterThanLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.GreaterThanLayerParams)
}
GreaterThanLayerParams::GreaterThanLayerParams(const GreaterThanLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::memcpy(&usenonstrictinequality_, &from.usenonstrictinequality_,
    reinterpret_cast<char*>(&alpha_) -
    reinterpret_cast<char*>(&usenonstrictinequality_) + sizeof(alpha_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.GreaterThanLayerParams)
}

void GreaterThanLayerParams::SharedCtor() {
  ::memset(&usenonstrictinequality_, 0, reinterpret_cast<char*>(&alpha_) -
    reinterpret_cast<char*>(&usenonstrictinequality_) + sizeof(alpha_));
  _cached_size_ = 0;
}

GreaterThanLayerParams::~GreaterThanLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.GreaterThanLayerParams)
  SharedDtor();
}

void GreaterThanLayerParams::SharedDtor() {
}

void GreaterThanLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const GreaterThanLayerParams& GreaterThanLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

GreaterThanLayerParams* GreaterThanLayerParams::New(::google::protobuf::Arena* arena) const {
  GreaterThanLayerParams* n = new GreaterThanLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void GreaterThanLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.GreaterThanLayerParams)
  ::memset(&usenonstrictinequality_, 0, reinterpret_cast<char*>(&alpha_) -
    reinterpret_cast<char*>(&usenonstrictinequality_) + sizeof(alpha_));
}

bool GreaterThanLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.GreaterThanLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // bool useNonStrictInequality = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &usenonstrictinequality_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // float alpha = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(21u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &alpha_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.GreaterThanLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.GreaterThanLayerParams)
  return false;
#undef DO_
}

void GreaterThanLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.GreaterThanLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // bool useNonStrictInequality = 1;
  if (this->usenonstrictinequality() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(1, this->usenonstrictinequality(), output);
  }

  // float alpha = 2;
  if (this->alpha() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(2, this->alpha(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.GreaterThanLayerParams)
}

size_t GreaterThanLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.GreaterThanLayerParams)
  size_t total_size = 0;

  // bool useNonStrictInequality = 1;
  if (this->usenonstrictinequality() != 0) {
    total_size += 1 + 1;
  }

  // float alpha = 2;
  if (this->alpha() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void GreaterThanLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const GreaterThanLayerParams*>(&from));
}

void GreaterThanLayerParams::MergeFrom(const GreaterThanLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.GreaterThanLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.usenonstrictinequality() != 0) {
    set_usenonstrictinequality(from.usenonstrictinequality());
  }
  if (from.alpha() != 0) {
    set_alpha(from.alpha());
  }
}

void GreaterThanLayerParams::CopyFrom(const GreaterThanLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.GreaterThanLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool GreaterThanLayerParams::IsInitialized() const {
  return true;
}

void GreaterThanLayerParams::Swap(GreaterThanLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void GreaterThanLayerParams::InternalSwap(GreaterThanLayerParams* other) {
  std::swap(usenonstrictinequality_, other->usenonstrictinequality_);
  std::swap(alpha_, other->alpha_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string GreaterThanLayerParams::GetTypeName() const {
  return "CoreML.Specification.GreaterThanLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// GreaterThanLayerParams

// bool useNonStrictInequality = 1;
void GreaterThanLayerParams::clear_usenonstrictinequality() {
  usenonstrictinequality_ = false;
}
bool GreaterThanLayerParams::usenonstrictinequality() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GreaterThanLayerParams.useNonStrictInequality)
  return usenonstrictinequality_;
}
void GreaterThanLayerParams::set_usenonstrictinequality(bool value) {
  
  usenonstrictinequality_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.GreaterThanLayerParams.useNonStrictInequality)
}

// float alpha = 2;
void GreaterThanLayerParams::clear_alpha() {
  alpha_ = 0;
}
float GreaterThanLayerParams::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GreaterThanLayerParams.alpha)
  return alpha_;
}
void GreaterThanLayerParams::set_alpha(float value) {
  
  alpha_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.GreaterThanLayerParams.alpha)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int LessThanLayerParams::kUseNonStrictInequalityFieldNumber;
const int LessThanLayerParams::kAlphaFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

LessThanLayerParams::LessThanLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.LessThanLayerParams)
}
LessThanLayerParams::LessThanLayerParams(const LessThanLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::memcpy(&usenonstrictinequality_, &from.usenonstrictinequality_,
    reinterpret_cast<char*>(&alpha_) -
    reinterpret_cast<char*>(&usenonstrictinequality_) + sizeof(alpha_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.LessThanLayerParams)
}

void LessThanLayerParams::SharedCtor() {
  ::memset(&usenonstrictinequality_, 0, reinterpret_cast<char*>(&alpha_) -
    reinterpret_cast<char*>(&usenonstrictinequality_) + sizeof(alpha_));
  _cached_size_ = 0;
}

LessThanLayerParams::~LessThanLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.LessThanLayerParams)
  SharedDtor();
}

void LessThanLayerParams::SharedDtor() {
}

void LessThanLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const LessThanLayerParams& LessThanLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

LessThanLayerParams* LessThanLayerParams::New(::google::protobuf::Arena* arena) const {
  LessThanLayerParams* n = new LessThanLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void LessThanLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.LessThanLayerParams)
  ::memset(&usenonstrictinequality_, 0, reinterpret_cast<char*>(&alpha_) -
    reinterpret_cast<char*>(&usenonstrictinequality_) + sizeof(alpha_));
}

bool LessThanLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.LessThanLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // bool useNonStrictInequality = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &usenonstrictinequality_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // float alpha = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(21u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &alpha_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.LessThanLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.LessThanLayerParams)
  return false;
#undef DO_
}

void LessThanLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.LessThanLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // bool useNonStrictInequality = 1;
  if (this->usenonstrictinequality() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(1, this->usenonstrictinequality(), output);
  }

  // float alpha = 2;
  if (this->alpha() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(2, this->alpha(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.LessThanLayerParams)
}

size_t LessThanLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.LessThanLayerParams)
  size_t total_size = 0;

  // bool useNonStrictInequality = 1;
  if (this->usenonstrictinequality() != 0) {
    total_size += 1 + 1;
  }

  // float alpha = 2;
  if (this->alpha() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void LessThanLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const LessThanLayerParams*>(&from));
}

void LessThanLayerParams::MergeFrom(const LessThanLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.LessThanLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.usenonstrictinequality() != 0) {
    set_usenonstrictinequality(from.usenonstrictinequality());
  }
  if (from.alpha() != 0) {
    set_alpha(from.alpha());
  }
}

void LessThanLayerParams::CopyFrom(const LessThanLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.LessThanLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool LessThanLayerParams::IsInitialized() const {
  return true;
}

void LessThanLayerParams::Swap(LessThanLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void LessThanLayerParams::InternalSwap(LessThanLayerParams* other) {
  std::swap(usenonstrictinequality_, other->usenonstrictinequality_);
  std::swap(alpha_, other->alpha_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string LessThanLayerParams::GetTypeName() const {
  return "CoreML.Specification.LessThanLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// LessThanLayerParams

// bool useNonStrictInequality = 1;
void LessThanLayerParams::clear_usenonstrictinequality() {
  usenonstrictinequality_ = false;
}
bool LessThanLayerParams::usenonstrictinequality() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LessThanLayerParams.useNonStrictInequality)
  return usenonstrictinequality_;
}
void LessThanLayerParams::set_usenonstrictinequality(bool value) {
  
  usenonstrictinequality_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.LessThanLayerParams.useNonStrictInequality)
}

// float alpha = 2;
void LessThanLayerParams::clear_alpha() {
  alpha_ = 0;
}
float LessThanLayerParams::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LessThanLayerParams.alpha)
  return alpha_;
}
void LessThanLayerParams::set_alpha(float value) {
  
  alpha_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.LessThanLayerParams.alpha)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int EqualLayerParams::kAlphaFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

EqualLayerParams::EqualLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.EqualLayerParams)
}
EqualLayerParams::EqualLayerParams(const EqualLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  alpha_ = from.alpha_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.EqualLayerParams)
}

void EqualLayerParams::SharedCtor() {
  alpha_ = 0;
  _cached_size_ = 0;
}

EqualLayerParams::~EqualLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.EqualLayerParams)
  SharedDtor();
}

void EqualLayerParams::SharedDtor() {
}

void EqualLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const EqualLayerParams& EqualLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

EqualLayerParams* EqualLayerParams::New(::google::protobuf::Arena* arena) const {
  EqualLayerParams* n = new EqualLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void EqualLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.EqualLayerParams)
  alpha_ = 0;
}

bool EqualLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.EqualLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // float alpha = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(13u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &alpha_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.EqualLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.EqualLayerParams)
  return false;
#undef DO_
}

void EqualLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.EqualLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // float alpha = 1;
  if (this->alpha() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(1, this->alpha(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.EqualLayerParams)
}

size_t EqualLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.EqualLayerParams)
  size_t total_size = 0;

  // float alpha = 1;
  if (this->alpha() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void EqualLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const EqualLayerParams*>(&from));
}

void EqualLayerParams::MergeFrom(const EqualLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.EqualLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.alpha() != 0) {
    set_alpha(from.alpha());
  }
}

void EqualLayerParams::CopyFrom(const EqualLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.EqualLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool EqualLayerParams::IsInitialized() const {
  return true;
}

void EqualLayerParams::Swap(EqualLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void EqualLayerParams::InternalSwap(EqualLayerParams* other) {
  std::swap(alpha_, other->alpha_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string EqualLayerParams::GetTypeName() const {
  return "CoreML.Specification.EqualLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// EqualLayerParams

// float alpha = 1;
void EqualLayerParams::clear_alpha() {
  alpha_ = 0;
}
float EqualLayerParams::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.EqualLayerParams.alpha)
  return alpha_;
}
void EqualLayerParams::set_alpha(float value) {
  
  alpha_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.EqualLayerParams.alpha)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int NotEqualLayerParams::kAlphaFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

NotEqualLayerParams::NotEqualLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.NotEqualLayerParams)
}
NotEqualLayerParams::NotEqualLayerParams(const NotEqualLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  alpha_ = from.alpha_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.NotEqualLayerParams)
}

void NotEqualLayerParams::SharedCtor() {
  alpha_ = 0;
  _cached_size_ = 0;
}

NotEqualLayerParams::~NotEqualLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.NotEqualLayerParams)
  SharedDtor();
}

void NotEqualLayerParams::SharedDtor() {
}

void NotEqualLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const NotEqualLayerParams& NotEqualLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

NotEqualLayerParams* NotEqualLayerParams::New(::google::protobuf::Arena* arena) const {
  NotEqualLayerParams* n = new NotEqualLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void NotEqualLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.NotEqualLayerParams)
  alpha_ = 0;
}

bool NotEqualLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.NotEqualLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // float alpha = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(13u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &alpha_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.NotEqualLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.NotEqualLayerParams)
  return false;
#undef DO_
}

void NotEqualLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.NotEqualLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // float alpha = 1;
  if (this->alpha() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(1, this->alpha(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.NotEqualLayerParams)
}

size_t NotEqualLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.NotEqualLayerParams)
  size_t total_size = 0;

  // float alpha = 1;
  if (this->alpha() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void NotEqualLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const NotEqualLayerParams*>(&from));
}

void NotEqualLayerParams::MergeFrom(const NotEqualLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.NotEqualLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.alpha() != 0) {
    set_alpha(from.alpha());
  }
}

void NotEqualLayerParams::CopyFrom(const NotEqualLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.NotEqualLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool NotEqualLayerParams::IsInitialized() const {
  return true;
}

void NotEqualLayerParams::Swap(NotEqualLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void NotEqualLayerParams::InternalSwap(NotEqualLayerParams* other) {
  std::swap(alpha_, other->alpha_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string NotEqualLayerParams::GetTypeName() const {
  return "CoreML.Specification.NotEqualLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// NotEqualLayerParams

// float alpha = 1;
void NotEqualLayerParams::clear_alpha() {
  alpha_ = 0;
}
float NotEqualLayerParams::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NotEqualLayerParams.alpha)
  return alpha_;
}
void NotEqualLayerParams::set_alpha(float value) {
  
  alpha_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.NotEqualLayerParams.alpha)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

LogicalAndLayerParams::LogicalAndLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.LogicalAndLayerParams)
}
LogicalAndLayerParams::LogicalAndLayerParams(const LogicalAndLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.LogicalAndLayerParams)
}

void LogicalAndLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

LogicalAndLayerParams::~LogicalAndLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.LogicalAndLayerParams)
  SharedDtor();
}

void LogicalAndLayerParams::SharedDtor() {
}

void LogicalAndLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const LogicalAndLayerParams& LogicalAndLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

LogicalAndLayerParams* LogicalAndLayerParams::New(::google::protobuf::Arena* arena) const {
  LogicalAndLayerParams* n = new LogicalAndLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void LogicalAndLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.LogicalAndLayerParams)
}

bool LogicalAndLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.LogicalAndLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.LogicalAndLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.LogicalAndLayerParams)
  return false;
#undef DO_
}

void LogicalAndLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.LogicalAndLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.LogicalAndLayerParams)
}

size_t LogicalAndLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.LogicalAndLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void LogicalAndLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const LogicalAndLayerParams*>(&from));
}

void LogicalAndLayerParams::MergeFrom(const LogicalAndLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.LogicalAndLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void LogicalAndLayerParams::CopyFrom(const LogicalAndLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.LogicalAndLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool LogicalAndLayerParams::IsInitialized() const {
  return true;
}

void LogicalAndLayerParams::Swap(LogicalAndLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void LogicalAndLayerParams::InternalSwap(LogicalAndLayerParams* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string LogicalAndLayerParams::GetTypeName() const {
  return "CoreML.Specification.LogicalAndLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// LogicalAndLayerParams

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

LogicalOrLayerParams::LogicalOrLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.LogicalOrLayerParams)
}
LogicalOrLayerParams::LogicalOrLayerParams(const LogicalOrLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.LogicalOrLayerParams)
}

void LogicalOrLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

LogicalOrLayerParams::~LogicalOrLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.LogicalOrLayerParams)
  SharedDtor();
}

void LogicalOrLayerParams::SharedDtor() {
}

void LogicalOrLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const LogicalOrLayerParams& LogicalOrLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

LogicalOrLayerParams* LogicalOrLayerParams::New(::google::protobuf::Arena* arena) const {
  LogicalOrLayerParams* n = new LogicalOrLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void LogicalOrLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.LogicalOrLayerParams)
}

bool LogicalOrLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.LogicalOrLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.LogicalOrLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.LogicalOrLayerParams)
  return false;
#undef DO_
}

void LogicalOrLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.LogicalOrLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.LogicalOrLayerParams)
}

size_t LogicalOrLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.LogicalOrLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void LogicalOrLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const LogicalOrLayerParams*>(&from));
}

void LogicalOrLayerParams::MergeFrom(const LogicalOrLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.LogicalOrLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void LogicalOrLayerParams::CopyFrom(const LogicalOrLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.LogicalOrLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool LogicalOrLayerParams::IsInitialized() const {
  return true;
}

void LogicalOrLayerParams::Swap(LogicalOrLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void LogicalOrLayerParams::InternalSwap(LogicalOrLayerParams* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string LogicalOrLayerParams::GetTypeName() const {
  return "CoreML.Specification.LogicalOrLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// LogicalOrLayerParams

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

LogicalXorLayerParams::LogicalXorLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.LogicalXorLayerParams)
}
LogicalXorLayerParams::LogicalXorLayerParams(const LogicalXorLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.LogicalXorLayerParams)
}

void LogicalXorLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

LogicalXorLayerParams::~LogicalXorLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.LogicalXorLayerParams)
  SharedDtor();
}

void LogicalXorLayerParams::SharedDtor() {
}

void LogicalXorLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const LogicalXorLayerParams& LogicalXorLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

LogicalXorLayerParams* LogicalXorLayerParams::New(::google::protobuf::Arena* arena) const {
  LogicalXorLayerParams* n = new LogicalXorLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void LogicalXorLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.LogicalXorLayerParams)
}

bool LogicalXorLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.LogicalXorLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.LogicalXorLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.LogicalXorLayerParams)
  return false;
#undef DO_
}

void LogicalXorLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.LogicalXorLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.LogicalXorLayerParams)
}

size_t LogicalXorLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.LogicalXorLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void LogicalXorLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const LogicalXorLayerParams*>(&from));
}

void LogicalXorLayerParams::MergeFrom(const LogicalXorLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.LogicalXorLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void LogicalXorLayerParams::CopyFrom(const LogicalXorLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.LogicalXorLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool LogicalXorLayerParams::IsInitialized() const {
  return true;
}

void LogicalXorLayerParams::Swap(LogicalXorLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void LogicalXorLayerParams::InternalSwap(LogicalXorLayerParams* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string LogicalXorLayerParams::GetTypeName() const {
  return "CoreML.Specification.LogicalXorLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// LogicalXorLayerParams

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

LogicalNotLayerParams::LogicalNotLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.LogicalNotLayerParams)
}
LogicalNotLayerParams::LogicalNotLayerParams(const LogicalNotLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.LogicalNotLayerParams)
}

void LogicalNotLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

LogicalNotLayerParams::~LogicalNotLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.LogicalNotLayerParams)
  SharedDtor();
}

void LogicalNotLayerParams::SharedDtor() {
}

void LogicalNotLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const LogicalNotLayerParams& LogicalNotLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

LogicalNotLayerParams* LogicalNotLayerParams::New(::google::protobuf::Arena* arena) const {
  LogicalNotLayerParams* n = new LogicalNotLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void LogicalNotLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.LogicalNotLayerParams)
}

bool LogicalNotLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.LogicalNotLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.LogicalNotLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.LogicalNotLayerParams)
  return false;
#undef DO_
}

void LogicalNotLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.LogicalNotLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.LogicalNotLayerParams)
}

size_t LogicalNotLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.LogicalNotLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void LogicalNotLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const LogicalNotLayerParams*>(&from));
}

void LogicalNotLayerParams::MergeFrom(const LogicalNotLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.LogicalNotLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void LogicalNotLayerParams::CopyFrom(const LogicalNotLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.LogicalNotLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool LogicalNotLayerParams::IsInitialized() const {
  return true;
}

void LogicalNotLayerParams::Swap(LogicalNotLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void LogicalNotLayerParams::InternalSwap(LogicalNotLayerParams* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string LogicalNotLayerParams::GetTypeName() const {
  return "CoreML.Specification.LogicalNotLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// LogicalNotLayerParams

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int BorderAmounts_EdgeSizes::kStartEdgeSizeFieldNumber;
const int BorderAmounts_EdgeSizes::kEndEdgeSizeFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

BorderAmounts_EdgeSizes::BorderAmounts_EdgeSizes()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.BorderAmounts.EdgeSizes)
}
BorderAmounts_EdgeSizes::BorderAmounts_EdgeSizes(const BorderAmounts_EdgeSizes& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::memcpy(&startedgesize_, &from.startedgesize_,
    reinterpret_cast<char*>(&endedgesize_) -
    reinterpret_cast<char*>(&startedgesize_) + sizeof(endedgesize_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.BorderAmounts.EdgeSizes)
}

void BorderAmounts_EdgeSizes::SharedCtor() {
  ::memset(&startedgesize_, 0, reinterpret_cast<char*>(&endedgesize_) -
    reinterpret_cast<char*>(&startedgesize_) + sizeof(endedgesize_));
  _cached_size_ = 0;
}

BorderAmounts_EdgeSizes::~BorderAmounts_EdgeSizes() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.BorderAmounts.EdgeSizes)
  SharedDtor();
}

void BorderAmounts_EdgeSizes::SharedDtor() {
}

void BorderAmounts_EdgeSizes::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const BorderAmounts_EdgeSizes& BorderAmounts_EdgeSizes::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

BorderAmounts_EdgeSizes* BorderAmounts_EdgeSizes::New(::google::protobuf::Arena* arena) const {
  BorderAmounts_EdgeSizes* n = new BorderAmounts_EdgeSizes;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void BorderAmounts_EdgeSizes::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.BorderAmounts.EdgeSizes)
  ::memset(&startedgesize_, 0, reinterpret_cast<char*>(&endedgesize_) -
    reinterpret_cast<char*>(&startedgesize_) + sizeof(endedgesize_));
}

bool BorderAmounts_EdgeSizes::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.BorderAmounts.EdgeSizes)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // uint64 startEdgeSize = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &startedgesize_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // uint64 endEdgeSize = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(16u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &endedgesize_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.BorderAmounts.EdgeSizes)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.BorderAmounts.EdgeSizes)
  return false;
#undef DO_
}

void BorderAmounts_EdgeSizes::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.BorderAmounts.EdgeSizes)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // uint64 startEdgeSize = 1;
  if (this->startedgesize() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(1, this->startedgesize(), output);
  }

  // uint64 endEdgeSize = 2;
  if (this->endedgesize() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(2, this->endedgesize(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.BorderAmounts.EdgeSizes)
}

size_t BorderAmounts_EdgeSizes::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.BorderAmounts.EdgeSizes)
  size_t total_size = 0;

  // uint64 startEdgeSize = 1;
  if (this->startedgesize() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->startedgesize());
  }

  // uint64 endEdgeSize = 2;
  if (this->endedgesize() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->endedgesize());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void BorderAmounts_EdgeSizes::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const BorderAmounts_EdgeSizes*>(&from));
}

void BorderAmounts_EdgeSizes::MergeFrom(const BorderAmounts_EdgeSizes& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.BorderAmounts.EdgeSizes)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.startedgesize() != 0) {
    set_startedgesize(from.startedgesize());
  }
  if (from.endedgesize() != 0) {
    set_endedgesize(from.endedgesize());
  }
}

void BorderAmounts_EdgeSizes::CopyFrom(const BorderAmounts_EdgeSizes& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.BorderAmounts.EdgeSizes)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool BorderAmounts_EdgeSizes::IsInitialized() const {
  return true;
}

void BorderAmounts_EdgeSizes::Swap(BorderAmounts_EdgeSizes* other) {
  if (other == this) return;
  InternalSwap(other);
}
void BorderAmounts_EdgeSizes::InternalSwap(BorderAmounts_EdgeSizes* other) {
  std::swap(startedgesize_, other->startedgesize_);
  std::swap(endedgesize_, other->endedgesize_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string BorderAmounts_EdgeSizes::GetTypeName() const {
  return "CoreML.Specification.BorderAmounts.EdgeSizes";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// BorderAmounts_EdgeSizes

// uint64 startEdgeSize = 1;
void BorderAmounts_EdgeSizes::clear_startedgesize() {
  startedgesize_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 BorderAmounts_EdgeSizes::startedgesize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BorderAmounts.EdgeSizes.startEdgeSize)
  return startedgesize_;
}
void BorderAmounts_EdgeSizes::set_startedgesize(::google::protobuf::uint64 value) {
  
  startedgesize_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.BorderAmounts.EdgeSizes.startEdgeSize)
}

// uint64 endEdgeSize = 2;
void BorderAmounts_EdgeSizes::clear_endedgesize() {
  endedgesize_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 BorderAmounts_EdgeSizes::endedgesize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BorderAmounts.EdgeSizes.endEdgeSize)
  return endedgesize_;
}
void BorderAmounts_EdgeSizes::set_endedgesize(::google::protobuf::uint64 value) {
  
  endedgesize_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.BorderAmounts.EdgeSizes.endEdgeSize)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int BorderAmounts::kBorderAmountsFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

BorderAmounts::BorderAmounts()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.BorderAmounts)
}
BorderAmounts::BorderAmounts(const BorderAmounts& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      borderamounts_(from.borderamounts_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.BorderAmounts)
}

void BorderAmounts::SharedCtor() {
  _cached_size_ = 0;
}

BorderAmounts::~BorderAmounts() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.BorderAmounts)
  SharedDtor();
}

void BorderAmounts::SharedDtor() {
}

void BorderAmounts::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const BorderAmounts& BorderAmounts::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

BorderAmounts* BorderAmounts::New(::google::protobuf::Arena* arena) const {
  BorderAmounts* n = new BorderAmounts;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void BorderAmounts::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.BorderAmounts)
  borderamounts_.Clear();
}

bool BorderAmounts::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.BorderAmounts)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated .CoreML.Specification.BorderAmounts.EdgeSizes borderAmounts = 10;
      case 10: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(82u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
                input, add_borderamounts()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.BorderAmounts)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.BorderAmounts)
  return false;
#undef DO_
}

void BorderAmounts::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.BorderAmounts)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated .CoreML.Specification.BorderAmounts.EdgeSizes borderAmounts = 10;
  for (unsigned int i = 0, n = this->borderamounts_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      10, this->borderamounts(i), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.BorderAmounts)
}

size_t BorderAmounts::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.BorderAmounts)
  size_t total_size = 0;

  // repeated .CoreML.Specification.BorderAmounts.EdgeSizes borderAmounts = 10;
  {
    unsigned int count = this->borderamounts_size();
    total_size += 1UL * count;
    for (unsigned int i = 0; i < count; i++) {
      total_size +=
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          this->borderamounts(i));
    }
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void BorderAmounts::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const BorderAmounts*>(&from));
}

void BorderAmounts::MergeFrom(const BorderAmounts& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.BorderAmounts)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  borderamounts_.MergeFrom(from.borderamounts_);
}

void BorderAmounts::CopyFrom(const BorderAmounts& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.BorderAmounts)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool BorderAmounts::IsInitialized() const {
  return true;
}

void BorderAmounts::Swap(BorderAmounts* other) {
  if (other == this) return;
  InternalSwap(other);
}
void BorderAmounts::InternalSwap(BorderAmounts* other) {
  borderamounts_.InternalSwap(&other->borderamounts_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string BorderAmounts::GetTypeName() const {
  return "CoreML.Specification.BorderAmounts";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// BorderAmounts

// repeated .CoreML.Specification.BorderAmounts.EdgeSizes borderAmounts = 10;
int BorderAmounts::borderamounts_size() const {
  return borderamounts_.size();
}
void BorderAmounts::clear_borderamounts() {
  borderamounts_.Clear();
}
const ::CoreML::Specification::BorderAmounts_EdgeSizes& BorderAmounts::borderamounts(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BorderAmounts.borderAmounts)
  return borderamounts_.Get(index);
}
::CoreML::Specification::BorderAmounts_EdgeSizes* BorderAmounts::mutable_borderamounts(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BorderAmounts.borderAmounts)
  return borderamounts_.Mutable(index);
}
::CoreML::Specification::BorderAmounts_EdgeSizes* BorderAmounts::add_borderamounts() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.BorderAmounts.borderAmounts)
  return borderamounts_.Add();
}
::google::protobuf::RepeatedPtrField< ::CoreML::Specification::BorderAmounts_EdgeSizes >*
BorderAmounts::mutable_borderamounts() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.BorderAmounts.borderAmounts)
  return &borderamounts_;
}
const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::BorderAmounts_EdgeSizes >&
BorderAmounts::borderamounts() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.BorderAmounts.borderAmounts)
  return borderamounts_;
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ValidPadding::kPaddingAmountsFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ValidPadding::ValidPadding()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ValidPadding)
}
ValidPadding::ValidPadding(const ValidPadding& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_paddingamounts()) {
    paddingamounts_ = new ::CoreML::Specification::BorderAmounts(*from.paddingamounts_);
  } else {
    paddingamounts_ = NULL;
  }
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ValidPadding)
}

void ValidPadding::SharedCtor() {
  paddingamounts_ = NULL;
  _cached_size_ = 0;
}

ValidPadding::~ValidPadding() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ValidPadding)
  SharedDtor();
}

void ValidPadding::SharedDtor() {
  if (this != internal_default_instance()) {
    delete paddingamounts_;
  }
}

void ValidPadding::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ValidPadding& ValidPadding::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ValidPadding* ValidPadding::New(::google::protobuf::Arena* arena) const {
  ValidPadding* n = new ValidPadding;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ValidPadding::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ValidPadding)
  if (GetArenaNoVirtual() == NULL && paddingamounts_ != NULL) {
    delete paddingamounts_;
  }
  paddingamounts_ = NULL;
}

bool ValidPadding::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ValidPadding)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // .CoreML.Specification.BorderAmounts paddingAmounts = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_paddingamounts()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ValidPadding)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ValidPadding)
  return false;
#undef DO_
}

void ValidPadding::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ValidPadding)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // .CoreML.Specification.BorderAmounts paddingAmounts = 1;
  if (this->has_paddingamounts()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1, *this->paddingamounts_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ValidPadding)
}

size_t ValidPadding::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ValidPadding)
  size_t total_size = 0;

  // .CoreML.Specification.BorderAmounts paddingAmounts = 1;
  if (this->has_paddingamounts()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->paddingamounts_);
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ValidPadding::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ValidPadding*>(&from));
}

void ValidPadding::MergeFrom(const ValidPadding& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ValidPadding)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.has_paddingamounts()) {
    mutable_paddingamounts()->::CoreML::Specification::BorderAmounts::MergeFrom(from.paddingamounts());
  }
}

void ValidPadding::CopyFrom(const ValidPadding& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ValidPadding)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ValidPadding::IsInitialized() const {
  return true;
}

void ValidPadding::Swap(ValidPadding* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ValidPadding::InternalSwap(ValidPadding* other) {
  std::swap(paddingamounts_, other->paddingamounts_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ValidPadding::GetTypeName() const {
  return "CoreML.Specification.ValidPadding";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ValidPadding

// .CoreML.Specification.BorderAmounts paddingAmounts = 1;
bool ValidPadding::has_paddingamounts() const {
  return this != internal_default_instance() && paddingamounts_ != NULL;
}
void ValidPadding::clear_paddingamounts() {
  if (GetArenaNoVirtual() == NULL && paddingamounts_ != NULL) delete paddingamounts_;
  paddingamounts_ = NULL;
}
const ::CoreML::Specification::BorderAmounts& ValidPadding::paddingamounts() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ValidPadding.paddingAmounts)
  return paddingamounts_ != NULL ? *paddingamounts_
                         : *::CoreML::Specification::BorderAmounts::internal_default_instance();
}
::CoreML::Specification::BorderAmounts* ValidPadding::mutable_paddingamounts() {
  
  if (paddingamounts_ == NULL) {
    paddingamounts_ = new ::CoreML::Specification::BorderAmounts;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ValidPadding.paddingAmounts)
  return paddingamounts_;
}
::CoreML::Specification::BorderAmounts* ValidPadding::release_paddingamounts() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ValidPadding.paddingAmounts)
  
  ::CoreML::Specification::BorderAmounts* temp = paddingamounts_;
  paddingamounts_ = NULL;
  return temp;
}
void ValidPadding::set_allocated_paddingamounts(::CoreML::Specification::BorderAmounts* paddingamounts) {
  delete paddingamounts_;
  paddingamounts_ = paddingamounts;
  if (paddingamounts) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ValidPadding.paddingAmounts)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int SamePadding::kAsymmetryModeFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

SamePadding::SamePadding()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.SamePadding)
}
SamePadding::SamePadding(const SamePadding& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  asymmetrymode_ = from.asymmetrymode_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.SamePadding)
}

void SamePadding::SharedCtor() {
  asymmetrymode_ = 0;
  _cached_size_ = 0;
}

SamePadding::~SamePadding() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.SamePadding)
  SharedDtor();
}

void SamePadding::SharedDtor() {
}

void SamePadding::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const SamePadding& SamePadding::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

SamePadding* SamePadding::New(::google::protobuf::Arena* arena) const {
  SamePadding* n = new SamePadding;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void SamePadding::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.SamePadding)
  asymmetrymode_ = 0;
}

bool SamePadding::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.SamePadding)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // .CoreML.Specification.SamePadding.SamePaddingMode asymmetryMode = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {
          int value;
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   int, ::google::protobuf::internal::WireFormatLite::TYPE_ENUM>(
                 input, &value)));
          set_asymmetrymode(static_cast< ::CoreML::Specification::SamePadding_SamePaddingMode >(value));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.SamePadding)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.SamePadding)
  return false;
#undef DO_
}

void SamePadding::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.SamePadding)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // .CoreML.Specification.SamePadding.SamePaddingMode asymmetryMode = 1;
  if (this->asymmetrymode() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteEnum(
      1, this->asymmetrymode(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.SamePadding)
}

size_t SamePadding::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.SamePadding)
  size_t total_size = 0;

  // .CoreML.Specification.SamePadding.SamePaddingMode asymmetryMode = 1;
  if (this->asymmetrymode() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::EnumSize(this->asymmetrymode());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void SamePadding::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const SamePadding*>(&from));
}

void SamePadding::MergeFrom(const SamePadding& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.SamePadding)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.asymmetrymode() != 0) {
    set_asymmetrymode(from.asymmetrymode());
  }
}

void SamePadding::CopyFrom(const SamePadding& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.SamePadding)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool SamePadding::IsInitialized() const {
  return true;
}

void SamePadding::Swap(SamePadding* other) {
  if (other == this) return;
  InternalSwap(other);
}
void SamePadding::InternalSwap(SamePadding* other) {
  std::swap(asymmetrymode_, other->asymmetrymode_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string SamePadding::GetTypeName() const {
  return "CoreML.Specification.SamePadding";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// SamePadding

// .CoreML.Specification.SamePadding.SamePaddingMode asymmetryMode = 1;
void SamePadding::clear_asymmetrymode() {
  asymmetrymode_ = 0;
}
::CoreML::Specification::SamePadding_SamePaddingMode SamePadding::asymmetrymode() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SamePadding.asymmetryMode)
  return static_cast< ::CoreML::Specification::SamePadding_SamePaddingMode >(asymmetrymode_);
}
void SamePadding::set_asymmetrymode(::CoreML::Specification::SamePadding_SamePaddingMode value) {
  
  asymmetrymode_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SamePadding.asymmetryMode)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int SamplingMode::kSamplingMethodFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

SamplingMode::SamplingMode()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.SamplingMode)
}
SamplingMode::SamplingMode(const SamplingMode& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  samplingmethod_ = from.samplingmethod_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.SamplingMode)
}

void SamplingMode::SharedCtor() {
  samplingmethod_ = 0;
  _cached_size_ = 0;
}

SamplingMode::~SamplingMode() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.SamplingMode)
  SharedDtor();
}

void SamplingMode::SharedDtor() {
}

void SamplingMode::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const SamplingMode& SamplingMode::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

SamplingMode* SamplingMode::New(::google::protobuf::Arena* arena) const {
  SamplingMode* n = new SamplingMode;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void SamplingMode::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.SamplingMode)
  samplingmethod_ = 0;
}

bool SamplingMode::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.SamplingMode)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // .CoreML.Specification.SamplingMode.Method samplingMethod = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {
          int value;
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   int, ::google::protobuf::internal::WireFormatLite::TYPE_ENUM>(
                 input, &value)));
          set_samplingmethod(static_cast< ::CoreML::Specification::SamplingMode_Method >(value));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.SamplingMode)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.SamplingMode)
  return false;
#undef DO_
}

void SamplingMode::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.SamplingMode)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // .CoreML.Specification.SamplingMode.Method samplingMethod = 1;
  if (this->samplingmethod() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteEnum(
      1, this->samplingmethod(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.SamplingMode)
}

size_t SamplingMode::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.SamplingMode)
  size_t total_size = 0;

  // .CoreML.Specification.SamplingMode.Method samplingMethod = 1;
  if (this->samplingmethod() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::EnumSize(this->samplingmethod());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void SamplingMode::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const SamplingMode*>(&from));
}

void SamplingMode::MergeFrom(const SamplingMode& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.SamplingMode)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.samplingmethod() != 0) {
    set_samplingmethod(from.samplingmethod());
  }
}

void SamplingMode::CopyFrom(const SamplingMode& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.SamplingMode)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool SamplingMode::IsInitialized() const {
  return true;
}

void SamplingMode::Swap(SamplingMode* other) {
  if (other == this) return;
  InternalSwap(other);
}
void SamplingMode::InternalSwap(SamplingMode* other) {
  std::swap(samplingmethod_, other->samplingmethod_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string SamplingMode::GetTypeName() const {
  return "CoreML.Specification.SamplingMode";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// SamplingMode

// .CoreML.Specification.SamplingMode.Method samplingMethod = 1;
void SamplingMode::clear_samplingmethod() {
  samplingmethod_ = 0;
}
::CoreML::Specification::SamplingMode_Method SamplingMode::samplingmethod() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SamplingMode.samplingMethod)
  return static_cast< ::CoreML::Specification::SamplingMode_Method >(samplingmethod_);
}
void SamplingMode::set_samplingmethod(::CoreML::Specification::SamplingMode_Method value) {
  
  samplingmethod_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SamplingMode.samplingMethod)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int BoxCoordinatesMode::kBoxModeFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

BoxCoordinatesMode::BoxCoordinatesMode()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.BoxCoordinatesMode)
}
BoxCoordinatesMode::BoxCoordinatesMode(const BoxCoordinatesMode& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  boxmode_ = from.boxmode_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.BoxCoordinatesMode)
}

void BoxCoordinatesMode::SharedCtor() {
  boxmode_ = 0;
  _cached_size_ = 0;
}

BoxCoordinatesMode::~BoxCoordinatesMode() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.BoxCoordinatesMode)
  SharedDtor();
}

void BoxCoordinatesMode::SharedDtor() {
}

void BoxCoordinatesMode::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const BoxCoordinatesMode& BoxCoordinatesMode::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

BoxCoordinatesMode* BoxCoordinatesMode::New(::google::protobuf::Arena* arena) const {
  BoxCoordinatesMode* n = new BoxCoordinatesMode;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void BoxCoordinatesMode::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.BoxCoordinatesMode)
  boxmode_ = 0;
}

bool BoxCoordinatesMode::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.BoxCoordinatesMode)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // .CoreML.Specification.BoxCoordinatesMode.Coordinates boxMode = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {
          int value;
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   int, ::google::protobuf::internal::WireFormatLite::TYPE_ENUM>(
                 input, &value)));
          set_boxmode(static_cast< ::CoreML::Specification::BoxCoordinatesMode_Coordinates >(value));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.BoxCoordinatesMode)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.BoxCoordinatesMode)
  return false;
#undef DO_
}

void BoxCoordinatesMode::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.BoxCoordinatesMode)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // .CoreML.Specification.BoxCoordinatesMode.Coordinates boxMode = 1;
  if (this->boxmode() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteEnum(
      1, this->boxmode(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.BoxCoordinatesMode)
}

size_t BoxCoordinatesMode::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.BoxCoordinatesMode)
  size_t total_size = 0;

  // .CoreML.Specification.BoxCoordinatesMode.Coordinates boxMode = 1;
  if (this->boxmode() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::EnumSize(this->boxmode());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void BoxCoordinatesMode::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const BoxCoordinatesMode*>(&from));
}

void BoxCoordinatesMode::MergeFrom(const BoxCoordinatesMode& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.BoxCoordinatesMode)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.boxmode() != 0) {
    set_boxmode(from.boxmode());
  }
}

void BoxCoordinatesMode::CopyFrom(const BoxCoordinatesMode& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.BoxCoordinatesMode)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool BoxCoordinatesMode::IsInitialized() const {
  return true;
}

void BoxCoordinatesMode::Swap(BoxCoordinatesMode* other) {
  if (other == this) return;
  InternalSwap(other);
}
void BoxCoordinatesMode::InternalSwap(BoxCoordinatesMode* other) {
  std::swap(boxmode_, other->boxmode_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string BoxCoordinatesMode::GetTypeName() const {
  return "CoreML.Specification.BoxCoordinatesMode";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// BoxCoordinatesMode

// .CoreML.Specification.BoxCoordinatesMode.Coordinates boxMode = 1;
void BoxCoordinatesMode::clear_boxmode() {
  boxmode_ = 0;
}
::CoreML::Specification::BoxCoordinatesMode_Coordinates BoxCoordinatesMode::boxmode() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BoxCoordinatesMode.boxMode)
  return static_cast< ::CoreML::Specification::BoxCoordinatesMode_Coordinates >(boxmode_);
}
void BoxCoordinatesMode::set_boxmode(::CoreML::Specification::BoxCoordinatesMode_Coordinates value) {
  
  boxmode_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.BoxCoordinatesMode.boxMode)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int WeightParams::kFloatValueFieldNumber;
const int WeightParams::kFloat16ValueFieldNumber;
const int WeightParams::kRawValueFieldNumber;
const int WeightParams::kQuantizationFieldNumber;
const int WeightParams::kIsUpdatableFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

WeightParams::WeightParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.WeightParams)
}
WeightParams::WeightParams(const WeightParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      floatvalue_(from.floatvalue_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  float16value_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (from.float16value().size() > 0) {
    float16value_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.float16value_);
  }
  rawvalue_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (from.rawvalue().size() > 0) {
    rawvalue_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.rawvalue_);
  }
  if (from.has_quantization()) {
    quantization_ = new ::CoreML::Specification::QuantizationParams(*from.quantization_);
  } else {
    quantization_ = NULL;
  }
  isupdatable_ = from.isupdatable_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.WeightParams)
}

void WeightParams::SharedCtor() {
  float16value_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  rawvalue_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  ::memset(&quantization_, 0, reinterpret_cast<char*>(&isupdatable_) -
    reinterpret_cast<char*>(&quantization_) + sizeof(isupdatable_));
  _cached_size_ = 0;
}

WeightParams::~WeightParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.WeightParams)
  SharedDtor();
}

void WeightParams::SharedDtor() {
  float16value_.DestroyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  rawvalue_.DestroyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (this != internal_default_instance()) {
    delete quantization_;
  }
}

void WeightParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const WeightParams& WeightParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

WeightParams* WeightParams::New(::google::protobuf::Arena* arena) const {
  WeightParams* n = new WeightParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void WeightParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.WeightParams)
  floatvalue_.Clear();
  float16value_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  rawvalue_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (GetArenaNoVirtual() == NULL && quantization_ != NULL) {
    delete quantization_;
  }
  quantization_ = NULL;
  isupdatable_ = false;
}

bool WeightParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.WeightParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(16383u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated float floatValue = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, this->mutable_floatvalue())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(13u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 1, 10u, input, this->mutable_floatvalue())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bytes float16Value = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(18u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadBytes(
                input, this->mutable_float16value()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bytes rawValue = 30;
      case 30: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(242u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadBytes(
                input, this->mutable_rawvalue()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.QuantizationParams quantization = 40;
      case 40: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(322u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_quantization()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool isUpdatable = 50;
      case 50: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(400u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &isupdatable_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.WeightParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.WeightParams)
  return false;
#undef DO_
}

void WeightParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.WeightParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated float floatValue = 1;
  if (this->floatvalue_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(1, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_floatvalue_cached_byte_size_);
    ::google::protobuf::internal::WireFormatLite::WriteFloatArray(
      this->floatvalue().data(), this->floatvalue_size(), output);
  }

  // bytes float16Value = 2;
  if (this->float16value().size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBytesMaybeAliased(
      2, this->float16value(), output);
  }

  // bytes rawValue = 30;
  if (this->rawvalue().size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBytesMaybeAliased(
      30, this->rawvalue(), output);
  }

  // .CoreML.Specification.QuantizationParams quantization = 40;
  if (this->has_quantization()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      40, *this->quantization_, output);
  }

  // bool isUpdatable = 50;
  if (this->isupdatable() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(50, this->isupdatable(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.WeightParams)
}

size_t WeightParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.WeightParams)
  size_t total_size = 0;

  // repeated float floatValue = 1;
  {
    unsigned int count = this->floatvalue_size();
    size_t data_size = 4UL * count;
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _floatvalue_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // bytes float16Value = 2;
  if (this->float16value().size() > 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::BytesSize(
        this->float16value());
  }

  // bytes rawValue = 30;
  if (this->rawvalue().size() > 0) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::BytesSize(
        this->rawvalue());
  }

  // .CoreML.Specification.QuantizationParams quantization = 40;
  if (this->has_quantization()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->quantization_);
  }

  // bool isUpdatable = 50;
  if (this->isupdatable() != 0) {
    total_size += 2 + 1;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void WeightParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const WeightParams*>(&from));
}

void WeightParams::MergeFrom(const WeightParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.WeightParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  floatvalue_.MergeFrom(from.floatvalue_);
  if (from.float16value().size() > 0) {

    float16value_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.float16value_);
  }
  if (from.rawvalue().size() > 0) {

    rawvalue_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.rawvalue_);
  }
  if (from.has_quantization()) {
    mutable_quantization()->::CoreML::Specification::QuantizationParams::MergeFrom(from.quantization());
  }
  if (from.isupdatable() != 0) {
    set_isupdatable(from.isupdatable());
  }
}

void WeightParams::CopyFrom(const WeightParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.WeightParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool WeightParams::IsInitialized() const {
  return true;
}

void WeightParams::Swap(WeightParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void WeightParams::InternalSwap(WeightParams* other) {
  floatvalue_.InternalSwap(&other->floatvalue_);
  float16value_.Swap(&other->float16value_);
  rawvalue_.Swap(&other->rawvalue_);
  std::swap(quantization_, other->quantization_);
  std::swap(isupdatable_, other->isupdatable_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string WeightParams::GetTypeName() const {
  return "CoreML.Specification.WeightParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// WeightParams

// repeated float floatValue = 1;
int WeightParams::floatvalue_size() const {
  return floatvalue_.size();
}
void WeightParams::clear_floatvalue() {
  floatvalue_.Clear();
}
float WeightParams::floatvalue(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.WeightParams.floatValue)
  return floatvalue_.Get(index);
}
void WeightParams::set_floatvalue(int index, float value) {
  floatvalue_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.WeightParams.floatValue)
}
void WeightParams::add_floatvalue(float value) {
  floatvalue_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.WeightParams.floatValue)
}
const ::google::protobuf::RepeatedField< float >&
WeightParams::floatvalue() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.WeightParams.floatValue)
  return floatvalue_;
}
::google::protobuf::RepeatedField< float >*
WeightParams::mutable_floatvalue() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.WeightParams.floatValue)
  return &floatvalue_;
}

// bytes float16Value = 2;
void WeightParams::clear_float16value() {
  float16value_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
const ::std::string& WeightParams::float16value() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.WeightParams.float16Value)
  return float16value_.GetNoArena();
}
void WeightParams::set_float16value(const ::std::string& value) {
  
  float16value_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.WeightParams.float16Value)
}
#if LANG_CXX11
void WeightParams::set_float16value(::std::string&& value) {
  
  float16value_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:CoreML.Specification.WeightParams.float16Value)
}
#endif
void WeightParams::set_float16value(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  
  float16value_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:CoreML.Specification.WeightParams.float16Value)
}
void WeightParams::set_float16value(const void* value, size_t size) {
  
  float16value_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:CoreML.Specification.WeightParams.float16Value)
}
::std::string* WeightParams::mutable_float16value() {
  
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.WeightParams.float16Value)
  return float16value_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
::std::string* WeightParams::release_float16value() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.WeightParams.float16Value)
  
  return float16value_.ReleaseNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
void WeightParams::set_allocated_float16value(::std::string* float16value) {
  if (float16value != NULL) {
    
  } else {
    
  }
  float16value_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), float16value);
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.WeightParams.float16Value)
}

// bytes rawValue = 30;
void WeightParams::clear_rawvalue() {
  rawvalue_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
const ::std::string& WeightParams::rawvalue() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.WeightParams.rawValue)
  return rawvalue_.GetNoArena();
}
void WeightParams::set_rawvalue(const ::std::string& value) {
  
  rawvalue_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.WeightParams.rawValue)
}
#if LANG_CXX11
void WeightParams::set_rawvalue(::std::string&& value) {
  
  rawvalue_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:CoreML.Specification.WeightParams.rawValue)
}
#endif
void WeightParams::set_rawvalue(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  
  rawvalue_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:CoreML.Specification.WeightParams.rawValue)
}
void WeightParams::set_rawvalue(const void* value, size_t size) {
  
  rawvalue_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:CoreML.Specification.WeightParams.rawValue)
}
::std::string* WeightParams::mutable_rawvalue() {
  
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.WeightParams.rawValue)
  return rawvalue_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
::std::string* WeightParams::release_rawvalue() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.WeightParams.rawValue)
  
  return rawvalue_.ReleaseNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
void WeightParams::set_allocated_rawvalue(::std::string* rawvalue) {
  if (rawvalue != NULL) {
    
  } else {
    
  }
  rawvalue_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), rawvalue);
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.WeightParams.rawValue)
}

// .CoreML.Specification.QuantizationParams quantization = 40;
bool WeightParams::has_quantization() const {
  return this != internal_default_instance() && quantization_ != NULL;
}
void WeightParams::clear_quantization() {
  if (GetArenaNoVirtual() == NULL && quantization_ != NULL) delete quantization_;
  quantization_ = NULL;
}
const ::CoreML::Specification::QuantizationParams& WeightParams::quantization() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.WeightParams.quantization)
  return quantization_ != NULL ? *quantization_
                         : *::CoreML::Specification::QuantizationParams::internal_default_instance();
}
::CoreML::Specification::QuantizationParams* WeightParams::mutable_quantization() {
  
  if (quantization_ == NULL) {
    quantization_ = new ::CoreML::Specification::QuantizationParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.WeightParams.quantization)
  return quantization_;
}
::CoreML::Specification::QuantizationParams* WeightParams::release_quantization() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.WeightParams.quantization)
  
  ::CoreML::Specification::QuantizationParams* temp = quantization_;
  quantization_ = NULL;
  return temp;
}
void WeightParams::set_allocated_quantization(::CoreML::Specification::QuantizationParams* quantization) {
  delete quantization_;
  quantization_ = quantization;
  if (quantization) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.WeightParams.quantization)
}

// bool isUpdatable = 50;
void WeightParams::clear_isupdatable() {
  isupdatable_ = false;
}
bool WeightParams::isupdatable() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.WeightParams.isUpdatable)
  return isupdatable_;
}
void WeightParams::set_isupdatable(bool value) {
  
  isupdatable_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.WeightParams.isUpdatable)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int QuantizationParams::kNumberOfBitsFieldNumber;
const int QuantizationParams::kLinearQuantizationFieldNumber;
const int QuantizationParams::kLookupTableQuantizationFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

QuantizationParams::QuantizationParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.QuantizationParams)
}
QuantizationParams::QuantizationParams(const QuantizationParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  numberofbits_ = from.numberofbits_;
  clear_has_QuantizationType();
  switch (from.QuantizationType_case()) {
    case kLinearQuantization: {
      mutable_linearquantization()->::CoreML::Specification::LinearQuantizationParams::MergeFrom(from.linearquantization());
      break;
    }
    case kLookupTableQuantization: {
      mutable_lookuptablequantization()->::CoreML::Specification::LookUpTableQuantizationParams::MergeFrom(from.lookuptablequantization());
      break;
    }
    case QUANTIZATIONTYPE_NOT_SET: {
      break;
    }
  }
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.QuantizationParams)
}

void QuantizationParams::SharedCtor() {
  numberofbits_ = GOOGLE_ULONGLONG(0);
  clear_has_QuantizationType();
  _cached_size_ = 0;
}

QuantizationParams::~QuantizationParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.QuantizationParams)
  SharedDtor();
}

void QuantizationParams::SharedDtor() {
  if (has_QuantizationType()) {
    clear_QuantizationType();
  }
}

void QuantizationParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const QuantizationParams& QuantizationParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

QuantizationParams* QuantizationParams::New(::google::protobuf::Arena* arena) const {
  QuantizationParams* n = new QuantizationParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void QuantizationParams::clear_QuantizationType() {
// @@protoc_insertion_point(one_of_clear_start:CoreML.Specification.QuantizationParams)
  switch (QuantizationType_case()) {
    case kLinearQuantization: {
      delete QuantizationType_.linearquantization_;
      break;
    }
    case kLookupTableQuantization: {
      delete QuantizationType_.lookuptablequantization_;
      break;
    }
    case QUANTIZATIONTYPE_NOT_SET: {
      break;
    }
  }
  _oneof_case_[0] = QUANTIZATIONTYPE_NOT_SET;
}


void QuantizationParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.QuantizationParams)
  numberofbits_ = GOOGLE_ULONGLONG(0);
  clear_QuantizationType();
}

bool QuantizationParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.QuantizationParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(16383u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // uint64 numberOfBits = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &numberofbits_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.LinearQuantizationParams linearQuantization = 101;
      case 101: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(810u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_linearquantization()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.LookUpTableQuantizationParams lookupTableQuantization = 102;
      case 102: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(818u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_lookuptablequantization()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.QuantizationParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.QuantizationParams)
  return false;
#undef DO_
}

void QuantizationParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.QuantizationParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // uint64 numberOfBits = 1;
  if (this->numberofbits() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(1, this->numberofbits(), output);
  }

  // .CoreML.Specification.LinearQuantizationParams linearQuantization = 101;
  if (has_linearquantization()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      101, *QuantizationType_.linearquantization_, output);
  }

  // .CoreML.Specification.LookUpTableQuantizationParams lookupTableQuantization = 102;
  if (has_lookuptablequantization()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      102, *QuantizationType_.lookuptablequantization_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.QuantizationParams)
}

size_t QuantizationParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.QuantizationParams)
  size_t total_size = 0;

  // uint64 numberOfBits = 1;
  if (this->numberofbits() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->numberofbits());
  }

  switch (QuantizationType_case()) {
    // .CoreML.Specification.LinearQuantizationParams linearQuantization = 101;
    case kLinearQuantization: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *QuantizationType_.linearquantization_);
      break;
    }
    // .CoreML.Specification.LookUpTableQuantizationParams lookupTableQuantization = 102;
    case kLookupTableQuantization: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *QuantizationType_.lookuptablequantization_);
      break;
    }
    case QUANTIZATIONTYPE_NOT_SET: {
      break;
    }
  }
  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void QuantizationParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const QuantizationParams*>(&from));
}

void QuantizationParams::MergeFrom(const QuantizationParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.QuantizationParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.numberofbits() != 0) {
    set_numberofbits(from.numberofbits());
  }
  switch (from.QuantizationType_case()) {
    case kLinearQuantization: {
      mutable_linearquantization()->::CoreML::Specification::LinearQuantizationParams::MergeFrom(from.linearquantization());
      break;
    }
    case kLookupTableQuantization: {
      mutable_lookuptablequantization()->::CoreML::Specification::LookUpTableQuantizationParams::MergeFrom(from.lookuptablequantization());
      break;
    }
    case QUANTIZATIONTYPE_NOT_SET: {
      break;
    }
  }
}

void QuantizationParams::CopyFrom(const QuantizationParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.QuantizationParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool QuantizationParams::IsInitialized() const {
  return true;
}

void QuantizationParams::Swap(QuantizationParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void QuantizationParams::InternalSwap(QuantizationParams* other) {
  std::swap(numberofbits_, other->numberofbits_);
  std::swap(QuantizationType_, other->QuantizationType_);
  std::swap(_oneof_case_[0], other->_oneof_case_[0]);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string QuantizationParams::GetTypeName() const {
  return "CoreML.Specification.QuantizationParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// QuantizationParams

// uint64 numberOfBits = 1;
void QuantizationParams::clear_numberofbits() {
  numberofbits_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 QuantizationParams::numberofbits() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.QuantizationParams.numberOfBits)
  return numberofbits_;
}
void QuantizationParams::set_numberofbits(::google::protobuf::uint64 value) {
  
  numberofbits_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.QuantizationParams.numberOfBits)
}

// .CoreML.Specification.LinearQuantizationParams linearQuantization = 101;
bool QuantizationParams::has_linearquantization() const {
  return QuantizationType_case() == kLinearQuantization;
}
void QuantizationParams::set_has_linearquantization() {
  _oneof_case_[0] = kLinearQuantization;
}
void QuantizationParams::clear_linearquantization() {
  if (has_linearquantization()) {
    delete QuantizationType_.linearquantization_;
    clear_has_QuantizationType();
  }
}
 const ::CoreML::Specification::LinearQuantizationParams& QuantizationParams::linearquantization() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.QuantizationParams.linearQuantization)
  return has_linearquantization()
      ? *QuantizationType_.linearquantization_
      : ::CoreML::Specification::LinearQuantizationParams::default_instance();
}
::CoreML::Specification::LinearQuantizationParams* QuantizationParams::mutable_linearquantization() {
  if (!has_linearquantization()) {
    clear_QuantizationType();
    set_has_linearquantization();
    QuantizationType_.linearquantization_ = new ::CoreML::Specification::LinearQuantizationParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.QuantizationParams.linearQuantization)
  return QuantizationType_.linearquantization_;
}
::CoreML::Specification::LinearQuantizationParams* QuantizationParams::release_linearquantization() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.QuantizationParams.linearQuantization)
  if (has_linearquantization()) {
    clear_has_QuantizationType();
    ::CoreML::Specification::LinearQuantizationParams* temp = QuantizationType_.linearquantization_;
    QuantizationType_.linearquantization_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void QuantizationParams::set_allocated_linearquantization(::CoreML::Specification::LinearQuantizationParams* linearquantization) {
  clear_QuantizationType();
  if (linearquantization) {
    set_has_linearquantization();
    QuantizationType_.linearquantization_ = linearquantization;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.QuantizationParams.linearQuantization)
}

// .CoreML.Specification.LookUpTableQuantizationParams lookupTableQuantization = 102;
bool QuantizationParams::has_lookuptablequantization() const {
  return QuantizationType_case() == kLookupTableQuantization;
}
void QuantizationParams::set_has_lookuptablequantization() {
  _oneof_case_[0] = kLookupTableQuantization;
}
void QuantizationParams::clear_lookuptablequantization() {
  if (has_lookuptablequantization()) {
    delete QuantizationType_.lookuptablequantization_;
    clear_has_QuantizationType();
  }
}
 const ::CoreML::Specification::LookUpTableQuantizationParams& QuantizationParams::lookuptablequantization() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.QuantizationParams.lookupTableQuantization)
  return has_lookuptablequantization()
      ? *QuantizationType_.lookuptablequantization_
      : ::CoreML::Specification::LookUpTableQuantizationParams::default_instance();
}
::CoreML::Specification::LookUpTableQuantizationParams* QuantizationParams::mutable_lookuptablequantization() {
  if (!has_lookuptablequantization()) {
    clear_QuantizationType();
    set_has_lookuptablequantization();
    QuantizationType_.lookuptablequantization_ = new ::CoreML::Specification::LookUpTableQuantizationParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.QuantizationParams.lookupTableQuantization)
  return QuantizationType_.lookuptablequantization_;
}
::CoreML::Specification::LookUpTableQuantizationParams* QuantizationParams::release_lookuptablequantization() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.QuantizationParams.lookupTableQuantization)
  if (has_lookuptablequantization()) {
    clear_has_QuantizationType();
    ::CoreML::Specification::LookUpTableQuantizationParams* temp = QuantizationType_.lookuptablequantization_;
    QuantizationType_.lookuptablequantization_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void QuantizationParams::set_allocated_lookuptablequantization(::CoreML::Specification::LookUpTableQuantizationParams* lookuptablequantization) {
  clear_QuantizationType();
  if (lookuptablequantization) {
    set_has_lookuptablequantization();
    QuantizationType_.lookuptablequantization_ = lookuptablequantization;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.QuantizationParams.lookupTableQuantization)
}

bool QuantizationParams::has_QuantizationType() const {
  return QuantizationType_case() != QUANTIZATIONTYPE_NOT_SET;
}
void QuantizationParams::clear_has_QuantizationType() {
  _oneof_case_[0] = QUANTIZATIONTYPE_NOT_SET;
}
QuantizationParams::QuantizationTypeCase QuantizationParams::QuantizationType_case() const {
  return QuantizationParams::QuantizationTypeCase(_oneof_case_[0]);
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int LinearQuantizationParams::kScaleFieldNumber;
const int LinearQuantizationParams::kBiasFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

LinearQuantizationParams::LinearQuantizationParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.LinearQuantizationParams)
}
LinearQuantizationParams::LinearQuantizationParams(const LinearQuantizationParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      scale_(from.scale_),
      bias_(from.bias_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.LinearQuantizationParams)
}

void LinearQuantizationParams::SharedCtor() {
  _cached_size_ = 0;
}

LinearQuantizationParams::~LinearQuantizationParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.LinearQuantizationParams)
  SharedDtor();
}

void LinearQuantizationParams::SharedDtor() {
}

void LinearQuantizationParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const LinearQuantizationParams& LinearQuantizationParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

LinearQuantizationParams* LinearQuantizationParams::New(::google::protobuf::Arena* arena) const {
  LinearQuantizationParams* n = new LinearQuantizationParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void LinearQuantizationParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.LinearQuantizationParams)
  scale_.Clear();
  bias_.Clear();
}

bool LinearQuantizationParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.LinearQuantizationParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated float scale = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, this->mutable_scale())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(13u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 1, 10u, input, this->mutable_scale())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated float bias = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(18u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, this->mutable_bias())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(21u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 1, 18u, input, this->mutable_bias())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.LinearQuantizationParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.LinearQuantizationParams)
  return false;
#undef DO_
}

void LinearQuantizationParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.LinearQuantizationParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated float scale = 1;
  if (this->scale_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(1, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_scale_cached_byte_size_);
    ::google::protobuf::internal::WireFormatLite::WriteFloatArray(
      this->scale().data(), this->scale_size(), output);
  }

  // repeated float bias = 2;
  if (this->bias_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(2, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_bias_cached_byte_size_);
    ::google::protobuf::internal::WireFormatLite::WriteFloatArray(
      this->bias().data(), this->bias_size(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.LinearQuantizationParams)
}

size_t LinearQuantizationParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.LinearQuantizationParams)
  size_t total_size = 0;

  // repeated float scale = 1;
  {
    unsigned int count = this->scale_size();
    size_t data_size = 4UL * count;
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _scale_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // repeated float bias = 2;
  {
    unsigned int count = this->bias_size();
    size_t data_size = 4UL * count;
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _bias_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void LinearQuantizationParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const LinearQuantizationParams*>(&from));
}

void LinearQuantizationParams::MergeFrom(const LinearQuantizationParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.LinearQuantizationParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  scale_.MergeFrom(from.scale_);
  bias_.MergeFrom(from.bias_);
}

void LinearQuantizationParams::CopyFrom(const LinearQuantizationParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.LinearQuantizationParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool LinearQuantizationParams::IsInitialized() const {
  return true;
}

void LinearQuantizationParams::Swap(LinearQuantizationParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void LinearQuantizationParams::InternalSwap(LinearQuantizationParams* other) {
  scale_.InternalSwap(&other->scale_);
  bias_.InternalSwap(&other->bias_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string LinearQuantizationParams::GetTypeName() const {
  return "CoreML.Specification.LinearQuantizationParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// LinearQuantizationParams

// repeated float scale = 1;
int LinearQuantizationParams::scale_size() const {
  return scale_.size();
}
void LinearQuantizationParams::clear_scale() {
  scale_.Clear();
}
float LinearQuantizationParams::scale(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LinearQuantizationParams.scale)
  return scale_.Get(index);
}
void LinearQuantizationParams::set_scale(int index, float value) {
  scale_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.LinearQuantizationParams.scale)
}
void LinearQuantizationParams::add_scale(float value) {
  scale_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.LinearQuantizationParams.scale)
}
const ::google::protobuf::RepeatedField< float >&
LinearQuantizationParams::scale() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.LinearQuantizationParams.scale)
  return scale_;
}
::google::protobuf::RepeatedField< float >*
LinearQuantizationParams::mutable_scale() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.LinearQuantizationParams.scale)
  return &scale_;
}

// repeated float bias = 2;
int LinearQuantizationParams::bias_size() const {
  return bias_.size();
}
void LinearQuantizationParams::clear_bias() {
  bias_.Clear();
}
float LinearQuantizationParams::bias(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LinearQuantizationParams.bias)
  return bias_.Get(index);
}
void LinearQuantizationParams::set_bias(int index, float value) {
  bias_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.LinearQuantizationParams.bias)
}
void LinearQuantizationParams::add_bias(float value) {
  bias_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.LinearQuantizationParams.bias)
}
const ::google::protobuf::RepeatedField< float >&
LinearQuantizationParams::bias() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.LinearQuantizationParams.bias)
  return bias_;
}
::google::protobuf::RepeatedField< float >*
LinearQuantizationParams::mutable_bias() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.LinearQuantizationParams.bias)
  return &bias_;
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int LookUpTableQuantizationParams::kFloatValueFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

LookUpTableQuantizationParams::LookUpTableQuantizationParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.LookUpTableQuantizationParams)
}
LookUpTableQuantizationParams::LookUpTableQuantizationParams(const LookUpTableQuantizationParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      floatvalue_(from.floatvalue_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.LookUpTableQuantizationParams)
}

void LookUpTableQuantizationParams::SharedCtor() {
  _cached_size_ = 0;
}

LookUpTableQuantizationParams::~LookUpTableQuantizationParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.LookUpTableQuantizationParams)
  SharedDtor();
}

void LookUpTableQuantizationParams::SharedDtor() {
}

void LookUpTableQuantizationParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const LookUpTableQuantizationParams& LookUpTableQuantizationParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

LookUpTableQuantizationParams* LookUpTableQuantizationParams::New(::google::protobuf::Arena* arena) const {
  LookUpTableQuantizationParams* n = new LookUpTableQuantizationParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void LookUpTableQuantizationParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.LookUpTableQuantizationParams)
  floatvalue_.Clear();
}

bool LookUpTableQuantizationParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.LookUpTableQuantizationParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated float floatValue = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, this->mutable_floatvalue())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(13u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 1, 10u, input, this->mutable_floatvalue())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.LookUpTableQuantizationParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.LookUpTableQuantizationParams)
  return false;
#undef DO_
}

void LookUpTableQuantizationParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.LookUpTableQuantizationParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated float floatValue = 1;
  if (this->floatvalue_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(1, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_floatvalue_cached_byte_size_);
    ::google::protobuf::internal::WireFormatLite::WriteFloatArray(
      this->floatvalue().data(), this->floatvalue_size(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.LookUpTableQuantizationParams)
}

size_t LookUpTableQuantizationParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.LookUpTableQuantizationParams)
  size_t total_size = 0;

  // repeated float floatValue = 1;
  {
    unsigned int count = this->floatvalue_size();
    size_t data_size = 4UL * count;
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _floatvalue_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void LookUpTableQuantizationParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const LookUpTableQuantizationParams*>(&from));
}

void LookUpTableQuantizationParams::MergeFrom(const LookUpTableQuantizationParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.LookUpTableQuantizationParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  floatvalue_.MergeFrom(from.floatvalue_);
}

void LookUpTableQuantizationParams::CopyFrom(const LookUpTableQuantizationParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.LookUpTableQuantizationParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool LookUpTableQuantizationParams::IsInitialized() const {
  return true;
}

void LookUpTableQuantizationParams::Swap(LookUpTableQuantizationParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void LookUpTableQuantizationParams::InternalSwap(LookUpTableQuantizationParams* other) {
  floatvalue_.InternalSwap(&other->floatvalue_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string LookUpTableQuantizationParams::GetTypeName() const {
  return "CoreML.Specification.LookUpTableQuantizationParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// LookUpTableQuantizationParams

// repeated float floatValue = 1;
int LookUpTableQuantizationParams::floatvalue_size() const {
  return floatvalue_.size();
}
void LookUpTableQuantizationParams::clear_floatvalue() {
  floatvalue_.Clear();
}
float LookUpTableQuantizationParams::floatvalue(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LookUpTableQuantizationParams.floatValue)
  return floatvalue_.Get(index);
}
void LookUpTableQuantizationParams::set_floatvalue(int index, float value) {
  floatvalue_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.LookUpTableQuantizationParams.floatValue)
}
void LookUpTableQuantizationParams::add_floatvalue(float value) {
  floatvalue_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.LookUpTableQuantizationParams.floatValue)
}
const ::google::protobuf::RepeatedField< float >&
LookUpTableQuantizationParams::floatvalue() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.LookUpTableQuantizationParams.floatValue)
  return floatvalue_;
}
::google::protobuf::RepeatedField< float >*
LookUpTableQuantizationParams::mutable_floatvalue() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.LookUpTableQuantizationParams.floatValue)
  return &floatvalue_;
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ConvolutionLayerParams::kOutputChannelsFieldNumber;
const int ConvolutionLayerParams::kKernelChannelsFieldNumber;
const int ConvolutionLayerParams::kNGroupsFieldNumber;
const int ConvolutionLayerParams::kKernelSizeFieldNumber;
const int ConvolutionLayerParams::kStrideFieldNumber;
const int ConvolutionLayerParams::kDilationFactorFieldNumber;
const int ConvolutionLayerParams::kValidFieldNumber;
const int ConvolutionLayerParams::kSameFieldNumber;
const int ConvolutionLayerParams::kIsDeconvolutionFieldNumber;
const int ConvolutionLayerParams::kHasBiasFieldNumber;
const int ConvolutionLayerParams::kWeightsFieldNumber;
const int ConvolutionLayerParams::kBiasFieldNumber;
const int ConvolutionLayerParams::kOutputShapeFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ConvolutionLayerParams::ConvolutionLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ConvolutionLayerParams)
}
ConvolutionLayerParams::ConvolutionLayerParams(const ConvolutionLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      kernelsize_(from.kernelsize_),
      stride_(from.stride_),
      dilationfactor_(from.dilationfactor_),
      outputshape_(from.outputshape_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_weights()) {
    weights_ = new ::CoreML::Specification::WeightParams(*from.weights_);
  } else {
    weights_ = NULL;
  }
  if (from.has_bias()) {
    bias_ = new ::CoreML::Specification::WeightParams(*from.bias_);
  } else {
    bias_ = NULL;
  }
  ::memcpy(&outputchannels_, &from.outputchannels_,
    reinterpret_cast<char*>(&hasbias_) -
    reinterpret_cast<char*>(&outputchannels_) + sizeof(hasbias_));
  clear_has_ConvolutionPaddingType();
  switch (from.ConvolutionPaddingType_case()) {
    case kValid: {
      mutable_valid()->::CoreML::Specification::ValidPadding::MergeFrom(from.valid());
      break;
    }
    case kSame: {
      mutable_same()->::CoreML::Specification::SamePadding::MergeFrom(from.same());
      break;
    }
    case CONVOLUTIONPADDINGTYPE_NOT_SET: {
      break;
    }
  }
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ConvolutionLayerParams)
}

void ConvolutionLayerParams::SharedCtor() {
  ::memset(&weights_, 0, reinterpret_cast<char*>(&hasbias_) -
    reinterpret_cast<char*>(&weights_) + sizeof(hasbias_));
  clear_has_ConvolutionPaddingType();
  _cached_size_ = 0;
}

ConvolutionLayerParams::~ConvolutionLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ConvolutionLayerParams)
  SharedDtor();
}

void ConvolutionLayerParams::SharedDtor() {
  if (this != internal_default_instance()) {
    delete weights_;
  }
  if (this != internal_default_instance()) {
    delete bias_;
  }
  if (has_ConvolutionPaddingType()) {
    clear_ConvolutionPaddingType();
  }
}

void ConvolutionLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ConvolutionLayerParams& ConvolutionLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ConvolutionLayerParams* ConvolutionLayerParams::New(::google::protobuf::Arena* arena) const {
  ConvolutionLayerParams* n = new ConvolutionLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ConvolutionLayerParams::clear_ConvolutionPaddingType() {
// @@protoc_insertion_point(one_of_clear_start:CoreML.Specification.ConvolutionLayerParams)
  switch (ConvolutionPaddingType_case()) {
    case kValid: {
      delete ConvolutionPaddingType_.valid_;
      break;
    }
    case kSame: {
      delete ConvolutionPaddingType_.same_;
      break;
    }
    case CONVOLUTIONPADDINGTYPE_NOT_SET: {
      break;
    }
  }
  _oneof_case_[0] = CONVOLUTIONPADDINGTYPE_NOT_SET;
}


void ConvolutionLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ConvolutionLayerParams)
  kernelsize_.Clear();
  stride_.Clear();
  dilationfactor_.Clear();
  outputshape_.Clear();
  if (GetArenaNoVirtual() == NULL && weights_ != NULL) {
    delete weights_;
  }
  weights_ = NULL;
  if (GetArenaNoVirtual() == NULL && bias_ != NULL) {
    delete bias_;
  }
  bias_ = NULL;
  ::memset(&outputchannels_, 0, reinterpret_cast<char*>(&hasbias_) -
    reinterpret_cast<char*>(&outputchannels_) + sizeof(hasbias_));
  clear_ConvolutionPaddingType();
}

bool ConvolutionLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ConvolutionLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(16383u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // uint64 outputChannels = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &outputchannels_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // uint64 kernelChannels = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(16u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &kernelchannels_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // uint64 nGroups = 10;
      case 10: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(80u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &ngroups_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated uint64 kernelSize = 20;
      case 20: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(162u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, this->mutable_kernelsize())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(160u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 2, 162u, input, this->mutable_kernelsize())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated uint64 stride = 30;
      case 30: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(242u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, this->mutable_stride())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(240u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 2, 242u, input, this->mutable_stride())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated uint64 dilationFactor = 40;
      case 40: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(322u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, this->mutable_dilationfactor())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(320u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 2, 322u, input, this->mutable_dilationfactor())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ValidPadding valid = 50;
      case 50: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(402u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_valid()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.SamePadding same = 51;
      case 51: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(410u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_same()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool isDeconvolution = 60;
      case 60: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(480u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &isdeconvolution_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool hasBias = 70;
      case 70: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(560u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &hasbias_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams weights = 90;
      case 90: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(722u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_weights()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams bias = 91;
      case 91: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(730u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_bias()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated uint64 outputShape = 100;
      case 100: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(802u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, this->mutable_outputshape())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(800u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 2, 802u, input, this->mutable_outputshape())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ConvolutionLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ConvolutionLayerParams)
  return false;
#undef DO_
}

void ConvolutionLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ConvolutionLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // uint64 outputChannels = 1;
  if (this->outputchannels() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(1, this->outputchannels(), output);
  }

  // uint64 kernelChannels = 2;
  if (this->kernelchannels() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(2, this->kernelchannels(), output);
  }

  // uint64 nGroups = 10;
  if (this->ngroups() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(10, this->ngroups(), output);
  }

  // repeated uint64 kernelSize = 20;
  if (this->kernelsize_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(20, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_kernelsize_cached_byte_size_);
  }
  for (int i = 0, n = this->kernelsize_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64NoTag(
      this->kernelsize(i), output);
  }

  // repeated uint64 stride = 30;
  if (this->stride_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(30, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_stride_cached_byte_size_);
  }
  for (int i = 0, n = this->stride_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64NoTag(
      this->stride(i), output);
  }

  // repeated uint64 dilationFactor = 40;
  if (this->dilationfactor_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(40, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_dilationfactor_cached_byte_size_);
  }
  for (int i = 0, n = this->dilationfactor_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64NoTag(
      this->dilationfactor(i), output);
  }

  // .CoreML.Specification.ValidPadding valid = 50;
  if (has_valid()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      50, *ConvolutionPaddingType_.valid_, output);
  }

  // .CoreML.Specification.SamePadding same = 51;
  if (has_same()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      51, *ConvolutionPaddingType_.same_, output);
  }

  // bool isDeconvolution = 60;
  if (this->isdeconvolution() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(60, this->isdeconvolution(), output);
  }

  // bool hasBias = 70;
  if (this->hasbias() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(70, this->hasbias(), output);
  }

  // .CoreML.Specification.WeightParams weights = 90;
  if (this->has_weights()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      90, *this->weights_, output);
  }

  // .CoreML.Specification.WeightParams bias = 91;
  if (this->has_bias()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      91, *this->bias_, output);
  }

  // repeated uint64 outputShape = 100;
  if (this->outputshape_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(100, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_outputshape_cached_byte_size_);
  }
  for (int i = 0, n = this->outputshape_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64NoTag(
      this->outputshape(i), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ConvolutionLayerParams)
}

size_t ConvolutionLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ConvolutionLayerParams)
  size_t total_size = 0;

  // repeated uint64 kernelSize = 20;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      UInt64Size(this->kernelsize_);
    if (data_size > 0) {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _kernelsize_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // repeated uint64 stride = 30;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      UInt64Size(this->stride_);
    if (data_size > 0) {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _stride_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // repeated uint64 dilationFactor = 40;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      UInt64Size(this->dilationfactor_);
    if (data_size > 0) {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _dilationfactor_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // repeated uint64 outputShape = 100;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      UInt64Size(this->outputshape_);
    if (data_size > 0) {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _outputshape_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // .CoreML.Specification.WeightParams weights = 90;
  if (this->has_weights()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->weights_);
  }

  // .CoreML.Specification.WeightParams bias = 91;
  if (this->has_bias()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->bias_);
  }

  // uint64 outputChannels = 1;
  if (this->outputchannels() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->outputchannels());
  }

  // uint64 kernelChannels = 2;
  if (this->kernelchannels() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->kernelchannels());
  }

  // uint64 nGroups = 10;
  if (this->ngroups() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->ngroups());
  }

  // bool isDeconvolution = 60;
  if (this->isdeconvolution() != 0) {
    total_size += 2 + 1;
  }

  // bool hasBias = 70;
  if (this->hasbias() != 0) {
    total_size += 2 + 1;
  }

  switch (ConvolutionPaddingType_case()) {
    // .CoreML.Specification.ValidPadding valid = 50;
    case kValid: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *ConvolutionPaddingType_.valid_);
      break;
    }
    // .CoreML.Specification.SamePadding same = 51;
    case kSame: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *ConvolutionPaddingType_.same_);
      break;
    }
    case CONVOLUTIONPADDINGTYPE_NOT_SET: {
      break;
    }
  }
  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ConvolutionLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ConvolutionLayerParams*>(&from));
}

void ConvolutionLayerParams::MergeFrom(const ConvolutionLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ConvolutionLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  kernelsize_.MergeFrom(from.kernelsize_);
  stride_.MergeFrom(from.stride_);
  dilationfactor_.MergeFrom(from.dilationfactor_);
  outputshape_.MergeFrom(from.outputshape_);
  if (from.has_weights()) {
    mutable_weights()->::CoreML::Specification::WeightParams::MergeFrom(from.weights());
  }
  if (from.has_bias()) {
    mutable_bias()->::CoreML::Specification::WeightParams::MergeFrom(from.bias());
  }
  if (from.outputchannels() != 0) {
    set_outputchannels(from.outputchannels());
  }
  if (from.kernelchannels() != 0) {
    set_kernelchannels(from.kernelchannels());
  }
  if (from.ngroups() != 0) {
    set_ngroups(from.ngroups());
  }
  if (from.isdeconvolution() != 0) {
    set_isdeconvolution(from.isdeconvolution());
  }
  if (from.hasbias() != 0) {
    set_hasbias(from.hasbias());
  }
  switch (from.ConvolutionPaddingType_case()) {
    case kValid: {
      mutable_valid()->::CoreML::Specification::ValidPadding::MergeFrom(from.valid());
      break;
    }
    case kSame: {
      mutable_same()->::CoreML::Specification::SamePadding::MergeFrom(from.same());
      break;
    }
    case CONVOLUTIONPADDINGTYPE_NOT_SET: {
      break;
    }
  }
}

void ConvolutionLayerParams::CopyFrom(const ConvolutionLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ConvolutionLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ConvolutionLayerParams::IsInitialized() const {
  return true;
}

void ConvolutionLayerParams::Swap(ConvolutionLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ConvolutionLayerParams::InternalSwap(ConvolutionLayerParams* other) {
  kernelsize_.InternalSwap(&other->kernelsize_);
  stride_.InternalSwap(&other->stride_);
  dilationfactor_.InternalSwap(&other->dilationfactor_);
  outputshape_.InternalSwap(&other->outputshape_);
  std::swap(weights_, other->weights_);
  std::swap(bias_, other->bias_);
  std::swap(outputchannels_, other->outputchannels_);
  std::swap(kernelchannels_, other->kernelchannels_);
  std::swap(ngroups_, other->ngroups_);
  std::swap(isdeconvolution_, other->isdeconvolution_);
  std::swap(hasbias_, other->hasbias_);
  std::swap(ConvolutionPaddingType_, other->ConvolutionPaddingType_);
  std::swap(_oneof_case_[0], other->_oneof_case_[0]);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ConvolutionLayerParams::GetTypeName() const {
  return "CoreML.Specification.ConvolutionLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ConvolutionLayerParams

// uint64 outputChannels = 1;
void ConvolutionLayerParams::clear_outputchannels() {
  outputchannels_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 ConvolutionLayerParams::outputchannels() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.outputChannels)
  return outputchannels_;
}
void ConvolutionLayerParams::set_outputchannels(::google::protobuf::uint64 value) {
  
  outputchannels_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConvolutionLayerParams.outputChannels)
}

// uint64 kernelChannels = 2;
void ConvolutionLayerParams::clear_kernelchannels() {
  kernelchannels_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 ConvolutionLayerParams::kernelchannels() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.kernelChannels)
  return kernelchannels_;
}
void ConvolutionLayerParams::set_kernelchannels(::google::protobuf::uint64 value) {
  
  kernelchannels_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConvolutionLayerParams.kernelChannels)
}

// uint64 nGroups = 10;
void ConvolutionLayerParams::clear_ngroups() {
  ngroups_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 ConvolutionLayerParams::ngroups() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.nGroups)
  return ngroups_;
}
void ConvolutionLayerParams::set_ngroups(::google::protobuf::uint64 value) {
  
  ngroups_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConvolutionLayerParams.nGroups)
}

// repeated uint64 kernelSize = 20;
int ConvolutionLayerParams::kernelsize_size() const {
  return kernelsize_.size();
}
void ConvolutionLayerParams::clear_kernelsize() {
  kernelsize_.Clear();
}
::google::protobuf::uint64 ConvolutionLayerParams::kernelsize(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.kernelSize)
  return kernelsize_.Get(index);
}
void ConvolutionLayerParams::set_kernelsize(int index, ::google::protobuf::uint64 value) {
  kernelsize_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConvolutionLayerParams.kernelSize)
}
void ConvolutionLayerParams::add_kernelsize(::google::protobuf::uint64 value) {
  kernelsize_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ConvolutionLayerParams.kernelSize)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
ConvolutionLayerParams::kernelsize() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ConvolutionLayerParams.kernelSize)
  return kernelsize_;
}
::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
ConvolutionLayerParams::mutable_kernelsize() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ConvolutionLayerParams.kernelSize)
  return &kernelsize_;
}

// repeated uint64 stride = 30;
int ConvolutionLayerParams::stride_size() const {
  return stride_.size();
}
void ConvolutionLayerParams::clear_stride() {
  stride_.Clear();
}
::google::protobuf::uint64 ConvolutionLayerParams::stride(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.stride)
  return stride_.Get(index);
}
void ConvolutionLayerParams::set_stride(int index, ::google::protobuf::uint64 value) {
  stride_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConvolutionLayerParams.stride)
}
void ConvolutionLayerParams::add_stride(::google::protobuf::uint64 value) {
  stride_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ConvolutionLayerParams.stride)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
ConvolutionLayerParams::stride() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ConvolutionLayerParams.stride)
  return stride_;
}
::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
ConvolutionLayerParams::mutable_stride() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ConvolutionLayerParams.stride)
  return &stride_;
}

// repeated uint64 dilationFactor = 40;
int ConvolutionLayerParams::dilationfactor_size() const {
  return dilationfactor_.size();
}
void ConvolutionLayerParams::clear_dilationfactor() {
  dilationfactor_.Clear();
}
::google::protobuf::uint64 ConvolutionLayerParams::dilationfactor(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.dilationFactor)
  return dilationfactor_.Get(index);
}
void ConvolutionLayerParams::set_dilationfactor(int index, ::google::protobuf::uint64 value) {
  dilationfactor_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConvolutionLayerParams.dilationFactor)
}
void ConvolutionLayerParams::add_dilationfactor(::google::protobuf::uint64 value) {
  dilationfactor_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ConvolutionLayerParams.dilationFactor)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
ConvolutionLayerParams::dilationfactor() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ConvolutionLayerParams.dilationFactor)
  return dilationfactor_;
}
::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
ConvolutionLayerParams::mutable_dilationfactor() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ConvolutionLayerParams.dilationFactor)
  return &dilationfactor_;
}

// .CoreML.Specification.ValidPadding valid = 50;
bool ConvolutionLayerParams::has_valid() const {
  return ConvolutionPaddingType_case() == kValid;
}
void ConvolutionLayerParams::set_has_valid() {
  _oneof_case_[0] = kValid;
}
void ConvolutionLayerParams::clear_valid() {
  if (has_valid()) {
    delete ConvolutionPaddingType_.valid_;
    clear_has_ConvolutionPaddingType();
  }
}
 const ::CoreML::Specification::ValidPadding& ConvolutionLayerParams::valid() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.valid)
  return has_valid()
      ? *ConvolutionPaddingType_.valid_
      : ::CoreML::Specification::ValidPadding::default_instance();
}
::CoreML::Specification::ValidPadding* ConvolutionLayerParams::mutable_valid() {
  if (!has_valid()) {
    clear_ConvolutionPaddingType();
    set_has_valid();
    ConvolutionPaddingType_.valid_ = new ::CoreML::Specification::ValidPadding;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ConvolutionLayerParams.valid)
  return ConvolutionPaddingType_.valid_;
}
::CoreML::Specification::ValidPadding* ConvolutionLayerParams::release_valid() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ConvolutionLayerParams.valid)
  if (has_valid()) {
    clear_has_ConvolutionPaddingType();
    ::CoreML::Specification::ValidPadding* temp = ConvolutionPaddingType_.valid_;
    ConvolutionPaddingType_.valid_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void ConvolutionLayerParams::set_allocated_valid(::CoreML::Specification::ValidPadding* valid) {
  clear_ConvolutionPaddingType();
  if (valid) {
    set_has_valid();
    ConvolutionPaddingType_.valid_ = valid;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ConvolutionLayerParams.valid)
}

// .CoreML.Specification.SamePadding same = 51;
bool ConvolutionLayerParams::has_same() const {
  return ConvolutionPaddingType_case() == kSame;
}
void ConvolutionLayerParams::set_has_same() {
  _oneof_case_[0] = kSame;
}
void ConvolutionLayerParams::clear_same() {
  if (has_same()) {
    delete ConvolutionPaddingType_.same_;
    clear_has_ConvolutionPaddingType();
  }
}
 const ::CoreML::Specification::SamePadding& ConvolutionLayerParams::same() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.same)
  return has_same()
      ? *ConvolutionPaddingType_.same_
      : ::CoreML::Specification::SamePadding::default_instance();
}
::CoreML::Specification::SamePadding* ConvolutionLayerParams::mutable_same() {
  if (!has_same()) {
    clear_ConvolutionPaddingType();
    set_has_same();
    ConvolutionPaddingType_.same_ = new ::CoreML::Specification::SamePadding;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ConvolutionLayerParams.same)
  return ConvolutionPaddingType_.same_;
}
::CoreML::Specification::SamePadding* ConvolutionLayerParams::release_same() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ConvolutionLayerParams.same)
  if (has_same()) {
    clear_has_ConvolutionPaddingType();
    ::CoreML::Specification::SamePadding* temp = ConvolutionPaddingType_.same_;
    ConvolutionPaddingType_.same_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void ConvolutionLayerParams::set_allocated_same(::CoreML::Specification::SamePadding* same) {
  clear_ConvolutionPaddingType();
  if (same) {
    set_has_same();
    ConvolutionPaddingType_.same_ = same;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ConvolutionLayerParams.same)
}

// bool isDeconvolution = 60;
void ConvolutionLayerParams::clear_isdeconvolution() {
  isdeconvolution_ = false;
}
bool ConvolutionLayerParams::isdeconvolution() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.isDeconvolution)
  return isdeconvolution_;
}
void ConvolutionLayerParams::set_isdeconvolution(bool value) {
  
  isdeconvolution_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConvolutionLayerParams.isDeconvolution)
}

// bool hasBias = 70;
void ConvolutionLayerParams::clear_hasbias() {
  hasbias_ = false;
}
bool ConvolutionLayerParams::hasbias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.hasBias)
  return hasbias_;
}
void ConvolutionLayerParams::set_hasbias(bool value) {
  
  hasbias_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConvolutionLayerParams.hasBias)
}

// .CoreML.Specification.WeightParams weights = 90;
bool ConvolutionLayerParams::has_weights() const {
  return this != internal_default_instance() && weights_ != NULL;
}
void ConvolutionLayerParams::clear_weights() {
  if (GetArenaNoVirtual() == NULL && weights_ != NULL) delete weights_;
  weights_ = NULL;
}
const ::CoreML::Specification::WeightParams& ConvolutionLayerParams::weights() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.weights)
  return weights_ != NULL ? *weights_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* ConvolutionLayerParams::mutable_weights() {
  
  if (weights_ == NULL) {
    weights_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ConvolutionLayerParams.weights)
  return weights_;
}
::CoreML::Specification::WeightParams* ConvolutionLayerParams::release_weights() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ConvolutionLayerParams.weights)
  
  ::CoreML::Specification::WeightParams* temp = weights_;
  weights_ = NULL;
  return temp;
}
void ConvolutionLayerParams::set_allocated_weights(::CoreML::Specification::WeightParams* weights) {
  delete weights_;
  weights_ = weights;
  if (weights) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ConvolutionLayerParams.weights)
}

// .CoreML.Specification.WeightParams bias = 91;
bool ConvolutionLayerParams::has_bias() const {
  return this != internal_default_instance() && bias_ != NULL;
}
void ConvolutionLayerParams::clear_bias() {
  if (GetArenaNoVirtual() == NULL && bias_ != NULL) delete bias_;
  bias_ = NULL;
}
const ::CoreML::Specification::WeightParams& ConvolutionLayerParams::bias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.bias)
  return bias_ != NULL ? *bias_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* ConvolutionLayerParams::mutable_bias() {
  
  if (bias_ == NULL) {
    bias_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ConvolutionLayerParams.bias)
  return bias_;
}
::CoreML::Specification::WeightParams* ConvolutionLayerParams::release_bias() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ConvolutionLayerParams.bias)
  
  ::CoreML::Specification::WeightParams* temp = bias_;
  bias_ = NULL;
  return temp;
}
void ConvolutionLayerParams::set_allocated_bias(::CoreML::Specification::WeightParams* bias) {
  delete bias_;
  bias_ = bias;
  if (bias) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ConvolutionLayerParams.bias)
}

// repeated uint64 outputShape = 100;
int ConvolutionLayerParams::outputshape_size() const {
  return outputshape_.size();
}
void ConvolutionLayerParams::clear_outputshape() {
  outputshape_.Clear();
}
::google::protobuf::uint64 ConvolutionLayerParams::outputshape(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConvolutionLayerParams.outputShape)
  return outputshape_.Get(index);
}
void ConvolutionLayerParams::set_outputshape(int index, ::google::protobuf::uint64 value) {
  outputshape_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConvolutionLayerParams.outputShape)
}
void ConvolutionLayerParams::add_outputshape(::google::protobuf::uint64 value) {
  outputshape_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ConvolutionLayerParams.outputShape)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
ConvolutionLayerParams::outputshape() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ConvolutionLayerParams.outputShape)
  return outputshape_;
}
::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
ConvolutionLayerParams::mutable_outputshape() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ConvolutionLayerParams.outputShape)
  return &outputshape_;
}

bool ConvolutionLayerParams::has_ConvolutionPaddingType() const {
  return ConvolutionPaddingType_case() != CONVOLUTIONPADDINGTYPE_NOT_SET;
}
void ConvolutionLayerParams::clear_has_ConvolutionPaddingType() {
  _oneof_case_[0] = CONVOLUTIONPADDINGTYPE_NOT_SET;
}
ConvolutionLayerParams::ConvolutionPaddingTypeCase ConvolutionLayerParams::ConvolutionPaddingType_case() const {
  return ConvolutionLayerParams::ConvolutionPaddingTypeCase(_oneof_case_[0]);
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int InnerProductLayerParams::kInputChannelsFieldNumber;
const int InnerProductLayerParams::kOutputChannelsFieldNumber;
const int InnerProductLayerParams::kHasBiasFieldNumber;
const int InnerProductLayerParams::kWeightsFieldNumber;
const int InnerProductLayerParams::kBiasFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

InnerProductLayerParams::InnerProductLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.InnerProductLayerParams)
}
InnerProductLayerParams::InnerProductLayerParams(const InnerProductLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_weights()) {
    weights_ = new ::CoreML::Specification::WeightParams(*from.weights_);
  } else {
    weights_ = NULL;
  }
  if (from.has_bias()) {
    bias_ = new ::CoreML::Specification::WeightParams(*from.bias_);
  } else {
    bias_ = NULL;
  }
  ::memcpy(&inputchannels_, &from.inputchannels_,
    reinterpret_cast<char*>(&hasbias_) -
    reinterpret_cast<char*>(&inputchannels_) + sizeof(hasbias_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.InnerProductLayerParams)
}

void InnerProductLayerParams::SharedCtor() {
  ::memset(&weights_, 0, reinterpret_cast<char*>(&hasbias_) -
    reinterpret_cast<char*>(&weights_) + sizeof(hasbias_));
  _cached_size_ = 0;
}

InnerProductLayerParams::~InnerProductLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.InnerProductLayerParams)
  SharedDtor();
}

void InnerProductLayerParams::SharedDtor() {
  if (this != internal_default_instance()) {
    delete weights_;
  }
  if (this != internal_default_instance()) {
    delete bias_;
  }
}

void InnerProductLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const InnerProductLayerParams& InnerProductLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

InnerProductLayerParams* InnerProductLayerParams::New(::google::protobuf::Arena* arena) const {
  InnerProductLayerParams* n = new InnerProductLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void InnerProductLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.InnerProductLayerParams)
  if (GetArenaNoVirtual() == NULL && weights_ != NULL) {
    delete weights_;
  }
  weights_ = NULL;
  if (GetArenaNoVirtual() == NULL && bias_ != NULL) {
    delete bias_;
  }
  bias_ = NULL;
  ::memset(&inputchannels_, 0, reinterpret_cast<char*>(&hasbias_) -
    reinterpret_cast<char*>(&inputchannels_) + sizeof(hasbias_));
}

bool InnerProductLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.InnerProductLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(16383u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // uint64 inputChannels = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &inputchannels_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // uint64 outputChannels = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(16u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &outputchannels_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool hasBias = 10;
      case 10: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(80u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &hasbias_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams weights = 20;
      case 20: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(162u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_weights()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams bias = 21;
      case 21: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(170u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_bias()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.InnerProductLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.InnerProductLayerParams)
  return false;
#undef DO_
}

void InnerProductLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.InnerProductLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // uint64 inputChannels = 1;
  if (this->inputchannels() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(1, this->inputchannels(), output);
  }

  // uint64 outputChannels = 2;
  if (this->outputchannels() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(2, this->outputchannels(), output);
  }

  // bool hasBias = 10;
  if (this->hasbias() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(10, this->hasbias(), output);
  }

  // .CoreML.Specification.WeightParams weights = 20;
  if (this->has_weights()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      20, *this->weights_, output);
  }

  // .CoreML.Specification.WeightParams bias = 21;
  if (this->has_bias()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      21, *this->bias_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.InnerProductLayerParams)
}

size_t InnerProductLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.InnerProductLayerParams)
  size_t total_size = 0;

  // .CoreML.Specification.WeightParams weights = 20;
  if (this->has_weights()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->weights_);
  }

  // .CoreML.Specification.WeightParams bias = 21;
  if (this->has_bias()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->bias_);
  }

  // uint64 inputChannels = 1;
  if (this->inputchannels() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->inputchannels());
  }

  // uint64 outputChannels = 2;
  if (this->outputchannels() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->outputchannels());
  }

  // bool hasBias = 10;
  if (this->hasbias() != 0) {
    total_size += 1 + 1;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void InnerProductLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const InnerProductLayerParams*>(&from));
}

void InnerProductLayerParams::MergeFrom(const InnerProductLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.InnerProductLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.has_weights()) {
    mutable_weights()->::CoreML::Specification::WeightParams::MergeFrom(from.weights());
  }
  if (from.has_bias()) {
    mutable_bias()->::CoreML::Specification::WeightParams::MergeFrom(from.bias());
  }
  if (from.inputchannels() != 0) {
    set_inputchannels(from.inputchannels());
  }
  if (from.outputchannels() != 0) {
    set_outputchannels(from.outputchannels());
  }
  if (from.hasbias() != 0) {
    set_hasbias(from.hasbias());
  }
}

void InnerProductLayerParams::CopyFrom(const InnerProductLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.InnerProductLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool InnerProductLayerParams::IsInitialized() const {
  return true;
}

void InnerProductLayerParams::Swap(InnerProductLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void InnerProductLayerParams::InternalSwap(InnerProductLayerParams* other) {
  std::swap(weights_, other->weights_);
  std::swap(bias_, other->bias_);
  std::swap(inputchannels_, other->inputchannels_);
  std::swap(outputchannels_, other->outputchannels_);
  std::swap(hasbias_, other->hasbias_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string InnerProductLayerParams::GetTypeName() const {
  return "CoreML.Specification.InnerProductLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// InnerProductLayerParams

// uint64 inputChannels = 1;
void InnerProductLayerParams::clear_inputchannels() {
  inputchannels_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 InnerProductLayerParams::inputchannels() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.InnerProductLayerParams.inputChannels)
  return inputchannels_;
}
void InnerProductLayerParams::set_inputchannels(::google::protobuf::uint64 value) {
  
  inputchannels_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.InnerProductLayerParams.inputChannels)
}

// uint64 outputChannels = 2;
void InnerProductLayerParams::clear_outputchannels() {
  outputchannels_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 InnerProductLayerParams::outputchannels() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.InnerProductLayerParams.outputChannels)
  return outputchannels_;
}
void InnerProductLayerParams::set_outputchannels(::google::protobuf::uint64 value) {
  
  outputchannels_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.InnerProductLayerParams.outputChannels)
}

// bool hasBias = 10;
void InnerProductLayerParams::clear_hasbias() {
  hasbias_ = false;
}
bool InnerProductLayerParams::hasbias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.InnerProductLayerParams.hasBias)
  return hasbias_;
}
void InnerProductLayerParams::set_hasbias(bool value) {
  
  hasbias_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.InnerProductLayerParams.hasBias)
}

// .CoreML.Specification.WeightParams weights = 20;
bool InnerProductLayerParams::has_weights() const {
  return this != internal_default_instance() && weights_ != NULL;
}
void InnerProductLayerParams::clear_weights() {
  if (GetArenaNoVirtual() == NULL && weights_ != NULL) delete weights_;
  weights_ = NULL;
}
const ::CoreML::Specification::WeightParams& InnerProductLayerParams::weights() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.InnerProductLayerParams.weights)
  return weights_ != NULL ? *weights_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* InnerProductLayerParams::mutable_weights() {
  
  if (weights_ == NULL) {
    weights_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.InnerProductLayerParams.weights)
  return weights_;
}
::CoreML::Specification::WeightParams* InnerProductLayerParams::release_weights() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.InnerProductLayerParams.weights)
  
  ::CoreML::Specification::WeightParams* temp = weights_;
  weights_ = NULL;
  return temp;
}
void InnerProductLayerParams::set_allocated_weights(::CoreML::Specification::WeightParams* weights) {
  delete weights_;
  weights_ = weights;
  if (weights) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.InnerProductLayerParams.weights)
}

// .CoreML.Specification.WeightParams bias = 21;
bool InnerProductLayerParams::has_bias() const {
  return this != internal_default_instance() && bias_ != NULL;
}
void InnerProductLayerParams::clear_bias() {
  if (GetArenaNoVirtual() == NULL && bias_ != NULL) delete bias_;
  bias_ = NULL;
}
const ::CoreML::Specification::WeightParams& InnerProductLayerParams::bias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.InnerProductLayerParams.bias)
  return bias_ != NULL ? *bias_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* InnerProductLayerParams::mutable_bias() {
  
  if (bias_ == NULL) {
    bias_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.InnerProductLayerParams.bias)
  return bias_;
}
::CoreML::Specification::WeightParams* InnerProductLayerParams::release_bias() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.InnerProductLayerParams.bias)
  
  ::CoreML::Specification::WeightParams* temp = bias_;
  bias_ = NULL;
  return temp;
}
void InnerProductLayerParams::set_allocated_bias(::CoreML::Specification::WeightParams* bias) {
  delete bias_;
  bias_ = bias;
  if (bias) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.InnerProductLayerParams.bias)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int EmbeddingLayerParams::kInputDimFieldNumber;
const int EmbeddingLayerParams::kOutputChannelsFieldNumber;
const int EmbeddingLayerParams::kHasBiasFieldNumber;
const int EmbeddingLayerParams::kWeightsFieldNumber;
const int EmbeddingLayerParams::kBiasFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

EmbeddingLayerParams::EmbeddingLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.EmbeddingLayerParams)
}
EmbeddingLayerParams::EmbeddingLayerParams(const EmbeddingLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_weights()) {
    weights_ = new ::CoreML::Specification::WeightParams(*from.weights_);
  } else {
    weights_ = NULL;
  }
  if (from.has_bias()) {
    bias_ = new ::CoreML::Specification::WeightParams(*from.bias_);
  } else {
    bias_ = NULL;
  }
  ::memcpy(&inputdim_, &from.inputdim_,
    reinterpret_cast<char*>(&hasbias_) -
    reinterpret_cast<char*>(&inputdim_) + sizeof(hasbias_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.EmbeddingLayerParams)
}

void EmbeddingLayerParams::SharedCtor() {
  ::memset(&weights_, 0, reinterpret_cast<char*>(&hasbias_) -
    reinterpret_cast<char*>(&weights_) + sizeof(hasbias_));
  _cached_size_ = 0;
}

EmbeddingLayerParams::~EmbeddingLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.EmbeddingLayerParams)
  SharedDtor();
}

void EmbeddingLayerParams::SharedDtor() {
  if (this != internal_default_instance()) {
    delete weights_;
  }
  if (this != internal_default_instance()) {
    delete bias_;
  }
}

void EmbeddingLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const EmbeddingLayerParams& EmbeddingLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

EmbeddingLayerParams* EmbeddingLayerParams::New(::google::protobuf::Arena* arena) const {
  EmbeddingLayerParams* n = new EmbeddingLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void EmbeddingLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.EmbeddingLayerParams)
  if (GetArenaNoVirtual() == NULL && weights_ != NULL) {
    delete weights_;
  }
  weights_ = NULL;
  if (GetArenaNoVirtual() == NULL && bias_ != NULL) {
    delete bias_;
  }
  bias_ = NULL;
  ::memset(&inputdim_, 0, reinterpret_cast<char*>(&hasbias_) -
    reinterpret_cast<char*>(&inputdim_) + sizeof(hasbias_));
}

bool EmbeddingLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.EmbeddingLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(16383u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // uint64 inputDim = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &inputdim_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // uint64 outputChannels = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(16u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &outputchannels_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool hasBias = 10;
      case 10: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(80u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &hasbias_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams weights = 20;
      case 20: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(162u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_weights()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams bias = 21;
      case 21: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(170u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_bias()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.EmbeddingLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.EmbeddingLayerParams)
  return false;
#undef DO_
}

void EmbeddingLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.EmbeddingLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // uint64 inputDim = 1;
  if (this->inputdim() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(1, this->inputdim(), output);
  }

  // uint64 outputChannels = 2;
  if (this->outputchannels() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(2, this->outputchannels(), output);
  }

  // bool hasBias = 10;
  if (this->hasbias() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(10, this->hasbias(), output);
  }

  // .CoreML.Specification.WeightParams weights = 20;
  if (this->has_weights()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      20, *this->weights_, output);
  }

  // .CoreML.Specification.WeightParams bias = 21;
  if (this->has_bias()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      21, *this->bias_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.EmbeddingLayerParams)
}

size_t EmbeddingLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.EmbeddingLayerParams)
  size_t total_size = 0;

  // .CoreML.Specification.WeightParams weights = 20;
  if (this->has_weights()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->weights_);
  }

  // .CoreML.Specification.WeightParams bias = 21;
  if (this->has_bias()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->bias_);
  }

  // uint64 inputDim = 1;
  if (this->inputdim() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->inputdim());
  }

  // uint64 outputChannels = 2;
  if (this->outputchannels() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->outputchannels());
  }

  // bool hasBias = 10;
  if (this->hasbias() != 0) {
    total_size += 1 + 1;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void EmbeddingLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const EmbeddingLayerParams*>(&from));
}

void EmbeddingLayerParams::MergeFrom(const EmbeddingLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.EmbeddingLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.has_weights()) {
    mutable_weights()->::CoreML::Specification::WeightParams::MergeFrom(from.weights());
  }
  if (from.has_bias()) {
    mutable_bias()->::CoreML::Specification::WeightParams::MergeFrom(from.bias());
  }
  if (from.inputdim() != 0) {
    set_inputdim(from.inputdim());
  }
  if (from.outputchannels() != 0) {
    set_outputchannels(from.outputchannels());
  }
  if (from.hasbias() != 0) {
    set_hasbias(from.hasbias());
  }
}

void EmbeddingLayerParams::CopyFrom(const EmbeddingLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.EmbeddingLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool EmbeddingLayerParams::IsInitialized() const {
  return true;
}

void EmbeddingLayerParams::Swap(EmbeddingLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void EmbeddingLayerParams::InternalSwap(EmbeddingLayerParams* other) {
  std::swap(weights_, other->weights_);
  std::swap(bias_, other->bias_);
  std::swap(inputdim_, other->inputdim_);
  std::swap(outputchannels_, other->outputchannels_);
  std::swap(hasbias_, other->hasbias_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string EmbeddingLayerParams::GetTypeName() const {
  return "CoreML.Specification.EmbeddingLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// EmbeddingLayerParams

// uint64 inputDim = 1;
void EmbeddingLayerParams::clear_inputdim() {
  inputdim_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 EmbeddingLayerParams::inputdim() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.EmbeddingLayerParams.inputDim)
  return inputdim_;
}
void EmbeddingLayerParams::set_inputdim(::google::protobuf::uint64 value) {
  
  inputdim_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.EmbeddingLayerParams.inputDim)
}

// uint64 outputChannels = 2;
void EmbeddingLayerParams::clear_outputchannels() {
  outputchannels_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 EmbeddingLayerParams::outputchannels() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.EmbeddingLayerParams.outputChannels)
  return outputchannels_;
}
void EmbeddingLayerParams::set_outputchannels(::google::protobuf::uint64 value) {
  
  outputchannels_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.EmbeddingLayerParams.outputChannels)
}

// bool hasBias = 10;
void EmbeddingLayerParams::clear_hasbias() {
  hasbias_ = false;
}
bool EmbeddingLayerParams::hasbias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.EmbeddingLayerParams.hasBias)
  return hasbias_;
}
void EmbeddingLayerParams::set_hasbias(bool value) {
  
  hasbias_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.EmbeddingLayerParams.hasBias)
}

// .CoreML.Specification.WeightParams weights = 20;
bool EmbeddingLayerParams::has_weights() const {
  return this != internal_default_instance() && weights_ != NULL;
}
void EmbeddingLayerParams::clear_weights() {
  if (GetArenaNoVirtual() == NULL && weights_ != NULL) delete weights_;
  weights_ = NULL;
}
const ::CoreML::Specification::WeightParams& EmbeddingLayerParams::weights() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.EmbeddingLayerParams.weights)
  return weights_ != NULL ? *weights_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* EmbeddingLayerParams::mutable_weights() {
  
  if (weights_ == NULL) {
    weights_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.EmbeddingLayerParams.weights)
  return weights_;
}
::CoreML::Specification::WeightParams* EmbeddingLayerParams::release_weights() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.EmbeddingLayerParams.weights)
  
  ::CoreML::Specification::WeightParams* temp = weights_;
  weights_ = NULL;
  return temp;
}
void EmbeddingLayerParams::set_allocated_weights(::CoreML::Specification::WeightParams* weights) {
  delete weights_;
  weights_ = weights;
  if (weights) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.EmbeddingLayerParams.weights)
}

// .CoreML.Specification.WeightParams bias = 21;
bool EmbeddingLayerParams::has_bias() const {
  return this != internal_default_instance() && bias_ != NULL;
}
void EmbeddingLayerParams::clear_bias() {
  if (GetArenaNoVirtual() == NULL && bias_ != NULL) delete bias_;
  bias_ = NULL;
}
const ::CoreML::Specification::WeightParams& EmbeddingLayerParams::bias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.EmbeddingLayerParams.bias)
  return bias_ != NULL ? *bias_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* EmbeddingLayerParams::mutable_bias() {
  
  if (bias_ == NULL) {
    bias_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.EmbeddingLayerParams.bias)
  return bias_;
}
::CoreML::Specification::WeightParams* EmbeddingLayerParams::release_bias() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.EmbeddingLayerParams.bias)
  
  ::CoreML::Specification::WeightParams* temp = bias_;
  bias_ = NULL;
  return temp;
}
void EmbeddingLayerParams::set_allocated_bias(::CoreML::Specification::WeightParams* bias) {
  delete bias_;
  bias_ = bias;
  if (bias) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.EmbeddingLayerParams.bias)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int EmbeddingNDLayerParams::kVocabSizeFieldNumber;
const int EmbeddingNDLayerParams::kEmbeddingSizeFieldNumber;
const int EmbeddingNDLayerParams::kHasBiasFieldNumber;
const int EmbeddingNDLayerParams::kWeightsFieldNumber;
const int EmbeddingNDLayerParams::kBiasFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

EmbeddingNDLayerParams::EmbeddingNDLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.EmbeddingNDLayerParams)
}
EmbeddingNDLayerParams::EmbeddingNDLayerParams(const EmbeddingNDLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_weights()) {
    weights_ = new ::CoreML::Specification::WeightParams(*from.weights_);
  } else {
    weights_ = NULL;
  }
  if (from.has_bias()) {
    bias_ = new ::CoreML::Specification::WeightParams(*from.bias_);
  } else {
    bias_ = NULL;
  }
  ::memcpy(&vocabsize_, &from.vocabsize_,
    reinterpret_cast<char*>(&hasbias_) -
    reinterpret_cast<char*>(&vocabsize_) + sizeof(hasbias_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.EmbeddingNDLayerParams)
}

void EmbeddingNDLayerParams::SharedCtor() {
  ::memset(&weights_, 0, reinterpret_cast<char*>(&hasbias_) -
    reinterpret_cast<char*>(&weights_) + sizeof(hasbias_));
  _cached_size_ = 0;
}

EmbeddingNDLayerParams::~EmbeddingNDLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.EmbeddingNDLayerParams)
  SharedDtor();
}

void EmbeddingNDLayerParams::SharedDtor() {
  if (this != internal_default_instance()) {
    delete weights_;
  }
  if (this != internal_default_instance()) {
    delete bias_;
  }
}

void EmbeddingNDLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const EmbeddingNDLayerParams& EmbeddingNDLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

EmbeddingNDLayerParams* EmbeddingNDLayerParams::New(::google::protobuf::Arena* arena) const {
  EmbeddingNDLayerParams* n = new EmbeddingNDLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void EmbeddingNDLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.EmbeddingNDLayerParams)
  if (GetArenaNoVirtual() == NULL && weights_ != NULL) {
    delete weights_;
  }
  weights_ = NULL;
  if (GetArenaNoVirtual() == NULL && bias_ != NULL) {
    delete bias_;
  }
  bias_ = NULL;
  ::memset(&vocabsize_, 0, reinterpret_cast<char*>(&hasbias_) -
    reinterpret_cast<char*>(&vocabsize_) + sizeof(hasbias_));
}

bool EmbeddingNDLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.EmbeddingNDLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(16383u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // uint64 vocabSize = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &vocabsize_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // uint64 embeddingSize = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(16u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &embeddingsize_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool hasBias = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(24u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &hasbias_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams weights = 20;
      case 20: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(162u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_weights()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams bias = 21;
      case 21: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(170u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_bias()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.EmbeddingNDLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.EmbeddingNDLayerParams)
  return false;
#undef DO_
}

void EmbeddingNDLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.EmbeddingNDLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // uint64 vocabSize = 1;
  if (this->vocabsize() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(1, this->vocabsize(), output);
  }

  // uint64 embeddingSize = 2;
  if (this->embeddingsize() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(2, this->embeddingsize(), output);
  }

  // bool hasBias = 3;
  if (this->hasbias() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(3, this->hasbias(), output);
  }

  // .CoreML.Specification.WeightParams weights = 20;
  if (this->has_weights()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      20, *this->weights_, output);
  }

  // .CoreML.Specification.WeightParams bias = 21;
  if (this->has_bias()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      21, *this->bias_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.EmbeddingNDLayerParams)
}

size_t EmbeddingNDLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.EmbeddingNDLayerParams)
  size_t total_size = 0;

  // .CoreML.Specification.WeightParams weights = 20;
  if (this->has_weights()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->weights_);
  }

  // .CoreML.Specification.WeightParams bias = 21;
  if (this->has_bias()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->bias_);
  }

  // uint64 vocabSize = 1;
  if (this->vocabsize() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->vocabsize());
  }

  // uint64 embeddingSize = 2;
  if (this->embeddingsize() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->embeddingsize());
  }

  // bool hasBias = 3;
  if (this->hasbias() != 0) {
    total_size += 1 + 1;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void EmbeddingNDLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const EmbeddingNDLayerParams*>(&from));
}

void EmbeddingNDLayerParams::MergeFrom(const EmbeddingNDLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.EmbeddingNDLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.has_weights()) {
    mutable_weights()->::CoreML::Specification::WeightParams::MergeFrom(from.weights());
  }
  if (from.has_bias()) {
    mutable_bias()->::CoreML::Specification::WeightParams::MergeFrom(from.bias());
  }
  if (from.vocabsize() != 0) {
    set_vocabsize(from.vocabsize());
  }
  if (from.embeddingsize() != 0) {
    set_embeddingsize(from.embeddingsize());
  }
  if (from.hasbias() != 0) {
    set_hasbias(from.hasbias());
  }
}

void EmbeddingNDLayerParams::CopyFrom(const EmbeddingNDLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.EmbeddingNDLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool EmbeddingNDLayerParams::IsInitialized() const {
  return true;
}

void EmbeddingNDLayerParams::Swap(EmbeddingNDLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void EmbeddingNDLayerParams::InternalSwap(EmbeddingNDLayerParams* other) {
  std::swap(weights_, other->weights_);
  std::swap(bias_, other->bias_);
  std::swap(vocabsize_, other->vocabsize_);
  std::swap(embeddingsize_, other->embeddingsize_);
  std::swap(hasbias_, other->hasbias_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string EmbeddingNDLayerParams::GetTypeName() const {
  return "CoreML.Specification.EmbeddingNDLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// EmbeddingNDLayerParams

// uint64 vocabSize = 1;
void EmbeddingNDLayerParams::clear_vocabsize() {
  vocabsize_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 EmbeddingNDLayerParams::vocabsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.EmbeddingNDLayerParams.vocabSize)
  return vocabsize_;
}
void EmbeddingNDLayerParams::set_vocabsize(::google::protobuf::uint64 value) {
  
  vocabsize_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.EmbeddingNDLayerParams.vocabSize)
}

// uint64 embeddingSize = 2;
void EmbeddingNDLayerParams::clear_embeddingsize() {
  embeddingsize_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 EmbeddingNDLayerParams::embeddingsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.EmbeddingNDLayerParams.embeddingSize)
  return embeddingsize_;
}
void EmbeddingNDLayerParams::set_embeddingsize(::google::protobuf::uint64 value) {
  
  embeddingsize_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.EmbeddingNDLayerParams.embeddingSize)
}

// bool hasBias = 3;
void EmbeddingNDLayerParams::clear_hasbias() {
  hasbias_ = false;
}
bool EmbeddingNDLayerParams::hasbias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.EmbeddingNDLayerParams.hasBias)
  return hasbias_;
}
void EmbeddingNDLayerParams::set_hasbias(bool value) {
  
  hasbias_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.EmbeddingNDLayerParams.hasBias)
}

// .CoreML.Specification.WeightParams weights = 20;
bool EmbeddingNDLayerParams::has_weights() const {
  return this != internal_default_instance() && weights_ != NULL;
}
void EmbeddingNDLayerParams::clear_weights() {
  if (GetArenaNoVirtual() == NULL && weights_ != NULL) delete weights_;
  weights_ = NULL;
}
const ::CoreML::Specification::WeightParams& EmbeddingNDLayerParams::weights() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.EmbeddingNDLayerParams.weights)
  return weights_ != NULL ? *weights_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* EmbeddingNDLayerParams::mutable_weights() {
  
  if (weights_ == NULL) {
    weights_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.EmbeddingNDLayerParams.weights)
  return weights_;
}
::CoreML::Specification::WeightParams* EmbeddingNDLayerParams::release_weights() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.EmbeddingNDLayerParams.weights)
  
  ::CoreML::Specification::WeightParams* temp = weights_;
  weights_ = NULL;
  return temp;
}
void EmbeddingNDLayerParams::set_allocated_weights(::CoreML::Specification::WeightParams* weights) {
  delete weights_;
  weights_ = weights;
  if (weights) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.EmbeddingNDLayerParams.weights)
}

// .CoreML.Specification.WeightParams bias = 21;
bool EmbeddingNDLayerParams::has_bias() const {
  return this != internal_default_instance() && bias_ != NULL;
}
void EmbeddingNDLayerParams::clear_bias() {
  if (GetArenaNoVirtual() == NULL && bias_ != NULL) delete bias_;
  bias_ = NULL;
}
const ::CoreML::Specification::WeightParams& EmbeddingNDLayerParams::bias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.EmbeddingNDLayerParams.bias)
  return bias_ != NULL ? *bias_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* EmbeddingNDLayerParams::mutable_bias() {
  
  if (bias_ == NULL) {
    bias_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.EmbeddingNDLayerParams.bias)
  return bias_;
}
::CoreML::Specification::WeightParams* EmbeddingNDLayerParams::release_bias() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.EmbeddingNDLayerParams.bias)
  
  ::CoreML::Specification::WeightParams* temp = bias_;
  bias_ = NULL;
  return temp;
}
void EmbeddingNDLayerParams::set_allocated_bias(::CoreML::Specification::WeightParams* bias) {
  delete bias_;
  bias_ = bias;
  if (bias) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.EmbeddingNDLayerParams.bias)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int BatchnormLayerParams::kChannelsFieldNumber;
const int BatchnormLayerParams::kComputeMeanVarFieldNumber;
const int BatchnormLayerParams::kInstanceNormalizationFieldNumber;
const int BatchnormLayerParams::kEpsilonFieldNumber;
const int BatchnormLayerParams::kGammaFieldNumber;
const int BatchnormLayerParams::kBetaFieldNumber;
const int BatchnormLayerParams::kMeanFieldNumber;
const int BatchnormLayerParams::kVarianceFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

BatchnormLayerParams::BatchnormLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.BatchnormLayerParams)
}
BatchnormLayerParams::BatchnormLayerParams(const BatchnormLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_gamma()) {
    gamma_ = new ::CoreML::Specification::WeightParams(*from.gamma_);
  } else {
    gamma_ = NULL;
  }
  if (from.has_beta()) {
    beta_ = new ::CoreML::Specification::WeightParams(*from.beta_);
  } else {
    beta_ = NULL;
  }
  if (from.has_mean()) {
    mean_ = new ::CoreML::Specification::WeightParams(*from.mean_);
  } else {
    mean_ = NULL;
  }
  if (from.has_variance()) {
    variance_ = new ::CoreML::Specification::WeightParams(*from.variance_);
  } else {
    variance_ = NULL;
  }
  ::memcpy(&channels_, &from.channels_,
    reinterpret_cast<char*>(&epsilon_) -
    reinterpret_cast<char*>(&channels_) + sizeof(epsilon_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.BatchnormLayerParams)
}

void BatchnormLayerParams::SharedCtor() {
  ::memset(&gamma_, 0, reinterpret_cast<char*>(&epsilon_) -
    reinterpret_cast<char*>(&gamma_) + sizeof(epsilon_));
  _cached_size_ = 0;
}

BatchnormLayerParams::~BatchnormLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.BatchnormLayerParams)
  SharedDtor();
}

void BatchnormLayerParams::SharedDtor() {
  if (this != internal_default_instance()) {
    delete gamma_;
  }
  if (this != internal_default_instance()) {
    delete beta_;
  }
  if (this != internal_default_instance()) {
    delete mean_;
  }
  if (this != internal_default_instance()) {
    delete variance_;
  }
}

void BatchnormLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const BatchnormLayerParams& BatchnormLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

BatchnormLayerParams* BatchnormLayerParams::New(::google::protobuf::Arena* arena) const {
  BatchnormLayerParams* n = new BatchnormLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void BatchnormLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.BatchnormLayerParams)
  if (GetArenaNoVirtual() == NULL && gamma_ != NULL) {
    delete gamma_;
  }
  gamma_ = NULL;
  if (GetArenaNoVirtual() == NULL && beta_ != NULL) {
    delete beta_;
  }
  beta_ = NULL;
  if (GetArenaNoVirtual() == NULL && mean_ != NULL) {
    delete mean_;
  }
  mean_ = NULL;
  if (GetArenaNoVirtual() == NULL && variance_ != NULL) {
    delete variance_;
  }
  variance_ = NULL;
  ::memset(&channels_, 0, reinterpret_cast<char*>(&epsilon_) -
    reinterpret_cast<char*>(&channels_) + sizeof(epsilon_));
}

bool BatchnormLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.BatchnormLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(16383u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // uint64 channels = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &channels_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool computeMeanVar = 5;
      case 5: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(40u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &computemeanvar_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool instanceNormalization = 6;
      case 6: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(48u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &instancenormalization_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // float epsilon = 10;
      case 10: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(85u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &epsilon_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams gamma = 15;
      case 15: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(122u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_gamma()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams beta = 16;
      case 16: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(130u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_beta()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams mean = 17;
      case 17: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(138u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_mean()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams variance = 18;
      case 18: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(146u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_variance()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.BatchnormLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.BatchnormLayerParams)
  return false;
#undef DO_
}

void BatchnormLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.BatchnormLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // uint64 channels = 1;
  if (this->channels() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(1, this->channels(), output);
  }

  // bool computeMeanVar = 5;
  if (this->computemeanvar() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(5, this->computemeanvar(), output);
  }

  // bool instanceNormalization = 6;
  if (this->instancenormalization() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(6, this->instancenormalization(), output);
  }

  // float epsilon = 10;
  if (this->epsilon() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(10, this->epsilon(), output);
  }

  // .CoreML.Specification.WeightParams gamma = 15;
  if (this->has_gamma()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      15, *this->gamma_, output);
  }

  // .CoreML.Specification.WeightParams beta = 16;
  if (this->has_beta()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      16, *this->beta_, output);
  }

  // .CoreML.Specification.WeightParams mean = 17;
  if (this->has_mean()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      17, *this->mean_, output);
  }

  // .CoreML.Specification.WeightParams variance = 18;
  if (this->has_variance()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      18, *this->variance_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.BatchnormLayerParams)
}

size_t BatchnormLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.BatchnormLayerParams)
  size_t total_size = 0;

  // .CoreML.Specification.WeightParams gamma = 15;
  if (this->has_gamma()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->gamma_);
  }

  // .CoreML.Specification.WeightParams beta = 16;
  if (this->has_beta()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->beta_);
  }

  // .CoreML.Specification.WeightParams mean = 17;
  if (this->has_mean()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->mean_);
  }

  // .CoreML.Specification.WeightParams variance = 18;
  if (this->has_variance()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->variance_);
  }

  // uint64 channels = 1;
  if (this->channels() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->channels());
  }

  // bool computeMeanVar = 5;
  if (this->computemeanvar() != 0) {
    total_size += 1 + 1;
  }

  // bool instanceNormalization = 6;
  if (this->instancenormalization() != 0) {
    total_size += 1 + 1;
  }

  // float epsilon = 10;
  if (this->epsilon() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void BatchnormLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const BatchnormLayerParams*>(&from));
}

void BatchnormLayerParams::MergeFrom(const BatchnormLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.BatchnormLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.has_gamma()) {
    mutable_gamma()->::CoreML::Specification::WeightParams::MergeFrom(from.gamma());
  }
  if (from.has_beta()) {
    mutable_beta()->::CoreML::Specification::WeightParams::MergeFrom(from.beta());
  }
  if (from.has_mean()) {
    mutable_mean()->::CoreML::Specification::WeightParams::MergeFrom(from.mean());
  }
  if (from.has_variance()) {
    mutable_variance()->::CoreML::Specification::WeightParams::MergeFrom(from.variance());
  }
  if (from.channels() != 0) {
    set_channels(from.channels());
  }
  if (from.computemeanvar() != 0) {
    set_computemeanvar(from.computemeanvar());
  }
  if (from.instancenormalization() != 0) {
    set_instancenormalization(from.instancenormalization());
  }
  if (from.epsilon() != 0) {
    set_epsilon(from.epsilon());
  }
}

void BatchnormLayerParams::CopyFrom(const BatchnormLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.BatchnormLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool BatchnormLayerParams::IsInitialized() const {
  return true;
}

void BatchnormLayerParams::Swap(BatchnormLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void BatchnormLayerParams::InternalSwap(BatchnormLayerParams* other) {
  std::swap(gamma_, other->gamma_);
  std::swap(beta_, other->beta_);
  std::swap(mean_, other->mean_);
  std::swap(variance_, other->variance_);
  std::swap(channels_, other->channels_);
  std::swap(computemeanvar_, other->computemeanvar_);
  std::swap(instancenormalization_, other->instancenormalization_);
  std::swap(epsilon_, other->epsilon_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string BatchnormLayerParams::GetTypeName() const {
  return "CoreML.Specification.BatchnormLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// BatchnormLayerParams

// uint64 channels = 1;
void BatchnormLayerParams::clear_channels() {
  channels_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 BatchnormLayerParams::channels() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchnormLayerParams.channels)
  return channels_;
}
void BatchnormLayerParams::set_channels(::google::protobuf::uint64 value) {
  
  channels_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.BatchnormLayerParams.channels)
}

// bool computeMeanVar = 5;
void BatchnormLayerParams::clear_computemeanvar() {
  computemeanvar_ = false;
}
bool BatchnormLayerParams::computemeanvar() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchnormLayerParams.computeMeanVar)
  return computemeanvar_;
}
void BatchnormLayerParams::set_computemeanvar(bool value) {
  
  computemeanvar_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.BatchnormLayerParams.computeMeanVar)
}

// bool instanceNormalization = 6;
void BatchnormLayerParams::clear_instancenormalization() {
  instancenormalization_ = false;
}
bool BatchnormLayerParams::instancenormalization() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchnormLayerParams.instanceNormalization)
  return instancenormalization_;
}
void BatchnormLayerParams::set_instancenormalization(bool value) {
  
  instancenormalization_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.BatchnormLayerParams.instanceNormalization)
}

// float epsilon = 10;
void BatchnormLayerParams::clear_epsilon() {
  epsilon_ = 0;
}
float BatchnormLayerParams::epsilon() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchnormLayerParams.epsilon)
  return epsilon_;
}
void BatchnormLayerParams::set_epsilon(float value) {
  
  epsilon_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.BatchnormLayerParams.epsilon)
}

// .CoreML.Specification.WeightParams gamma = 15;
bool BatchnormLayerParams::has_gamma() const {
  return this != internal_default_instance() && gamma_ != NULL;
}
void BatchnormLayerParams::clear_gamma() {
  if (GetArenaNoVirtual() == NULL && gamma_ != NULL) delete gamma_;
  gamma_ = NULL;
}
const ::CoreML::Specification::WeightParams& BatchnormLayerParams::gamma() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchnormLayerParams.gamma)
  return gamma_ != NULL ? *gamma_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* BatchnormLayerParams::mutable_gamma() {
  
  if (gamma_ == NULL) {
    gamma_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BatchnormLayerParams.gamma)
  return gamma_;
}
::CoreML::Specification::WeightParams* BatchnormLayerParams::release_gamma() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.BatchnormLayerParams.gamma)
  
  ::CoreML::Specification::WeightParams* temp = gamma_;
  gamma_ = NULL;
  return temp;
}
void BatchnormLayerParams::set_allocated_gamma(::CoreML::Specification::WeightParams* gamma) {
  delete gamma_;
  gamma_ = gamma;
  if (gamma) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.BatchnormLayerParams.gamma)
}

// .CoreML.Specification.WeightParams beta = 16;
bool BatchnormLayerParams::has_beta() const {
  return this != internal_default_instance() && beta_ != NULL;
}
void BatchnormLayerParams::clear_beta() {
  if (GetArenaNoVirtual() == NULL && beta_ != NULL) delete beta_;
  beta_ = NULL;
}
const ::CoreML::Specification::WeightParams& BatchnormLayerParams::beta() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchnormLayerParams.beta)
  return beta_ != NULL ? *beta_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* BatchnormLayerParams::mutable_beta() {
  
  if (beta_ == NULL) {
    beta_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BatchnormLayerParams.beta)
  return beta_;
}
::CoreML::Specification::WeightParams* BatchnormLayerParams::release_beta() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.BatchnormLayerParams.beta)
  
  ::CoreML::Specification::WeightParams* temp = beta_;
  beta_ = NULL;
  return temp;
}
void BatchnormLayerParams::set_allocated_beta(::CoreML::Specification::WeightParams* beta) {
  delete beta_;
  beta_ = beta;
  if (beta) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.BatchnormLayerParams.beta)
}

// .CoreML.Specification.WeightParams mean = 17;
bool BatchnormLayerParams::has_mean() const {
  return this != internal_default_instance() && mean_ != NULL;
}
void BatchnormLayerParams::clear_mean() {
  if (GetArenaNoVirtual() == NULL && mean_ != NULL) delete mean_;
  mean_ = NULL;
}
const ::CoreML::Specification::WeightParams& BatchnormLayerParams::mean() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchnormLayerParams.mean)
  return mean_ != NULL ? *mean_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* BatchnormLayerParams::mutable_mean() {
  
  if (mean_ == NULL) {
    mean_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BatchnormLayerParams.mean)
  return mean_;
}
::CoreML::Specification::WeightParams* BatchnormLayerParams::release_mean() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.BatchnormLayerParams.mean)
  
  ::CoreML::Specification::WeightParams* temp = mean_;
  mean_ = NULL;
  return temp;
}
void BatchnormLayerParams::set_allocated_mean(::CoreML::Specification::WeightParams* mean) {
  delete mean_;
  mean_ = mean;
  if (mean) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.BatchnormLayerParams.mean)
}

// .CoreML.Specification.WeightParams variance = 18;
bool BatchnormLayerParams::has_variance() const {
  return this != internal_default_instance() && variance_ != NULL;
}
void BatchnormLayerParams::clear_variance() {
  if (GetArenaNoVirtual() == NULL && variance_ != NULL) delete variance_;
  variance_ = NULL;
}
const ::CoreML::Specification::WeightParams& BatchnormLayerParams::variance() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchnormLayerParams.variance)
  return variance_ != NULL ? *variance_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* BatchnormLayerParams::mutable_variance() {
  
  if (variance_ == NULL) {
    variance_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BatchnormLayerParams.variance)
  return variance_;
}
::CoreML::Specification::WeightParams* BatchnormLayerParams::release_variance() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.BatchnormLayerParams.variance)
  
  ::CoreML::Specification::WeightParams* temp = variance_;
  variance_ = NULL;
  return temp;
}
void BatchnormLayerParams::set_allocated_variance(::CoreML::Specification::WeightParams* variance) {
  delete variance_;
  variance_ = variance;
  if (variance) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.BatchnormLayerParams.variance)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int PoolingLayerParams_ValidCompletePadding::kPaddingAmountsFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

PoolingLayerParams_ValidCompletePadding::PoolingLayerParams_ValidCompletePadding()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.PoolingLayerParams.ValidCompletePadding)
}
PoolingLayerParams_ValidCompletePadding::PoolingLayerParams_ValidCompletePadding(const PoolingLayerParams_ValidCompletePadding& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      paddingamounts_(from.paddingamounts_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.PoolingLayerParams.ValidCompletePadding)
}

void PoolingLayerParams_ValidCompletePadding::SharedCtor() {
  _cached_size_ = 0;
}

PoolingLayerParams_ValidCompletePadding::~PoolingLayerParams_ValidCompletePadding() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.PoolingLayerParams.ValidCompletePadding)
  SharedDtor();
}

void PoolingLayerParams_ValidCompletePadding::SharedDtor() {
}

void PoolingLayerParams_ValidCompletePadding::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const PoolingLayerParams_ValidCompletePadding& PoolingLayerParams_ValidCompletePadding::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

PoolingLayerParams_ValidCompletePadding* PoolingLayerParams_ValidCompletePadding::New(::google::protobuf::Arena* arena) const {
  PoolingLayerParams_ValidCompletePadding* n = new PoolingLayerParams_ValidCompletePadding;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void PoolingLayerParams_ValidCompletePadding::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.PoolingLayerParams.ValidCompletePadding)
  paddingamounts_.Clear();
}

bool PoolingLayerParams_ValidCompletePadding::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.PoolingLayerParams.ValidCompletePadding)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated uint64 paddingAmounts = 10;
      case 10: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(82u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, this->mutable_paddingamounts())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(80u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 1, 82u, input, this->mutable_paddingamounts())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.PoolingLayerParams.ValidCompletePadding)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.PoolingLayerParams.ValidCompletePadding)
  return false;
#undef DO_
}

void PoolingLayerParams_ValidCompletePadding::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.PoolingLayerParams.ValidCompletePadding)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated uint64 paddingAmounts = 10;
  if (this->paddingamounts_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(10, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_paddingamounts_cached_byte_size_);
  }
  for (int i = 0, n = this->paddingamounts_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64NoTag(
      this->paddingamounts(i), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.PoolingLayerParams.ValidCompletePadding)
}

size_t PoolingLayerParams_ValidCompletePadding::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.PoolingLayerParams.ValidCompletePadding)
  size_t total_size = 0;

  // repeated uint64 paddingAmounts = 10;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      UInt64Size(this->paddingamounts_);
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _paddingamounts_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void PoolingLayerParams_ValidCompletePadding::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const PoolingLayerParams_ValidCompletePadding*>(&from));
}

void PoolingLayerParams_ValidCompletePadding::MergeFrom(const PoolingLayerParams_ValidCompletePadding& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.PoolingLayerParams.ValidCompletePadding)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  paddingamounts_.MergeFrom(from.paddingamounts_);
}

void PoolingLayerParams_ValidCompletePadding::CopyFrom(const PoolingLayerParams_ValidCompletePadding& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.PoolingLayerParams.ValidCompletePadding)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool PoolingLayerParams_ValidCompletePadding::IsInitialized() const {
  return true;
}

void PoolingLayerParams_ValidCompletePadding::Swap(PoolingLayerParams_ValidCompletePadding* other) {
  if (other == this) return;
  InternalSwap(other);
}
void PoolingLayerParams_ValidCompletePadding::InternalSwap(PoolingLayerParams_ValidCompletePadding* other) {
  paddingamounts_.InternalSwap(&other->paddingamounts_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string PoolingLayerParams_ValidCompletePadding::GetTypeName() const {
  return "CoreML.Specification.PoolingLayerParams.ValidCompletePadding";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// PoolingLayerParams_ValidCompletePadding

// repeated uint64 paddingAmounts = 10;
int PoolingLayerParams_ValidCompletePadding::paddingamounts_size() const {
  return paddingamounts_.size();
}
void PoolingLayerParams_ValidCompletePadding::clear_paddingamounts() {
  paddingamounts_.Clear();
}
::google::protobuf::uint64 PoolingLayerParams_ValidCompletePadding::paddingamounts(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PoolingLayerParams.ValidCompletePadding.paddingAmounts)
  return paddingamounts_.Get(index);
}
void PoolingLayerParams_ValidCompletePadding::set_paddingamounts(int index, ::google::protobuf::uint64 value) {
  paddingamounts_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.PoolingLayerParams.ValidCompletePadding.paddingAmounts)
}
void PoolingLayerParams_ValidCompletePadding::add_paddingamounts(::google::protobuf::uint64 value) {
  paddingamounts_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.PoolingLayerParams.ValidCompletePadding.paddingAmounts)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
PoolingLayerParams_ValidCompletePadding::paddingamounts() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.PoolingLayerParams.ValidCompletePadding.paddingAmounts)
  return paddingamounts_;
}
::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
PoolingLayerParams_ValidCompletePadding::mutable_paddingamounts() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.PoolingLayerParams.ValidCompletePadding.paddingAmounts)
  return &paddingamounts_;
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int PoolingLayerParams::kTypeFieldNumber;
const int PoolingLayerParams::kKernelSizeFieldNumber;
const int PoolingLayerParams::kStrideFieldNumber;
const int PoolingLayerParams::kValidFieldNumber;
const int PoolingLayerParams::kSameFieldNumber;
const int PoolingLayerParams::kIncludeLastPixelFieldNumber;
const int PoolingLayerParams::kAvgPoolExcludePaddingFieldNumber;
const int PoolingLayerParams::kGlobalPoolingFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

PoolingLayerParams::PoolingLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.PoolingLayerParams)
}
PoolingLayerParams::PoolingLayerParams(const PoolingLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      kernelsize_(from.kernelsize_),
      stride_(from.stride_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::memcpy(&type_, &from.type_,
    reinterpret_cast<char*>(&globalpooling_) -
    reinterpret_cast<char*>(&type_) + sizeof(globalpooling_));
  clear_has_PoolingPaddingType();
  switch (from.PoolingPaddingType_case()) {
    case kValid: {
      mutable_valid()->::CoreML::Specification::ValidPadding::MergeFrom(from.valid());
      break;
    }
    case kSame: {
      mutable_same()->::CoreML::Specification::SamePadding::MergeFrom(from.same());
      break;
    }
    case kIncludeLastPixel: {
      mutable_includelastpixel()->::CoreML::Specification::PoolingLayerParams_ValidCompletePadding::MergeFrom(from.includelastpixel());
      break;
    }
    case POOLINGPADDINGTYPE_NOT_SET: {
      break;
    }
  }
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.PoolingLayerParams)
}

void PoolingLayerParams::SharedCtor() {
  ::memset(&type_, 0, reinterpret_cast<char*>(&globalpooling_) -
    reinterpret_cast<char*>(&type_) + sizeof(globalpooling_));
  clear_has_PoolingPaddingType();
  _cached_size_ = 0;
}

PoolingLayerParams::~PoolingLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.PoolingLayerParams)
  SharedDtor();
}

void PoolingLayerParams::SharedDtor() {
  if (has_PoolingPaddingType()) {
    clear_PoolingPaddingType();
  }
}

void PoolingLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const PoolingLayerParams& PoolingLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

PoolingLayerParams* PoolingLayerParams::New(::google::protobuf::Arena* arena) const {
  PoolingLayerParams* n = new PoolingLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void PoolingLayerParams::clear_PoolingPaddingType() {
// @@protoc_insertion_point(one_of_clear_start:CoreML.Specification.PoolingLayerParams)
  switch (PoolingPaddingType_case()) {
    case kValid: {
      delete PoolingPaddingType_.valid_;
      break;
    }
    case kSame: {
      delete PoolingPaddingType_.same_;
      break;
    }
    case kIncludeLastPixel: {
      delete PoolingPaddingType_.includelastpixel_;
      break;
    }
    case POOLINGPADDINGTYPE_NOT_SET: {
      break;
    }
  }
  _oneof_case_[0] = POOLINGPADDINGTYPE_NOT_SET;
}


void PoolingLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.PoolingLayerParams)
  kernelsize_.Clear();
  stride_.Clear();
  ::memset(&type_, 0, reinterpret_cast<char*>(&globalpooling_) -
    reinterpret_cast<char*>(&type_) + sizeof(globalpooling_));
  clear_PoolingPaddingType();
}

bool PoolingLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.PoolingLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(16383u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // .CoreML.Specification.PoolingLayerParams.PoolingType type = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {
          int value;
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   int, ::google::protobuf::internal::WireFormatLite::TYPE_ENUM>(
                 input, &value)));
          set_type(static_cast< ::CoreML::Specification::PoolingLayerParams_PoolingType >(value));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated uint64 kernelSize = 10;
      case 10: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(82u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, this->mutable_kernelsize())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(80u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 1, 82u, input, this->mutable_kernelsize())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated uint64 stride = 20;
      case 20: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(162u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, this->mutable_stride())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(160u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 2, 162u, input, this->mutable_stride())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ValidPadding valid = 30;
      case 30: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(242u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_valid()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.SamePadding same = 31;
      case 31: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(250u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_same()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.PoolingLayerParams.ValidCompletePadding includeLastPixel = 32;
      case 32: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(258u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_includelastpixel()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool avgPoolExcludePadding = 50;
      case 50: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(400u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &avgpoolexcludepadding_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool globalPooling = 60;
      case 60: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(480u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &globalpooling_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.PoolingLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.PoolingLayerParams)
  return false;
#undef DO_
}

void PoolingLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.PoolingLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // .CoreML.Specification.PoolingLayerParams.PoolingType type = 1;
  if (this->type() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteEnum(
      1, this->type(), output);
  }

  // repeated uint64 kernelSize = 10;
  if (this->kernelsize_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(10, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_kernelsize_cached_byte_size_);
  }
  for (int i = 0, n = this->kernelsize_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64NoTag(
      this->kernelsize(i), output);
  }

  // repeated uint64 stride = 20;
  if (this->stride_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(20, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_stride_cached_byte_size_);
  }
  for (int i = 0, n = this->stride_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64NoTag(
      this->stride(i), output);
  }

  // .CoreML.Specification.ValidPadding valid = 30;
  if (has_valid()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      30, *PoolingPaddingType_.valid_, output);
  }

  // .CoreML.Specification.SamePadding same = 31;
  if (has_same()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      31, *PoolingPaddingType_.same_, output);
  }

  // .CoreML.Specification.PoolingLayerParams.ValidCompletePadding includeLastPixel = 32;
  if (has_includelastpixel()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      32, *PoolingPaddingType_.includelastpixel_, output);
  }

  // bool avgPoolExcludePadding = 50;
  if (this->avgpoolexcludepadding() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(50, this->avgpoolexcludepadding(), output);
  }

  // bool globalPooling = 60;
  if (this->globalpooling() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(60, this->globalpooling(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.PoolingLayerParams)
}

size_t PoolingLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.PoolingLayerParams)
  size_t total_size = 0;

  // repeated uint64 kernelSize = 10;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      UInt64Size(this->kernelsize_);
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _kernelsize_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // repeated uint64 stride = 20;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      UInt64Size(this->stride_);
    if (data_size > 0) {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _stride_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // .CoreML.Specification.PoolingLayerParams.PoolingType type = 1;
  if (this->type() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::EnumSize(this->type());
  }

  // bool avgPoolExcludePadding = 50;
  if (this->avgpoolexcludepadding() != 0) {
    total_size += 2 + 1;
  }

  // bool globalPooling = 60;
  if (this->globalpooling() != 0) {
    total_size += 2 + 1;
  }

  switch (PoolingPaddingType_case()) {
    // .CoreML.Specification.ValidPadding valid = 30;
    case kValid: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *PoolingPaddingType_.valid_);
      break;
    }
    // .CoreML.Specification.SamePadding same = 31;
    case kSame: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *PoolingPaddingType_.same_);
      break;
    }
    // .CoreML.Specification.PoolingLayerParams.ValidCompletePadding includeLastPixel = 32;
    case kIncludeLastPixel: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *PoolingPaddingType_.includelastpixel_);
      break;
    }
    case POOLINGPADDINGTYPE_NOT_SET: {
      break;
    }
  }
  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void PoolingLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const PoolingLayerParams*>(&from));
}

void PoolingLayerParams::MergeFrom(const PoolingLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.PoolingLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  kernelsize_.MergeFrom(from.kernelsize_);
  stride_.MergeFrom(from.stride_);
  if (from.type() != 0) {
    set_type(from.type());
  }
  if (from.avgpoolexcludepadding() != 0) {
    set_avgpoolexcludepadding(from.avgpoolexcludepadding());
  }
  if (from.globalpooling() != 0) {
    set_globalpooling(from.globalpooling());
  }
  switch (from.PoolingPaddingType_case()) {
    case kValid: {
      mutable_valid()->::CoreML::Specification::ValidPadding::MergeFrom(from.valid());
      break;
    }
    case kSame: {
      mutable_same()->::CoreML::Specification::SamePadding::MergeFrom(from.same());
      break;
    }
    case kIncludeLastPixel: {
      mutable_includelastpixel()->::CoreML::Specification::PoolingLayerParams_ValidCompletePadding::MergeFrom(from.includelastpixel());
      break;
    }
    case POOLINGPADDINGTYPE_NOT_SET: {
      break;
    }
  }
}

void PoolingLayerParams::CopyFrom(const PoolingLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.PoolingLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool PoolingLayerParams::IsInitialized() const {
  return true;
}

void PoolingLayerParams::Swap(PoolingLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void PoolingLayerParams::InternalSwap(PoolingLayerParams* other) {
  kernelsize_.InternalSwap(&other->kernelsize_);
  stride_.InternalSwap(&other->stride_);
  std::swap(type_, other->type_);
  std::swap(avgpoolexcludepadding_, other->avgpoolexcludepadding_);
  std::swap(globalpooling_, other->globalpooling_);
  std::swap(PoolingPaddingType_, other->PoolingPaddingType_);
  std::swap(_oneof_case_[0], other->_oneof_case_[0]);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string PoolingLayerParams::GetTypeName() const {
  return "CoreML.Specification.PoolingLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// PoolingLayerParams

// .CoreML.Specification.PoolingLayerParams.PoolingType type = 1;
void PoolingLayerParams::clear_type() {
  type_ = 0;
}
::CoreML::Specification::PoolingLayerParams_PoolingType PoolingLayerParams::type() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PoolingLayerParams.type)
  return static_cast< ::CoreML::Specification::PoolingLayerParams_PoolingType >(type_);
}
void PoolingLayerParams::set_type(::CoreML::Specification::PoolingLayerParams_PoolingType value) {
  
  type_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.PoolingLayerParams.type)
}

// repeated uint64 kernelSize = 10;
int PoolingLayerParams::kernelsize_size() const {
  return kernelsize_.size();
}
void PoolingLayerParams::clear_kernelsize() {
  kernelsize_.Clear();
}
::google::protobuf::uint64 PoolingLayerParams::kernelsize(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PoolingLayerParams.kernelSize)
  return kernelsize_.Get(index);
}
void PoolingLayerParams::set_kernelsize(int index, ::google::protobuf::uint64 value) {
  kernelsize_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.PoolingLayerParams.kernelSize)
}
void PoolingLayerParams::add_kernelsize(::google::protobuf::uint64 value) {
  kernelsize_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.PoolingLayerParams.kernelSize)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
PoolingLayerParams::kernelsize() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.PoolingLayerParams.kernelSize)
  return kernelsize_;
}
::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
PoolingLayerParams::mutable_kernelsize() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.PoolingLayerParams.kernelSize)
  return &kernelsize_;
}

// repeated uint64 stride = 20;
int PoolingLayerParams::stride_size() const {
  return stride_.size();
}
void PoolingLayerParams::clear_stride() {
  stride_.Clear();
}
::google::protobuf::uint64 PoolingLayerParams::stride(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PoolingLayerParams.stride)
  return stride_.Get(index);
}
void PoolingLayerParams::set_stride(int index, ::google::protobuf::uint64 value) {
  stride_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.PoolingLayerParams.stride)
}
void PoolingLayerParams::add_stride(::google::protobuf::uint64 value) {
  stride_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.PoolingLayerParams.stride)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
PoolingLayerParams::stride() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.PoolingLayerParams.stride)
  return stride_;
}
::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
PoolingLayerParams::mutable_stride() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.PoolingLayerParams.stride)
  return &stride_;
}

// .CoreML.Specification.ValidPadding valid = 30;
bool PoolingLayerParams::has_valid() const {
  return PoolingPaddingType_case() == kValid;
}
void PoolingLayerParams::set_has_valid() {
  _oneof_case_[0] = kValid;
}
void PoolingLayerParams::clear_valid() {
  if (has_valid()) {
    delete PoolingPaddingType_.valid_;
    clear_has_PoolingPaddingType();
  }
}
 const ::CoreML::Specification::ValidPadding& PoolingLayerParams::valid() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PoolingLayerParams.valid)
  return has_valid()
      ? *PoolingPaddingType_.valid_
      : ::CoreML::Specification::ValidPadding::default_instance();
}
::CoreML::Specification::ValidPadding* PoolingLayerParams::mutable_valid() {
  if (!has_valid()) {
    clear_PoolingPaddingType();
    set_has_valid();
    PoolingPaddingType_.valid_ = new ::CoreML::Specification::ValidPadding;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.PoolingLayerParams.valid)
  return PoolingPaddingType_.valid_;
}
::CoreML::Specification::ValidPadding* PoolingLayerParams::release_valid() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.PoolingLayerParams.valid)
  if (has_valid()) {
    clear_has_PoolingPaddingType();
    ::CoreML::Specification::ValidPadding* temp = PoolingPaddingType_.valid_;
    PoolingPaddingType_.valid_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void PoolingLayerParams::set_allocated_valid(::CoreML::Specification::ValidPadding* valid) {
  clear_PoolingPaddingType();
  if (valid) {
    set_has_valid();
    PoolingPaddingType_.valid_ = valid;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.PoolingLayerParams.valid)
}

// .CoreML.Specification.SamePadding same = 31;
bool PoolingLayerParams::has_same() const {
  return PoolingPaddingType_case() == kSame;
}
void PoolingLayerParams::set_has_same() {
  _oneof_case_[0] = kSame;
}
void PoolingLayerParams::clear_same() {
  if (has_same()) {
    delete PoolingPaddingType_.same_;
    clear_has_PoolingPaddingType();
  }
}
 const ::CoreML::Specification::SamePadding& PoolingLayerParams::same() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PoolingLayerParams.same)
  return has_same()
      ? *PoolingPaddingType_.same_
      : ::CoreML::Specification::SamePadding::default_instance();
}
::CoreML::Specification::SamePadding* PoolingLayerParams::mutable_same() {
  if (!has_same()) {
    clear_PoolingPaddingType();
    set_has_same();
    PoolingPaddingType_.same_ = new ::CoreML::Specification::SamePadding;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.PoolingLayerParams.same)
  return PoolingPaddingType_.same_;
}
::CoreML::Specification::SamePadding* PoolingLayerParams::release_same() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.PoolingLayerParams.same)
  if (has_same()) {
    clear_has_PoolingPaddingType();
    ::CoreML::Specification::SamePadding* temp = PoolingPaddingType_.same_;
    PoolingPaddingType_.same_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void PoolingLayerParams::set_allocated_same(::CoreML::Specification::SamePadding* same) {
  clear_PoolingPaddingType();
  if (same) {
    set_has_same();
    PoolingPaddingType_.same_ = same;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.PoolingLayerParams.same)
}

// .CoreML.Specification.PoolingLayerParams.ValidCompletePadding includeLastPixel = 32;
bool PoolingLayerParams::has_includelastpixel() const {
  return PoolingPaddingType_case() == kIncludeLastPixel;
}
void PoolingLayerParams::set_has_includelastpixel() {
  _oneof_case_[0] = kIncludeLastPixel;
}
void PoolingLayerParams::clear_includelastpixel() {
  if (has_includelastpixel()) {
    delete PoolingPaddingType_.includelastpixel_;
    clear_has_PoolingPaddingType();
  }
}
 const ::CoreML::Specification::PoolingLayerParams_ValidCompletePadding& PoolingLayerParams::includelastpixel() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PoolingLayerParams.includeLastPixel)
  return has_includelastpixel()
      ? *PoolingPaddingType_.includelastpixel_
      : ::CoreML::Specification::PoolingLayerParams_ValidCompletePadding::default_instance();
}
::CoreML::Specification::PoolingLayerParams_ValidCompletePadding* PoolingLayerParams::mutable_includelastpixel() {
  if (!has_includelastpixel()) {
    clear_PoolingPaddingType();
    set_has_includelastpixel();
    PoolingPaddingType_.includelastpixel_ = new ::CoreML::Specification::PoolingLayerParams_ValidCompletePadding;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.PoolingLayerParams.includeLastPixel)
  return PoolingPaddingType_.includelastpixel_;
}
::CoreML::Specification::PoolingLayerParams_ValidCompletePadding* PoolingLayerParams::release_includelastpixel() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.PoolingLayerParams.includeLastPixel)
  if (has_includelastpixel()) {
    clear_has_PoolingPaddingType();
    ::CoreML::Specification::PoolingLayerParams_ValidCompletePadding* temp = PoolingPaddingType_.includelastpixel_;
    PoolingPaddingType_.includelastpixel_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void PoolingLayerParams::set_allocated_includelastpixel(::CoreML::Specification::PoolingLayerParams_ValidCompletePadding* includelastpixel) {
  clear_PoolingPaddingType();
  if (includelastpixel) {
    set_has_includelastpixel();
    PoolingPaddingType_.includelastpixel_ = includelastpixel;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.PoolingLayerParams.includeLastPixel)
}

// bool avgPoolExcludePadding = 50;
void PoolingLayerParams::clear_avgpoolexcludepadding() {
  avgpoolexcludepadding_ = false;
}
bool PoolingLayerParams::avgpoolexcludepadding() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PoolingLayerParams.avgPoolExcludePadding)
  return avgpoolexcludepadding_;
}
void PoolingLayerParams::set_avgpoolexcludepadding(bool value) {
  
  avgpoolexcludepadding_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.PoolingLayerParams.avgPoolExcludePadding)
}

// bool globalPooling = 60;
void PoolingLayerParams::clear_globalpooling() {
  globalpooling_ = false;
}
bool PoolingLayerParams::globalpooling() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PoolingLayerParams.globalPooling)
  return globalpooling_;
}
void PoolingLayerParams::set_globalpooling(bool value) {
  
  globalpooling_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.PoolingLayerParams.globalPooling)
}

bool PoolingLayerParams::has_PoolingPaddingType() const {
  return PoolingPaddingType_case() != POOLINGPADDINGTYPE_NOT_SET;
}
void PoolingLayerParams::clear_has_PoolingPaddingType() {
  _oneof_case_[0] = POOLINGPADDINGTYPE_NOT_SET;
}
PoolingLayerParams::PoolingPaddingTypeCase PoolingLayerParams::PoolingPaddingType_case() const {
  return PoolingLayerParams::PoolingPaddingTypeCase(_oneof_case_[0]);
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int PaddingLayerParams_PaddingConstant::kValueFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

PaddingLayerParams_PaddingConstant::PaddingLayerParams_PaddingConstant()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.PaddingLayerParams.PaddingConstant)
}
PaddingLayerParams_PaddingConstant::PaddingLayerParams_PaddingConstant(const PaddingLayerParams_PaddingConstant& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  value_ = from.value_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.PaddingLayerParams.PaddingConstant)
}

void PaddingLayerParams_PaddingConstant::SharedCtor() {
  value_ = 0;
  _cached_size_ = 0;
}

PaddingLayerParams_PaddingConstant::~PaddingLayerParams_PaddingConstant() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.PaddingLayerParams.PaddingConstant)
  SharedDtor();
}

void PaddingLayerParams_PaddingConstant::SharedDtor() {
}

void PaddingLayerParams_PaddingConstant::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const PaddingLayerParams_PaddingConstant& PaddingLayerParams_PaddingConstant::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

PaddingLayerParams_PaddingConstant* PaddingLayerParams_PaddingConstant::New(::google::protobuf::Arena* arena) const {
  PaddingLayerParams_PaddingConstant* n = new PaddingLayerParams_PaddingConstant;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void PaddingLayerParams_PaddingConstant::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.PaddingLayerParams.PaddingConstant)
  value_ = 0;
}

bool PaddingLayerParams_PaddingConstant::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.PaddingLayerParams.PaddingConstant)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // float value = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(13u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &value_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.PaddingLayerParams.PaddingConstant)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.PaddingLayerParams.PaddingConstant)
  return false;
#undef DO_
}

void PaddingLayerParams_PaddingConstant::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.PaddingLayerParams.PaddingConstant)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // float value = 1;
  if (this->value() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(1, this->value(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.PaddingLayerParams.PaddingConstant)
}

size_t PaddingLayerParams_PaddingConstant::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.PaddingLayerParams.PaddingConstant)
  size_t total_size = 0;

  // float value = 1;
  if (this->value() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void PaddingLayerParams_PaddingConstant::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const PaddingLayerParams_PaddingConstant*>(&from));
}

void PaddingLayerParams_PaddingConstant::MergeFrom(const PaddingLayerParams_PaddingConstant& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.PaddingLayerParams.PaddingConstant)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.value() != 0) {
    set_value(from.value());
  }
}

void PaddingLayerParams_PaddingConstant::CopyFrom(const PaddingLayerParams_PaddingConstant& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.PaddingLayerParams.PaddingConstant)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool PaddingLayerParams_PaddingConstant::IsInitialized() const {
  return true;
}

void PaddingLayerParams_PaddingConstant::Swap(PaddingLayerParams_PaddingConstant* other) {
  if (other == this) return;
  InternalSwap(other);
}
void PaddingLayerParams_PaddingConstant::InternalSwap(PaddingLayerParams_PaddingConstant* other) {
  std::swap(value_, other->value_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string PaddingLayerParams_PaddingConstant::GetTypeName() const {
  return "CoreML.Specification.PaddingLayerParams.PaddingConstant";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// PaddingLayerParams_PaddingConstant

// float value = 1;
void PaddingLayerParams_PaddingConstant::clear_value() {
  value_ = 0;
}
float PaddingLayerParams_PaddingConstant::value() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PaddingLayerParams.PaddingConstant.value)
  return value_;
}
void PaddingLayerParams_PaddingConstant::set_value(float value) {
  
  value_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.PaddingLayerParams.PaddingConstant.value)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

PaddingLayerParams_PaddingReflection::PaddingLayerParams_PaddingReflection()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.PaddingLayerParams.PaddingReflection)
}
PaddingLayerParams_PaddingReflection::PaddingLayerParams_PaddingReflection(const PaddingLayerParams_PaddingReflection& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.PaddingLayerParams.PaddingReflection)
}

void PaddingLayerParams_PaddingReflection::SharedCtor() {
  _cached_size_ = 0;
}

PaddingLayerParams_PaddingReflection::~PaddingLayerParams_PaddingReflection() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.PaddingLayerParams.PaddingReflection)
  SharedDtor();
}

void PaddingLayerParams_PaddingReflection::SharedDtor() {
}

void PaddingLayerParams_PaddingReflection::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const PaddingLayerParams_PaddingReflection& PaddingLayerParams_PaddingReflection::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

PaddingLayerParams_PaddingReflection* PaddingLayerParams_PaddingReflection::New(::google::protobuf::Arena* arena) const {
  PaddingLayerParams_PaddingReflection* n = new PaddingLayerParams_PaddingReflection;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void PaddingLayerParams_PaddingReflection::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.PaddingLayerParams.PaddingReflection)
}

bool PaddingLayerParams_PaddingReflection::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.PaddingLayerParams.PaddingReflection)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.PaddingLayerParams.PaddingReflection)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.PaddingLayerParams.PaddingReflection)
  return false;
#undef DO_
}

void PaddingLayerParams_PaddingReflection::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.PaddingLayerParams.PaddingReflection)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.PaddingLayerParams.PaddingReflection)
}

size_t PaddingLayerParams_PaddingReflection::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.PaddingLayerParams.PaddingReflection)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void PaddingLayerParams_PaddingReflection::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const PaddingLayerParams_PaddingReflection*>(&from));
}

void PaddingLayerParams_PaddingReflection::MergeFrom(const PaddingLayerParams_PaddingReflection& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.PaddingLayerParams.PaddingReflection)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void PaddingLayerParams_PaddingReflection::CopyFrom(const PaddingLayerParams_PaddingReflection& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.PaddingLayerParams.PaddingReflection)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool PaddingLayerParams_PaddingReflection::IsInitialized() const {
  return true;
}

void PaddingLayerParams_PaddingReflection::Swap(PaddingLayerParams_PaddingReflection* other) {
  if (other == this) return;
  InternalSwap(other);
}
void PaddingLayerParams_PaddingReflection::InternalSwap(PaddingLayerParams_PaddingReflection* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string PaddingLayerParams_PaddingReflection::GetTypeName() const {
  return "CoreML.Specification.PaddingLayerParams.PaddingReflection";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// PaddingLayerParams_PaddingReflection

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

PaddingLayerParams_PaddingReplication::PaddingLayerParams_PaddingReplication()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.PaddingLayerParams.PaddingReplication)
}
PaddingLayerParams_PaddingReplication::PaddingLayerParams_PaddingReplication(const PaddingLayerParams_PaddingReplication& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.PaddingLayerParams.PaddingReplication)
}

void PaddingLayerParams_PaddingReplication::SharedCtor() {
  _cached_size_ = 0;
}

PaddingLayerParams_PaddingReplication::~PaddingLayerParams_PaddingReplication() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.PaddingLayerParams.PaddingReplication)
  SharedDtor();
}

void PaddingLayerParams_PaddingReplication::SharedDtor() {
}

void PaddingLayerParams_PaddingReplication::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const PaddingLayerParams_PaddingReplication& PaddingLayerParams_PaddingReplication::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

PaddingLayerParams_PaddingReplication* PaddingLayerParams_PaddingReplication::New(::google::protobuf::Arena* arena) const {
  PaddingLayerParams_PaddingReplication* n = new PaddingLayerParams_PaddingReplication;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void PaddingLayerParams_PaddingReplication::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.PaddingLayerParams.PaddingReplication)
}

bool PaddingLayerParams_PaddingReplication::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.PaddingLayerParams.PaddingReplication)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.PaddingLayerParams.PaddingReplication)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.PaddingLayerParams.PaddingReplication)
  return false;
#undef DO_
}

void PaddingLayerParams_PaddingReplication::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.PaddingLayerParams.PaddingReplication)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.PaddingLayerParams.PaddingReplication)
}

size_t PaddingLayerParams_PaddingReplication::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.PaddingLayerParams.PaddingReplication)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void PaddingLayerParams_PaddingReplication::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const PaddingLayerParams_PaddingReplication*>(&from));
}

void PaddingLayerParams_PaddingReplication::MergeFrom(const PaddingLayerParams_PaddingReplication& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.PaddingLayerParams.PaddingReplication)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void PaddingLayerParams_PaddingReplication::CopyFrom(const PaddingLayerParams_PaddingReplication& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.PaddingLayerParams.PaddingReplication)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool PaddingLayerParams_PaddingReplication::IsInitialized() const {
  return true;
}

void PaddingLayerParams_PaddingReplication::Swap(PaddingLayerParams_PaddingReplication* other) {
  if (other == this) return;
  InternalSwap(other);
}
void PaddingLayerParams_PaddingReplication::InternalSwap(PaddingLayerParams_PaddingReplication* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string PaddingLayerParams_PaddingReplication::GetTypeName() const {
  return "CoreML.Specification.PaddingLayerParams.PaddingReplication";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// PaddingLayerParams_PaddingReplication

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int PaddingLayerParams::kConstantFieldNumber;
const int PaddingLayerParams::kReflectionFieldNumber;
const int PaddingLayerParams::kReplicationFieldNumber;
const int PaddingLayerParams::kPaddingAmountsFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

PaddingLayerParams::PaddingLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.PaddingLayerParams)
}
PaddingLayerParams::PaddingLayerParams(const PaddingLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_paddingamounts()) {
    paddingamounts_ = new ::CoreML::Specification::BorderAmounts(*from.paddingamounts_);
  } else {
    paddingamounts_ = NULL;
  }
  clear_has_PaddingType();
  switch (from.PaddingType_case()) {
    case kConstant: {
      mutable_constant()->::CoreML::Specification::PaddingLayerParams_PaddingConstant::MergeFrom(from.constant());
      break;
    }
    case kReflection: {
      mutable_reflection()->::CoreML::Specification::PaddingLayerParams_PaddingReflection::MergeFrom(from.reflection());
      break;
    }
    case kReplication: {
      mutable_replication()->::CoreML::Specification::PaddingLayerParams_PaddingReplication::MergeFrom(from.replication());
      break;
    }
    case PADDINGTYPE_NOT_SET: {
      break;
    }
  }
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.PaddingLayerParams)
}

void PaddingLayerParams::SharedCtor() {
  paddingamounts_ = NULL;
  clear_has_PaddingType();
  _cached_size_ = 0;
}

PaddingLayerParams::~PaddingLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.PaddingLayerParams)
  SharedDtor();
}

void PaddingLayerParams::SharedDtor() {
  if (this != internal_default_instance()) {
    delete paddingamounts_;
  }
  if (has_PaddingType()) {
    clear_PaddingType();
  }
}

void PaddingLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const PaddingLayerParams& PaddingLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

PaddingLayerParams* PaddingLayerParams::New(::google::protobuf::Arena* arena) const {
  PaddingLayerParams* n = new PaddingLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void PaddingLayerParams::clear_PaddingType() {
// @@protoc_insertion_point(one_of_clear_start:CoreML.Specification.PaddingLayerParams)
  switch (PaddingType_case()) {
    case kConstant: {
      delete PaddingType_.constant_;
      break;
    }
    case kReflection: {
      delete PaddingType_.reflection_;
      break;
    }
    case kReplication: {
      delete PaddingType_.replication_;
      break;
    }
    case PADDINGTYPE_NOT_SET: {
      break;
    }
  }
  _oneof_case_[0] = PADDINGTYPE_NOT_SET;
}


void PaddingLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.PaddingLayerParams)
  if (GetArenaNoVirtual() == NULL && paddingamounts_ != NULL) {
    delete paddingamounts_;
  }
  paddingamounts_ = NULL;
  clear_PaddingType();
}

bool PaddingLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.PaddingLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // .CoreML.Specification.PaddingLayerParams.PaddingConstant constant = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_constant()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.PaddingLayerParams.PaddingReflection reflection = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(18u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_reflection()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.PaddingLayerParams.PaddingReplication replication = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(26u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_replication()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.BorderAmounts paddingAmounts = 10;
      case 10: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(82u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_paddingamounts()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.PaddingLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.PaddingLayerParams)
  return false;
#undef DO_
}

void PaddingLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.PaddingLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // .CoreML.Specification.PaddingLayerParams.PaddingConstant constant = 1;
  if (has_constant()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1, *PaddingType_.constant_, output);
  }

  // .CoreML.Specification.PaddingLayerParams.PaddingReflection reflection = 2;
  if (has_reflection()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      2, *PaddingType_.reflection_, output);
  }

  // .CoreML.Specification.PaddingLayerParams.PaddingReplication replication = 3;
  if (has_replication()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      3, *PaddingType_.replication_, output);
  }

  // .CoreML.Specification.BorderAmounts paddingAmounts = 10;
  if (this->has_paddingamounts()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      10, *this->paddingamounts_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.PaddingLayerParams)
}

size_t PaddingLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.PaddingLayerParams)
  size_t total_size = 0;

  // .CoreML.Specification.BorderAmounts paddingAmounts = 10;
  if (this->has_paddingamounts()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->paddingamounts_);
  }

  switch (PaddingType_case()) {
    // .CoreML.Specification.PaddingLayerParams.PaddingConstant constant = 1;
    case kConstant: {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *PaddingType_.constant_);
      break;
    }
    // .CoreML.Specification.PaddingLayerParams.PaddingReflection reflection = 2;
    case kReflection: {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *PaddingType_.reflection_);
      break;
    }
    // .CoreML.Specification.PaddingLayerParams.PaddingReplication replication = 3;
    case kReplication: {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *PaddingType_.replication_);
      break;
    }
    case PADDINGTYPE_NOT_SET: {
      break;
    }
  }
  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void PaddingLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const PaddingLayerParams*>(&from));
}

void PaddingLayerParams::MergeFrom(const PaddingLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.PaddingLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.has_paddingamounts()) {
    mutable_paddingamounts()->::CoreML::Specification::BorderAmounts::MergeFrom(from.paddingamounts());
  }
  switch (from.PaddingType_case()) {
    case kConstant: {
      mutable_constant()->::CoreML::Specification::PaddingLayerParams_PaddingConstant::MergeFrom(from.constant());
      break;
    }
    case kReflection: {
      mutable_reflection()->::CoreML::Specification::PaddingLayerParams_PaddingReflection::MergeFrom(from.reflection());
      break;
    }
    case kReplication: {
      mutable_replication()->::CoreML::Specification::PaddingLayerParams_PaddingReplication::MergeFrom(from.replication());
      break;
    }
    case PADDINGTYPE_NOT_SET: {
      break;
    }
  }
}

void PaddingLayerParams::CopyFrom(const PaddingLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.PaddingLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool PaddingLayerParams::IsInitialized() const {
  return true;
}

void PaddingLayerParams::Swap(PaddingLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void PaddingLayerParams::InternalSwap(PaddingLayerParams* other) {
  std::swap(paddingamounts_, other->paddingamounts_);
  std::swap(PaddingType_, other->PaddingType_);
  std::swap(_oneof_case_[0], other->_oneof_case_[0]);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string PaddingLayerParams::GetTypeName() const {
  return "CoreML.Specification.PaddingLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// PaddingLayerParams

// .CoreML.Specification.PaddingLayerParams.PaddingConstant constant = 1;
bool PaddingLayerParams::has_constant() const {
  return PaddingType_case() == kConstant;
}
void PaddingLayerParams::set_has_constant() {
  _oneof_case_[0] = kConstant;
}
void PaddingLayerParams::clear_constant() {
  if (has_constant()) {
    delete PaddingType_.constant_;
    clear_has_PaddingType();
  }
}
 const ::CoreML::Specification::PaddingLayerParams_PaddingConstant& PaddingLayerParams::constant() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PaddingLayerParams.constant)
  return has_constant()
      ? *PaddingType_.constant_
      : ::CoreML::Specification::PaddingLayerParams_PaddingConstant::default_instance();
}
::CoreML::Specification::PaddingLayerParams_PaddingConstant* PaddingLayerParams::mutable_constant() {
  if (!has_constant()) {
    clear_PaddingType();
    set_has_constant();
    PaddingType_.constant_ = new ::CoreML::Specification::PaddingLayerParams_PaddingConstant;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.PaddingLayerParams.constant)
  return PaddingType_.constant_;
}
::CoreML::Specification::PaddingLayerParams_PaddingConstant* PaddingLayerParams::release_constant() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.PaddingLayerParams.constant)
  if (has_constant()) {
    clear_has_PaddingType();
    ::CoreML::Specification::PaddingLayerParams_PaddingConstant* temp = PaddingType_.constant_;
    PaddingType_.constant_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void PaddingLayerParams::set_allocated_constant(::CoreML::Specification::PaddingLayerParams_PaddingConstant* constant) {
  clear_PaddingType();
  if (constant) {
    set_has_constant();
    PaddingType_.constant_ = constant;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.PaddingLayerParams.constant)
}

// .CoreML.Specification.PaddingLayerParams.PaddingReflection reflection = 2;
bool PaddingLayerParams::has_reflection() const {
  return PaddingType_case() == kReflection;
}
void PaddingLayerParams::set_has_reflection() {
  _oneof_case_[0] = kReflection;
}
void PaddingLayerParams::clear_reflection() {
  if (has_reflection()) {
    delete PaddingType_.reflection_;
    clear_has_PaddingType();
  }
}
 const ::CoreML::Specification::PaddingLayerParams_PaddingReflection& PaddingLayerParams::reflection() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PaddingLayerParams.reflection)
  return has_reflection()
      ? *PaddingType_.reflection_
      : ::CoreML::Specification::PaddingLayerParams_PaddingReflection::default_instance();
}
::CoreML::Specification::PaddingLayerParams_PaddingReflection* PaddingLayerParams::mutable_reflection() {
  if (!has_reflection()) {
    clear_PaddingType();
    set_has_reflection();
    PaddingType_.reflection_ = new ::CoreML::Specification::PaddingLayerParams_PaddingReflection;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.PaddingLayerParams.reflection)
  return PaddingType_.reflection_;
}
::CoreML::Specification::PaddingLayerParams_PaddingReflection* PaddingLayerParams::release_reflection() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.PaddingLayerParams.reflection)
  if (has_reflection()) {
    clear_has_PaddingType();
    ::CoreML::Specification::PaddingLayerParams_PaddingReflection* temp = PaddingType_.reflection_;
    PaddingType_.reflection_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void PaddingLayerParams::set_allocated_reflection(::CoreML::Specification::PaddingLayerParams_PaddingReflection* reflection) {
  clear_PaddingType();
  if (reflection) {
    set_has_reflection();
    PaddingType_.reflection_ = reflection;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.PaddingLayerParams.reflection)
}

// .CoreML.Specification.PaddingLayerParams.PaddingReplication replication = 3;
bool PaddingLayerParams::has_replication() const {
  return PaddingType_case() == kReplication;
}
void PaddingLayerParams::set_has_replication() {
  _oneof_case_[0] = kReplication;
}
void PaddingLayerParams::clear_replication() {
  if (has_replication()) {
    delete PaddingType_.replication_;
    clear_has_PaddingType();
  }
}
 const ::CoreML::Specification::PaddingLayerParams_PaddingReplication& PaddingLayerParams::replication() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PaddingLayerParams.replication)
  return has_replication()
      ? *PaddingType_.replication_
      : ::CoreML::Specification::PaddingLayerParams_PaddingReplication::default_instance();
}
::CoreML::Specification::PaddingLayerParams_PaddingReplication* PaddingLayerParams::mutable_replication() {
  if (!has_replication()) {
    clear_PaddingType();
    set_has_replication();
    PaddingType_.replication_ = new ::CoreML::Specification::PaddingLayerParams_PaddingReplication;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.PaddingLayerParams.replication)
  return PaddingType_.replication_;
}
::CoreML::Specification::PaddingLayerParams_PaddingReplication* PaddingLayerParams::release_replication() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.PaddingLayerParams.replication)
  if (has_replication()) {
    clear_has_PaddingType();
    ::CoreML::Specification::PaddingLayerParams_PaddingReplication* temp = PaddingType_.replication_;
    PaddingType_.replication_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void PaddingLayerParams::set_allocated_replication(::CoreML::Specification::PaddingLayerParams_PaddingReplication* replication) {
  clear_PaddingType();
  if (replication) {
    set_has_replication();
    PaddingType_.replication_ = replication;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.PaddingLayerParams.replication)
}

// .CoreML.Specification.BorderAmounts paddingAmounts = 10;
bool PaddingLayerParams::has_paddingamounts() const {
  return this != internal_default_instance() && paddingamounts_ != NULL;
}
void PaddingLayerParams::clear_paddingamounts() {
  if (GetArenaNoVirtual() == NULL && paddingamounts_ != NULL) delete paddingamounts_;
  paddingamounts_ = NULL;
}
const ::CoreML::Specification::BorderAmounts& PaddingLayerParams::paddingamounts() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PaddingLayerParams.paddingAmounts)
  return paddingamounts_ != NULL ? *paddingamounts_
                         : *::CoreML::Specification::BorderAmounts::internal_default_instance();
}
::CoreML::Specification::BorderAmounts* PaddingLayerParams::mutable_paddingamounts() {
  
  if (paddingamounts_ == NULL) {
    paddingamounts_ = new ::CoreML::Specification::BorderAmounts;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.PaddingLayerParams.paddingAmounts)
  return paddingamounts_;
}
::CoreML::Specification::BorderAmounts* PaddingLayerParams::release_paddingamounts() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.PaddingLayerParams.paddingAmounts)
  
  ::CoreML::Specification::BorderAmounts* temp = paddingamounts_;
  paddingamounts_ = NULL;
  return temp;
}
void PaddingLayerParams::set_allocated_paddingamounts(::CoreML::Specification::BorderAmounts* paddingamounts) {
  delete paddingamounts_;
  paddingamounts_ = paddingamounts;
  if (paddingamounts) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.PaddingLayerParams.paddingAmounts)
}

bool PaddingLayerParams::has_PaddingType() const {
  return PaddingType_case() != PADDINGTYPE_NOT_SET;
}
void PaddingLayerParams::clear_has_PaddingType() {
  _oneof_case_[0] = PADDINGTYPE_NOT_SET;
}
PaddingLayerParams::PaddingTypeCase PaddingLayerParams::PaddingType_case() const {
  return PaddingLayerParams::PaddingTypeCase(_oneof_case_[0]);
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ConcatLayerParams::kSequenceConcatFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ConcatLayerParams::ConcatLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ConcatLayerParams)
}
ConcatLayerParams::ConcatLayerParams(const ConcatLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  sequenceconcat_ = from.sequenceconcat_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ConcatLayerParams)
}

void ConcatLayerParams::SharedCtor() {
  sequenceconcat_ = false;
  _cached_size_ = 0;
}

ConcatLayerParams::~ConcatLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ConcatLayerParams)
  SharedDtor();
}

void ConcatLayerParams::SharedDtor() {
}

void ConcatLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ConcatLayerParams& ConcatLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ConcatLayerParams* ConcatLayerParams::New(::google::protobuf::Arena* arena) const {
  ConcatLayerParams* n = new ConcatLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ConcatLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ConcatLayerParams)
  sequenceconcat_ = false;
}

bool ConcatLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ConcatLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(16383u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // bool sequenceConcat = 100;
      case 100: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(800u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &sequenceconcat_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ConcatLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ConcatLayerParams)
  return false;
#undef DO_
}

void ConcatLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ConcatLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // bool sequenceConcat = 100;
  if (this->sequenceconcat() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(100, this->sequenceconcat(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ConcatLayerParams)
}

size_t ConcatLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ConcatLayerParams)
  size_t total_size = 0;

  // bool sequenceConcat = 100;
  if (this->sequenceconcat() != 0) {
    total_size += 2 + 1;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ConcatLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ConcatLayerParams*>(&from));
}

void ConcatLayerParams::MergeFrom(const ConcatLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ConcatLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.sequenceconcat() != 0) {
    set_sequenceconcat(from.sequenceconcat());
  }
}

void ConcatLayerParams::CopyFrom(const ConcatLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ConcatLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ConcatLayerParams::IsInitialized() const {
  return true;
}

void ConcatLayerParams::Swap(ConcatLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ConcatLayerParams::InternalSwap(ConcatLayerParams* other) {
  std::swap(sequenceconcat_, other->sequenceconcat_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ConcatLayerParams::GetTypeName() const {
  return "CoreML.Specification.ConcatLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ConcatLayerParams

// bool sequenceConcat = 100;
void ConcatLayerParams::clear_sequenceconcat() {
  sequenceconcat_ = false;
}
bool ConcatLayerParams::sequenceconcat() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConcatLayerParams.sequenceConcat)
  return sequenceconcat_;
}
void ConcatLayerParams::set_sequenceconcat(bool value) {
  
  sequenceconcat_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConcatLayerParams.sequenceConcat)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int LRNLayerParams::kAlphaFieldNumber;
const int LRNLayerParams::kBetaFieldNumber;
const int LRNLayerParams::kLocalSizeFieldNumber;
const int LRNLayerParams::kKFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

LRNLayerParams::LRNLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.LRNLayerParams)
}
LRNLayerParams::LRNLayerParams(const LRNLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::memcpy(&alpha_, &from.alpha_,
    reinterpret_cast<char*>(&k_) -
    reinterpret_cast<char*>(&alpha_) + sizeof(k_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.LRNLayerParams)
}

void LRNLayerParams::SharedCtor() {
  ::memset(&alpha_, 0, reinterpret_cast<char*>(&k_) -
    reinterpret_cast<char*>(&alpha_) + sizeof(k_));
  _cached_size_ = 0;
}

LRNLayerParams::~LRNLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.LRNLayerParams)
  SharedDtor();
}

void LRNLayerParams::SharedDtor() {
}

void LRNLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const LRNLayerParams& LRNLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

LRNLayerParams* LRNLayerParams::New(::google::protobuf::Arena* arena) const {
  LRNLayerParams* n = new LRNLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void LRNLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.LRNLayerParams)
  ::memset(&alpha_, 0, reinterpret_cast<char*>(&k_) -
    reinterpret_cast<char*>(&alpha_) + sizeof(k_));
}

bool LRNLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.LRNLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // float alpha = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(13u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &alpha_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // float beta = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(21u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &beta_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // uint64 localSize = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(24u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &localsize_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // float k = 4;
      case 4: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(37u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &k_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.LRNLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.LRNLayerParams)
  return false;
#undef DO_
}

void LRNLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.LRNLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // float alpha = 1;
  if (this->alpha() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(1, this->alpha(), output);
  }

  // float beta = 2;
  if (this->beta() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(2, this->beta(), output);
  }

  // uint64 localSize = 3;
  if (this->localsize() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(3, this->localsize(), output);
  }

  // float k = 4;
  if (this->k() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(4, this->k(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.LRNLayerParams)
}

size_t LRNLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.LRNLayerParams)
  size_t total_size = 0;

  // float alpha = 1;
  if (this->alpha() != 0) {
    total_size += 1 + 4;
  }

  // float beta = 2;
  if (this->beta() != 0) {
    total_size += 1 + 4;
  }

  // uint64 localSize = 3;
  if (this->localsize() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->localsize());
  }

  // float k = 4;
  if (this->k() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void LRNLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const LRNLayerParams*>(&from));
}

void LRNLayerParams::MergeFrom(const LRNLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.LRNLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.alpha() != 0) {
    set_alpha(from.alpha());
  }
  if (from.beta() != 0) {
    set_beta(from.beta());
  }
  if (from.localsize() != 0) {
    set_localsize(from.localsize());
  }
  if (from.k() != 0) {
    set_k(from.k());
  }
}

void LRNLayerParams::CopyFrom(const LRNLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.LRNLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool LRNLayerParams::IsInitialized() const {
  return true;
}

void LRNLayerParams::Swap(LRNLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void LRNLayerParams::InternalSwap(LRNLayerParams* other) {
  std::swap(alpha_, other->alpha_);
  std::swap(beta_, other->beta_);
  std::swap(localsize_, other->localsize_);
  std::swap(k_, other->k_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string LRNLayerParams::GetTypeName() const {
  return "CoreML.Specification.LRNLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// LRNLayerParams

// float alpha = 1;
void LRNLayerParams::clear_alpha() {
  alpha_ = 0;
}
float LRNLayerParams::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LRNLayerParams.alpha)
  return alpha_;
}
void LRNLayerParams::set_alpha(float value) {
  
  alpha_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.LRNLayerParams.alpha)
}

// float beta = 2;
void LRNLayerParams::clear_beta() {
  beta_ = 0;
}
float LRNLayerParams::beta() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LRNLayerParams.beta)
  return beta_;
}
void LRNLayerParams::set_beta(float value) {
  
  beta_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.LRNLayerParams.beta)
}

// uint64 localSize = 3;
void LRNLayerParams::clear_localsize() {
  localsize_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 LRNLayerParams::localsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LRNLayerParams.localSize)
  return localsize_;
}
void LRNLayerParams::set_localsize(::google::protobuf::uint64 value) {
  
  localsize_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.LRNLayerParams.localSize)
}

// float k = 4;
void LRNLayerParams::clear_k() {
  k_ = 0;
}
float LRNLayerParams::k() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LRNLayerParams.k)
  return k_;
}
void LRNLayerParams::set_k(float value) {
  
  k_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.LRNLayerParams.k)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

SoftmaxLayerParams::SoftmaxLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.SoftmaxLayerParams)
}
SoftmaxLayerParams::SoftmaxLayerParams(const SoftmaxLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.SoftmaxLayerParams)
}

void SoftmaxLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

SoftmaxLayerParams::~SoftmaxLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.SoftmaxLayerParams)
  SharedDtor();
}

void SoftmaxLayerParams::SharedDtor() {
}

void SoftmaxLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const SoftmaxLayerParams& SoftmaxLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

SoftmaxLayerParams* SoftmaxLayerParams::New(::google::protobuf::Arena* arena) const {
  SoftmaxLayerParams* n = new SoftmaxLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void SoftmaxLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.SoftmaxLayerParams)
}

bool SoftmaxLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.SoftmaxLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.SoftmaxLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.SoftmaxLayerParams)
  return false;
#undef DO_
}

void SoftmaxLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.SoftmaxLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.SoftmaxLayerParams)
}

size_t SoftmaxLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.SoftmaxLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void SoftmaxLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const SoftmaxLayerParams*>(&from));
}

void SoftmaxLayerParams::MergeFrom(const SoftmaxLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.SoftmaxLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void SoftmaxLayerParams::CopyFrom(const SoftmaxLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.SoftmaxLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool SoftmaxLayerParams::IsInitialized() const {
  return true;
}

void SoftmaxLayerParams::Swap(SoftmaxLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void SoftmaxLayerParams::InternalSwap(SoftmaxLayerParams* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string SoftmaxLayerParams::GetTypeName() const {
  return "CoreML.Specification.SoftmaxLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// SoftmaxLayerParams

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int SplitLayerParams::kNOutputsFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

SplitLayerParams::SplitLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.SplitLayerParams)
}
SplitLayerParams::SplitLayerParams(const SplitLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  noutputs_ = from.noutputs_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.SplitLayerParams)
}

void SplitLayerParams::SharedCtor() {
  noutputs_ = GOOGLE_ULONGLONG(0);
  _cached_size_ = 0;
}

SplitLayerParams::~SplitLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.SplitLayerParams)
  SharedDtor();
}

void SplitLayerParams::SharedDtor() {
}

void SplitLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const SplitLayerParams& SplitLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

SplitLayerParams* SplitLayerParams::New(::google::protobuf::Arena* arena) const {
  SplitLayerParams* n = new SplitLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void SplitLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.SplitLayerParams)
  noutputs_ = GOOGLE_ULONGLONG(0);
}

bool SplitLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.SplitLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // uint64 nOutputs = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &noutputs_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.SplitLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.SplitLayerParams)
  return false;
#undef DO_
}

void SplitLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.SplitLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // uint64 nOutputs = 1;
  if (this->noutputs() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(1, this->noutputs(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.SplitLayerParams)
}

size_t SplitLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.SplitLayerParams)
  size_t total_size = 0;

  // uint64 nOutputs = 1;
  if (this->noutputs() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->noutputs());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void SplitLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const SplitLayerParams*>(&from));
}

void SplitLayerParams::MergeFrom(const SplitLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.SplitLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.noutputs() != 0) {
    set_noutputs(from.noutputs());
  }
}

void SplitLayerParams::CopyFrom(const SplitLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.SplitLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool SplitLayerParams::IsInitialized() const {
  return true;
}

void SplitLayerParams::Swap(SplitLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void SplitLayerParams::InternalSwap(SplitLayerParams* other) {
  std::swap(noutputs_, other->noutputs_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string SplitLayerParams::GetTypeName() const {
  return "CoreML.Specification.SplitLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// SplitLayerParams

// uint64 nOutputs = 1;
void SplitLayerParams::clear_noutputs() {
  noutputs_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 SplitLayerParams::noutputs() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SplitLayerParams.nOutputs)
  return noutputs_;
}
void SplitLayerParams::set_noutputs(::google::protobuf::uint64 value) {
  
  noutputs_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SplitLayerParams.nOutputs)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int AddLayerParams::kAlphaFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

AddLayerParams::AddLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.AddLayerParams)
}
AddLayerParams::AddLayerParams(const AddLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  alpha_ = from.alpha_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.AddLayerParams)
}

void AddLayerParams::SharedCtor() {
  alpha_ = 0;
  _cached_size_ = 0;
}

AddLayerParams::~AddLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.AddLayerParams)
  SharedDtor();
}

void AddLayerParams::SharedDtor() {
}

void AddLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const AddLayerParams& AddLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

AddLayerParams* AddLayerParams::New(::google::protobuf::Arena* arena) const {
  AddLayerParams* n = new AddLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void AddLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.AddLayerParams)
  alpha_ = 0;
}

bool AddLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.AddLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // float alpha = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(13u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &alpha_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.AddLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.AddLayerParams)
  return false;
#undef DO_
}

void AddLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.AddLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // float alpha = 1;
  if (this->alpha() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(1, this->alpha(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.AddLayerParams)
}

size_t AddLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.AddLayerParams)
  size_t total_size = 0;

  // float alpha = 1;
  if (this->alpha() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void AddLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const AddLayerParams*>(&from));
}

void AddLayerParams::MergeFrom(const AddLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.AddLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.alpha() != 0) {
    set_alpha(from.alpha());
  }
}

void AddLayerParams::CopyFrom(const AddLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.AddLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool AddLayerParams::IsInitialized() const {
  return true;
}

void AddLayerParams::Swap(AddLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void AddLayerParams::InternalSwap(AddLayerParams* other) {
  std::swap(alpha_, other->alpha_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string AddLayerParams::GetTypeName() const {
  return "CoreML.Specification.AddLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// AddLayerParams

// float alpha = 1;
void AddLayerParams::clear_alpha() {
  alpha_ = 0;
}
float AddLayerParams::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.AddLayerParams.alpha)
  return alpha_;
}
void AddLayerParams::set_alpha(float value) {
  
  alpha_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.AddLayerParams.alpha)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int MultiplyLayerParams::kAlphaFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

MultiplyLayerParams::MultiplyLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.MultiplyLayerParams)
}
MultiplyLayerParams::MultiplyLayerParams(const MultiplyLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  alpha_ = from.alpha_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.MultiplyLayerParams)
}

void MultiplyLayerParams::SharedCtor() {
  alpha_ = 0;
  _cached_size_ = 0;
}

MultiplyLayerParams::~MultiplyLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.MultiplyLayerParams)
  SharedDtor();
}

void MultiplyLayerParams::SharedDtor() {
}

void MultiplyLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const MultiplyLayerParams& MultiplyLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

MultiplyLayerParams* MultiplyLayerParams::New(::google::protobuf::Arena* arena) const {
  MultiplyLayerParams* n = new MultiplyLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void MultiplyLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.MultiplyLayerParams)
  alpha_ = 0;
}

bool MultiplyLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.MultiplyLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // float alpha = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(13u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &alpha_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.MultiplyLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.MultiplyLayerParams)
  return false;
#undef DO_
}

void MultiplyLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.MultiplyLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // float alpha = 1;
  if (this->alpha() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(1, this->alpha(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.MultiplyLayerParams)
}

size_t MultiplyLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.MultiplyLayerParams)
  size_t total_size = 0;

  // float alpha = 1;
  if (this->alpha() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void MultiplyLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const MultiplyLayerParams*>(&from));
}

void MultiplyLayerParams::MergeFrom(const MultiplyLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.MultiplyLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.alpha() != 0) {
    set_alpha(from.alpha());
  }
}

void MultiplyLayerParams::CopyFrom(const MultiplyLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.MultiplyLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool MultiplyLayerParams::IsInitialized() const {
  return true;
}

void MultiplyLayerParams::Swap(MultiplyLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void MultiplyLayerParams::InternalSwap(MultiplyLayerParams* other) {
  std::swap(alpha_, other->alpha_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string MultiplyLayerParams::GetTypeName() const {
  return "CoreML.Specification.MultiplyLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// MultiplyLayerParams

// float alpha = 1;
void MultiplyLayerParams::clear_alpha() {
  alpha_ = 0;
}
float MultiplyLayerParams::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.MultiplyLayerParams.alpha)
  return alpha_;
}
void MultiplyLayerParams::set_alpha(float value) {
  
  alpha_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.MultiplyLayerParams.alpha)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int UnaryFunctionLayerParams::kTypeFieldNumber;
const int UnaryFunctionLayerParams::kAlphaFieldNumber;
const int UnaryFunctionLayerParams::kEpsilonFieldNumber;
const int UnaryFunctionLayerParams::kShiftFieldNumber;
const int UnaryFunctionLayerParams::kScaleFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

UnaryFunctionLayerParams::UnaryFunctionLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.UnaryFunctionLayerParams)
}
UnaryFunctionLayerParams::UnaryFunctionLayerParams(const UnaryFunctionLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::memcpy(&type_, &from.type_,
    reinterpret_cast<char*>(&scale_) -
    reinterpret_cast<char*>(&type_) + sizeof(scale_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.UnaryFunctionLayerParams)
}

void UnaryFunctionLayerParams::SharedCtor() {
  ::memset(&type_, 0, reinterpret_cast<char*>(&scale_) -
    reinterpret_cast<char*>(&type_) + sizeof(scale_));
  _cached_size_ = 0;
}

UnaryFunctionLayerParams::~UnaryFunctionLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.UnaryFunctionLayerParams)
  SharedDtor();
}

void UnaryFunctionLayerParams::SharedDtor() {
}

void UnaryFunctionLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const UnaryFunctionLayerParams& UnaryFunctionLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

UnaryFunctionLayerParams* UnaryFunctionLayerParams::New(::google::protobuf::Arena* arena) const {
  UnaryFunctionLayerParams* n = new UnaryFunctionLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void UnaryFunctionLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.UnaryFunctionLayerParams)
  ::memset(&type_, 0, reinterpret_cast<char*>(&scale_) -
    reinterpret_cast<char*>(&type_) + sizeof(scale_));
}

bool UnaryFunctionLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.UnaryFunctionLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // .CoreML.Specification.UnaryFunctionLayerParams.Operation type = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {
          int value;
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   int, ::google::protobuf::internal::WireFormatLite::TYPE_ENUM>(
                 input, &value)));
          set_type(static_cast< ::CoreML::Specification::UnaryFunctionLayerParams_Operation >(value));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // float alpha = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(21u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &alpha_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // float epsilon = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(29u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &epsilon_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // float shift = 4;
      case 4: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(37u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &shift_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // float scale = 5;
      case 5: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(45u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &scale_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.UnaryFunctionLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.UnaryFunctionLayerParams)
  return false;
#undef DO_
}

void UnaryFunctionLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.UnaryFunctionLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // .CoreML.Specification.UnaryFunctionLayerParams.Operation type = 1;
  if (this->type() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteEnum(
      1, this->type(), output);
  }

  // float alpha = 2;
  if (this->alpha() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(2, this->alpha(), output);
  }

  // float epsilon = 3;
  if (this->epsilon() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(3, this->epsilon(), output);
  }

  // float shift = 4;
  if (this->shift() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(4, this->shift(), output);
  }

  // float scale = 5;
  if (this->scale() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(5, this->scale(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.UnaryFunctionLayerParams)
}

size_t UnaryFunctionLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.UnaryFunctionLayerParams)
  size_t total_size = 0;

  // .CoreML.Specification.UnaryFunctionLayerParams.Operation type = 1;
  if (this->type() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::EnumSize(this->type());
  }

  // float alpha = 2;
  if (this->alpha() != 0) {
    total_size += 1 + 4;
  }

  // float epsilon = 3;
  if (this->epsilon() != 0) {
    total_size += 1 + 4;
  }

  // float shift = 4;
  if (this->shift() != 0) {
    total_size += 1 + 4;
  }

  // float scale = 5;
  if (this->scale() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void UnaryFunctionLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const UnaryFunctionLayerParams*>(&from));
}

void UnaryFunctionLayerParams::MergeFrom(const UnaryFunctionLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.UnaryFunctionLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.type() != 0) {
    set_type(from.type());
  }
  if (from.alpha() != 0) {
    set_alpha(from.alpha());
  }
  if (from.epsilon() != 0) {
    set_epsilon(from.epsilon());
  }
  if (from.shift() != 0) {
    set_shift(from.shift());
  }
  if (from.scale() != 0) {
    set_scale(from.scale());
  }
}

void UnaryFunctionLayerParams::CopyFrom(const UnaryFunctionLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.UnaryFunctionLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool UnaryFunctionLayerParams::IsInitialized() const {
  return true;
}

void UnaryFunctionLayerParams::Swap(UnaryFunctionLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void UnaryFunctionLayerParams::InternalSwap(UnaryFunctionLayerParams* other) {
  std::swap(type_, other->type_);
  std::swap(alpha_, other->alpha_);
  std::swap(epsilon_, other->epsilon_);
  std::swap(shift_, other->shift_);
  std::swap(scale_, other->scale_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string UnaryFunctionLayerParams::GetTypeName() const {
  return "CoreML.Specification.UnaryFunctionLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// UnaryFunctionLayerParams

// .CoreML.Specification.UnaryFunctionLayerParams.Operation type = 1;
void UnaryFunctionLayerParams::clear_type() {
  type_ = 0;
}
::CoreML::Specification::UnaryFunctionLayerParams_Operation UnaryFunctionLayerParams::type() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UnaryFunctionLayerParams.type)
  return static_cast< ::CoreML::Specification::UnaryFunctionLayerParams_Operation >(type_);
}
void UnaryFunctionLayerParams::set_type(::CoreML::Specification::UnaryFunctionLayerParams_Operation value) {
  
  type_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.UnaryFunctionLayerParams.type)
}

// float alpha = 2;
void UnaryFunctionLayerParams::clear_alpha() {
  alpha_ = 0;
}
float UnaryFunctionLayerParams::alpha() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UnaryFunctionLayerParams.alpha)
  return alpha_;
}
void UnaryFunctionLayerParams::set_alpha(float value) {
  
  alpha_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.UnaryFunctionLayerParams.alpha)
}

// float epsilon = 3;
void UnaryFunctionLayerParams::clear_epsilon() {
  epsilon_ = 0;
}
float UnaryFunctionLayerParams::epsilon() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UnaryFunctionLayerParams.epsilon)
  return epsilon_;
}
void UnaryFunctionLayerParams::set_epsilon(float value) {
  
  epsilon_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.UnaryFunctionLayerParams.epsilon)
}

// float shift = 4;
void UnaryFunctionLayerParams::clear_shift() {
  shift_ = 0;
}
float UnaryFunctionLayerParams::shift() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UnaryFunctionLayerParams.shift)
  return shift_;
}
void UnaryFunctionLayerParams::set_shift(float value) {
  
  shift_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.UnaryFunctionLayerParams.shift)
}

// float scale = 5;
void UnaryFunctionLayerParams::clear_scale() {
  scale_ = 0;
}
float UnaryFunctionLayerParams::scale() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UnaryFunctionLayerParams.scale)
  return scale_;
}
void UnaryFunctionLayerParams::set_scale(float value) {
  
  scale_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.UnaryFunctionLayerParams.scale)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int UpsampleLayerParams::kScalingFactorFieldNumber;
const int UpsampleLayerParams::kModeFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

UpsampleLayerParams::UpsampleLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.UpsampleLayerParams)
}
UpsampleLayerParams::UpsampleLayerParams(const UpsampleLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      scalingfactor_(from.scalingfactor_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  mode_ = from.mode_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.UpsampleLayerParams)
}

void UpsampleLayerParams::SharedCtor() {
  mode_ = 0;
  _cached_size_ = 0;
}

UpsampleLayerParams::~UpsampleLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.UpsampleLayerParams)
  SharedDtor();
}

void UpsampleLayerParams::SharedDtor() {
}

void UpsampleLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const UpsampleLayerParams& UpsampleLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

UpsampleLayerParams* UpsampleLayerParams::New(::google::protobuf::Arena* arena) const {
  UpsampleLayerParams* n = new UpsampleLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void UpsampleLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.UpsampleLayerParams)
  scalingfactor_.Clear();
  mode_ = 0;
}

bool UpsampleLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.UpsampleLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated uint64 scalingFactor = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, this->mutable_scalingfactor())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(8u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 1, 10u, input, this->mutable_scalingfactor())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.UpsampleLayerParams.InterpolationMode mode = 5;
      case 5: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(40u)) {
          int value;
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   int, ::google::protobuf::internal::WireFormatLite::TYPE_ENUM>(
                 input, &value)));
          set_mode(static_cast< ::CoreML::Specification::UpsampleLayerParams_InterpolationMode >(value));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.UpsampleLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.UpsampleLayerParams)
  return false;
#undef DO_
}

void UpsampleLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.UpsampleLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated uint64 scalingFactor = 1;
  if (this->scalingfactor_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(1, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_scalingfactor_cached_byte_size_);
  }
  for (int i = 0, n = this->scalingfactor_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64NoTag(
      this->scalingfactor(i), output);
  }

  // .CoreML.Specification.UpsampleLayerParams.InterpolationMode mode = 5;
  if (this->mode() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteEnum(
      5, this->mode(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.UpsampleLayerParams)
}

size_t UpsampleLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.UpsampleLayerParams)
  size_t total_size = 0;

  // repeated uint64 scalingFactor = 1;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      UInt64Size(this->scalingfactor_);
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _scalingfactor_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // .CoreML.Specification.UpsampleLayerParams.InterpolationMode mode = 5;
  if (this->mode() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::EnumSize(this->mode());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void UpsampleLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const UpsampleLayerParams*>(&from));
}

void UpsampleLayerParams::MergeFrom(const UpsampleLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.UpsampleLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  scalingfactor_.MergeFrom(from.scalingfactor_);
  if (from.mode() != 0) {
    set_mode(from.mode());
  }
}

void UpsampleLayerParams::CopyFrom(const UpsampleLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.UpsampleLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool UpsampleLayerParams::IsInitialized() const {
  return true;
}

void UpsampleLayerParams::Swap(UpsampleLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void UpsampleLayerParams::InternalSwap(UpsampleLayerParams* other) {
  scalingfactor_.InternalSwap(&other->scalingfactor_);
  std::swap(mode_, other->mode_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string UpsampleLayerParams::GetTypeName() const {
  return "CoreML.Specification.UpsampleLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// UpsampleLayerParams

// repeated uint64 scalingFactor = 1;
int UpsampleLayerParams::scalingfactor_size() const {
  return scalingfactor_.size();
}
void UpsampleLayerParams::clear_scalingfactor() {
  scalingfactor_.Clear();
}
::google::protobuf::uint64 UpsampleLayerParams::scalingfactor(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UpsampleLayerParams.scalingFactor)
  return scalingfactor_.Get(index);
}
void UpsampleLayerParams::set_scalingfactor(int index, ::google::protobuf::uint64 value) {
  scalingfactor_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.UpsampleLayerParams.scalingFactor)
}
void UpsampleLayerParams::add_scalingfactor(::google::protobuf::uint64 value) {
  scalingfactor_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.UpsampleLayerParams.scalingFactor)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
UpsampleLayerParams::scalingfactor() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.UpsampleLayerParams.scalingFactor)
  return scalingfactor_;
}
::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
UpsampleLayerParams::mutable_scalingfactor() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.UpsampleLayerParams.scalingFactor)
  return &scalingfactor_;
}

// .CoreML.Specification.UpsampleLayerParams.InterpolationMode mode = 5;
void UpsampleLayerParams::clear_mode() {
  mode_ = 0;
}
::CoreML::Specification::UpsampleLayerParams_InterpolationMode UpsampleLayerParams::mode() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UpsampleLayerParams.mode)
  return static_cast< ::CoreML::Specification::UpsampleLayerParams_InterpolationMode >(mode_);
}
void UpsampleLayerParams::set_mode(::CoreML::Specification::UpsampleLayerParams_InterpolationMode value) {
  
  mode_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.UpsampleLayerParams.mode)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ResizeBilinearLayerParams::kTargetSizeFieldNumber;
const int ResizeBilinearLayerParams::kModeFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ResizeBilinearLayerParams::ResizeBilinearLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ResizeBilinearLayerParams)
}
ResizeBilinearLayerParams::ResizeBilinearLayerParams(const ResizeBilinearLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      targetsize_(from.targetsize_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_mode()) {
    mode_ = new ::CoreML::Specification::SamplingMode(*from.mode_);
  } else {
    mode_ = NULL;
  }
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ResizeBilinearLayerParams)
}

void ResizeBilinearLayerParams::SharedCtor() {
  mode_ = NULL;
  _cached_size_ = 0;
}

ResizeBilinearLayerParams::~ResizeBilinearLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ResizeBilinearLayerParams)
  SharedDtor();
}

void ResizeBilinearLayerParams::SharedDtor() {
  if (this != internal_default_instance()) {
    delete mode_;
  }
}

void ResizeBilinearLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ResizeBilinearLayerParams& ResizeBilinearLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ResizeBilinearLayerParams* ResizeBilinearLayerParams::New(::google::protobuf::Arena* arena) const {
  ResizeBilinearLayerParams* n = new ResizeBilinearLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ResizeBilinearLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ResizeBilinearLayerParams)
  targetsize_.Clear();
  if (GetArenaNoVirtual() == NULL && mode_ != NULL) {
    delete mode_;
  }
  mode_ = NULL;
}

bool ResizeBilinearLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ResizeBilinearLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated uint64 targetSize = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, this->mutable_targetsize())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(8u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 1, 10u, input, this->mutable_targetsize())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.SamplingMode mode = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(18u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_mode()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ResizeBilinearLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ResizeBilinearLayerParams)
  return false;
#undef DO_
}

void ResizeBilinearLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ResizeBilinearLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated uint64 targetSize = 1;
  if (this->targetsize_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(1, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_targetsize_cached_byte_size_);
  }
  for (int i = 0, n = this->targetsize_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64NoTag(
      this->targetsize(i), output);
  }

  // .CoreML.Specification.SamplingMode mode = 2;
  if (this->has_mode()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      2, *this->mode_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ResizeBilinearLayerParams)
}

size_t ResizeBilinearLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ResizeBilinearLayerParams)
  size_t total_size = 0;

  // repeated uint64 targetSize = 1;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      UInt64Size(this->targetsize_);
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _targetsize_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // .CoreML.Specification.SamplingMode mode = 2;
  if (this->has_mode()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->mode_);
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ResizeBilinearLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ResizeBilinearLayerParams*>(&from));
}

void ResizeBilinearLayerParams::MergeFrom(const ResizeBilinearLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ResizeBilinearLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  targetsize_.MergeFrom(from.targetsize_);
  if (from.has_mode()) {
    mutable_mode()->::CoreML::Specification::SamplingMode::MergeFrom(from.mode());
  }
}

void ResizeBilinearLayerParams::CopyFrom(const ResizeBilinearLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ResizeBilinearLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ResizeBilinearLayerParams::IsInitialized() const {
  return true;
}

void ResizeBilinearLayerParams::Swap(ResizeBilinearLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ResizeBilinearLayerParams::InternalSwap(ResizeBilinearLayerParams* other) {
  targetsize_.InternalSwap(&other->targetsize_);
  std::swap(mode_, other->mode_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ResizeBilinearLayerParams::GetTypeName() const {
  return "CoreML.Specification.ResizeBilinearLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ResizeBilinearLayerParams

// repeated uint64 targetSize = 1;
int ResizeBilinearLayerParams::targetsize_size() const {
  return targetsize_.size();
}
void ResizeBilinearLayerParams::clear_targetsize() {
  targetsize_.Clear();
}
::google::protobuf::uint64 ResizeBilinearLayerParams::targetsize(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ResizeBilinearLayerParams.targetSize)
  return targetsize_.Get(index);
}
void ResizeBilinearLayerParams::set_targetsize(int index, ::google::protobuf::uint64 value) {
  targetsize_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ResizeBilinearLayerParams.targetSize)
}
void ResizeBilinearLayerParams::add_targetsize(::google::protobuf::uint64 value) {
  targetsize_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ResizeBilinearLayerParams.targetSize)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
ResizeBilinearLayerParams::targetsize() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ResizeBilinearLayerParams.targetSize)
  return targetsize_;
}
::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
ResizeBilinearLayerParams::mutable_targetsize() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ResizeBilinearLayerParams.targetSize)
  return &targetsize_;
}

// .CoreML.Specification.SamplingMode mode = 2;
bool ResizeBilinearLayerParams::has_mode() const {
  return this != internal_default_instance() && mode_ != NULL;
}
void ResizeBilinearLayerParams::clear_mode() {
  if (GetArenaNoVirtual() == NULL && mode_ != NULL) delete mode_;
  mode_ = NULL;
}
const ::CoreML::Specification::SamplingMode& ResizeBilinearLayerParams::mode() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ResizeBilinearLayerParams.mode)
  return mode_ != NULL ? *mode_
                         : *::CoreML::Specification::SamplingMode::internal_default_instance();
}
::CoreML::Specification::SamplingMode* ResizeBilinearLayerParams::mutable_mode() {
  
  if (mode_ == NULL) {
    mode_ = new ::CoreML::Specification::SamplingMode;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ResizeBilinearLayerParams.mode)
  return mode_;
}
::CoreML::Specification::SamplingMode* ResizeBilinearLayerParams::release_mode() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ResizeBilinearLayerParams.mode)
  
  ::CoreML::Specification::SamplingMode* temp = mode_;
  mode_ = NULL;
  return temp;
}
void ResizeBilinearLayerParams::set_allocated_mode(::CoreML::Specification::SamplingMode* mode) {
  delete mode_;
  mode_ = mode;
  if (mode) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ResizeBilinearLayerParams.mode)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int CropResizeLayerParams::kTargetSizeFieldNumber;
const int CropResizeLayerParams::kNormalizedCoordinatesFieldNumber;
const int CropResizeLayerParams::kModeFieldNumber;
const int CropResizeLayerParams::kBoxIndicesModeFieldNumber;
const int CropResizeLayerParams::kSpatialScaleFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

CropResizeLayerParams::CropResizeLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.CropResizeLayerParams)
}
CropResizeLayerParams::CropResizeLayerParams(const CropResizeLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      targetsize_(from.targetsize_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_mode()) {
    mode_ = new ::CoreML::Specification::SamplingMode(*from.mode_);
  } else {
    mode_ = NULL;
  }
  if (from.has_boxindicesmode()) {
    boxindicesmode_ = new ::CoreML::Specification::BoxCoordinatesMode(*from.boxindicesmode_);
  } else {
    boxindicesmode_ = NULL;
  }
  ::memcpy(&normalizedcoordinates_, &from.normalizedcoordinates_,
    reinterpret_cast<char*>(&spatialscale_) -
    reinterpret_cast<char*>(&normalizedcoordinates_) + sizeof(spatialscale_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.CropResizeLayerParams)
}

void CropResizeLayerParams::SharedCtor() {
  ::memset(&mode_, 0, reinterpret_cast<char*>(&spatialscale_) -
    reinterpret_cast<char*>(&mode_) + sizeof(spatialscale_));
  _cached_size_ = 0;
}

CropResizeLayerParams::~CropResizeLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.CropResizeLayerParams)
  SharedDtor();
}

void CropResizeLayerParams::SharedDtor() {
  if (this != internal_default_instance()) {
    delete mode_;
  }
  if (this != internal_default_instance()) {
    delete boxindicesmode_;
  }
}

void CropResizeLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const CropResizeLayerParams& CropResizeLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

CropResizeLayerParams* CropResizeLayerParams::New(::google::protobuf::Arena* arena) const {
  CropResizeLayerParams* n = new CropResizeLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void CropResizeLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.CropResizeLayerParams)
  targetsize_.Clear();
  if (GetArenaNoVirtual() == NULL && mode_ != NULL) {
    delete mode_;
  }
  mode_ = NULL;
  if (GetArenaNoVirtual() == NULL && boxindicesmode_ != NULL) {
    delete boxindicesmode_;
  }
  boxindicesmode_ = NULL;
  ::memset(&normalizedcoordinates_, 0, reinterpret_cast<char*>(&spatialscale_) -
    reinterpret_cast<char*>(&normalizedcoordinates_) + sizeof(spatialscale_));
}

bool CropResizeLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.CropResizeLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated uint64 targetSize = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, this->mutable_targetsize())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(8u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 1, 10u, input, this->mutable_targetsize())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool normalizedCoordinates = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(16u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &normalizedcoordinates_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.SamplingMode mode = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(26u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_mode()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.BoxCoordinatesMode boxIndicesMode = 4;
      case 4: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(34u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_boxindicesmode()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // float spatialScale = 5;
      case 5: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(45u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &spatialscale_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.CropResizeLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.CropResizeLayerParams)
  return false;
#undef DO_
}

void CropResizeLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.CropResizeLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated uint64 targetSize = 1;
  if (this->targetsize_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(1, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_targetsize_cached_byte_size_);
  }
  for (int i = 0, n = this->targetsize_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64NoTag(
      this->targetsize(i), output);
  }

  // bool normalizedCoordinates = 2;
  if (this->normalizedcoordinates() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(2, this->normalizedcoordinates(), output);
  }

  // .CoreML.Specification.SamplingMode mode = 3;
  if (this->has_mode()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      3, *this->mode_, output);
  }

  // .CoreML.Specification.BoxCoordinatesMode boxIndicesMode = 4;
  if (this->has_boxindicesmode()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      4, *this->boxindicesmode_, output);
  }

  // float spatialScale = 5;
  if (this->spatialscale() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(5, this->spatialscale(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.CropResizeLayerParams)
}

size_t CropResizeLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.CropResizeLayerParams)
  size_t total_size = 0;

  // repeated uint64 targetSize = 1;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      UInt64Size(this->targetsize_);
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _targetsize_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // .CoreML.Specification.SamplingMode mode = 3;
  if (this->has_mode()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->mode_);
  }

  // .CoreML.Specification.BoxCoordinatesMode boxIndicesMode = 4;
  if (this->has_boxindicesmode()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->boxindicesmode_);
  }

  // bool normalizedCoordinates = 2;
  if (this->normalizedcoordinates() != 0) {
    total_size += 1 + 1;
  }

  // float spatialScale = 5;
  if (this->spatialscale() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void CropResizeLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const CropResizeLayerParams*>(&from));
}

void CropResizeLayerParams::MergeFrom(const CropResizeLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.CropResizeLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  targetsize_.MergeFrom(from.targetsize_);
  if (from.has_mode()) {
    mutable_mode()->::CoreML::Specification::SamplingMode::MergeFrom(from.mode());
  }
  if (from.has_boxindicesmode()) {
    mutable_boxindicesmode()->::CoreML::Specification::BoxCoordinatesMode::MergeFrom(from.boxindicesmode());
  }
  if (from.normalizedcoordinates() != 0) {
    set_normalizedcoordinates(from.normalizedcoordinates());
  }
  if (from.spatialscale() != 0) {
    set_spatialscale(from.spatialscale());
  }
}

void CropResizeLayerParams::CopyFrom(const CropResizeLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.CropResizeLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool CropResizeLayerParams::IsInitialized() const {
  return true;
}

void CropResizeLayerParams::Swap(CropResizeLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void CropResizeLayerParams::InternalSwap(CropResizeLayerParams* other) {
  targetsize_.InternalSwap(&other->targetsize_);
  std::swap(mode_, other->mode_);
  std::swap(boxindicesmode_, other->boxindicesmode_);
  std::swap(normalizedcoordinates_, other->normalizedcoordinates_);
  std::swap(spatialscale_, other->spatialscale_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string CropResizeLayerParams::GetTypeName() const {
  return "CoreML.Specification.CropResizeLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// CropResizeLayerParams

// repeated uint64 targetSize = 1;
int CropResizeLayerParams::targetsize_size() const {
  return targetsize_.size();
}
void CropResizeLayerParams::clear_targetsize() {
  targetsize_.Clear();
}
::google::protobuf::uint64 CropResizeLayerParams::targetsize(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CropResizeLayerParams.targetSize)
  return targetsize_.Get(index);
}
void CropResizeLayerParams::set_targetsize(int index, ::google::protobuf::uint64 value) {
  targetsize_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.CropResizeLayerParams.targetSize)
}
void CropResizeLayerParams::add_targetsize(::google::protobuf::uint64 value) {
  targetsize_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.CropResizeLayerParams.targetSize)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
CropResizeLayerParams::targetsize() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.CropResizeLayerParams.targetSize)
  return targetsize_;
}
::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
CropResizeLayerParams::mutable_targetsize() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.CropResizeLayerParams.targetSize)
  return &targetsize_;
}

// bool normalizedCoordinates = 2;
void CropResizeLayerParams::clear_normalizedcoordinates() {
  normalizedcoordinates_ = false;
}
bool CropResizeLayerParams::normalizedcoordinates() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CropResizeLayerParams.normalizedCoordinates)
  return normalizedcoordinates_;
}
void CropResizeLayerParams::set_normalizedcoordinates(bool value) {
  
  normalizedcoordinates_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.CropResizeLayerParams.normalizedCoordinates)
}

// .CoreML.Specification.SamplingMode mode = 3;
bool CropResizeLayerParams::has_mode() const {
  return this != internal_default_instance() && mode_ != NULL;
}
void CropResizeLayerParams::clear_mode() {
  if (GetArenaNoVirtual() == NULL && mode_ != NULL) delete mode_;
  mode_ = NULL;
}
const ::CoreML::Specification::SamplingMode& CropResizeLayerParams::mode() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CropResizeLayerParams.mode)
  return mode_ != NULL ? *mode_
                         : *::CoreML::Specification::SamplingMode::internal_default_instance();
}
::CoreML::Specification::SamplingMode* CropResizeLayerParams::mutable_mode() {
  
  if (mode_ == NULL) {
    mode_ = new ::CoreML::Specification::SamplingMode;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.CropResizeLayerParams.mode)
  return mode_;
}
::CoreML::Specification::SamplingMode* CropResizeLayerParams::release_mode() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.CropResizeLayerParams.mode)
  
  ::CoreML::Specification::SamplingMode* temp = mode_;
  mode_ = NULL;
  return temp;
}
void CropResizeLayerParams::set_allocated_mode(::CoreML::Specification::SamplingMode* mode) {
  delete mode_;
  mode_ = mode;
  if (mode) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.CropResizeLayerParams.mode)
}

// .CoreML.Specification.BoxCoordinatesMode boxIndicesMode = 4;
bool CropResizeLayerParams::has_boxindicesmode() const {
  return this != internal_default_instance() && boxindicesmode_ != NULL;
}
void CropResizeLayerParams::clear_boxindicesmode() {
  if (GetArenaNoVirtual() == NULL && boxindicesmode_ != NULL) delete boxindicesmode_;
  boxindicesmode_ = NULL;
}
const ::CoreML::Specification::BoxCoordinatesMode& CropResizeLayerParams::boxindicesmode() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CropResizeLayerParams.boxIndicesMode)
  return boxindicesmode_ != NULL ? *boxindicesmode_
                         : *::CoreML::Specification::BoxCoordinatesMode::internal_default_instance();
}
::CoreML::Specification::BoxCoordinatesMode* CropResizeLayerParams::mutable_boxindicesmode() {
  
  if (boxindicesmode_ == NULL) {
    boxindicesmode_ = new ::CoreML::Specification::BoxCoordinatesMode;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.CropResizeLayerParams.boxIndicesMode)
  return boxindicesmode_;
}
::CoreML::Specification::BoxCoordinatesMode* CropResizeLayerParams::release_boxindicesmode() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.CropResizeLayerParams.boxIndicesMode)
  
  ::CoreML::Specification::BoxCoordinatesMode* temp = boxindicesmode_;
  boxindicesmode_ = NULL;
  return temp;
}
void CropResizeLayerParams::set_allocated_boxindicesmode(::CoreML::Specification::BoxCoordinatesMode* boxindicesmode) {
  delete boxindicesmode_;
  boxindicesmode_ = boxindicesmode;
  if (boxindicesmode) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.CropResizeLayerParams.boxIndicesMode)
}

// float spatialScale = 5;
void CropResizeLayerParams::clear_spatialscale() {
  spatialscale_ = 0;
}
float CropResizeLayerParams::spatialscale() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CropResizeLayerParams.spatialScale)
  return spatialscale_;
}
void CropResizeLayerParams::set_spatialscale(float value) {
  
  spatialscale_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.CropResizeLayerParams.spatialScale)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int BiasLayerParams::kShapeFieldNumber;
const int BiasLayerParams::kBiasFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

BiasLayerParams::BiasLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.BiasLayerParams)
}
BiasLayerParams::BiasLayerParams(const BiasLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      shape_(from.shape_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_bias()) {
    bias_ = new ::CoreML::Specification::WeightParams(*from.bias_);
  } else {
    bias_ = NULL;
  }
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.BiasLayerParams)
}

void BiasLayerParams::SharedCtor() {
  bias_ = NULL;
  _cached_size_ = 0;
}

BiasLayerParams::~BiasLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.BiasLayerParams)
  SharedDtor();
}

void BiasLayerParams::SharedDtor() {
  if (this != internal_default_instance()) {
    delete bias_;
  }
}

void BiasLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const BiasLayerParams& BiasLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

BiasLayerParams* BiasLayerParams::New(::google::protobuf::Arena* arena) const {
  BiasLayerParams* n = new BiasLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void BiasLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.BiasLayerParams)
  shape_.Clear();
  if (GetArenaNoVirtual() == NULL && bias_ != NULL) {
    delete bias_;
  }
  bias_ = NULL;
}

bool BiasLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.BiasLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated uint64 shape = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, this->mutable_shape())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(8u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 1, 10u, input, this->mutable_shape())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams bias = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(18u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_bias()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.BiasLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.BiasLayerParams)
  return false;
#undef DO_
}

void BiasLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.BiasLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated uint64 shape = 1;
  if (this->shape_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(1, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_shape_cached_byte_size_);
  }
  for (int i = 0, n = this->shape_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64NoTag(
      this->shape(i), output);
  }

  // .CoreML.Specification.WeightParams bias = 2;
  if (this->has_bias()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      2, *this->bias_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.BiasLayerParams)
}

size_t BiasLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.BiasLayerParams)
  size_t total_size = 0;

  // repeated uint64 shape = 1;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      UInt64Size(this->shape_);
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _shape_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // .CoreML.Specification.WeightParams bias = 2;
  if (this->has_bias()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->bias_);
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void BiasLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const BiasLayerParams*>(&from));
}

void BiasLayerParams::MergeFrom(const BiasLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.BiasLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  shape_.MergeFrom(from.shape_);
  if (from.has_bias()) {
    mutable_bias()->::CoreML::Specification::WeightParams::MergeFrom(from.bias());
  }
}

void BiasLayerParams::CopyFrom(const BiasLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.BiasLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool BiasLayerParams::IsInitialized() const {
  return true;
}

void BiasLayerParams::Swap(BiasLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void BiasLayerParams::InternalSwap(BiasLayerParams* other) {
  shape_.InternalSwap(&other->shape_);
  std::swap(bias_, other->bias_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string BiasLayerParams::GetTypeName() const {
  return "CoreML.Specification.BiasLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// BiasLayerParams

// repeated uint64 shape = 1;
int BiasLayerParams::shape_size() const {
  return shape_.size();
}
void BiasLayerParams::clear_shape() {
  shape_.Clear();
}
::google::protobuf::uint64 BiasLayerParams::shape(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BiasLayerParams.shape)
  return shape_.Get(index);
}
void BiasLayerParams::set_shape(int index, ::google::protobuf::uint64 value) {
  shape_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.BiasLayerParams.shape)
}
void BiasLayerParams::add_shape(::google::protobuf::uint64 value) {
  shape_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.BiasLayerParams.shape)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
BiasLayerParams::shape() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.BiasLayerParams.shape)
  return shape_;
}
::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
BiasLayerParams::mutable_shape() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.BiasLayerParams.shape)
  return &shape_;
}

// .CoreML.Specification.WeightParams bias = 2;
bool BiasLayerParams::has_bias() const {
  return this != internal_default_instance() && bias_ != NULL;
}
void BiasLayerParams::clear_bias() {
  if (GetArenaNoVirtual() == NULL && bias_ != NULL) delete bias_;
  bias_ = NULL;
}
const ::CoreML::Specification::WeightParams& BiasLayerParams::bias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BiasLayerParams.bias)
  return bias_ != NULL ? *bias_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* BiasLayerParams::mutable_bias() {
  
  if (bias_ == NULL) {
    bias_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BiasLayerParams.bias)
  return bias_;
}
::CoreML::Specification::WeightParams* BiasLayerParams::release_bias() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.BiasLayerParams.bias)
  
  ::CoreML::Specification::WeightParams* temp = bias_;
  bias_ = NULL;
  return temp;
}
void BiasLayerParams::set_allocated_bias(::CoreML::Specification::WeightParams* bias) {
  delete bias_;
  bias_ = bias;
  if (bias) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.BiasLayerParams.bias)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ScaleLayerParams::kShapeScaleFieldNumber;
const int ScaleLayerParams::kScaleFieldNumber;
const int ScaleLayerParams::kHasBiasFieldNumber;
const int ScaleLayerParams::kShapeBiasFieldNumber;
const int ScaleLayerParams::kBiasFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ScaleLayerParams::ScaleLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ScaleLayerParams)
}
ScaleLayerParams::ScaleLayerParams(const ScaleLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      shapescale_(from.shapescale_),
      shapebias_(from.shapebias_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_scale()) {
    scale_ = new ::CoreML::Specification::WeightParams(*from.scale_);
  } else {
    scale_ = NULL;
  }
  if (from.has_bias()) {
    bias_ = new ::CoreML::Specification::WeightParams(*from.bias_);
  } else {
    bias_ = NULL;
  }
  hasbias_ = from.hasbias_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ScaleLayerParams)
}

void ScaleLayerParams::SharedCtor() {
  ::memset(&scale_, 0, reinterpret_cast<char*>(&hasbias_) -
    reinterpret_cast<char*>(&scale_) + sizeof(hasbias_));
  _cached_size_ = 0;
}

ScaleLayerParams::~ScaleLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ScaleLayerParams)
  SharedDtor();
}

void ScaleLayerParams::SharedDtor() {
  if (this != internal_default_instance()) {
    delete scale_;
  }
  if (this != internal_default_instance()) {
    delete bias_;
  }
}

void ScaleLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ScaleLayerParams& ScaleLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ScaleLayerParams* ScaleLayerParams::New(::google::protobuf::Arena* arena) const {
  ScaleLayerParams* n = new ScaleLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ScaleLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ScaleLayerParams)
  shapescale_.Clear();
  shapebias_.Clear();
  if (GetArenaNoVirtual() == NULL && scale_ != NULL) {
    delete scale_;
  }
  scale_ = NULL;
  if (GetArenaNoVirtual() == NULL && bias_ != NULL) {
    delete bias_;
  }
  bias_ = NULL;
  hasbias_ = false;
}

bool ScaleLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ScaleLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated uint64 shapeScale = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, this->mutable_shapescale())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(8u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 1, 10u, input, this->mutable_shapescale())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams scale = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(18u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_scale()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool hasBias = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(24u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &hasbias_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated uint64 shapeBias = 4;
      case 4: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(34u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, this->mutable_shapebias())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(32u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 1, 34u, input, this->mutable_shapebias())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams bias = 5;
      case 5: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(42u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_bias()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ScaleLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ScaleLayerParams)
  return false;
#undef DO_
}

void ScaleLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ScaleLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated uint64 shapeScale = 1;
  if (this->shapescale_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(1, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_shapescale_cached_byte_size_);
  }
  for (int i = 0, n = this->shapescale_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64NoTag(
      this->shapescale(i), output);
  }

  // .CoreML.Specification.WeightParams scale = 2;
  if (this->has_scale()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      2, *this->scale_, output);
  }

  // bool hasBias = 3;
  if (this->hasbias() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(3, this->hasbias(), output);
  }

  // repeated uint64 shapeBias = 4;
  if (this->shapebias_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(4, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_shapebias_cached_byte_size_);
  }
  for (int i = 0, n = this->shapebias_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64NoTag(
      this->shapebias(i), output);
  }

  // .CoreML.Specification.WeightParams bias = 5;
  if (this->has_bias()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      5, *this->bias_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ScaleLayerParams)
}

size_t ScaleLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ScaleLayerParams)
  size_t total_size = 0;

  // repeated uint64 shapeScale = 1;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      UInt64Size(this->shapescale_);
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _shapescale_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // repeated uint64 shapeBias = 4;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      UInt64Size(this->shapebias_);
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _shapebias_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // .CoreML.Specification.WeightParams scale = 2;
  if (this->has_scale()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->scale_);
  }

  // .CoreML.Specification.WeightParams bias = 5;
  if (this->has_bias()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->bias_);
  }

  // bool hasBias = 3;
  if (this->hasbias() != 0) {
    total_size += 1 + 1;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ScaleLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ScaleLayerParams*>(&from));
}

void ScaleLayerParams::MergeFrom(const ScaleLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ScaleLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  shapescale_.MergeFrom(from.shapescale_);
  shapebias_.MergeFrom(from.shapebias_);
  if (from.has_scale()) {
    mutable_scale()->::CoreML::Specification::WeightParams::MergeFrom(from.scale());
  }
  if (from.has_bias()) {
    mutable_bias()->::CoreML::Specification::WeightParams::MergeFrom(from.bias());
  }
  if (from.hasbias() != 0) {
    set_hasbias(from.hasbias());
  }
}

void ScaleLayerParams::CopyFrom(const ScaleLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ScaleLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ScaleLayerParams::IsInitialized() const {
  return true;
}

void ScaleLayerParams::Swap(ScaleLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ScaleLayerParams::InternalSwap(ScaleLayerParams* other) {
  shapescale_.InternalSwap(&other->shapescale_);
  shapebias_.InternalSwap(&other->shapebias_);
  std::swap(scale_, other->scale_);
  std::swap(bias_, other->bias_);
  std::swap(hasbias_, other->hasbias_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ScaleLayerParams::GetTypeName() const {
  return "CoreML.Specification.ScaleLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ScaleLayerParams

// repeated uint64 shapeScale = 1;
int ScaleLayerParams::shapescale_size() const {
  return shapescale_.size();
}
void ScaleLayerParams::clear_shapescale() {
  shapescale_.Clear();
}
::google::protobuf::uint64 ScaleLayerParams::shapescale(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ScaleLayerParams.shapeScale)
  return shapescale_.Get(index);
}
void ScaleLayerParams::set_shapescale(int index, ::google::protobuf::uint64 value) {
  shapescale_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ScaleLayerParams.shapeScale)
}
void ScaleLayerParams::add_shapescale(::google::protobuf::uint64 value) {
  shapescale_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ScaleLayerParams.shapeScale)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
ScaleLayerParams::shapescale() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ScaleLayerParams.shapeScale)
  return shapescale_;
}
::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
ScaleLayerParams::mutable_shapescale() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ScaleLayerParams.shapeScale)
  return &shapescale_;
}

// .CoreML.Specification.WeightParams scale = 2;
bool ScaleLayerParams::has_scale() const {
  return this != internal_default_instance() && scale_ != NULL;
}
void ScaleLayerParams::clear_scale() {
  if (GetArenaNoVirtual() == NULL && scale_ != NULL) delete scale_;
  scale_ = NULL;
}
const ::CoreML::Specification::WeightParams& ScaleLayerParams::scale() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ScaleLayerParams.scale)
  return scale_ != NULL ? *scale_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* ScaleLayerParams::mutable_scale() {
  
  if (scale_ == NULL) {
    scale_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ScaleLayerParams.scale)
  return scale_;
}
::CoreML::Specification::WeightParams* ScaleLayerParams::release_scale() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ScaleLayerParams.scale)
  
  ::CoreML::Specification::WeightParams* temp = scale_;
  scale_ = NULL;
  return temp;
}
void ScaleLayerParams::set_allocated_scale(::CoreML::Specification::WeightParams* scale) {
  delete scale_;
  scale_ = scale;
  if (scale) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ScaleLayerParams.scale)
}

// bool hasBias = 3;
void ScaleLayerParams::clear_hasbias() {
  hasbias_ = false;
}
bool ScaleLayerParams::hasbias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ScaleLayerParams.hasBias)
  return hasbias_;
}
void ScaleLayerParams::set_hasbias(bool value) {
  
  hasbias_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ScaleLayerParams.hasBias)
}

// repeated uint64 shapeBias = 4;
int ScaleLayerParams::shapebias_size() const {
  return shapebias_.size();
}
void ScaleLayerParams::clear_shapebias() {
  shapebias_.Clear();
}
::google::protobuf::uint64 ScaleLayerParams::shapebias(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ScaleLayerParams.shapeBias)
  return shapebias_.Get(index);
}
void ScaleLayerParams::set_shapebias(int index, ::google::protobuf::uint64 value) {
  shapebias_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ScaleLayerParams.shapeBias)
}
void ScaleLayerParams::add_shapebias(::google::protobuf::uint64 value) {
  shapebias_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ScaleLayerParams.shapeBias)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
ScaleLayerParams::shapebias() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ScaleLayerParams.shapeBias)
  return shapebias_;
}
::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
ScaleLayerParams::mutable_shapebias() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ScaleLayerParams.shapeBias)
  return &shapebias_;
}

// .CoreML.Specification.WeightParams bias = 5;
bool ScaleLayerParams::has_bias() const {
  return this != internal_default_instance() && bias_ != NULL;
}
void ScaleLayerParams::clear_bias() {
  if (GetArenaNoVirtual() == NULL && bias_ != NULL) delete bias_;
  bias_ = NULL;
}
const ::CoreML::Specification::WeightParams& ScaleLayerParams::bias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ScaleLayerParams.bias)
  return bias_ != NULL ? *bias_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* ScaleLayerParams::mutable_bias() {
  
  if (bias_ == NULL) {
    bias_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.ScaleLayerParams.bias)
  return bias_;
}
::CoreML::Specification::WeightParams* ScaleLayerParams::release_bias() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.ScaleLayerParams.bias)
  
  ::CoreML::Specification::WeightParams* temp = bias_;
  bias_ = NULL;
  return temp;
}
void ScaleLayerParams::set_allocated_bias(::CoreML::Specification::WeightParams* bias) {
  delete bias_;
  bias_ = bias;
  if (bias) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.ScaleLayerParams.bias)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int LoadConstantLayerParams::kShapeFieldNumber;
const int LoadConstantLayerParams::kDataFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

LoadConstantLayerParams::LoadConstantLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.LoadConstantLayerParams)
}
LoadConstantLayerParams::LoadConstantLayerParams(const LoadConstantLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      shape_(from.shape_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_data()) {
    data_ = new ::CoreML::Specification::WeightParams(*from.data_);
  } else {
    data_ = NULL;
  }
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.LoadConstantLayerParams)
}

void LoadConstantLayerParams::SharedCtor() {
  data_ = NULL;
  _cached_size_ = 0;
}

LoadConstantLayerParams::~LoadConstantLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.LoadConstantLayerParams)
  SharedDtor();
}

void LoadConstantLayerParams::SharedDtor() {
  if (this != internal_default_instance()) {
    delete data_;
  }
}

void LoadConstantLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const LoadConstantLayerParams& LoadConstantLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

LoadConstantLayerParams* LoadConstantLayerParams::New(::google::protobuf::Arena* arena) const {
  LoadConstantLayerParams* n = new LoadConstantLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void LoadConstantLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.LoadConstantLayerParams)
  shape_.Clear();
  if (GetArenaNoVirtual() == NULL && data_ != NULL) {
    delete data_;
  }
  data_ = NULL;
}

bool LoadConstantLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.LoadConstantLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated uint64 shape = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, this->mutable_shape())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(8u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 1, 10u, input, this->mutable_shape())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams data = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(18u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_data()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.LoadConstantLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.LoadConstantLayerParams)
  return false;
#undef DO_
}

void LoadConstantLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.LoadConstantLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated uint64 shape = 1;
  if (this->shape_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(1, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_shape_cached_byte_size_);
  }
  for (int i = 0, n = this->shape_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64NoTag(
      this->shape(i), output);
  }

  // .CoreML.Specification.WeightParams data = 2;
  if (this->has_data()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      2, *this->data_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.LoadConstantLayerParams)
}

size_t LoadConstantLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.LoadConstantLayerParams)
  size_t total_size = 0;

  // repeated uint64 shape = 1;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      UInt64Size(this->shape_);
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _shape_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // .CoreML.Specification.WeightParams data = 2;
  if (this->has_data()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->data_);
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void LoadConstantLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const LoadConstantLayerParams*>(&from));
}

void LoadConstantLayerParams::MergeFrom(const LoadConstantLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.LoadConstantLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  shape_.MergeFrom(from.shape_);
  if (from.has_data()) {
    mutable_data()->::CoreML::Specification::WeightParams::MergeFrom(from.data());
  }
}

void LoadConstantLayerParams::CopyFrom(const LoadConstantLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.LoadConstantLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool LoadConstantLayerParams::IsInitialized() const {
  return true;
}

void LoadConstantLayerParams::Swap(LoadConstantLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void LoadConstantLayerParams::InternalSwap(LoadConstantLayerParams* other) {
  shape_.InternalSwap(&other->shape_);
  std::swap(data_, other->data_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string LoadConstantLayerParams::GetTypeName() const {
  return "CoreML.Specification.LoadConstantLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// LoadConstantLayerParams

// repeated uint64 shape = 1;
int LoadConstantLayerParams::shape_size() const {
  return shape_.size();
}
void LoadConstantLayerParams::clear_shape() {
  shape_.Clear();
}
::google::protobuf::uint64 LoadConstantLayerParams::shape(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LoadConstantLayerParams.shape)
  return shape_.Get(index);
}
void LoadConstantLayerParams::set_shape(int index, ::google::protobuf::uint64 value) {
  shape_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.LoadConstantLayerParams.shape)
}
void LoadConstantLayerParams::add_shape(::google::protobuf::uint64 value) {
  shape_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.LoadConstantLayerParams.shape)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
LoadConstantLayerParams::shape() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.LoadConstantLayerParams.shape)
  return shape_;
}
::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
LoadConstantLayerParams::mutable_shape() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.LoadConstantLayerParams.shape)
  return &shape_;
}

// .CoreML.Specification.WeightParams data = 2;
bool LoadConstantLayerParams::has_data() const {
  return this != internal_default_instance() && data_ != NULL;
}
void LoadConstantLayerParams::clear_data() {
  if (GetArenaNoVirtual() == NULL && data_ != NULL) delete data_;
  data_ = NULL;
}
const ::CoreML::Specification::WeightParams& LoadConstantLayerParams::data() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LoadConstantLayerParams.data)
  return data_ != NULL ? *data_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* LoadConstantLayerParams::mutable_data() {
  
  if (data_ == NULL) {
    data_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LoadConstantLayerParams.data)
  return data_;
}
::CoreML::Specification::WeightParams* LoadConstantLayerParams::release_data() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LoadConstantLayerParams.data)
  
  ::CoreML::Specification::WeightParams* temp = data_;
  data_ = NULL;
  return temp;
}
void LoadConstantLayerParams::set_allocated_data(::CoreML::Specification::WeightParams* data) {
  delete data_;
  data_ = data;
  if (data) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LoadConstantLayerParams.data)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int L2NormalizeLayerParams::kEpsilonFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

L2NormalizeLayerParams::L2NormalizeLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.L2NormalizeLayerParams)
}
L2NormalizeLayerParams::L2NormalizeLayerParams(const L2NormalizeLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  epsilon_ = from.epsilon_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.L2NormalizeLayerParams)
}

void L2NormalizeLayerParams::SharedCtor() {
  epsilon_ = 0;
  _cached_size_ = 0;
}

L2NormalizeLayerParams::~L2NormalizeLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.L2NormalizeLayerParams)
  SharedDtor();
}

void L2NormalizeLayerParams::SharedDtor() {
}

void L2NormalizeLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const L2NormalizeLayerParams& L2NormalizeLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

L2NormalizeLayerParams* L2NormalizeLayerParams::New(::google::protobuf::Arena* arena) const {
  L2NormalizeLayerParams* n = new L2NormalizeLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void L2NormalizeLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.L2NormalizeLayerParams)
  epsilon_ = 0;
}

bool L2NormalizeLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.L2NormalizeLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // float epsilon = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(13u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &epsilon_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.L2NormalizeLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.L2NormalizeLayerParams)
  return false;
#undef DO_
}

void L2NormalizeLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.L2NormalizeLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // float epsilon = 1;
  if (this->epsilon() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(1, this->epsilon(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.L2NormalizeLayerParams)
}

size_t L2NormalizeLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.L2NormalizeLayerParams)
  size_t total_size = 0;

  // float epsilon = 1;
  if (this->epsilon() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void L2NormalizeLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const L2NormalizeLayerParams*>(&from));
}

void L2NormalizeLayerParams::MergeFrom(const L2NormalizeLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.L2NormalizeLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.epsilon() != 0) {
    set_epsilon(from.epsilon());
  }
}

void L2NormalizeLayerParams::CopyFrom(const L2NormalizeLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.L2NormalizeLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool L2NormalizeLayerParams::IsInitialized() const {
  return true;
}

void L2NormalizeLayerParams::Swap(L2NormalizeLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void L2NormalizeLayerParams::InternalSwap(L2NormalizeLayerParams* other) {
  std::swap(epsilon_, other->epsilon_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string L2NormalizeLayerParams::GetTypeName() const {
  return "CoreML.Specification.L2NormalizeLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// L2NormalizeLayerParams

// float epsilon = 1;
void L2NormalizeLayerParams::clear_epsilon() {
  epsilon_ = 0;
}
float L2NormalizeLayerParams::epsilon() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.L2NormalizeLayerParams.epsilon)
  return epsilon_;
}
void L2NormalizeLayerParams::set_epsilon(float value) {
  
  epsilon_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.L2NormalizeLayerParams.epsilon)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int FlattenLayerParams::kModeFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

FlattenLayerParams::FlattenLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.FlattenLayerParams)
}
FlattenLayerParams::FlattenLayerParams(const FlattenLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  mode_ = from.mode_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.FlattenLayerParams)
}

void FlattenLayerParams::SharedCtor() {
  mode_ = 0;
  _cached_size_ = 0;
}

FlattenLayerParams::~FlattenLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.FlattenLayerParams)
  SharedDtor();
}

void FlattenLayerParams::SharedDtor() {
}

void FlattenLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const FlattenLayerParams& FlattenLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

FlattenLayerParams* FlattenLayerParams::New(::google::protobuf::Arena* arena) const {
  FlattenLayerParams* n = new FlattenLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void FlattenLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.FlattenLayerParams)
  mode_ = 0;
}

bool FlattenLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.FlattenLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // .CoreML.Specification.FlattenLayerParams.FlattenOrder mode = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {
          int value;
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   int, ::google::protobuf::internal::WireFormatLite::TYPE_ENUM>(
                 input, &value)));
          set_mode(static_cast< ::CoreML::Specification::FlattenLayerParams_FlattenOrder >(value));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.FlattenLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.FlattenLayerParams)
  return false;
#undef DO_
}

void FlattenLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.FlattenLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // .CoreML.Specification.FlattenLayerParams.FlattenOrder mode = 1;
  if (this->mode() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteEnum(
      1, this->mode(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.FlattenLayerParams)
}

size_t FlattenLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.FlattenLayerParams)
  size_t total_size = 0;

  // .CoreML.Specification.FlattenLayerParams.FlattenOrder mode = 1;
  if (this->mode() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::EnumSize(this->mode());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void FlattenLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const FlattenLayerParams*>(&from));
}

void FlattenLayerParams::MergeFrom(const FlattenLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.FlattenLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.mode() != 0) {
    set_mode(from.mode());
  }
}

void FlattenLayerParams::CopyFrom(const FlattenLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.FlattenLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool FlattenLayerParams::IsInitialized() const {
  return true;
}

void FlattenLayerParams::Swap(FlattenLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void FlattenLayerParams::InternalSwap(FlattenLayerParams* other) {
  std::swap(mode_, other->mode_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string FlattenLayerParams::GetTypeName() const {
  return "CoreML.Specification.FlattenLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// FlattenLayerParams

// .CoreML.Specification.FlattenLayerParams.FlattenOrder mode = 1;
void FlattenLayerParams::clear_mode() {
  mode_ = 0;
}
::CoreML::Specification::FlattenLayerParams_FlattenOrder FlattenLayerParams::mode() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.FlattenLayerParams.mode)
  return static_cast< ::CoreML::Specification::FlattenLayerParams_FlattenOrder >(mode_);
}
void FlattenLayerParams::set_mode(::CoreML::Specification::FlattenLayerParams_FlattenOrder value) {
  
  mode_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.FlattenLayerParams.mode)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ReshapeLayerParams::kTargetShapeFieldNumber;
const int ReshapeLayerParams::kModeFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ReshapeLayerParams::ReshapeLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ReshapeLayerParams)
}
ReshapeLayerParams::ReshapeLayerParams(const ReshapeLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      targetshape_(from.targetshape_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  mode_ = from.mode_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ReshapeLayerParams)
}

void ReshapeLayerParams::SharedCtor() {
  mode_ = 0;
  _cached_size_ = 0;
}

ReshapeLayerParams::~ReshapeLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ReshapeLayerParams)
  SharedDtor();
}

void ReshapeLayerParams::SharedDtor() {
}

void ReshapeLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ReshapeLayerParams& ReshapeLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ReshapeLayerParams* ReshapeLayerParams::New(::google::protobuf::Arena* arena) const {
  ReshapeLayerParams* n = new ReshapeLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ReshapeLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ReshapeLayerParams)
  targetshape_.Clear();
  mode_ = 0;
}

bool ReshapeLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ReshapeLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated int64 targetShape = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 input, this->mutable_targetshape())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(8u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 1, 10u, input, this->mutable_targetshape())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ReshapeLayerParams.ReshapeOrder mode = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(16u)) {
          int value;
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   int, ::google::protobuf::internal::WireFormatLite::TYPE_ENUM>(
                 input, &value)));
          set_mode(static_cast< ::CoreML::Specification::ReshapeLayerParams_ReshapeOrder >(value));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ReshapeLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ReshapeLayerParams)
  return false;
#undef DO_
}

void ReshapeLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ReshapeLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated int64 targetShape = 1;
  if (this->targetshape_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(1, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_targetshape_cached_byte_size_);
  }
  for (int i = 0, n = this->targetshape_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteInt64NoTag(
      this->targetshape(i), output);
  }

  // .CoreML.Specification.ReshapeLayerParams.ReshapeOrder mode = 2;
  if (this->mode() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteEnum(
      2, this->mode(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ReshapeLayerParams)
}

size_t ReshapeLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ReshapeLayerParams)
  size_t total_size = 0;

  // repeated int64 targetShape = 1;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      Int64Size(this->targetshape_);
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _targetshape_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // .CoreML.Specification.ReshapeLayerParams.ReshapeOrder mode = 2;
  if (this->mode() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::EnumSize(this->mode());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ReshapeLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ReshapeLayerParams*>(&from));
}

void ReshapeLayerParams::MergeFrom(const ReshapeLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ReshapeLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  targetshape_.MergeFrom(from.targetshape_);
  if (from.mode() != 0) {
    set_mode(from.mode());
  }
}

void ReshapeLayerParams::CopyFrom(const ReshapeLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ReshapeLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ReshapeLayerParams::IsInitialized() const {
  return true;
}

void ReshapeLayerParams::Swap(ReshapeLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ReshapeLayerParams::InternalSwap(ReshapeLayerParams* other) {
  targetshape_.InternalSwap(&other->targetshape_);
  std::swap(mode_, other->mode_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ReshapeLayerParams::GetTypeName() const {
  return "CoreML.Specification.ReshapeLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ReshapeLayerParams

// repeated int64 targetShape = 1;
int ReshapeLayerParams::targetshape_size() const {
  return targetshape_.size();
}
void ReshapeLayerParams::clear_targetshape() {
  targetshape_.Clear();
}
::google::protobuf::int64 ReshapeLayerParams::targetshape(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReshapeLayerParams.targetShape)
  return targetshape_.Get(index);
}
void ReshapeLayerParams::set_targetshape(int index, ::google::protobuf::int64 value) {
  targetshape_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReshapeLayerParams.targetShape)
}
void ReshapeLayerParams::add_targetshape(::google::protobuf::int64 value) {
  targetshape_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ReshapeLayerParams.targetShape)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
ReshapeLayerParams::targetshape() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ReshapeLayerParams.targetShape)
  return targetshape_;
}
::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
ReshapeLayerParams::mutable_targetshape() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ReshapeLayerParams.targetShape)
  return &targetshape_;
}

// .CoreML.Specification.ReshapeLayerParams.ReshapeOrder mode = 2;
void ReshapeLayerParams::clear_mode() {
  mode_ = 0;
}
::CoreML::Specification::ReshapeLayerParams_ReshapeOrder ReshapeLayerParams::mode() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReshapeLayerParams.mode)
  return static_cast< ::CoreML::Specification::ReshapeLayerParams_ReshapeOrder >(mode_);
}
void ReshapeLayerParams::set_mode(::CoreML::Specification::ReshapeLayerParams_ReshapeOrder value) {
  
  mode_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReshapeLayerParams.mode)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int PermuteLayerParams::kAxisFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

PermuteLayerParams::PermuteLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.PermuteLayerParams)
}
PermuteLayerParams::PermuteLayerParams(const PermuteLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      axis_(from.axis_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.PermuteLayerParams)
}

void PermuteLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

PermuteLayerParams::~PermuteLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.PermuteLayerParams)
  SharedDtor();
}

void PermuteLayerParams::SharedDtor() {
}

void PermuteLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const PermuteLayerParams& PermuteLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

PermuteLayerParams* PermuteLayerParams::New(::google::protobuf::Arena* arena) const {
  PermuteLayerParams* n = new PermuteLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void PermuteLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.PermuteLayerParams)
  axis_.Clear();
}

bool PermuteLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.PermuteLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated uint64 axis = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, this->mutable_axis())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(8u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 1, 10u, input, this->mutable_axis())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.PermuteLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.PermuteLayerParams)
  return false;
#undef DO_
}

void PermuteLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.PermuteLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated uint64 axis = 1;
  if (this->axis_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(1, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_axis_cached_byte_size_);
  }
  for (int i = 0, n = this->axis_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64NoTag(
      this->axis(i), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.PermuteLayerParams)
}

size_t PermuteLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.PermuteLayerParams)
  size_t total_size = 0;

  // repeated uint64 axis = 1;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      UInt64Size(this->axis_);
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _axis_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void PermuteLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const PermuteLayerParams*>(&from));
}

void PermuteLayerParams::MergeFrom(const PermuteLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.PermuteLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  axis_.MergeFrom(from.axis_);
}

void PermuteLayerParams::CopyFrom(const PermuteLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.PermuteLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool PermuteLayerParams::IsInitialized() const {
  return true;
}

void PermuteLayerParams::Swap(PermuteLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void PermuteLayerParams::InternalSwap(PermuteLayerParams* other) {
  axis_.InternalSwap(&other->axis_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string PermuteLayerParams::GetTypeName() const {
  return "CoreML.Specification.PermuteLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// PermuteLayerParams

// repeated uint64 axis = 1;
int PermuteLayerParams::axis_size() const {
  return axis_.size();
}
void PermuteLayerParams::clear_axis() {
  axis_.Clear();
}
::google::protobuf::uint64 PermuteLayerParams::axis(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.PermuteLayerParams.axis)
  return axis_.Get(index);
}
void PermuteLayerParams::set_axis(int index, ::google::protobuf::uint64 value) {
  axis_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.PermuteLayerParams.axis)
}
void PermuteLayerParams::add_axis(::google::protobuf::uint64 value) {
  axis_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.PermuteLayerParams.axis)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
PermuteLayerParams::axis() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.PermuteLayerParams.axis)
  return axis_;
}
::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
PermuteLayerParams::mutable_axis() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.PermuteLayerParams.axis)
  return &axis_;
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ReorganizeDataLayerParams::kModeFieldNumber;
const int ReorganizeDataLayerParams::kBlockSizeFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ReorganizeDataLayerParams::ReorganizeDataLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ReorganizeDataLayerParams)
}
ReorganizeDataLayerParams::ReorganizeDataLayerParams(const ReorganizeDataLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::memcpy(&blocksize_, &from.blocksize_,
    reinterpret_cast<char*>(&mode_) -
    reinterpret_cast<char*>(&blocksize_) + sizeof(mode_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ReorganizeDataLayerParams)
}

void ReorganizeDataLayerParams::SharedCtor() {
  ::memset(&blocksize_, 0, reinterpret_cast<char*>(&mode_) -
    reinterpret_cast<char*>(&blocksize_) + sizeof(mode_));
  _cached_size_ = 0;
}

ReorganizeDataLayerParams::~ReorganizeDataLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ReorganizeDataLayerParams)
  SharedDtor();
}

void ReorganizeDataLayerParams::SharedDtor() {
}

void ReorganizeDataLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ReorganizeDataLayerParams& ReorganizeDataLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ReorganizeDataLayerParams* ReorganizeDataLayerParams::New(::google::protobuf::Arena* arena) const {
  ReorganizeDataLayerParams* n = new ReorganizeDataLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ReorganizeDataLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ReorganizeDataLayerParams)
  ::memset(&blocksize_, 0, reinterpret_cast<char*>(&mode_) -
    reinterpret_cast<char*>(&blocksize_) + sizeof(mode_));
}

bool ReorganizeDataLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ReorganizeDataLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // .CoreML.Specification.ReorganizeDataLayerParams.ReorganizationType mode = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {
          int value;
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   int, ::google::protobuf::internal::WireFormatLite::TYPE_ENUM>(
                 input, &value)));
          set_mode(static_cast< ::CoreML::Specification::ReorganizeDataLayerParams_ReorganizationType >(value));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // uint64 blockSize = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(16u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &blocksize_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ReorganizeDataLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ReorganizeDataLayerParams)
  return false;
#undef DO_
}

void ReorganizeDataLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ReorganizeDataLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // .CoreML.Specification.ReorganizeDataLayerParams.ReorganizationType mode = 1;
  if (this->mode() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteEnum(
      1, this->mode(), output);
  }

  // uint64 blockSize = 2;
  if (this->blocksize() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(2, this->blocksize(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ReorganizeDataLayerParams)
}

size_t ReorganizeDataLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ReorganizeDataLayerParams)
  size_t total_size = 0;

  // uint64 blockSize = 2;
  if (this->blocksize() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->blocksize());
  }

  // .CoreML.Specification.ReorganizeDataLayerParams.ReorganizationType mode = 1;
  if (this->mode() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::EnumSize(this->mode());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ReorganizeDataLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ReorganizeDataLayerParams*>(&from));
}

void ReorganizeDataLayerParams::MergeFrom(const ReorganizeDataLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ReorganizeDataLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.blocksize() != 0) {
    set_blocksize(from.blocksize());
  }
  if (from.mode() != 0) {
    set_mode(from.mode());
  }
}

void ReorganizeDataLayerParams::CopyFrom(const ReorganizeDataLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ReorganizeDataLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ReorganizeDataLayerParams::IsInitialized() const {
  return true;
}

void ReorganizeDataLayerParams::Swap(ReorganizeDataLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ReorganizeDataLayerParams::InternalSwap(ReorganizeDataLayerParams* other) {
  std::swap(blocksize_, other->blocksize_);
  std::swap(mode_, other->mode_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ReorganizeDataLayerParams::GetTypeName() const {
  return "CoreML.Specification.ReorganizeDataLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ReorganizeDataLayerParams

// .CoreML.Specification.ReorganizeDataLayerParams.ReorganizationType mode = 1;
void ReorganizeDataLayerParams::clear_mode() {
  mode_ = 0;
}
::CoreML::Specification::ReorganizeDataLayerParams_ReorganizationType ReorganizeDataLayerParams::mode() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReorganizeDataLayerParams.mode)
  return static_cast< ::CoreML::Specification::ReorganizeDataLayerParams_ReorganizationType >(mode_);
}
void ReorganizeDataLayerParams::set_mode(::CoreML::Specification::ReorganizeDataLayerParams_ReorganizationType value) {
  
  mode_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReorganizeDataLayerParams.mode)
}

// uint64 blockSize = 2;
void ReorganizeDataLayerParams::clear_blocksize() {
  blocksize_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 ReorganizeDataLayerParams::blocksize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReorganizeDataLayerParams.blockSize)
  return blocksize_;
}
void ReorganizeDataLayerParams::set_blocksize(::google::protobuf::uint64 value) {
  
  blocksize_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReorganizeDataLayerParams.blockSize)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int SliceLayerParams::kStartIndexFieldNumber;
const int SliceLayerParams::kEndIndexFieldNumber;
const int SliceLayerParams::kStrideFieldNumber;
const int SliceLayerParams::kAxisFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

SliceLayerParams::SliceLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.SliceLayerParams)
}
SliceLayerParams::SliceLayerParams(const SliceLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::memcpy(&startindex_, &from.startindex_,
    reinterpret_cast<char*>(&axis_) -
    reinterpret_cast<char*>(&startindex_) + sizeof(axis_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.SliceLayerParams)
}

void SliceLayerParams::SharedCtor() {
  ::memset(&startindex_, 0, reinterpret_cast<char*>(&axis_) -
    reinterpret_cast<char*>(&startindex_) + sizeof(axis_));
  _cached_size_ = 0;
}

SliceLayerParams::~SliceLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.SliceLayerParams)
  SharedDtor();
}

void SliceLayerParams::SharedDtor() {
}

void SliceLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const SliceLayerParams& SliceLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

SliceLayerParams* SliceLayerParams::New(::google::protobuf::Arena* arena) const {
  SliceLayerParams* n = new SliceLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void SliceLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.SliceLayerParams)
  ::memset(&startindex_, 0, reinterpret_cast<char*>(&axis_) -
    reinterpret_cast<char*>(&startindex_) + sizeof(axis_));
}

bool SliceLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.SliceLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // int64 startIndex = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 input, &startindex_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // int64 endIndex = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(16u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 input, &endindex_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // uint64 stride = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(24u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &stride_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.SliceLayerParams.SliceAxis axis = 4;
      case 4: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(32u)) {
          int value;
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   int, ::google::protobuf::internal::WireFormatLite::TYPE_ENUM>(
                 input, &value)));
          set_axis(static_cast< ::CoreML::Specification::SliceLayerParams_SliceAxis >(value));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.SliceLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.SliceLayerParams)
  return false;
#undef DO_
}

void SliceLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.SliceLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // int64 startIndex = 1;
  if (this->startindex() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteInt64(1, this->startindex(), output);
  }

  // int64 endIndex = 2;
  if (this->endindex() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteInt64(2, this->endindex(), output);
  }

  // uint64 stride = 3;
  if (this->stride() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(3, this->stride(), output);
  }

  // .CoreML.Specification.SliceLayerParams.SliceAxis axis = 4;
  if (this->axis() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteEnum(
      4, this->axis(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.SliceLayerParams)
}

size_t SliceLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.SliceLayerParams)
  size_t total_size = 0;

  // int64 startIndex = 1;
  if (this->startindex() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::Int64Size(
        this->startindex());
  }

  // int64 endIndex = 2;
  if (this->endindex() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::Int64Size(
        this->endindex());
  }

  // uint64 stride = 3;
  if (this->stride() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->stride());
  }

  // .CoreML.Specification.SliceLayerParams.SliceAxis axis = 4;
  if (this->axis() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::EnumSize(this->axis());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void SliceLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const SliceLayerParams*>(&from));
}

void SliceLayerParams::MergeFrom(const SliceLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.SliceLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.startindex() != 0) {
    set_startindex(from.startindex());
  }
  if (from.endindex() != 0) {
    set_endindex(from.endindex());
  }
  if (from.stride() != 0) {
    set_stride(from.stride());
  }
  if (from.axis() != 0) {
    set_axis(from.axis());
  }
}

void SliceLayerParams::CopyFrom(const SliceLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.SliceLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool SliceLayerParams::IsInitialized() const {
  return true;
}

void SliceLayerParams::Swap(SliceLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void SliceLayerParams::InternalSwap(SliceLayerParams* other) {
  std::swap(startindex_, other->startindex_);
  std::swap(endindex_, other->endindex_);
  std::swap(stride_, other->stride_);
  std::swap(axis_, other->axis_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string SliceLayerParams::GetTypeName() const {
  return "CoreML.Specification.SliceLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// SliceLayerParams

// int64 startIndex = 1;
void SliceLayerParams::clear_startindex() {
  startindex_ = GOOGLE_LONGLONG(0);
}
::google::protobuf::int64 SliceLayerParams::startindex() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SliceLayerParams.startIndex)
  return startindex_;
}
void SliceLayerParams::set_startindex(::google::protobuf::int64 value) {
  
  startindex_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SliceLayerParams.startIndex)
}

// int64 endIndex = 2;
void SliceLayerParams::clear_endindex() {
  endindex_ = GOOGLE_LONGLONG(0);
}
::google::protobuf::int64 SliceLayerParams::endindex() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SliceLayerParams.endIndex)
  return endindex_;
}
void SliceLayerParams::set_endindex(::google::protobuf::int64 value) {
  
  endindex_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SliceLayerParams.endIndex)
}

// uint64 stride = 3;
void SliceLayerParams::clear_stride() {
  stride_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 SliceLayerParams::stride() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SliceLayerParams.stride)
  return stride_;
}
void SliceLayerParams::set_stride(::google::protobuf::uint64 value) {
  
  stride_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SliceLayerParams.stride)
}

// .CoreML.Specification.SliceLayerParams.SliceAxis axis = 4;
void SliceLayerParams::clear_axis() {
  axis_ = 0;
}
::CoreML::Specification::SliceLayerParams_SliceAxis SliceLayerParams::axis() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SliceLayerParams.axis)
  return static_cast< ::CoreML::Specification::SliceLayerParams_SliceAxis >(axis_);
}
void SliceLayerParams::set_axis(::CoreML::Specification::SliceLayerParams_SliceAxis value) {
  
  axis_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SliceLayerParams.axis)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ReduceLayerParams::kModeFieldNumber;
const int ReduceLayerParams::kEpsilonFieldNumber;
const int ReduceLayerParams::kAxisFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ReduceLayerParams::ReduceLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ReduceLayerParams)
}
ReduceLayerParams::ReduceLayerParams(const ReduceLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::memcpy(&mode_, &from.mode_,
    reinterpret_cast<char*>(&axis_) -
    reinterpret_cast<char*>(&mode_) + sizeof(axis_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ReduceLayerParams)
}

void ReduceLayerParams::SharedCtor() {
  ::memset(&mode_, 0, reinterpret_cast<char*>(&axis_) -
    reinterpret_cast<char*>(&mode_) + sizeof(axis_));
  _cached_size_ = 0;
}

ReduceLayerParams::~ReduceLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ReduceLayerParams)
  SharedDtor();
}

void ReduceLayerParams::SharedDtor() {
}

void ReduceLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ReduceLayerParams& ReduceLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ReduceLayerParams* ReduceLayerParams::New(::google::protobuf::Arena* arena) const {
  ReduceLayerParams* n = new ReduceLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ReduceLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ReduceLayerParams)
  ::memset(&mode_, 0, reinterpret_cast<char*>(&axis_) -
    reinterpret_cast<char*>(&mode_) + sizeof(axis_));
}

bool ReduceLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ReduceLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // .CoreML.Specification.ReduceLayerParams.ReduceOperation mode = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {
          int value;
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   int, ::google::protobuf::internal::WireFormatLite::TYPE_ENUM>(
                 input, &value)));
          set_mode(static_cast< ::CoreML::Specification::ReduceLayerParams_ReduceOperation >(value));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // float epsilon = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(21u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &epsilon_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ReduceLayerParams.ReduceAxis axis = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(24u)) {
          int value;
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   int, ::google::protobuf::internal::WireFormatLite::TYPE_ENUM>(
                 input, &value)));
          set_axis(static_cast< ::CoreML::Specification::ReduceLayerParams_ReduceAxis >(value));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ReduceLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ReduceLayerParams)
  return false;
#undef DO_
}

void ReduceLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ReduceLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // .CoreML.Specification.ReduceLayerParams.ReduceOperation mode = 1;
  if (this->mode() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteEnum(
      1, this->mode(), output);
  }

  // float epsilon = 2;
  if (this->epsilon() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(2, this->epsilon(), output);
  }

  // .CoreML.Specification.ReduceLayerParams.ReduceAxis axis = 3;
  if (this->axis() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteEnum(
      3, this->axis(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ReduceLayerParams)
}

size_t ReduceLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ReduceLayerParams)
  size_t total_size = 0;

  // .CoreML.Specification.ReduceLayerParams.ReduceOperation mode = 1;
  if (this->mode() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::EnumSize(this->mode());
  }

  // float epsilon = 2;
  if (this->epsilon() != 0) {
    total_size += 1 + 4;
  }

  // .CoreML.Specification.ReduceLayerParams.ReduceAxis axis = 3;
  if (this->axis() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::EnumSize(this->axis());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ReduceLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ReduceLayerParams*>(&from));
}

void ReduceLayerParams::MergeFrom(const ReduceLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ReduceLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.mode() != 0) {
    set_mode(from.mode());
  }
  if (from.epsilon() != 0) {
    set_epsilon(from.epsilon());
  }
  if (from.axis() != 0) {
    set_axis(from.axis());
  }
}

void ReduceLayerParams::CopyFrom(const ReduceLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ReduceLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ReduceLayerParams::IsInitialized() const {
  return true;
}

void ReduceLayerParams::Swap(ReduceLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ReduceLayerParams::InternalSwap(ReduceLayerParams* other) {
  std::swap(mode_, other->mode_);
  std::swap(epsilon_, other->epsilon_);
  std::swap(axis_, other->axis_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ReduceLayerParams::GetTypeName() const {
  return "CoreML.Specification.ReduceLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ReduceLayerParams

// .CoreML.Specification.ReduceLayerParams.ReduceOperation mode = 1;
void ReduceLayerParams::clear_mode() {
  mode_ = 0;
}
::CoreML::Specification::ReduceLayerParams_ReduceOperation ReduceLayerParams::mode() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceLayerParams.mode)
  return static_cast< ::CoreML::Specification::ReduceLayerParams_ReduceOperation >(mode_);
}
void ReduceLayerParams::set_mode(::CoreML::Specification::ReduceLayerParams_ReduceOperation value) {
  
  mode_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceLayerParams.mode)
}

// float epsilon = 2;
void ReduceLayerParams::clear_epsilon() {
  epsilon_ = 0;
}
float ReduceLayerParams::epsilon() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceLayerParams.epsilon)
  return epsilon_;
}
void ReduceLayerParams::set_epsilon(float value) {
  
  epsilon_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceLayerParams.epsilon)
}

// .CoreML.Specification.ReduceLayerParams.ReduceAxis axis = 3;
void ReduceLayerParams::clear_axis() {
  axis_ = 0;
}
::CoreML::Specification::ReduceLayerParams_ReduceAxis ReduceLayerParams::axis() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ReduceLayerParams.axis)
  return static_cast< ::CoreML::Specification::ReduceLayerParams_ReduceAxis >(axis_);
}
void ReduceLayerParams::set_axis(::CoreML::Specification::ReduceLayerParams_ReduceAxis value) {
  
  axis_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ReduceLayerParams.axis)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int CropLayerParams::kCropAmountsFieldNumber;
const int CropLayerParams::kOffsetFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

CropLayerParams::CropLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.CropLayerParams)
}
CropLayerParams::CropLayerParams(const CropLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      offset_(from.offset_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_cropamounts()) {
    cropamounts_ = new ::CoreML::Specification::BorderAmounts(*from.cropamounts_);
  } else {
    cropamounts_ = NULL;
  }
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.CropLayerParams)
}

void CropLayerParams::SharedCtor() {
  cropamounts_ = NULL;
  _cached_size_ = 0;
}

CropLayerParams::~CropLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.CropLayerParams)
  SharedDtor();
}

void CropLayerParams::SharedDtor() {
  if (this != internal_default_instance()) {
    delete cropamounts_;
  }
}

void CropLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const CropLayerParams& CropLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

CropLayerParams* CropLayerParams::New(::google::protobuf::Arena* arena) const {
  CropLayerParams* n = new CropLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void CropLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.CropLayerParams)
  offset_.Clear();
  if (GetArenaNoVirtual() == NULL && cropamounts_ != NULL) {
    delete cropamounts_;
  }
  cropamounts_ = NULL;
}

bool CropLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.CropLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // .CoreML.Specification.BorderAmounts cropAmounts = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_cropamounts()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated uint64 offset = 5;
      case 5: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(42u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, this->mutable_offset())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(40u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 1, 42u, input, this->mutable_offset())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.CropLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.CropLayerParams)
  return false;
#undef DO_
}

void CropLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.CropLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // .CoreML.Specification.BorderAmounts cropAmounts = 1;
  if (this->has_cropamounts()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1, *this->cropamounts_, output);
  }

  // repeated uint64 offset = 5;
  if (this->offset_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(5, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_offset_cached_byte_size_);
  }
  for (int i = 0, n = this->offset_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64NoTag(
      this->offset(i), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.CropLayerParams)
}

size_t CropLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.CropLayerParams)
  size_t total_size = 0;

  // repeated uint64 offset = 5;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      UInt64Size(this->offset_);
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _offset_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // .CoreML.Specification.BorderAmounts cropAmounts = 1;
  if (this->has_cropamounts()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->cropamounts_);
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void CropLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const CropLayerParams*>(&from));
}

void CropLayerParams::MergeFrom(const CropLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.CropLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  offset_.MergeFrom(from.offset_);
  if (from.has_cropamounts()) {
    mutable_cropamounts()->::CoreML::Specification::BorderAmounts::MergeFrom(from.cropamounts());
  }
}

void CropLayerParams::CopyFrom(const CropLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.CropLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool CropLayerParams::IsInitialized() const {
  return true;
}

void CropLayerParams::Swap(CropLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void CropLayerParams::InternalSwap(CropLayerParams* other) {
  offset_.InternalSwap(&other->offset_);
  std::swap(cropamounts_, other->cropamounts_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string CropLayerParams::GetTypeName() const {
  return "CoreML.Specification.CropLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// CropLayerParams

// .CoreML.Specification.BorderAmounts cropAmounts = 1;
bool CropLayerParams::has_cropamounts() const {
  return this != internal_default_instance() && cropamounts_ != NULL;
}
void CropLayerParams::clear_cropamounts() {
  if (GetArenaNoVirtual() == NULL && cropamounts_ != NULL) delete cropamounts_;
  cropamounts_ = NULL;
}
const ::CoreML::Specification::BorderAmounts& CropLayerParams::cropamounts() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CropLayerParams.cropAmounts)
  return cropamounts_ != NULL ? *cropamounts_
                         : *::CoreML::Specification::BorderAmounts::internal_default_instance();
}
::CoreML::Specification::BorderAmounts* CropLayerParams::mutable_cropamounts() {
  
  if (cropamounts_ == NULL) {
    cropamounts_ = new ::CoreML::Specification::BorderAmounts;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.CropLayerParams.cropAmounts)
  return cropamounts_;
}
::CoreML::Specification::BorderAmounts* CropLayerParams::release_cropamounts() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.CropLayerParams.cropAmounts)
  
  ::CoreML::Specification::BorderAmounts* temp = cropamounts_;
  cropamounts_ = NULL;
  return temp;
}
void CropLayerParams::set_allocated_cropamounts(::CoreML::Specification::BorderAmounts* cropamounts) {
  delete cropamounts_;
  cropamounts_ = cropamounts;
  if (cropamounts) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.CropLayerParams.cropAmounts)
}

// repeated uint64 offset = 5;
int CropLayerParams::offset_size() const {
  return offset_.size();
}
void CropLayerParams::clear_offset() {
  offset_.Clear();
}
::google::protobuf::uint64 CropLayerParams::offset(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CropLayerParams.offset)
  return offset_.Get(index);
}
void CropLayerParams::set_offset(int index, ::google::protobuf::uint64 value) {
  offset_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.CropLayerParams.offset)
}
void CropLayerParams::add_offset(::google::protobuf::uint64 value) {
  offset_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.CropLayerParams.offset)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
CropLayerParams::offset() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.CropLayerParams.offset)
  return offset_;
}
::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
CropLayerParams::mutable_offset() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.CropLayerParams.offset)
  return &offset_;
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

AverageLayerParams::AverageLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.AverageLayerParams)
}
AverageLayerParams::AverageLayerParams(const AverageLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.AverageLayerParams)
}

void AverageLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

AverageLayerParams::~AverageLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.AverageLayerParams)
  SharedDtor();
}

void AverageLayerParams::SharedDtor() {
}

void AverageLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const AverageLayerParams& AverageLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

AverageLayerParams* AverageLayerParams::New(::google::protobuf::Arena* arena) const {
  AverageLayerParams* n = new AverageLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void AverageLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.AverageLayerParams)
}

bool AverageLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.AverageLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.AverageLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.AverageLayerParams)
  return false;
#undef DO_
}

void AverageLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.AverageLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.AverageLayerParams)
}

size_t AverageLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.AverageLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void AverageLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const AverageLayerParams*>(&from));
}

void AverageLayerParams::MergeFrom(const AverageLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.AverageLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void AverageLayerParams::CopyFrom(const AverageLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.AverageLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool AverageLayerParams::IsInitialized() const {
  return true;
}

void AverageLayerParams::Swap(AverageLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void AverageLayerParams::InternalSwap(AverageLayerParams* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string AverageLayerParams::GetTypeName() const {
  return "CoreML.Specification.AverageLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// AverageLayerParams

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

MaxLayerParams::MaxLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.MaxLayerParams)
}
MaxLayerParams::MaxLayerParams(const MaxLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.MaxLayerParams)
}

void MaxLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

MaxLayerParams::~MaxLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.MaxLayerParams)
  SharedDtor();
}

void MaxLayerParams::SharedDtor() {
}

void MaxLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const MaxLayerParams& MaxLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

MaxLayerParams* MaxLayerParams::New(::google::protobuf::Arena* arena) const {
  MaxLayerParams* n = new MaxLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void MaxLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.MaxLayerParams)
}

bool MaxLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.MaxLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.MaxLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.MaxLayerParams)
  return false;
#undef DO_
}

void MaxLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.MaxLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.MaxLayerParams)
}

size_t MaxLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.MaxLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void MaxLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const MaxLayerParams*>(&from));
}

void MaxLayerParams::MergeFrom(const MaxLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.MaxLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void MaxLayerParams::CopyFrom(const MaxLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.MaxLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool MaxLayerParams::IsInitialized() const {
  return true;
}

void MaxLayerParams::Swap(MaxLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void MaxLayerParams::InternalSwap(MaxLayerParams* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string MaxLayerParams::GetTypeName() const {
  return "CoreML.Specification.MaxLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// MaxLayerParams

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

MinLayerParams::MinLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.MinLayerParams)
}
MinLayerParams::MinLayerParams(const MinLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.MinLayerParams)
}

void MinLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

MinLayerParams::~MinLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.MinLayerParams)
  SharedDtor();
}

void MinLayerParams::SharedDtor() {
}

void MinLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const MinLayerParams& MinLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

MinLayerParams* MinLayerParams::New(::google::protobuf::Arena* arena) const {
  MinLayerParams* n = new MinLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void MinLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.MinLayerParams)
}

bool MinLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.MinLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.MinLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.MinLayerParams)
  return false;
#undef DO_
}

void MinLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.MinLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.MinLayerParams)
}

size_t MinLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.MinLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void MinLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const MinLayerParams*>(&from));
}

void MinLayerParams::MergeFrom(const MinLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.MinLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void MinLayerParams::CopyFrom(const MinLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.MinLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool MinLayerParams::IsInitialized() const {
  return true;
}

void MinLayerParams::Swap(MinLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void MinLayerParams::InternalSwap(MinLayerParams* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string MinLayerParams::GetTypeName() const {
  return "CoreML.Specification.MinLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// MinLayerParams

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int DotProductLayerParams::kCosineSimilarityFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

DotProductLayerParams::DotProductLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.DotProductLayerParams)
}
DotProductLayerParams::DotProductLayerParams(const DotProductLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  cosinesimilarity_ = from.cosinesimilarity_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.DotProductLayerParams)
}

void DotProductLayerParams::SharedCtor() {
  cosinesimilarity_ = false;
  _cached_size_ = 0;
}

DotProductLayerParams::~DotProductLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.DotProductLayerParams)
  SharedDtor();
}

void DotProductLayerParams::SharedDtor() {
}

void DotProductLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const DotProductLayerParams& DotProductLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

DotProductLayerParams* DotProductLayerParams::New(::google::protobuf::Arena* arena) const {
  DotProductLayerParams* n = new DotProductLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void DotProductLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.DotProductLayerParams)
  cosinesimilarity_ = false;
}

bool DotProductLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.DotProductLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // bool cosineSimilarity = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &cosinesimilarity_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.DotProductLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.DotProductLayerParams)
  return false;
#undef DO_
}

void DotProductLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.DotProductLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // bool cosineSimilarity = 1;
  if (this->cosinesimilarity() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(1, this->cosinesimilarity(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.DotProductLayerParams)
}

size_t DotProductLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.DotProductLayerParams)
  size_t total_size = 0;

  // bool cosineSimilarity = 1;
  if (this->cosinesimilarity() != 0) {
    total_size += 1 + 1;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void DotProductLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const DotProductLayerParams*>(&from));
}

void DotProductLayerParams::MergeFrom(const DotProductLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.DotProductLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.cosinesimilarity() != 0) {
    set_cosinesimilarity(from.cosinesimilarity());
  }
}

void DotProductLayerParams::CopyFrom(const DotProductLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.DotProductLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool DotProductLayerParams::IsInitialized() const {
  return true;
}

void DotProductLayerParams::Swap(DotProductLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void DotProductLayerParams::InternalSwap(DotProductLayerParams* other) {
  std::swap(cosinesimilarity_, other->cosinesimilarity_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string DotProductLayerParams::GetTypeName() const {
  return "CoreML.Specification.DotProductLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// DotProductLayerParams

// bool cosineSimilarity = 1;
void DotProductLayerParams::clear_cosinesimilarity() {
  cosinesimilarity_ = false;
}
bool DotProductLayerParams::cosinesimilarity() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.DotProductLayerParams.cosineSimilarity)
  return cosinesimilarity_;
}
void DotProductLayerParams::set_cosinesimilarity(bool value) {
  
  cosinesimilarity_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.DotProductLayerParams.cosineSimilarity)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int MeanVarianceNormalizeLayerParams::kAcrossChannelsFieldNumber;
const int MeanVarianceNormalizeLayerParams::kNormalizeVarianceFieldNumber;
const int MeanVarianceNormalizeLayerParams::kEpsilonFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

MeanVarianceNormalizeLayerParams::MeanVarianceNormalizeLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.MeanVarianceNormalizeLayerParams)
}
MeanVarianceNormalizeLayerParams::MeanVarianceNormalizeLayerParams(const MeanVarianceNormalizeLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::memcpy(&acrosschannels_, &from.acrosschannels_,
    reinterpret_cast<char*>(&epsilon_) -
    reinterpret_cast<char*>(&acrosschannels_) + sizeof(epsilon_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.MeanVarianceNormalizeLayerParams)
}

void MeanVarianceNormalizeLayerParams::SharedCtor() {
  ::memset(&acrosschannels_, 0, reinterpret_cast<char*>(&epsilon_) -
    reinterpret_cast<char*>(&acrosschannels_) + sizeof(epsilon_));
  _cached_size_ = 0;
}

MeanVarianceNormalizeLayerParams::~MeanVarianceNormalizeLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.MeanVarianceNormalizeLayerParams)
  SharedDtor();
}

void MeanVarianceNormalizeLayerParams::SharedDtor() {
}

void MeanVarianceNormalizeLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const MeanVarianceNormalizeLayerParams& MeanVarianceNormalizeLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

MeanVarianceNormalizeLayerParams* MeanVarianceNormalizeLayerParams::New(::google::protobuf::Arena* arena) const {
  MeanVarianceNormalizeLayerParams* n = new MeanVarianceNormalizeLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void MeanVarianceNormalizeLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.MeanVarianceNormalizeLayerParams)
  ::memset(&acrosschannels_, 0, reinterpret_cast<char*>(&epsilon_) -
    reinterpret_cast<char*>(&acrosschannels_) + sizeof(epsilon_));
}

bool MeanVarianceNormalizeLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.MeanVarianceNormalizeLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // bool acrossChannels = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &acrosschannels_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool normalizeVariance = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(16u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &normalizevariance_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // float epsilon = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(29u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &epsilon_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.MeanVarianceNormalizeLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.MeanVarianceNormalizeLayerParams)
  return false;
#undef DO_
}

void MeanVarianceNormalizeLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.MeanVarianceNormalizeLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // bool acrossChannels = 1;
  if (this->acrosschannels() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(1, this->acrosschannels(), output);
  }

  // bool normalizeVariance = 2;
  if (this->normalizevariance() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(2, this->normalizevariance(), output);
  }

  // float epsilon = 3;
  if (this->epsilon() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(3, this->epsilon(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.MeanVarianceNormalizeLayerParams)
}

size_t MeanVarianceNormalizeLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.MeanVarianceNormalizeLayerParams)
  size_t total_size = 0;

  // bool acrossChannels = 1;
  if (this->acrosschannels() != 0) {
    total_size += 1 + 1;
  }

  // bool normalizeVariance = 2;
  if (this->normalizevariance() != 0) {
    total_size += 1 + 1;
  }

  // float epsilon = 3;
  if (this->epsilon() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void MeanVarianceNormalizeLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const MeanVarianceNormalizeLayerParams*>(&from));
}

void MeanVarianceNormalizeLayerParams::MergeFrom(const MeanVarianceNormalizeLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.MeanVarianceNormalizeLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.acrosschannels() != 0) {
    set_acrosschannels(from.acrosschannels());
  }
  if (from.normalizevariance() != 0) {
    set_normalizevariance(from.normalizevariance());
  }
  if (from.epsilon() != 0) {
    set_epsilon(from.epsilon());
  }
}

void MeanVarianceNormalizeLayerParams::CopyFrom(const MeanVarianceNormalizeLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.MeanVarianceNormalizeLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool MeanVarianceNormalizeLayerParams::IsInitialized() const {
  return true;
}

void MeanVarianceNormalizeLayerParams::Swap(MeanVarianceNormalizeLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void MeanVarianceNormalizeLayerParams::InternalSwap(MeanVarianceNormalizeLayerParams* other) {
  std::swap(acrosschannels_, other->acrosschannels_);
  std::swap(normalizevariance_, other->normalizevariance_);
  std::swap(epsilon_, other->epsilon_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string MeanVarianceNormalizeLayerParams::GetTypeName() const {
  return "CoreML.Specification.MeanVarianceNormalizeLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// MeanVarianceNormalizeLayerParams

// bool acrossChannels = 1;
void MeanVarianceNormalizeLayerParams::clear_acrosschannels() {
  acrosschannels_ = false;
}
bool MeanVarianceNormalizeLayerParams::acrosschannels() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.MeanVarianceNormalizeLayerParams.acrossChannels)
  return acrosschannels_;
}
void MeanVarianceNormalizeLayerParams::set_acrosschannels(bool value) {
  
  acrosschannels_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.MeanVarianceNormalizeLayerParams.acrossChannels)
}

// bool normalizeVariance = 2;
void MeanVarianceNormalizeLayerParams::clear_normalizevariance() {
  normalizevariance_ = false;
}
bool MeanVarianceNormalizeLayerParams::normalizevariance() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.MeanVarianceNormalizeLayerParams.normalizeVariance)
  return normalizevariance_;
}
void MeanVarianceNormalizeLayerParams::set_normalizevariance(bool value) {
  
  normalizevariance_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.MeanVarianceNormalizeLayerParams.normalizeVariance)
}

// float epsilon = 3;
void MeanVarianceNormalizeLayerParams::clear_epsilon() {
  epsilon_ = 0;
}
float MeanVarianceNormalizeLayerParams::epsilon() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.MeanVarianceNormalizeLayerParams.epsilon)
  return epsilon_;
}
void MeanVarianceNormalizeLayerParams::set_epsilon(float value) {
  
  epsilon_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.MeanVarianceNormalizeLayerParams.epsilon)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int SequenceRepeatLayerParams::kNRepetitionsFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

SequenceRepeatLayerParams::SequenceRepeatLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.SequenceRepeatLayerParams)
}
SequenceRepeatLayerParams::SequenceRepeatLayerParams(const SequenceRepeatLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  nrepetitions_ = from.nrepetitions_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.SequenceRepeatLayerParams)
}

void SequenceRepeatLayerParams::SharedCtor() {
  nrepetitions_ = GOOGLE_ULONGLONG(0);
  _cached_size_ = 0;
}

SequenceRepeatLayerParams::~SequenceRepeatLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.SequenceRepeatLayerParams)
  SharedDtor();
}

void SequenceRepeatLayerParams::SharedDtor() {
}

void SequenceRepeatLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const SequenceRepeatLayerParams& SequenceRepeatLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

SequenceRepeatLayerParams* SequenceRepeatLayerParams::New(::google::protobuf::Arena* arena) const {
  SequenceRepeatLayerParams* n = new SequenceRepeatLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void SequenceRepeatLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.SequenceRepeatLayerParams)
  nrepetitions_ = GOOGLE_ULONGLONG(0);
}

bool SequenceRepeatLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.SequenceRepeatLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // uint64 nRepetitions = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &nrepetitions_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.SequenceRepeatLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.SequenceRepeatLayerParams)
  return false;
#undef DO_
}

void SequenceRepeatLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.SequenceRepeatLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // uint64 nRepetitions = 1;
  if (this->nrepetitions() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(1, this->nrepetitions(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.SequenceRepeatLayerParams)
}

size_t SequenceRepeatLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.SequenceRepeatLayerParams)
  size_t total_size = 0;

  // uint64 nRepetitions = 1;
  if (this->nrepetitions() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->nrepetitions());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void SequenceRepeatLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const SequenceRepeatLayerParams*>(&from));
}

void SequenceRepeatLayerParams::MergeFrom(const SequenceRepeatLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.SequenceRepeatLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.nrepetitions() != 0) {
    set_nrepetitions(from.nrepetitions());
  }
}

void SequenceRepeatLayerParams::CopyFrom(const SequenceRepeatLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.SequenceRepeatLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool SequenceRepeatLayerParams::IsInitialized() const {
  return true;
}

void SequenceRepeatLayerParams::Swap(SequenceRepeatLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void SequenceRepeatLayerParams::InternalSwap(SequenceRepeatLayerParams* other) {
  std::swap(nrepetitions_, other->nrepetitions_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string SequenceRepeatLayerParams::GetTypeName() const {
  return "CoreML.Specification.SequenceRepeatLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// SequenceRepeatLayerParams

// uint64 nRepetitions = 1;
void SequenceRepeatLayerParams::clear_nrepetitions() {
  nrepetitions_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 SequenceRepeatLayerParams::nrepetitions() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SequenceRepeatLayerParams.nRepetitions)
  return nrepetitions_;
}
void SequenceRepeatLayerParams::set_nrepetitions(::google::protobuf::uint64 value) {
  
  nrepetitions_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SequenceRepeatLayerParams.nRepetitions)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int SimpleRecurrentLayerParams::kInputVectorSizeFieldNumber;
const int SimpleRecurrentLayerParams::kOutputVectorSizeFieldNumber;
const int SimpleRecurrentLayerParams::kActivationFieldNumber;
const int SimpleRecurrentLayerParams::kSequenceOutputFieldNumber;
const int SimpleRecurrentLayerParams::kHasBiasVectorFieldNumber;
const int SimpleRecurrentLayerParams::kWeightMatrixFieldNumber;
const int SimpleRecurrentLayerParams::kRecursionMatrixFieldNumber;
const int SimpleRecurrentLayerParams::kBiasVectorFieldNumber;
const int SimpleRecurrentLayerParams::kReverseInputFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

SimpleRecurrentLayerParams::SimpleRecurrentLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.SimpleRecurrentLayerParams)
}
SimpleRecurrentLayerParams::SimpleRecurrentLayerParams(const SimpleRecurrentLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_activation()) {
    activation_ = new ::CoreML::Specification::ActivationParams(*from.activation_);
  } else {
    activation_ = NULL;
  }
  if (from.has_weightmatrix()) {
    weightmatrix_ = new ::CoreML::Specification::WeightParams(*from.weightmatrix_);
  } else {
    weightmatrix_ = NULL;
  }
  if (from.has_recursionmatrix()) {
    recursionmatrix_ = new ::CoreML::Specification::WeightParams(*from.recursionmatrix_);
  } else {
    recursionmatrix_ = NULL;
  }
  if (from.has_biasvector()) {
    biasvector_ = new ::CoreML::Specification::WeightParams(*from.biasvector_);
  } else {
    biasvector_ = NULL;
  }
  ::memcpy(&inputvectorsize_, &from.inputvectorsize_,
    reinterpret_cast<char*>(&reverseinput_) -
    reinterpret_cast<char*>(&inputvectorsize_) + sizeof(reverseinput_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.SimpleRecurrentLayerParams)
}

void SimpleRecurrentLayerParams::SharedCtor() {
  ::memset(&activation_, 0, reinterpret_cast<char*>(&reverseinput_) -
    reinterpret_cast<char*>(&activation_) + sizeof(reverseinput_));
  _cached_size_ = 0;
}

SimpleRecurrentLayerParams::~SimpleRecurrentLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.SimpleRecurrentLayerParams)
  SharedDtor();
}

void SimpleRecurrentLayerParams::SharedDtor() {
  if (this != internal_default_instance()) {
    delete activation_;
  }
  if (this != internal_default_instance()) {
    delete weightmatrix_;
  }
  if (this != internal_default_instance()) {
    delete recursionmatrix_;
  }
  if (this != internal_default_instance()) {
    delete biasvector_;
  }
}

void SimpleRecurrentLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const SimpleRecurrentLayerParams& SimpleRecurrentLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

SimpleRecurrentLayerParams* SimpleRecurrentLayerParams::New(::google::protobuf::Arena* arena) const {
  SimpleRecurrentLayerParams* n = new SimpleRecurrentLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void SimpleRecurrentLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.SimpleRecurrentLayerParams)
  if (GetArenaNoVirtual() == NULL && activation_ != NULL) {
    delete activation_;
  }
  activation_ = NULL;
  if (GetArenaNoVirtual() == NULL && weightmatrix_ != NULL) {
    delete weightmatrix_;
  }
  weightmatrix_ = NULL;
  if (GetArenaNoVirtual() == NULL && recursionmatrix_ != NULL) {
    delete recursionmatrix_;
  }
  recursionmatrix_ = NULL;
  if (GetArenaNoVirtual() == NULL && biasvector_ != NULL) {
    delete biasvector_;
  }
  biasvector_ = NULL;
  ::memset(&inputvectorsize_, 0, reinterpret_cast<char*>(&reverseinput_) -
    reinterpret_cast<char*>(&inputvectorsize_) + sizeof(reverseinput_));
}

bool SimpleRecurrentLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.SimpleRecurrentLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(16383u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // uint64 inputVectorSize = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &inputvectorsize_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // uint64 outputVectorSize = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(16u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &outputvectorsize_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.ActivationParams activation = 10;
      case 10: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(82u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_activation()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool sequenceOutput = 15;
      case 15: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(120u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &sequenceoutput_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool hasBiasVector = 20;
      case 20: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(160u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &hasbiasvector_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams weightMatrix = 30;
      case 30: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(242u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_weightmatrix()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams recursionMatrix = 31;
      case 31: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(250u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_recursionmatrix()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams biasVector = 32;
      case 32: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(258u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_biasvector()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool reverseInput = 100;
      case 100: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(800u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &reverseinput_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.SimpleRecurrentLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.SimpleRecurrentLayerParams)
  return false;
#undef DO_
}

void SimpleRecurrentLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.SimpleRecurrentLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // uint64 inputVectorSize = 1;
  if (this->inputvectorsize() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(1, this->inputvectorsize(), output);
  }

  // uint64 outputVectorSize = 2;
  if (this->outputvectorsize() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(2, this->outputvectorsize(), output);
  }

  // .CoreML.Specification.ActivationParams activation = 10;
  if (this->has_activation()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      10, *this->activation_, output);
  }

  // bool sequenceOutput = 15;
  if (this->sequenceoutput() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(15, this->sequenceoutput(), output);
  }

  // bool hasBiasVector = 20;
  if (this->hasbiasvector() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(20, this->hasbiasvector(), output);
  }

  // .CoreML.Specification.WeightParams weightMatrix = 30;
  if (this->has_weightmatrix()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      30, *this->weightmatrix_, output);
  }

  // .CoreML.Specification.WeightParams recursionMatrix = 31;
  if (this->has_recursionmatrix()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      31, *this->recursionmatrix_, output);
  }

  // .CoreML.Specification.WeightParams biasVector = 32;
  if (this->has_biasvector()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      32, *this->biasvector_, output);
  }

  // bool reverseInput = 100;
  if (this->reverseinput() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(100, this->reverseinput(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.SimpleRecurrentLayerParams)
}

size_t SimpleRecurrentLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.SimpleRecurrentLayerParams)
  size_t total_size = 0;

  // .CoreML.Specification.ActivationParams activation = 10;
  if (this->has_activation()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->activation_);
  }

  // .CoreML.Specification.WeightParams weightMatrix = 30;
  if (this->has_weightmatrix()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->weightmatrix_);
  }

  // .CoreML.Specification.WeightParams recursionMatrix = 31;
  if (this->has_recursionmatrix()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->recursionmatrix_);
  }

  // .CoreML.Specification.WeightParams biasVector = 32;
  if (this->has_biasvector()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->biasvector_);
  }

  // uint64 inputVectorSize = 1;
  if (this->inputvectorsize() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->inputvectorsize());
  }

  // uint64 outputVectorSize = 2;
  if (this->outputvectorsize() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->outputvectorsize());
  }

  // bool sequenceOutput = 15;
  if (this->sequenceoutput() != 0) {
    total_size += 1 + 1;
  }

  // bool hasBiasVector = 20;
  if (this->hasbiasvector() != 0) {
    total_size += 2 + 1;
  }

  // bool reverseInput = 100;
  if (this->reverseinput() != 0) {
    total_size += 2 + 1;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void SimpleRecurrentLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const SimpleRecurrentLayerParams*>(&from));
}

void SimpleRecurrentLayerParams::MergeFrom(const SimpleRecurrentLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.SimpleRecurrentLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.has_activation()) {
    mutable_activation()->::CoreML::Specification::ActivationParams::MergeFrom(from.activation());
  }
  if (from.has_weightmatrix()) {
    mutable_weightmatrix()->::CoreML::Specification::WeightParams::MergeFrom(from.weightmatrix());
  }
  if (from.has_recursionmatrix()) {
    mutable_recursionmatrix()->::CoreML::Specification::WeightParams::MergeFrom(from.recursionmatrix());
  }
  if (from.has_biasvector()) {
    mutable_biasvector()->::CoreML::Specification::WeightParams::MergeFrom(from.biasvector());
  }
  if (from.inputvectorsize() != 0) {
    set_inputvectorsize(from.inputvectorsize());
  }
  if (from.outputvectorsize() != 0) {
    set_outputvectorsize(from.outputvectorsize());
  }
  if (from.sequenceoutput() != 0) {
    set_sequenceoutput(from.sequenceoutput());
  }
  if (from.hasbiasvector() != 0) {
    set_hasbiasvector(from.hasbiasvector());
  }
  if (from.reverseinput() != 0) {
    set_reverseinput(from.reverseinput());
  }
}

void SimpleRecurrentLayerParams::CopyFrom(const SimpleRecurrentLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.SimpleRecurrentLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool SimpleRecurrentLayerParams::IsInitialized() const {
  return true;
}

void SimpleRecurrentLayerParams::Swap(SimpleRecurrentLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void SimpleRecurrentLayerParams::InternalSwap(SimpleRecurrentLayerParams* other) {
  std::swap(activation_, other->activation_);
  std::swap(weightmatrix_, other->weightmatrix_);
  std::swap(recursionmatrix_, other->recursionmatrix_);
  std::swap(biasvector_, other->biasvector_);
  std::swap(inputvectorsize_, other->inputvectorsize_);
  std::swap(outputvectorsize_, other->outputvectorsize_);
  std::swap(sequenceoutput_, other->sequenceoutput_);
  std::swap(hasbiasvector_, other->hasbiasvector_);
  std::swap(reverseinput_, other->reverseinput_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string SimpleRecurrentLayerParams::GetTypeName() const {
  return "CoreML.Specification.SimpleRecurrentLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// SimpleRecurrentLayerParams

// uint64 inputVectorSize = 1;
void SimpleRecurrentLayerParams::clear_inputvectorsize() {
  inputvectorsize_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 SimpleRecurrentLayerParams::inputvectorsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SimpleRecurrentLayerParams.inputVectorSize)
  return inputvectorsize_;
}
void SimpleRecurrentLayerParams::set_inputvectorsize(::google::protobuf::uint64 value) {
  
  inputvectorsize_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SimpleRecurrentLayerParams.inputVectorSize)
}

// uint64 outputVectorSize = 2;
void SimpleRecurrentLayerParams::clear_outputvectorsize() {
  outputvectorsize_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 SimpleRecurrentLayerParams::outputvectorsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SimpleRecurrentLayerParams.outputVectorSize)
  return outputvectorsize_;
}
void SimpleRecurrentLayerParams::set_outputvectorsize(::google::protobuf::uint64 value) {
  
  outputvectorsize_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SimpleRecurrentLayerParams.outputVectorSize)
}

// .CoreML.Specification.ActivationParams activation = 10;
bool SimpleRecurrentLayerParams::has_activation() const {
  return this != internal_default_instance() && activation_ != NULL;
}
void SimpleRecurrentLayerParams::clear_activation() {
  if (GetArenaNoVirtual() == NULL && activation_ != NULL) delete activation_;
  activation_ = NULL;
}
const ::CoreML::Specification::ActivationParams& SimpleRecurrentLayerParams::activation() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SimpleRecurrentLayerParams.activation)
  return activation_ != NULL ? *activation_
                         : *::CoreML::Specification::ActivationParams::internal_default_instance();
}
::CoreML::Specification::ActivationParams* SimpleRecurrentLayerParams::mutable_activation() {
  
  if (activation_ == NULL) {
    activation_ = new ::CoreML::Specification::ActivationParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.SimpleRecurrentLayerParams.activation)
  return activation_;
}
::CoreML::Specification::ActivationParams* SimpleRecurrentLayerParams::release_activation() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.SimpleRecurrentLayerParams.activation)
  
  ::CoreML::Specification::ActivationParams* temp = activation_;
  activation_ = NULL;
  return temp;
}
void SimpleRecurrentLayerParams::set_allocated_activation(::CoreML::Specification::ActivationParams* activation) {
  delete activation_;
  activation_ = activation;
  if (activation) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.SimpleRecurrentLayerParams.activation)
}

// bool sequenceOutput = 15;
void SimpleRecurrentLayerParams::clear_sequenceoutput() {
  sequenceoutput_ = false;
}
bool SimpleRecurrentLayerParams::sequenceoutput() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SimpleRecurrentLayerParams.sequenceOutput)
  return sequenceoutput_;
}
void SimpleRecurrentLayerParams::set_sequenceoutput(bool value) {
  
  sequenceoutput_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SimpleRecurrentLayerParams.sequenceOutput)
}

// bool hasBiasVector = 20;
void SimpleRecurrentLayerParams::clear_hasbiasvector() {
  hasbiasvector_ = false;
}
bool SimpleRecurrentLayerParams::hasbiasvector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SimpleRecurrentLayerParams.hasBiasVector)
  return hasbiasvector_;
}
void SimpleRecurrentLayerParams::set_hasbiasvector(bool value) {
  
  hasbiasvector_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SimpleRecurrentLayerParams.hasBiasVector)
}

// .CoreML.Specification.WeightParams weightMatrix = 30;
bool SimpleRecurrentLayerParams::has_weightmatrix() const {
  return this != internal_default_instance() && weightmatrix_ != NULL;
}
void SimpleRecurrentLayerParams::clear_weightmatrix() {
  if (GetArenaNoVirtual() == NULL && weightmatrix_ != NULL) delete weightmatrix_;
  weightmatrix_ = NULL;
}
const ::CoreML::Specification::WeightParams& SimpleRecurrentLayerParams::weightmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SimpleRecurrentLayerParams.weightMatrix)
  return weightmatrix_ != NULL ? *weightmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* SimpleRecurrentLayerParams::mutable_weightmatrix() {
  
  if (weightmatrix_ == NULL) {
    weightmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.SimpleRecurrentLayerParams.weightMatrix)
  return weightmatrix_;
}
::CoreML::Specification::WeightParams* SimpleRecurrentLayerParams::release_weightmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.SimpleRecurrentLayerParams.weightMatrix)
  
  ::CoreML::Specification::WeightParams* temp = weightmatrix_;
  weightmatrix_ = NULL;
  return temp;
}
void SimpleRecurrentLayerParams::set_allocated_weightmatrix(::CoreML::Specification::WeightParams* weightmatrix) {
  delete weightmatrix_;
  weightmatrix_ = weightmatrix;
  if (weightmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.SimpleRecurrentLayerParams.weightMatrix)
}

// .CoreML.Specification.WeightParams recursionMatrix = 31;
bool SimpleRecurrentLayerParams::has_recursionmatrix() const {
  return this != internal_default_instance() && recursionmatrix_ != NULL;
}
void SimpleRecurrentLayerParams::clear_recursionmatrix() {
  if (GetArenaNoVirtual() == NULL && recursionmatrix_ != NULL) delete recursionmatrix_;
  recursionmatrix_ = NULL;
}
const ::CoreML::Specification::WeightParams& SimpleRecurrentLayerParams::recursionmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SimpleRecurrentLayerParams.recursionMatrix)
  return recursionmatrix_ != NULL ? *recursionmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* SimpleRecurrentLayerParams::mutable_recursionmatrix() {
  
  if (recursionmatrix_ == NULL) {
    recursionmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.SimpleRecurrentLayerParams.recursionMatrix)
  return recursionmatrix_;
}
::CoreML::Specification::WeightParams* SimpleRecurrentLayerParams::release_recursionmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.SimpleRecurrentLayerParams.recursionMatrix)
  
  ::CoreML::Specification::WeightParams* temp = recursionmatrix_;
  recursionmatrix_ = NULL;
  return temp;
}
void SimpleRecurrentLayerParams::set_allocated_recursionmatrix(::CoreML::Specification::WeightParams* recursionmatrix) {
  delete recursionmatrix_;
  recursionmatrix_ = recursionmatrix;
  if (recursionmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.SimpleRecurrentLayerParams.recursionMatrix)
}

// .CoreML.Specification.WeightParams biasVector = 32;
bool SimpleRecurrentLayerParams::has_biasvector() const {
  return this != internal_default_instance() && biasvector_ != NULL;
}
void SimpleRecurrentLayerParams::clear_biasvector() {
  if (GetArenaNoVirtual() == NULL && biasvector_ != NULL) delete biasvector_;
  biasvector_ = NULL;
}
const ::CoreML::Specification::WeightParams& SimpleRecurrentLayerParams::biasvector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SimpleRecurrentLayerParams.biasVector)
  return biasvector_ != NULL ? *biasvector_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* SimpleRecurrentLayerParams::mutable_biasvector() {
  
  if (biasvector_ == NULL) {
    biasvector_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.SimpleRecurrentLayerParams.biasVector)
  return biasvector_;
}
::CoreML::Specification::WeightParams* SimpleRecurrentLayerParams::release_biasvector() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.SimpleRecurrentLayerParams.biasVector)
  
  ::CoreML::Specification::WeightParams* temp = biasvector_;
  biasvector_ = NULL;
  return temp;
}
void SimpleRecurrentLayerParams::set_allocated_biasvector(::CoreML::Specification::WeightParams* biasvector) {
  delete biasvector_;
  biasvector_ = biasvector;
  if (biasvector) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.SimpleRecurrentLayerParams.biasVector)
}

// bool reverseInput = 100;
void SimpleRecurrentLayerParams::clear_reverseinput() {
  reverseinput_ = false;
}
bool SimpleRecurrentLayerParams::reverseinput() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SimpleRecurrentLayerParams.reverseInput)
  return reverseinput_;
}
void SimpleRecurrentLayerParams::set_reverseinput(bool value) {
  
  reverseinput_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SimpleRecurrentLayerParams.reverseInput)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int GRULayerParams::kInputVectorSizeFieldNumber;
const int GRULayerParams::kOutputVectorSizeFieldNumber;
const int GRULayerParams::kActivationsFieldNumber;
const int GRULayerParams::kSequenceOutputFieldNumber;
const int GRULayerParams::kHasBiasVectorsFieldNumber;
const int GRULayerParams::kUpdateGateWeightMatrixFieldNumber;
const int GRULayerParams::kResetGateWeightMatrixFieldNumber;
const int GRULayerParams::kOutputGateWeightMatrixFieldNumber;
const int GRULayerParams::kUpdateGateRecursionMatrixFieldNumber;
const int GRULayerParams::kResetGateRecursionMatrixFieldNumber;
const int GRULayerParams::kOutputGateRecursionMatrixFieldNumber;
const int GRULayerParams::kUpdateGateBiasVectorFieldNumber;
const int GRULayerParams::kResetGateBiasVectorFieldNumber;
const int GRULayerParams::kOutputGateBiasVectorFieldNumber;
const int GRULayerParams::kReverseInputFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

GRULayerParams::GRULayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.GRULayerParams)
}
GRULayerParams::GRULayerParams(const GRULayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      activations_(from.activations_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_updategateweightmatrix()) {
    updategateweightmatrix_ = new ::CoreML::Specification::WeightParams(*from.updategateweightmatrix_);
  } else {
    updategateweightmatrix_ = NULL;
  }
  if (from.has_resetgateweightmatrix()) {
    resetgateweightmatrix_ = new ::CoreML::Specification::WeightParams(*from.resetgateweightmatrix_);
  } else {
    resetgateweightmatrix_ = NULL;
  }
  if (from.has_outputgateweightmatrix()) {
    outputgateweightmatrix_ = new ::CoreML::Specification::WeightParams(*from.outputgateweightmatrix_);
  } else {
    outputgateweightmatrix_ = NULL;
  }
  if (from.has_updategaterecursionmatrix()) {
    updategaterecursionmatrix_ = new ::CoreML::Specification::WeightParams(*from.updategaterecursionmatrix_);
  } else {
    updategaterecursionmatrix_ = NULL;
  }
  if (from.has_resetgaterecursionmatrix()) {
    resetgaterecursionmatrix_ = new ::CoreML::Specification::WeightParams(*from.resetgaterecursionmatrix_);
  } else {
    resetgaterecursionmatrix_ = NULL;
  }
  if (from.has_outputgaterecursionmatrix()) {
    outputgaterecursionmatrix_ = new ::CoreML::Specification::WeightParams(*from.outputgaterecursionmatrix_);
  } else {
    outputgaterecursionmatrix_ = NULL;
  }
  if (from.has_updategatebiasvector()) {
    updategatebiasvector_ = new ::CoreML::Specification::WeightParams(*from.updategatebiasvector_);
  } else {
    updategatebiasvector_ = NULL;
  }
  if (from.has_resetgatebiasvector()) {
    resetgatebiasvector_ = new ::CoreML::Specification::WeightParams(*from.resetgatebiasvector_);
  } else {
    resetgatebiasvector_ = NULL;
  }
  if (from.has_outputgatebiasvector()) {
    outputgatebiasvector_ = new ::CoreML::Specification::WeightParams(*from.outputgatebiasvector_);
  } else {
    outputgatebiasvector_ = NULL;
  }
  ::memcpy(&inputvectorsize_, &from.inputvectorsize_,
    reinterpret_cast<char*>(&reverseinput_) -
    reinterpret_cast<char*>(&inputvectorsize_) + sizeof(reverseinput_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.GRULayerParams)
}

void GRULayerParams::SharedCtor() {
  ::memset(&updategateweightmatrix_, 0, reinterpret_cast<char*>(&reverseinput_) -
    reinterpret_cast<char*>(&updategateweightmatrix_) + sizeof(reverseinput_));
  _cached_size_ = 0;
}

GRULayerParams::~GRULayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.GRULayerParams)
  SharedDtor();
}

void GRULayerParams::SharedDtor() {
  if (this != internal_default_instance()) {
    delete updategateweightmatrix_;
  }
  if (this != internal_default_instance()) {
    delete resetgateweightmatrix_;
  }
  if (this != internal_default_instance()) {
    delete outputgateweightmatrix_;
  }
  if (this != internal_default_instance()) {
    delete updategaterecursionmatrix_;
  }
  if (this != internal_default_instance()) {
    delete resetgaterecursionmatrix_;
  }
  if (this != internal_default_instance()) {
    delete outputgaterecursionmatrix_;
  }
  if (this != internal_default_instance()) {
    delete updategatebiasvector_;
  }
  if (this != internal_default_instance()) {
    delete resetgatebiasvector_;
  }
  if (this != internal_default_instance()) {
    delete outputgatebiasvector_;
  }
}

void GRULayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const GRULayerParams& GRULayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

GRULayerParams* GRULayerParams::New(::google::protobuf::Arena* arena) const {
  GRULayerParams* n = new GRULayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void GRULayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.GRULayerParams)
  activations_.Clear();
  if (GetArenaNoVirtual() == NULL && updategateweightmatrix_ != NULL) {
    delete updategateweightmatrix_;
  }
  updategateweightmatrix_ = NULL;
  if (GetArenaNoVirtual() == NULL && resetgateweightmatrix_ != NULL) {
    delete resetgateweightmatrix_;
  }
  resetgateweightmatrix_ = NULL;
  if (GetArenaNoVirtual() == NULL && outputgateweightmatrix_ != NULL) {
    delete outputgateweightmatrix_;
  }
  outputgateweightmatrix_ = NULL;
  if (GetArenaNoVirtual() == NULL && updategaterecursionmatrix_ != NULL) {
    delete updategaterecursionmatrix_;
  }
  updategaterecursionmatrix_ = NULL;
  if (GetArenaNoVirtual() == NULL && resetgaterecursionmatrix_ != NULL) {
    delete resetgaterecursionmatrix_;
  }
  resetgaterecursionmatrix_ = NULL;
  if (GetArenaNoVirtual() == NULL && outputgaterecursionmatrix_ != NULL) {
    delete outputgaterecursionmatrix_;
  }
  outputgaterecursionmatrix_ = NULL;
  if (GetArenaNoVirtual() == NULL && updategatebiasvector_ != NULL) {
    delete updategatebiasvector_;
  }
  updategatebiasvector_ = NULL;
  if (GetArenaNoVirtual() == NULL && resetgatebiasvector_ != NULL) {
    delete resetgatebiasvector_;
  }
  resetgatebiasvector_ = NULL;
  if (GetArenaNoVirtual() == NULL && outputgatebiasvector_ != NULL) {
    delete outputgatebiasvector_;
  }
  outputgatebiasvector_ = NULL;
  ::memset(&inputvectorsize_, 0, reinterpret_cast<char*>(&reverseinput_) -
    reinterpret_cast<char*>(&inputvectorsize_) + sizeof(reverseinput_));
}

bool GRULayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.GRULayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(16383u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // uint64 inputVectorSize = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &inputvectorsize_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // uint64 outputVectorSize = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(16u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &outputvectorsize_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated .CoreML.Specification.ActivationParams activations = 10;
      case 10: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(82u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
                input, add_activations()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool sequenceOutput = 15;
      case 15: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(120u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &sequenceoutput_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool hasBiasVectors = 20;
      case 20: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(160u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &hasbiasvectors_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams updateGateWeightMatrix = 30;
      case 30: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(242u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_updategateweightmatrix()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams resetGateWeightMatrix = 31;
      case 31: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(250u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_resetgateweightmatrix()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams outputGateWeightMatrix = 32;
      case 32: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(258u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_outputgateweightmatrix()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams updateGateRecursionMatrix = 50;
      case 50: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(402u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_updategaterecursionmatrix()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams resetGateRecursionMatrix = 51;
      case 51: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(410u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_resetgaterecursionmatrix()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams outputGateRecursionMatrix = 52;
      case 52: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(418u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_outputgaterecursionmatrix()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams updateGateBiasVector = 70;
      case 70: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(562u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_updategatebiasvector()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams resetGateBiasVector = 71;
      case 71: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(570u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_resetgatebiasvector()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams outputGateBiasVector = 72;
      case 72: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(578u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_outputgatebiasvector()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool reverseInput = 100;
      case 100: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(800u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &reverseinput_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.GRULayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.GRULayerParams)
  return false;
#undef DO_
}

void GRULayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.GRULayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // uint64 inputVectorSize = 1;
  if (this->inputvectorsize() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(1, this->inputvectorsize(), output);
  }

  // uint64 outputVectorSize = 2;
  if (this->outputvectorsize() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(2, this->outputvectorsize(), output);
  }

  // repeated .CoreML.Specification.ActivationParams activations = 10;
  for (unsigned int i = 0, n = this->activations_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      10, this->activations(i), output);
  }

  // bool sequenceOutput = 15;
  if (this->sequenceoutput() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(15, this->sequenceoutput(), output);
  }

  // bool hasBiasVectors = 20;
  if (this->hasbiasvectors() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(20, this->hasbiasvectors(), output);
  }

  // .CoreML.Specification.WeightParams updateGateWeightMatrix = 30;
  if (this->has_updategateweightmatrix()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      30, *this->updategateweightmatrix_, output);
  }

  // .CoreML.Specification.WeightParams resetGateWeightMatrix = 31;
  if (this->has_resetgateweightmatrix()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      31, *this->resetgateweightmatrix_, output);
  }

  // .CoreML.Specification.WeightParams outputGateWeightMatrix = 32;
  if (this->has_outputgateweightmatrix()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      32, *this->outputgateweightmatrix_, output);
  }

  // .CoreML.Specification.WeightParams updateGateRecursionMatrix = 50;
  if (this->has_updategaterecursionmatrix()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      50, *this->updategaterecursionmatrix_, output);
  }

  // .CoreML.Specification.WeightParams resetGateRecursionMatrix = 51;
  if (this->has_resetgaterecursionmatrix()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      51, *this->resetgaterecursionmatrix_, output);
  }

  // .CoreML.Specification.WeightParams outputGateRecursionMatrix = 52;
  if (this->has_outputgaterecursionmatrix()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      52, *this->outputgaterecursionmatrix_, output);
  }

  // .CoreML.Specification.WeightParams updateGateBiasVector = 70;
  if (this->has_updategatebiasvector()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      70, *this->updategatebiasvector_, output);
  }

  // .CoreML.Specification.WeightParams resetGateBiasVector = 71;
  if (this->has_resetgatebiasvector()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      71, *this->resetgatebiasvector_, output);
  }

  // .CoreML.Specification.WeightParams outputGateBiasVector = 72;
  if (this->has_outputgatebiasvector()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      72, *this->outputgatebiasvector_, output);
  }

  // bool reverseInput = 100;
  if (this->reverseinput() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(100, this->reverseinput(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.GRULayerParams)
}

size_t GRULayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.GRULayerParams)
  size_t total_size = 0;

  // repeated .CoreML.Specification.ActivationParams activations = 10;
  {
    unsigned int count = this->activations_size();
    total_size += 1UL * count;
    for (unsigned int i = 0; i < count; i++) {
      total_size +=
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          this->activations(i));
    }
  }

  // .CoreML.Specification.WeightParams updateGateWeightMatrix = 30;
  if (this->has_updategateweightmatrix()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->updategateweightmatrix_);
  }

  // .CoreML.Specification.WeightParams resetGateWeightMatrix = 31;
  if (this->has_resetgateweightmatrix()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->resetgateweightmatrix_);
  }

  // .CoreML.Specification.WeightParams outputGateWeightMatrix = 32;
  if (this->has_outputgateweightmatrix()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->outputgateweightmatrix_);
  }

  // .CoreML.Specification.WeightParams updateGateRecursionMatrix = 50;
  if (this->has_updategaterecursionmatrix()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->updategaterecursionmatrix_);
  }

  // .CoreML.Specification.WeightParams resetGateRecursionMatrix = 51;
  if (this->has_resetgaterecursionmatrix()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->resetgaterecursionmatrix_);
  }

  // .CoreML.Specification.WeightParams outputGateRecursionMatrix = 52;
  if (this->has_outputgaterecursionmatrix()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->outputgaterecursionmatrix_);
  }

  // .CoreML.Specification.WeightParams updateGateBiasVector = 70;
  if (this->has_updategatebiasvector()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->updategatebiasvector_);
  }

  // .CoreML.Specification.WeightParams resetGateBiasVector = 71;
  if (this->has_resetgatebiasvector()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->resetgatebiasvector_);
  }

  // .CoreML.Specification.WeightParams outputGateBiasVector = 72;
  if (this->has_outputgatebiasvector()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->outputgatebiasvector_);
  }

  // uint64 inputVectorSize = 1;
  if (this->inputvectorsize() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->inputvectorsize());
  }

  // uint64 outputVectorSize = 2;
  if (this->outputvectorsize() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->outputvectorsize());
  }

  // bool sequenceOutput = 15;
  if (this->sequenceoutput() != 0) {
    total_size += 1 + 1;
  }

  // bool hasBiasVectors = 20;
  if (this->hasbiasvectors() != 0) {
    total_size += 2 + 1;
  }

  // bool reverseInput = 100;
  if (this->reverseinput() != 0) {
    total_size += 2 + 1;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void GRULayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const GRULayerParams*>(&from));
}

void GRULayerParams::MergeFrom(const GRULayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.GRULayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  activations_.MergeFrom(from.activations_);
  if (from.has_updategateweightmatrix()) {
    mutable_updategateweightmatrix()->::CoreML::Specification::WeightParams::MergeFrom(from.updategateweightmatrix());
  }
  if (from.has_resetgateweightmatrix()) {
    mutable_resetgateweightmatrix()->::CoreML::Specification::WeightParams::MergeFrom(from.resetgateweightmatrix());
  }
  if (from.has_outputgateweightmatrix()) {
    mutable_outputgateweightmatrix()->::CoreML::Specification::WeightParams::MergeFrom(from.outputgateweightmatrix());
  }
  if (from.has_updategaterecursionmatrix()) {
    mutable_updategaterecursionmatrix()->::CoreML::Specification::WeightParams::MergeFrom(from.updategaterecursionmatrix());
  }
  if (from.has_resetgaterecursionmatrix()) {
    mutable_resetgaterecursionmatrix()->::CoreML::Specification::WeightParams::MergeFrom(from.resetgaterecursionmatrix());
  }
  if (from.has_outputgaterecursionmatrix()) {
    mutable_outputgaterecursionmatrix()->::CoreML::Specification::WeightParams::MergeFrom(from.outputgaterecursionmatrix());
  }
  if (from.has_updategatebiasvector()) {
    mutable_updategatebiasvector()->::CoreML::Specification::WeightParams::MergeFrom(from.updategatebiasvector());
  }
  if (from.has_resetgatebiasvector()) {
    mutable_resetgatebiasvector()->::CoreML::Specification::WeightParams::MergeFrom(from.resetgatebiasvector());
  }
  if (from.has_outputgatebiasvector()) {
    mutable_outputgatebiasvector()->::CoreML::Specification::WeightParams::MergeFrom(from.outputgatebiasvector());
  }
  if (from.inputvectorsize() != 0) {
    set_inputvectorsize(from.inputvectorsize());
  }
  if (from.outputvectorsize() != 0) {
    set_outputvectorsize(from.outputvectorsize());
  }
  if (from.sequenceoutput() != 0) {
    set_sequenceoutput(from.sequenceoutput());
  }
  if (from.hasbiasvectors() != 0) {
    set_hasbiasvectors(from.hasbiasvectors());
  }
  if (from.reverseinput() != 0) {
    set_reverseinput(from.reverseinput());
  }
}

void GRULayerParams::CopyFrom(const GRULayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.GRULayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool GRULayerParams::IsInitialized() const {
  return true;
}

void GRULayerParams::Swap(GRULayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void GRULayerParams::InternalSwap(GRULayerParams* other) {
  activations_.InternalSwap(&other->activations_);
  std::swap(updategateweightmatrix_, other->updategateweightmatrix_);
  std::swap(resetgateweightmatrix_, other->resetgateweightmatrix_);
  std::swap(outputgateweightmatrix_, other->outputgateweightmatrix_);
  std::swap(updategaterecursionmatrix_, other->updategaterecursionmatrix_);
  std::swap(resetgaterecursionmatrix_, other->resetgaterecursionmatrix_);
  std::swap(outputgaterecursionmatrix_, other->outputgaterecursionmatrix_);
  std::swap(updategatebiasvector_, other->updategatebiasvector_);
  std::swap(resetgatebiasvector_, other->resetgatebiasvector_);
  std::swap(outputgatebiasvector_, other->outputgatebiasvector_);
  std::swap(inputvectorsize_, other->inputvectorsize_);
  std::swap(outputvectorsize_, other->outputvectorsize_);
  std::swap(sequenceoutput_, other->sequenceoutput_);
  std::swap(hasbiasvectors_, other->hasbiasvectors_);
  std::swap(reverseinput_, other->reverseinput_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string GRULayerParams::GetTypeName() const {
  return "CoreML.Specification.GRULayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// GRULayerParams

// uint64 inputVectorSize = 1;
void GRULayerParams::clear_inputvectorsize() {
  inputvectorsize_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 GRULayerParams::inputvectorsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.inputVectorSize)
  return inputvectorsize_;
}
void GRULayerParams::set_inputvectorsize(::google::protobuf::uint64 value) {
  
  inputvectorsize_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.GRULayerParams.inputVectorSize)
}

// uint64 outputVectorSize = 2;
void GRULayerParams::clear_outputvectorsize() {
  outputvectorsize_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 GRULayerParams::outputvectorsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.outputVectorSize)
  return outputvectorsize_;
}
void GRULayerParams::set_outputvectorsize(::google::protobuf::uint64 value) {
  
  outputvectorsize_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.GRULayerParams.outputVectorSize)
}

// repeated .CoreML.Specification.ActivationParams activations = 10;
int GRULayerParams::activations_size() const {
  return activations_.size();
}
void GRULayerParams::clear_activations() {
  activations_.Clear();
}
const ::CoreML::Specification::ActivationParams& GRULayerParams::activations(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.activations)
  return activations_.Get(index);
}
::CoreML::Specification::ActivationParams* GRULayerParams::mutable_activations(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.GRULayerParams.activations)
  return activations_.Mutable(index);
}
::CoreML::Specification::ActivationParams* GRULayerParams::add_activations() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.GRULayerParams.activations)
  return activations_.Add();
}
::google::protobuf::RepeatedPtrField< ::CoreML::Specification::ActivationParams >*
GRULayerParams::mutable_activations() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.GRULayerParams.activations)
  return &activations_;
}
const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::ActivationParams >&
GRULayerParams::activations() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.GRULayerParams.activations)
  return activations_;
}

// bool sequenceOutput = 15;
void GRULayerParams::clear_sequenceoutput() {
  sequenceoutput_ = false;
}
bool GRULayerParams::sequenceoutput() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.sequenceOutput)
  return sequenceoutput_;
}
void GRULayerParams::set_sequenceoutput(bool value) {
  
  sequenceoutput_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.GRULayerParams.sequenceOutput)
}

// bool hasBiasVectors = 20;
void GRULayerParams::clear_hasbiasvectors() {
  hasbiasvectors_ = false;
}
bool GRULayerParams::hasbiasvectors() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.hasBiasVectors)
  return hasbiasvectors_;
}
void GRULayerParams::set_hasbiasvectors(bool value) {
  
  hasbiasvectors_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.GRULayerParams.hasBiasVectors)
}

// .CoreML.Specification.WeightParams updateGateWeightMatrix = 30;
bool GRULayerParams::has_updategateweightmatrix() const {
  return this != internal_default_instance() && updategateweightmatrix_ != NULL;
}
void GRULayerParams::clear_updategateweightmatrix() {
  if (GetArenaNoVirtual() == NULL && updategateweightmatrix_ != NULL) delete updategateweightmatrix_;
  updategateweightmatrix_ = NULL;
}
const ::CoreML::Specification::WeightParams& GRULayerParams::updategateweightmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.updateGateWeightMatrix)
  return updategateweightmatrix_ != NULL ? *updategateweightmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* GRULayerParams::mutable_updategateweightmatrix() {
  
  if (updategateweightmatrix_ == NULL) {
    updategateweightmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.GRULayerParams.updateGateWeightMatrix)
  return updategateweightmatrix_;
}
::CoreML::Specification::WeightParams* GRULayerParams::release_updategateweightmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.GRULayerParams.updateGateWeightMatrix)
  
  ::CoreML::Specification::WeightParams* temp = updategateweightmatrix_;
  updategateweightmatrix_ = NULL;
  return temp;
}
void GRULayerParams::set_allocated_updategateweightmatrix(::CoreML::Specification::WeightParams* updategateweightmatrix) {
  delete updategateweightmatrix_;
  updategateweightmatrix_ = updategateweightmatrix;
  if (updategateweightmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.GRULayerParams.updateGateWeightMatrix)
}

// .CoreML.Specification.WeightParams resetGateWeightMatrix = 31;
bool GRULayerParams::has_resetgateweightmatrix() const {
  return this != internal_default_instance() && resetgateweightmatrix_ != NULL;
}
void GRULayerParams::clear_resetgateweightmatrix() {
  if (GetArenaNoVirtual() == NULL && resetgateweightmatrix_ != NULL) delete resetgateweightmatrix_;
  resetgateweightmatrix_ = NULL;
}
const ::CoreML::Specification::WeightParams& GRULayerParams::resetgateweightmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.resetGateWeightMatrix)
  return resetgateweightmatrix_ != NULL ? *resetgateweightmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* GRULayerParams::mutable_resetgateweightmatrix() {
  
  if (resetgateweightmatrix_ == NULL) {
    resetgateweightmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.GRULayerParams.resetGateWeightMatrix)
  return resetgateweightmatrix_;
}
::CoreML::Specification::WeightParams* GRULayerParams::release_resetgateweightmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.GRULayerParams.resetGateWeightMatrix)
  
  ::CoreML::Specification::WeightParams* temp = resetgateweightmatrix_;
  resetgateweightmatrix_ = NULL;
  return temp;
}
void GRULayerParams::set_allocated_resetgateweightmatrix(::CoreML::Specification::WeightParams* resetgateweightmatrix) {
  delete resetgateweightmatrix_;
  resetgateweightmatrix_ = resetgateweightmatrix;
  if (resetgateweightmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.GRULayerParams.resetGateWeightMatrix)
}

// .CoreML.Specification.WeightParams outputGateWeightMatrix = 32;
bool GRULayerParams::has_outputgateweightmatrix() const {
  return this != internal_default_instance() && outputgateweightmatrix_ != NULL;
}
void GRULayerParams::clear_outputgateweightmatrix() {
  if (GetArenaNoVirtual() == NULL && outputgateweightmatrix_ != NULL) delete outputgateweightmatrix_;
  outputgateweightmatrix_ = NULL;
}
const ::CoreML::Specification::WeightParams& GRULayerParams::outputgateweightmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.outputGateWeightMatrix)
  return outputgateweightmatrix_ != NULL ? *outputgateweightmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* GRULayerParams::mutable_outputgateweightmatrix() {
  
  if (outputgateweightmatrix_ == NULL) {
    outputgateweightmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.GRULayerParams.outputGateWeightMatrix)
  return outputgateweightmatrix_;
}
::CoreML::Specification::WeightParams* GRULayerParams::release_outputgateweightmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.GRULayerParams.outputGateWeightMatrix)
  
  ::CoreML::Specification::WeightParams* temp = outputgateweightmatrix_;
  outputgateweightmatrix_ = NULL;
  return temp;
}
void GRULayerParams::set_allocated_outputgateweightmatrix(::CoreML::Specification::WeightParams* outputgateweightmatrix) {
  delete outputgateweightmatrix_;
  outputgateweightmatrix_ = outputgateweightmatrix;
  if (outputgateweightmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.GRULayerParams.outputGateWeightMatrix)
}

// .CoreML.Specification.WeightParams updateGateRecursionMatrix = 50;
bool GRULayerParams::has_updategaterecursionmatrix() const {
  return this != internal_default_instance() && updategaterecursionmatrix_ != NULL;
}
void GRULayerParams::clear_updategaterecursionmatrix() {
  if (GetArenaNoVirtual() == NULL && updategaterecursionmatrix_ != NULL) delete updategaterecursionmatrix_;
  updategaterecursionmatrix_ = NULL;
}
const ::CoreML::Specification::WeightParams& GRULayerParams::updategaterecursionmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.updateGateRecursionMatrix)
  return updategaterecursionmatrix_ != NULL ? *updategaterecursionmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* GRULayerParams::mutable_updategaterecursionmatrix() {
  
  if (updategaterecursionmatrix_ == NULL) {
    updategaterecursionmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.GRULayerParams.updateGateRecursionMatrix)
  return updategaterecursionmatrix_;
}
::CoreML::Specification::WeightParams* GRULayerParams::release_updategaterecursionmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.GRULayerParams.updateGateRecursionMatrix)
  
  ::CoreML::Specification::WeightParams* temp = updategaterecursionmatrix_;
  updategaterecursionmatrix_ = NULL;
  return temp;
}
void GRULayerParams::set_allocated_updategaterecursionmatrix(::CoreML::Specification::WeightParams* updategaterecursionmatrix) {
  delete updategaterecursionmatrix_;
  updategaterecursionmatrix_ = updategaterecursionmatrix;
  if (updategaterecursionmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.GRULayerParams.updateGateRecursionMatrix)
}

// .CoreML.Specification.WeightParams resetGateRecursionMatrix = 51;
bool GRULayerParams::has_resetgaterecursionmatrix() const {
  return this != internal_default_instance() && resetgaterecursionmatrix_ != NULL;
}
void GRULayerParams::clear_resetgaterecursionmatrix() {
  if (GetArenaNoVirtual() == NULL && resetgaterecursionmatrix_ != NULL) delete resetgaterecursionmatrix_;
  resetgaterecursionmatrix_ = NULL;
}
const ::CoreML::Specification::WeightParams& GRULayerParams::resetgaterecursionmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.resetGateRecursionMatrix)
  return resetgaterecursionmatrix_ != NULL ? *resetgaterecursionmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* GRULayerParams::mutable_resetgaterecursionmatrix() {
  
  if (resetgaterecursionmatrix_ == NULL) {
    resetgaterecursionmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.GRULayerParams.resetGateRecursionMatrix)
  return resetgaterecursionmatrix_;
}
::CoreML::Specification::WeightParams* GRULayerParams::release_resetgaterecursionmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.GRULayerParams.resetGateRecursionMatrix)
  
  ::CoreML::Specification::WeightParams* temp = resetgaterecursionmatrix_;
  resetgaterecursionmatrix_ = NULL;
  return temp;
}
void GRULayerParams::set_allocated_resetgaterecursionmatrix(::CoreML::Specification::WeightParams* resetgaterecursionmatrix) {
  delete resetgaterecursionmatrix_;
  resetgaterecursionmatrix_ = resetgaterecursionmatrix;
  if (resetgaterecursionmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.GRULayerParams.resetGateRecursionMatrix)
}

// .CoreML.Specification.WeightParams outputGateRecursionMatrix = 52;
bool GRULayerParams::has_outputgaterecursionmatrix() const {
  return this != internal_default_instance() && outputgaterecursionmatrix_ != NULL;
}
void GRULayerParams::clear_outputgaterecursionmatrix() {
  if (GetArenaNoVirtual() == NULL && outputgaterecursionmatrix_ != NULL) delete outputgaterecursionmatrix_;
  outputgaterecursionmatrix_ = NULL;
}
const ::CoreML::Specification::WeightParams& GRULayerParams::outputgaterecursionmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.outputGateRecursionMatrix)
  return outputgaterecursionmatrix_ != NULL ? *outputgaterecursionmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* GRULayerParams::mutable_outputgaterecursionmatrix() {
  
  if (outputgaterecursionmatrix_ == NULL) {
    outputgaterecursionmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.GRULayerParams.outputGateRecursionMatrix)
  return outputgaterecursionmatrix_;
}
::CoreML::Specification::WeightParams* GRULayerParams::release_outputgaterecursionmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.GRULayerParams.outputGateRecursionMatrix)
  
  ::CoreML::Specification::WeightParams* temp = outputgaterecursionmatrix_;
  outputgaterecursionmatrix_ = NULL;
  return temp;
}
void GRULayerParams::set_allocated_outputgaterecursionmatrix(::CoreML::Specification::WeightParams* outputgaterecursionmatrix) {
  delete outputgaterecursionmatrix_;
  outputgaterecursionmatrix_ = outputgaterecursionmatrix;
  if (outputgaterecursionmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.GRULayerParams.outputGateRecursionMatrix)
}

// .CoreML.Specification.WeightParams updateGateBiasVector = 70;
bool GRULayerParams::has_updategatebiasvector() const {
  return this != internal_default_instance() && updategatebiasvector_ != NULL;
}
void GRULayerParams::clear_updategatebiasvector() {
  if (GetArenaNoVirtual() == NULL && updategatebiasvector_ != NULL) delete updategatebiasvector_;
  updategatebiasvector_ = NULL;
}
const ::CoreML::Specification::WeightParams& GRULayerParams::updategatebiasvector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.updateGateBiasVector)
  return updategatebiasvector_ != NULL ? *updategatebiasvector_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* GRULayerParams::mutable_updategatebiasvector() {
  
  if (updategatebiasvector_ == NULL) {
    updategatebiasvector_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.GRULayerParams.updateGateBiasVector)
  return updategatebiasvector_;
}
::CoreML::Specification::WeightParams* GRULayerParams::release_updategatebiasvector() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.GRULayerParams.updateGateBiasVector)
  
  ::CoreML::Specification::WeightParams* temp = updategatebiasvector_;
  updategatebiasvector_ = NULL;
  return temp;
}
void GRULayerParams::set_allocated_updategatebiasvector(::CoreML::Specification::WeightParams* updategatebiasvector) {
  delete updategatebiasvector_;
  updategatebiasvector_ = updategatebiasvector;
  if (updategatebiasvector) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.GRULayerParams.updateGateBiasVector)
}

// .CoreML.Specification.WeightParams resetGateBiasVector = 71;
bool GRULayerParams::has_resetgatebiasvector() const {
  return this != internal_default_instance() && resetgatebiasvector_ != NULL;
}
void GRULayerParams::clear_resetgatebiasvector() {
  if (GetArenaNoVirtual() == NULL && resetgatebiasvector_ != NULL) delete resetgatebiasvector_;
  resetgatebiasvector_ = NULL;
}
const ::CoreML::Specification::WeightParams& GRULayerParams::resetgatebiasvector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.resetGateBiasVector)
  return resetgatebiasvector_ != NULL ? *resetgatebiasvector_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* GRULayerParams::mutable_resetgatebiasvector() {
  
  if (resetgatebiasvector_ == NULL) {
    resetgatebiasvector_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.GRULayerParams.resetGateBiasVector)
  return resetgatebiasvector_;
}
::CoreML::Specification::WeightParams* GRULayerParams::release_resetgatebiasvector() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.GRULayerParams.resetGateBiasVector)
  
  ::CoreML::Specification::WeightParams* temp = resetgatebiasvector_;
  resetgatebiasvector_ = NULL;
  return temp;
}
void GRULayerParams::set_allocated_resetgatebiasvector(::CoreML::Specification::WeightParams* resetgatebiasvector) {
  delete resetgatebiasvector_;
  resetgatebiasvector_ = resetgatebiasvector;
  if (resetgatebiasvector) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.GRULayerParams.resetGateBiasVector)
}

// .CoreML.Specification.WeightParams outputGateBiasVector = 72;
bool GRULayerParams::has_outputgatebiasvector() const {
  return this != internal_default_instance() && outputgatebiasvector_ != NULL;
}
void GRULayerParams::clear_outputgatebiasvector() {
  if (GetArenaNoVirtual() == NULL && outputgatebiasvector_ != NULL) delete outputgatebiasvector_;
  outputgatebiasvector_ = NULL;
}
const ::CoreML::Specification::WeightParams& GRULayerParams::outputgatebiasvector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.outputGateBiasVector)
  return outputgatebiasvector_ != NULL ? *outputgatebiasvector_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* GRULayerParams::mutable_outputgatebiasvector() {
  
  if (outputgatebiasvector_ == NULL) {
    outputgatebiasvector_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.GRULayerParams.outputGateBiasVector)
  return outputgatebiasvector_;
}
::CoreML::Specification::WeightParams* GRULayerParams::release_outputgatebiasvector() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.GRULayerParams.outputGateBiasVector)
  
  ::CoreML::Specification::WeightParams* temp = outputgatebiasvector_;
  outputgatebiasvector_ = NULL;
  return temp;
}
void GRULayerParams::set_allocated_outputgatebiasvector(::CoreML::Specification::WeightParams* outputgatebiasvector) {
  delete outputgatebiasvector_;
  outputgatebiasvector_ = outputgatebiasvector;
  if (outputgatebiasvector) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.GRULayerParams.outputGateBiasVector)
}

// bool reverseInput = 100;
void GRULayerParams::clear_reverseinput() {
  reverseinput_ = false;
}
bool GRULayerParams::reverseinput() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GRULayerParams.reverseInput)
  return reverseinput_;
}
void GRULayerParams::set_reverseinput(bool value) {
  
  reverseinput_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.GRULayerParams.reverseInput)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int LSTMParams::kSequenceOutputFieldNumber;
const int LSTMParams::kHasBiasVectorsFieldNumber;
const int LSTMParams::kForgetBiasFieldNumber;
const int LSTMParams::kHasPeepholeVectorsFieldNumber;
const int LSTMParams::kCoupledInputAndForgetGateFieldNumber;
const int LSTMParams::kCellClipThresholdFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

LSTMParams::LSTMParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.LSTMParams)
}
LSTMParams::LSTMParams(const LSTMParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::memcpy(&cellclipthreshold_, &from.cellclipthreshold_,
    reinterpret_cast<char*>(&coupledinputandforgetgate_) -
    reinterpret_cast<char*>(&cellclipthreshold_) + sizeof(coupledinputandforgetgate_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.LSTMParams)
}

void LSTMParams::SharedCtor() {
  ::memset(&cellclipthreshold_, 0, reinterpret_cast<char*>(&coupledinputandforgetgate_) -
    reinterpret_cast<char*>(&cellclipthreshold_) + sizeof(coupledinputandforgetgate_));
  _cached_size_ = 0;
}

LSTMParams::~LSTMParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.LSTMParams)
  SharedDtor();
}

void LSTMParams::SharedDtor() {
}

void LSTMParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const LSTMParams& LSTMParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

LSTMParams* LSTMParams::New(::google::protobuf::Arena* arena) const {
  LSTMParams* n = new LSTMParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void LSTMParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.LSTMParams)
  ::memset(&cellclipthreshold_, 0, reinterpret_cast<char*>(&coupledinputandforgetgate_) -
    reinterpret_cast<char*>(&cellclipthreshold_) + sizeof(coupledinputandforgetgate_));
}

bool LSTMParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.LSTMParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(16383u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // bool sequenceOutput = 10;
      case 10: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(80u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &sequenceoutput_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool hasBiasVectors = 20;
      case 20: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(160u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &hasbiasvectors_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool forgetBias = 30;
      case 30: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(240u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &forgetbias_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool hasPeepholeVectors = 40;
      case 40: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(320u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &haspeepholevectors_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool coupledInputAndForgetGate = 50;
      case 50: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(400u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &coupledinputandforgetgate_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // float cellClipThreshold = 60;
      case 60: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(485u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &cellclipthreshold_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.LSTMParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.LSTMParams)
  return false;
#undef DO_
}

void LSTMParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.LSTMParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // bool sequenceOutput = 10;
  if (this->sequenceoutput() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(10, this->sequenceoutput(), output);
  }

  // bool hasBiasVectors = 20;
  if (this->hasbiasvectors() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(20, this->hasbiasvectors(), output);
  }

  // bool forgetBias = 30;
  if (this->forgetbias() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(30, this->forgetbias(), output);
  }

  // bool hasPeepholeVectors = 40;
  if (this->haspeepholevectors() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(40, this->haspeepholevectors(), output);
  }

  // bool coupledInputAndForgetGate = 50;
  if (this->coupledinputandforgetgate() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(50, this->coupledinputandforgetgate(), output);
  }

  // float cellClipThreshold = 60;
  if (this->cellclipthreshold() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(60, this->cellclipthreshold(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.LSTMParams)
}

size_t LSTMParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.LSTMParams)
  size_t total_size = 0;

  // float cellClipThreshold = 60;
  if (this->cellclipthreshold() != 0) {
    total_size += 2 + 4;
  }

  // bool sequenceOutput = 10;
  if (this->sequenceoutput() != 0) {
    total_size += 1 + 1;
  }

  // bool hasBiasVectors = 20;
  if (this->hasbiasvectors() != 0) {
    total_size += 2 + 1;
  }

  // bool forgetBias = 30;
  if (this->forgetbias() != 0) {
    total_size += 2 + 1;
  }

  // bool hasPeepholeVectors = 40;
  if (this->haspeepholevectors() != 0) {
    total_size += 2 + 1;
  }

  // bool coupledInputAndForgetGate = 50;
  if (this->coupledinputandforgetgate() != 0) {
    total_size += 2 + 1;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void LSTMParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const LSTMParams*>(&from));
}

void LSTMParams::MergeFrom(const LSTMParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.LSTMParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.cellclipthreshold() != 0) {
    set_cellclipthreshold(from.cellclipthreshold());
  }
  if (from.sequenceoutput() != 0) {
    set_sequenceoutput(from.sequenceoutput());
  }
  if (from.hasbiasvectors() != 0) {
    set_hasbiasvectors(from.hasbiasvectors());
  }
  if (from.forgetbias() != 0) {
    set_forgetbias(from.forgetbias());
  }
  if (from.haspeepholevectors() != 0) {
    set_haspeepholevectors(from.haspeepholevectors());
  }
  if (from.coupledinputandforgetgate() != 0) {
    set_coupledinputandforgetgate(from.coupledinputandforgetgate());
  }
}

void LSTMParams::CopyFrom(const LSTMParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.LSTMParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool LSTMParams::IsInitialized() const {
  return true;
}

void LSTMParams::Swap(LSTMParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void LSTMParams::InternalSwap(LSTMParams* other) {
  std::swap(cellclipthreshold_, other->cellclipthreshold_);
  std::swap(sequenceoutput_, other->sequenceoutput_);
  std::swap(hasbiasvectors_, other->hasbiasvectors_);
  std::swap(forgetbias_, other->forgetbias_);
  std::swap(haspeepholevectors_, other->haspeepholevectors_);
  std::swap(coupledinputandforgetgate_, other->coupledinputandforgetgate_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string LSTMParams::GetTypeName() const {
  return "CoreML.Specification.LSTMParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// LSTMParams

// bool sequenceOutput = 10;
void LSTMParams::clear_sequenceoutput() {
  sequenceoutput_ = false;
}
bool LSTMParams::sequenceoutput() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMParams.sequenceOutput)
  return sequenceoutput_;
}
void LSTMParams::set_sequenceoutput(bool value) {
  
  sequenceoutput_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.LSTMParams.sequenceOutput)
}

// bool hasBiasVectors = 20;
void LSTMParams::clear_hasbiasvectors() {
  hasbiasvectors_ = false;
}
bool LSTMParams::hasbiasvectors() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMParams.hasBiasVectors)
  return hasbiasvectors_;
}
void LSTMParams::set_hasbiasvectors(bool value) {
  
  hasbiasvectors_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.LSTMParams.hasBiasVectors)
}

// bool forgetBias = 30;
void LSTMParams::clear_forgetbias() {
  forgetbias_ = false;
}
bool LSTMParams::forgetbias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMParams.forgetBias)
  return forgetbias_;
}
void LSTMParams::set_forgetbias(bool value) {
  
  forgetbias_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.LSTMParams.forgetBias)
}

// bool hasPeepholeVectors = 40;
void LSTMParams::clear_haspeepholevectors() {
  haspeepholevectors_ = false;
}
bool LSTMParams::haspeepholevectors() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMParams.hasPeepholeVectors)
  return haspeepholevectors_;
}
void LSTMParams::set_haspeepholevectors(bool value) {
  
  haspeepholevectors_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.LSTMParams.hasPeepholeVectors)
}

// bool coupledInputAndForgetGate = 50;
void LSTMParams::clear_coupledinputandforgetgate() {
  coupledinputandforgetgate_ = false;
}
bool LSTMParams::coupledinputandforgetgate() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMParams.coupledInputAndForgetGate)
  return coupledinputandforgetgate_;
}
void LSTMParams::set_coupledinputandforgetgate(bool value) {
  
  coupledinputandforgetgate_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.LSTMParams.coupledInputAndForgetGate)
}

// float cellClipThreshold = 60;
void LSTMParams::clear_cellclipthreshold() {
  cellclipthreshold_ = 0;
}
float LSTMParams::cellclipthreshold() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMParams.cellClipThreshold)
  return cellclipthreshold_;
}
void LSTMParams::set_cellclipthreshold(float value) {
  
  cellclipthreshold_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.LSTMParams.cellClipThreshold)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int LSTMWeightParams::kInputGateWeightMatrixFieldNumber;
const int LSTMWeightParams::kForgetGateWeightMatrixFieldNumber;
const int LSTMWeightParams::kBlockInputWeightMatrixFieldNumber;
const int LSTMWeightParams::kOutputGateWeightMatrixFieldNumber;
const int LSTMWeightParams::kInputGateRecursionMatrixFieldNumber;
const int LSTMWeightParams::kForgetGateRecursionMatrixFieldNumber;
const int LSTMWeightParams::kBlockInputRecursionMatrixFieldNumber;
const int LSTMWeightParams::kOutputGateRecursionMatrixFieldNumber;
const int LSTMWeightParams::kInputGateBiasVectorFieldNumber;
const int LSTMWeightParams::kForgetGateBiasVectorFieldNumber;
const int LSTMWeightParams::kBlockInputBiasVectorFieldNumber;
const int LSTMWeightParams::kOutputGateBiasVectorFieldNumber;
const int LSTMWeightParams::kInputGatePeepholeVectorFieldNumber;
const int LSTMWeightParams::kForgetGatePeepholeVectorFieldNumber;
const int LSTMWeightParams::kOutputGatePeepholeVectorFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

LSTMWeightParams::LSTMWeightParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.LSTMWeightParams)
}
LSTMWeightParams::LSTMWeightParams(const LSTMWeightParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_inputgateweightmatrix()) {
    inputgateweightmatrix_ = new ::CoreML::Specification::WeightParams(*from.inputgateweightmatrix_);
  } else {
    inputgateweightmatrix_ = NULL;
  }
  if (from.has_forgetgateweightmatrix()) {
    forgetgateweightmatrix_ = new ::CoreML::Specification::WeightParams(*from.forgetgateweightmatrix_);
  } else {
    forgetgateweightmatrix_ = NULL;
  }
  if (from.has_blockinputweightmatrix()) {
    blockinputweightmatrix_ = new ::CoreML::Specification::WeightParams(*from.blockinputweightmatrix_);
  } else {
    blockinputweightmatrix_ = NULL;
  }
  if (from.has_outputgateweightmatrix()) {
    outputgateweightmatrix_ = new ::CoreML::Specification::WeightParams(*from.outputgateweightmatrix_);
  } else {
    outputgateweightmatrix_ = NULL;
  }
  if (from.has_inputgaterecursionmatrix()) {
    inputgaterecursionmatrix_ = new ::CoreML::Specification::WeightParams(*from.inputgaterecursionmatrix_);
  } else {
    inputgaterecursionmatrix_ = NULL;
  }
  if (from.has_forgetgaterecursionmatrix()) {
    forgetgaterecursionmatrix_ = new ::CoreML::Specification::WeightParams(*from.forgetgaterecursionmatrix_);
  } else {
    forgetgaterecursionmatrix_ = NULL;
  }
  if (from.has_blockinputrecursionmatrix()) {
    blockinputrecursionmatrix_ = new ::CoreML::Specification::WeightParams(*from.blockinputrecursionmatrix_);
  } else {
    blockinputrecursionmatrix_ = NULL;
  }
  if (from.has_outputgaterecursionmatrix()) {
    outputgaterecursionmatrix_ = new ::CoreML::Specification::WeightParams(*from.outputgaterecursionmatrix_);
  } else {
    outputgaterecursionmatrix_ = NULL;
  }
  if (from.has_inputgatebiasvector()) {
    inputgatebiasvector_ = new ::CoreML::Specification::WeightParams(*from.inputgatebiasvector_);
  } else {
    inputgatebiasvector_ = NULL;
  }
  if (from.has_forgetgatebiasvector()) {
    forgetgatebiasvector_ = new ::CoreML::Specification::WeightParams(*from.forgetgatebiasvector_);
  } else {
    forgetgatebiasvector_ = NULL;
  }
  if (from.has_blockinputbiasvector()) {
    blockinputbiasvector_ = new ::CoreML::Specification::WeightParams(*from.blockinputbiasvector_);
  } else {
    blockinputbiasvector_ = NULL;
  }
  if (from.has_outputgatebiasvector()) {
    outputgatebiasvector_ = new ::CoreML::Specification::WeightParams(*from.outputgatebiasvector_);
  } else {
    outputgatebiasvector_ = NULL;
  }
  if (from.has_inputgatepeepholevector()) {
    inputgatepeepholevector_ = new ::CoreML::Specification::WeightParams(*from.inputgatepeepholevector_);
  } else {
    inputgatepeepholevector_ = NULL;
  }
  if (from.has_forgetgatepeepholevector()) {
    forgetgatepeepholevector_ = new ::CoreML::Specification::WeightParams(*from.forgetgatepeepholevector_);
  } else {
    forgetgatepeepholevector_ = NULL;
  }
  if (from.has_outputgatepeepholevector()) {
    outputgatepeepholevector_ = new ::CoreML::Specification::WeightParams(*from.outputgatepeepholevector_);
  } else {
    outputgatepeepholevector_ = NULL;
  }
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.LSTMWeightParams)
}

void LSTMWeightParams::SharedCtor() {
  ::memset(&inputgateweightmatrix_, 0, reinterpret_cast<char*>(&outputgatepeepholevector_) -
    reinterpret_cast<char*>(&inputgateweightmatrix_) + sizeof(outputgatepeepholevector_));
  _cached_size_ = 0;
}

LSTMWeightParams::~LSTMWeightParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.LSTMWeightParams)
  SharedDtor();
}

void LSTMWeightParams::SharedDtor() {
  if (this != internal_default_instance()) {
    delete inputgateweightmatrix_;
  }
  if (this != internal_default_instance()) {
    delete forgetgateweightmatrix_;
  }
  if (this != internal_default_instance()) {
    delete blockinputweightmatrix_;
  }
  if (this != internal_default_instance()) {
    delete outputgateweightmatrix_;
  }
  if (this != internal_default_instance()) {
    delete inputgaterecursionmatrix_;
  }
  if (this != internal_default_instance()) {
    delete forgetgaterecursionmatrix_;
  }
  if (this != internal_default_instance()) {
    delete blockinputrecursionmatrix_;
  }
  if (this != internal_default_instance()) {
    delete outputgaterecursionmatrix_;
  }
  if (this != internal_default_instance()) {
    delete inputgatebiasvector_;
  }
  if (this != internal_default_instance()) {
    delete forgetgatebiasvector_;
  }
  if (this != internal_default_instance()) {
    delete blockinputbiasvector_;
  }
  if (this != internal_default_instance()) {
    delete outputgatebiasvector_;
  }
  if (this != internal_default_instance()) {
    delete inputgatepeepholevector_;
  }
  if (this != internal_default_instance()) {
    delete forgetgatepeepholevector_;
  }
  if (this != internal_default_instance()) {
    delete outputgatepeepholevector_;
  }
}

void LSTMWeightParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const LSTMWeightParams& LSTMWeightParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

LSTMWeightParams* LSTMWeightParams::New(::google::protobuf::Arena* arena) const {
  LSTMWeightParams* n = new LSTMWeightParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void LSTMWeightParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.LSTMWeightParams)
  if (GetArenaNoVirtual() == NULL && inputgateweightmatrix_ != NULL) {
    delete inputgateweightmatrix_;
  }
  inputgateweightmatrix_ = NULL;
  if (GetArenaNoVirtual() == NULL && forgetgateweightmatrix_ != NULL) {
    delete forgetgateweightmatrix_;
  }
  forgetgateweightmatrix_ = NULL;
  if (GetArenaNoVirtual() == NULL && blockinputweightmatrix_ != NULL) {
    delete blockinputweightmatrix_;
  }
  blockinputweightmatrix_ = NULL;
  if (GetArenaNoVirtual() == NULL && outputgateweightmatrix_ != NULL) {
    delete outputgateweightmatrix_;
  }
  outputgateweightmatrix_ = NULL;
  if (GetArenaNoVirtual() == NULL && inputgaterecursionmatrix_ != NULL) {
    delete inputgaterecursionmatrix_;
  }
  inputgaterecursionmatrix_ = NULL;
  if (GetArenaNoVirtual() == NULL && forgetgaterecursionmatrix_ != NULL) {
    delete forgetgaterecursionmatrix_;
  }
  forgetgaterecursionmatrix_ = NULL;
  if (GetArenaNoVirtual() == NULL && blockinputrecursionmatrix_ != NULL) {
    delete blockinputrecursionmatrix_;
  }
  blockinputrecursionmatrix_ = NULL;
  if (GetArenaNoVirtual() == NULL && outputgaterecursionmatrix_ != NULL) {
    delete outputgaterecursionmatrix_;
  }
  outputgaterecursionmatrix_ = NULL;
  if (GetArenaNoVirtual() == NULL && inputgatebiasvector_ != NULL) {
    delete inputgatebiasvector_;
  }
  inputgatebiasvector_ = NULL;
  if (GetArenaNoVirtual() == NULL && forgetgatebiasvector_ != NULL) {
    delete forgetgatebiasvector_;
  }
  forgetgatebiasvector_ = NULL;
  if (GetArenaNoVirtual() == NULL && blockinputbiasvector_ != NULL) {
    delete blockinputbiasvector_;
  }
  blockinputbiasvector_ = NULL;
  if (GetArenaNoVirtual() == NULL && outputgatebiasvector_ != NULL) {
    delete outputgatebiasvector_;
  }
  outputgatebiasvector_ = NULL;
  if (GetArenaNoVirtual() == NULL && inputgatepeepholevector_ != NULL) {
    delete inputgatepeepholevector_;
  }
  inputgatepeepholevector_ = NULL;
  if (GetArenaNoVirtual() == NULL && forgetgatepeepholevector_ != NULL) {
    delete forgetgatepeepholevector_;
  }
  forgetgatepeepholevector_ = NULL;
  if (GetArenaNoVirtual() == NULL && outputgatepeepholevector_ != NULL) {
    delete outputgatepeepholevector_;
  }
  outputgatepeepholevector_ = NULL;
}

bool LSTMWeightParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.LSTMWeightParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(16383u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // .CoreML.Specification.WeightParams inputGateWeightMatrix = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_inputgateweightmatrix()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams forgetGateWeightMatrix = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(18u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_forgetgateweightmatrix()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams blockInputWeightMatrix = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(26u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_blockinputweightmatrix()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams outputGateWeightMatrix = 4;
      case 4: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(34u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_outputgateweightmatrix()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams inputGateRecursionMatrix = 20;
      case 20: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(162u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_inputgaterecursionmatrix()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams forgetGateRecursionMatrix = 21;
      case 21: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(170u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_forgetgaterecursionmatrix()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams blockInputRecursionMatrix = 22;
      case 22: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(178u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_blockinputrecursionmatrix()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams outputGateRecursionMatrix = 23;
      case 23: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(186u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_outputgaterecursionmatrix()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams inputGateBiasVector = 40;
      case 40: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(322u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_inputgatebiasvector()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams forgetGateBiasVector = 41;
      case 41: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(330u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_forgetgatebiasvector()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams blockInputBiasVector = 42;
      case 42: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(338u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_blockinputbiasvector()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams outputGateBiasVector = 43;
      case 43: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(346u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_outputgatebiasvector()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams inputGatePeepholeVector = 60;
      case 60: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(482u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_inputgatepeepholevector()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams forgetGatePeepholeVector = 61;
      case 61: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(490u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_forgetgatepeepholevector()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams outputGatePeepholeVector = 62;
      case 62: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(498u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_outputgatepeepholevector()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.LSTMWeightParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.LSTMWeightParams)
  return false;
#undef DO_
}

void LSTMWeightParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.LSTMWeightParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // .CoreML.Specification.WeightParams inputGateWeightMatrix = 1;
  if (this->has_inputgateweightmatrix()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1, *this->inputgateweightmatrix_, output);
  }

  // .CoreML.Specification.WeightParams forgetGateWeightMatrix = 2;
  if (this->has_forgetgateweightmatrix()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      2, *this->forgetgateweightmatrix_, output);
  }

  // .CoreML.Specification.WeightParams blockInputWeightMatrix = 3;
  if (this->has_blockinputweightmatrix()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      3, *this->blockinputweightmatrix_, output);
  }

  // .CoreML.Specification.WeightParams outputGateWeightMatrix = 4;
  if (this->has_outputgateweightmatrix()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      4, *this->outputgateweightmatrix_, output);
  }

  // .CoreML.Specification.WeightParams inputGateRecursionMatrix = 20;
  if (this->has_inputgaterecursionmatrix()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      20, *this->inputgaterecursionmatrix_, output);
  }

  // .CoreML.Specification.WeightParams forgetGateRecursionMatrix = 21;
  if (this->has_forgetgaterecursionmatrix()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      21, *this->forgetgaterecursionmatrix_, output);
  }

  // .CoreML.Specification.WeightParams blockInputRecursionMatrix = 22;
  if (this->has_blockinputrecursionmatrix()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      22, *this->blockinputrecursionmatrix_, output);
  }

  // .CoreML.Specification.WeightParams outputGateRecursionMatrix = 23;
  if (this->has_outputgaterecursionmatrix()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      23, *this->outputgaterecursionmatrix_, output);
  }

  // .CoreML.Specification.WeightParams inputGateBiasVector = 40;
  if (this->has_inputgatebiasvector()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      40, *this->inputgatebiasvector_, output);
  }

  // .CoreML.Specification.WeightParams forgetGateBiasVector = 41;
  if (this->has_forgetgatebiasvector()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      41, *this->forgetgatebiasvector_, output);
  }

  // .CoreML.Specification.WeightParams blockInputBiasVector = 42;
  if (this->has_blockinputbiasvector()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      42, *this->blockinputbiasvector_, output);
  }

  // .CoreML.Specification.WeightParams outputGateBiasVector = 43;
  if (this->has_outputgatebiasvector()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      43, *this->outputgatebiasvector_, output);
  }

  // .CoreML.Specification.WeightParams inputGatePeepholeVector = 60;
  if (this->has_inputgatepeepholevector()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      60, *this->inputgatepeepholevector_, output);
  }

  // .CoreML.Specification.WeightParams forgetGatePeepholeVector = 61;
  if (this->has_forgetgatepeepholevector()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      61, *this->forgetgatepeepholevector_, output);
  }

  // .CoreML.Specification.WeightParams outputGatePeepholeVector = 62;
  if (this->has_outputgatepeepholevector()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      62, *this->outputgatepeepholevector_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.LSTMWeightParams)
}

size_t LSTMWeightParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.LSTMWeightParams)
  size_t total_size = 0;

  // .CoreML.Specification.WeightParams inputGateWeightMatrix = 1;
  if (this->has_inputgateweightmatrix()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->inputgateweightmatrix_);
  }

  // .CoreML.Specification.WeightParams forgetGateWeightMatrix = 2;
  if (this->has_forgetgateweightmatrix()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->forgetgateweightmatrix_);
  }

  // .CoreML.Specification.WeightParams blockInputWeightMatrix = 3;
  if (this->has_blockinputweightmatrix()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->blockinputweightmatrix_);
  }

  // .CoreML.Specification.WeightParams outputGateWeightMatrix = 4;
  if (this->has_outputgateweightmatrix()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->outputgateweightmatrix_);
  }

  // .CoreML.Specification.WeightParams inputGateRecursionMatrix = 20;
  if (this->has_inputgaterecursionmatrix()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->inputgaterecursionmatrix_);
  }

  // .CoreML.Specification.WeightParams forgetGateRecursionMatrix = 21;
  if (this->has_forgetgaterecursionmatrix()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->forgetgaterecursionmatrix_);
  }

  // .CoreML.Specification.WeightParams blockInputRecursionMatrix = 22;
  if (this->has_blockinputrecursionmatrix()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->blockinputrecursionmatrix_);
  }

  // .CoreML.Specification.WeightParams outputGateRecursionMatrix = 23;
  if (this->has_outputgaterecursionmatrix()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->outputgaterecursionmatrix_);
  }

  // .CoreML.Specification.WeightParams inputGateBiasVector = 40;
  if (this->has_inputgatebiasvector()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->inputgatebiasvector_);
  }

  // .CoreML.Specification.WeightParams forgetGateBiasVector = 41;
  if (this->has_forgetgatebiasvector()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->forgetgatebiasvector_);
  }

  // .CoreML.Specification.WeightParams blockInputBiasVector = 42;
  if (this->has_blockinputbiasvector()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->blockinputbiasvector_);
  }

  // .CoreML.Specification.WeightParams outputGateBiasVector = 43;
  if (this->has_outputgatebiasvector()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->outputgatebiasvector_);
  }

  // .CoreML.Specification.WeightParams inputGatePeepholeVector = 60;
  if (this->has_inputgatepeepholevector()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->inputgatepeepholevector_);
  }

  // .CoreML.Specification.WeightParams forgetGatePeepholeVector = 61;
  if (this->has_forgetgatepeepholevector()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->forgetgatepeepholevector_);
  }

  // .CoreML.Specification.WeightParams outputGatePeepholeVector = 62;
  if (this->has_outputgatepeepholevector()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->outputgatepeepholevector_);
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void LSTMWeightParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const LSTMWeightParams*>(&from));
}

void LSTMWeightParams::MergeFrom(const LSTMWeightParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.LSTMWeightParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.has_inputgateweightmatrix()) {
    mutable_inputgateweightmatrix()->::CoreML::Specification::WeightParams::MergeFrom(from.inputgateweightmatrix());
  }
  if (from.has_forgetgateweightmatrix()) {
    mutable_forgetgateweightmatrix()->::CoreML::Specification::WeightParams::MergeFrom(from.forgetgateweightmatrix());
  }
  if (from.has_blockinputweightmatrix()) {
    mutable_blockinputweightmatrix()->::CoreML::Specification::WeightParams::MergeFrom(from.blockinputweightmatrix());
  }
  if (from.has_outputgateweightmatrix()) {
    mutable_outputgateweightmatrix()->::CoreML::Specification::WeightParams::MergeFrom(from.outputgateweightmatrix());
  }
  if (from.has_inputgaterecursionmatrix()) {
    mutable_inputgaterecursionmatrix()->::CoreML::Specification::WeightParams::MergeFrom(from.inputgaterecursionmatrix());
  }
  if (from.has_forgetgaterecursionmatrix()) {
    mutable_forgetgaterecursionmatrix()->::CoreML::Specification::WeightParams::MergeFrom(from.forgetgaterecursionmatrix());
  }
  if (from.has_blockinputrecursionmatrix()) {
    mutable_blockinputrecursionmatrix()->::CoreML::Specification::WeightParams::MergeFrom(from.blockinputrecursionmatrix());
  }
  if (from.has_outputgaterecursionmatrix()) {
    mutable_outputgaterecursionmatrix()->::CoreML::Specification::WeightParams::MergeFrom(from.outputgaterecursionmatrix());
  }
  if (from.has_inputgatebiasvector()) {
    mutable_inputgatebiasvector()->::CoreML::Specification::WeightParams::MergeFrom(from.inputgatebiasvector());
  }
  if (from.has_forgetgatebiasvector()) {
    mutable_forgetgatebiasvector()->::CoreML::Specification::WeightParams::MergeFrom(from.forgetgatebiasvector());
  }
  if (from.has_blockinputbiasvector()) {
    mutable_blockinputbiasvector()->::CoreML::Specification::WeightParams::MergeFrom(from.blockinputbiasvector());
  }
  if (from.has_outputgatebiasvector()) {
    mutable_outputgatebiasvector()->::CoreML::Specification::WeightParams::MergeFrom(from.outputgatebiasvector());
  }
  if (from.has_inputgatepeepholevector()) {
    mutable_inputgatepeepholevector()->::CoreML::Specification::WeightParams::MergeFrom(from.inputgatepeepholevector());
  }
  if (from.has_forgetgatepeepholevector()) {
    mutable_forgetgatepeepholevector()->::CoreML::Specification::WeightParams::MergeFrom(from.forgetgatepeepholevector());
  }
  if (from.has_outputgatepeepholevector()) {
    mutable_outputgatepeepholevector()->::CoreML::Specification::WeightParams::MergeFrom(from.outputgatepeepholevector());
  }
}

void LSTMWeightParams::CopyFrom(const LSTMWeightParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.LSTMWeightParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool LSTMWeightParams::IsInitialized() const {
  return true;
}

void LSTMWeightParams::Swap(LSTMWeightParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void LSTMWeightParams::InternalSwap(LSTMWeightParams* other) {
  std::swap(inputgateweightmatrix_, other->inputgateweightmatrix_);
  std::swap(forgetgateweightmatrix_, other->forgetgateweightmatrix_);
  std::swap(blockinputweightmatrix_, other->blockinputweightmatrix_);
  std::swap(outputgateweightmatrix_, other->outputgateweightmatrix_);
  std::swap(inputgaterecursionmatrix_, other->inputgaterecursionmatrix_);
  std::swap(forgetgaterecursionmatrix_, other->forgetgaterecursionmatrix_);
  std::swap(blockinputrecursionmatrix_, other->blockinputrecursionmatrix_);
  std::swap(outputgaterecursionmatrix_, other->outputgaterecursionmatrix_);
  std::swap(inputgatebiasvector_, other->inputgatebiasvector_);
  std::swap(forgetgatebiasvector_, other->forgetgatebiasvector_);
  std::swap(blockinputbiasvector_, other->blockinputbiasvector_);
  std::swap(outputgatebiasvector_, other->outputgatebiasvector_);
  std::swap(inputgatepeepholevector_, other->inputgatepeepholevector_);
  std::swap(forgetgatepeepholevector_, other->forgetgatepeepholevector_);
  std::swap(outputgatepeepholevector_, other->outputgatepeepholevector_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string LSTMWeightParams::GetTypeName() const {
  return "CoreML.Specification.LSTMWeightParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// LSTMWeightParams

// .CoreML.Specification.WeightParams inputGateWeightMatrix = 1;
bool LSTMWeightParams::has_inputgateweightmatrix() const {
  return this != internal_default_instance() && inputgateweightmatrix_ != NULL;
}
void LSTMWeightParams::clear_inputgateweightmatrix() {
  if (GetArenaNoVirtual() == NULL && inputgateweightmatrix_ != NULL) delete inputgateweightmatrix_;
  inputgateweightmatrix_ = NULL;
}
const ::CoreML::Specification::WeightParams& LSTMWeightParams::inputgateweightmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.inputGateWeightMatrix)
  return inputgateweightmatrix_ != NULL ? *inputgateweightmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_inputgateweightmatrix() {
  
  if (inputgateweightmatrix_ == NULL) {
    inputgateweightmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.inputGateWeightMatrix)
  return inputgateweightmatrix_;
}
::CoreML::Specification::WeightParams* LSTMWeightParams::release_inputgateweightmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.inputGateWeightMatrix)
  
  ::CoreML::Specification::WeightParams* temp = inputgateweightmatrix_;
  inputgateweightmatrix_ = NULL;
  return temp;
}
void LSTMWeightParams::set_allocated_inputgateweightmatrix(::CoreML::Specification::WeightParams* inputgateweightmatrix) {
  delete inputgateweightmatrix_;
  inputgateweightmatrix_ = inputgateweightmatrix;
  if (inputgateweightmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.inputGateWeightMatrix)
}

// .CoreML.Specification.WeightParams forgetGateWeightMatrix = 2;
bool LSTMWeightParams::has_forgetgateweightmatrix() const {
  return this != internal_default_instance() && forgetgateweightmatrix_ != NULL;
}
void LSTMWeightParams::clear_forgetgateweightmatrix() {
  if (GetArenaNoVirtual() == NULL && forgetgateweightmatrix_ != NULL) delete forgetgateweightmatrix_;
  forgetgateweightmatrix_ = NULL;
}
const ::CoreML::Specification::WeightParams& LSTMWeightParams::forgetgateweightmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.forgetGateWeightMatrix)
  return forgetgateweightmatrix_ != NULL ? *forgetgateweightmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_forgetgateweightmatrix() {
  
  if (forgetgateweightmatrix_ == NULL) {
    forgetgateweightmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.forgetGateWeightMatrix)
  return forgetgateweightmatrix_;
}
::CoreML::Specification::WeightParams* LSTMWeightParams::release_forgetgateweightmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.forgetGateWeightMatrix)
  
  ::CoreML::Specification::WeightParams* temp = forgetgateweightmatrix_;
  forgetgateweightmatrix_ = NULL;
  return temp;
}
void LSTMWeightParams::set_allocated_forgetgateweightmatrix(::CoreML::Specification::WeightParams* forgetgateweightmatrix) {
  delete forgetgateweightmatrix_;
  forgetgateweightmatrix_ = forgetgateweightmatrix;
  if (forgetgateweightmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.forgetGateWeightMatrix)
}

// .CoreML.Specification.WeightParams blockInputWeightMatrix = 3;
bool LSTMWeightParams::has_blockinputweightmatrix() const {
  return this != internal_default_instance() && blockinputweightmatrix_ != NULL;
}
void LSTMWeightParams::clear_blockinputweightmatrix() {
  if (GetArenaNoVirtual() == NULL && blockinputweightmatrix_ != NULL) delete blockinputweightmatrix_;
  blockinputweightmatrix_ = NULL;
}
const ::CoreML::Specification::WeightParams& LSTMWeightParams::blockinputweightmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.blockInputWeightMatrix)
  return blockinputweightmatrix_ != NULL ? *blockinputweightmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_blockinputweightmatrix() {
  
  if (blockinputweightmatrix_ == NULL) {
    blockinputweightmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.blockInputWeightMatrix)
  return blockinputweightmatrix_;
}
::CoreML::Specification::WeightParams* LSTMWeightParams::release_blockinputweightmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.blockInputWeightMatrix)
  
  ::CoreML::Specification::WeightParams* temp = blockinputweightmatrix_;
  blockinputweightmatrix_ = NULL;
  return temp;
}
void LSTMWeightParams::set_allocated_blockinputweightmatrix(::CoreML::Specification::WeightParams* blockinputweightmatrix) {
  delete blockinputweightmatrix_;
  blockinputweightmatrix_ = blockinputweightmatrix;
  if (blockinputweightmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.blockInputWeightMatrix)
}

// .CoreML.Specification.WeightParams outputGateWeightMatrix = 4;
bool LSTMWeightParams::has_outputgateweightmatrix() const {
  return this != internal_default_instance() && outputgateweightmatrix_ != NULL;
}
void LSTMWeightParams::clear_outputgateweightmatrix() {
  if (GetArenaNoVirtual() == NULL && outputgateweightmatrix_ != NULL) delete outputgateweightmatrix_;
  outputgateweightmatrix_ = NULL;
}
const ::CoreML::Specification::WeightParams& LSTMWeightParams::outputgateweightmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.outputGateWeightMatrix)
  return outputgateweightmatrix_ != NULL ? *outputgateweightmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_outputgateweightmatrix() {
  
  if (outputgateweightmatrix_ == NULL) {
    outputgateweightmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.outputGateWeightMatrix)
  return outputgateweightmatrix_;
}
::CoreML::Specification::WeightParams* LSTMWeightParams::release_outputgateweightmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.outputGateWeightMatrix)
  
  ::CoreML::Specification::WeightParams* temp = outputgateweightmatrix_;
  outputgateweightmatrix_ = NULL;
  return temp;
}
void LSTMWeightParams::set_allocated_outputgateweightmatrix(::CoreML::Specification::WeightParams* outputgateweightmatrix) {
  delete outputgateweightmatrix_;
  outputgateweightmatrix_ = outputgateweightmatrix;
  if (outputgateweightmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.outputGateWeightMatrix)
}

// .CoreML.Specification.WeightParams inputGateRecursionMatrix = 20;
bool LSTMWeightParams::has_inputgaterecursionmatrix() const {
  return this != internal_default_instance() && inputgaterecursionmatrix_ != NULL;
}
void LSTMWeightParams::clear_inputgaterecursionmatrix() {
  if (GetArenaNoVirtual() == NULL && inputgaterecursionmatrix_ != NULL) delete inputgaterecursionmatrix_;
  inputgaterecursionmatrix_ = NULL;
}
const ::CoreML::Specification::WeightParams& LSTMWeightParams::inputgaterecursionmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.inputGateRecursionMatrix)
  return inputgaterecursionmatrix_ != NULL ? *inputgaterecursionmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_inputgaterecursionmatrix() {
  
  if (inputgaterecursionmatrix_ == NULL) {
    inputgaterecursionmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.inputGateRecursionMatrix)
  return inputgaterecursionmatrix_;
}
::CoreML::Specification::WeightParams* LSTMWeightParams::release_inputgaterecursionmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.inputGateRecursionMatrix)
  
  ::CoreML::Specification::WeightParams* temp = inputgaterecursionmatrix_;
  inputgaterecursionmatrix_ = NULL;
  return temp;
}
void LSTMWeightParams::set_allocated_inputgaterecursionmatrix(::CoreML::Specification::WeightParams* inputgaterecursionmatrix) {
  delete inputgaterecursionmatrix_;
  inputgaterecursionmatrix_ = inputgaterecursionmatrix;
  if (inputgaterecursionmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.inputGateRecursionMatrix)
}

// .CoreML.Specification.WeightParams forgetGateRecursionMatrix = 21;
bool LSTMWeightParams::has_forgetgaterecursionmatrix() const {
  return this != internal_default_instance() && forgetgaterecursionmatrix_ != NULL;
}
void LSTMWeightParams::clear_forgetgaterecursionmatrix() {
  if (GetArenaNoVirtual() == NULL && forgetgaterecursionmatrix_ != NULL) delete forgetgaterecursionmatrix_;
  forgetgaterecursionmatrix_ = NULL;
}
const ::CoreML::Specification::WeightParams& LSTMWeightParams::forgetgaterecursionmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.forgetGateRecursionMatrix)
  return forgetgaterecursionmatrix_ != NULL ? *forgetgaterecursionmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_forgetgaterecursionmatrix() {
  
  if (forgetgaterecursionmatrix_ == NULL) {
    forgetgaterecursionmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.forgetGateRecursionMatrix)
  return forgetgaterecursionmatrix_;
}
::CoreML::Specification::WeightParams* LSTMWeightParams::release_forgetgaterecursionmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.forgetGateRecursionMatrix)
  
  ::CoreML::Specification::WeightParams* temp = forgetgaterecursionmatrix_;
  forgetgaterecursionmatrix_ = NULL;
  return temp;
}
void LSTMWeightParams::set_allocated_forgetgaterecursionmatrix(::CoreML::Specification::WeightParams* forgetgaterecursionmatrix) {
  delete forgetgaterecursionmatrix_;
  forgetgaterecursionmatrix_ = forgetgaterecursionmatrix;
  if (forgetgaterecursionmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.forgetGateRecursionMatrix)
}

// .CoreML.Specification.WeightParams blockInputRecursionMatrix = 22;
bool LSTMWeightParams::has_blockinputrecursionmatrix() const {
  return this != internal_default_instance() && blockinputrecursionmatrix_ != NULL;
}
void LSTMWeightParams::clear_blockinputrecursionmatrix() {
  if (GetArenaNoVirtual() == NULL && blockinputrecursionmatrix_ != NULL) delete blockinputrecursionmatrix_;
  blockinputrecursionmatrix_ = NULL;
}
const ::CoreML::Specification::WeightParams& LSTMWeightParams::blockinputrecursionmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.blockInputRecursionMatrix)
  return blockinputrecursionmatrix_ != NULL ? *blockinputrecursionmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_blockinputrecursionmatrix() {
  
  if (blockinputrecursionmatrix_ == NULL) {
    blockinputrecursionmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.blockInputRecursionMatrix)
  return blockinputrecursionmatrix_;
}
::CoreML::Specification::WeightParams* LSTMWeightParams::release_blockinputrecursionmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.blockInputRecursionMatrix)
  
  ::CoreML::Specification::WeightParams* temp = blockinputrecursionmatrix_;
  blockinputrecursionmatrix_ = NULL;
  return temp;
}
void LSTMWeightParams::set_allocated_blockinputrecursionmatrix(::CoreML::Specification::WeightParams* blockinputrecursionmatrix) {
  delete blockinputrecursionmatrix_;
  blockinputrecursionmatrix_ = blockinputrecursionmatrix;
  if (blockinputrecursionmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.blockInputRecursionMatrix)
}

// .CoreML.Specification.WeightParams outputGateRecursionMatrix = 23;
bool LSTMWeightParams::has_outputgaterecursionmatrix() const {
  return this != internal_default_instance() && outputgaterecursionmatrix_ != NULL;
}
void LSTMWeightParams::clear_outputgaterecursionmatrix() {
  if (GetArenaNoVirtual() == NULL && outputgaterecursionmatrix_ != NULL) delete outputgaterecursionmatrix_;
  outputgaterecursionmatrix_ = NULL;
}
const ::CoreML::Specification::WeightParams& LSTMWeightParams::outputgaterecursionmatrix() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.outputGateRecursionMatrix)
  return outputgaterecursionmatrix_ != NULL ? *outputgaterecursionmatrix_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_outputgaterecursionmatrix() {
  
  if (outputgaterecursionmatrix_ == NULL) {
    outputgaterecursionmatrix_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.outputGateRecursionMatrix)
  return outputgaterecursionmatrix_;
}
::CoreML::Specification::WeightParams* LSTMWeightParams::release_outputgaterecursionmatrix() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.outputGateRecursionMatrix)
  
  ::CoreML::Specification::WeightParams* temp = outputgaterecursionmatrix_;
  outputgaterecursionmatrix_ = NULL;
  return temp;
}
void LSTMWeightParams::set_allocated_outputgaterecursionmatrix(::CoreML::Specification::WeightParams* outputgaterecursionmatrix) {
  delete outputgaterecursionmatrix_;
  outputgaterecursionmatrix_ = outputgaterecursionmatrix;
  if (outputgaterecursionmatrix) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.outputGateRecursionMatrix)
}

// .CoreML.Specification.WeightParams inputGateBiasVector = 40;
bool LSTMWeightParams::has_inputgatebiasvector() const {
  return this != internal_default_instance() && inputgatebiasvector_ != NULL;
}
void LSTMWeightParams::clear_inputgatebiasvector() {
  if (GetArenaNoVirtual() == NULL && inputgatebiasvector_ != NULL) delete inputgatebiasvector_;
  inputgatebiasvector_ = NULL;
}
const ::CoreML::Specification::WeightParams& LSTMWeightParams::inputgatebiasvector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.inputGateBiasVector)
  return inputgatebiasvector_ != NULL ? *inputgatebiasvector_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_inputgatebiasvector() {
  
  if (inputgatebiasvector_ == NULL) {
    inputgatebiasvector_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.inputGateBiasVector)
  return inputgatebiasvector_;
}
::CoreML::Specification::WeightParams* LSTMWeightParams::release_inputgatebiasvector() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.inputGateBiasVector)
  
  ::CoreML::Specification::WeightParams* temp = inputgatebiasvector_;
  inputgatebiasvector_ = NULL;
  return temp;
}
void LSTMWeightParams::set_allocated_inputgatebiasvector(::CoreML::Specification::WeightParams* inputgatebiasvector) {
  delete inputgatebiasvector_;
  inputgatebiasvector_ = inputgatebiasvector;
  if (inputgatebiasvector) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.inputGateBiasVector)
}

// .CoreML.Specification.WeightParams forgetGateBiasVector = 41;
bool LSTMWeightParams::has_forgetgatebiasvector() const {
  return this != internal_default_instance() && forgetgatebiasvector_ != NULL;
}
void LSTMWeightParams::clear_forgetgatebiasvector() {
  if (GetArenaNoVirtual() == NULL && forgetgatebiasvector_ != NULL) delete forgetgatebiasvector_;
  forgetgatebiasvector_ = NULL;
}
const ::CoreML::Specification::WeightParams& LSTMWeightParams::forgetgatebiasvector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.forgetGateBiasVector)
  return forgetgatebiasvector_ != NULL ? *forgetgatebiasvector_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_forgetgatebiasvector() {
  
  if (forgetgatebiasvector_ == NULL) {
    forgetgatebiasvector_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.forgetGateBiasVector)
  return forgetgatebiasvector_;
}
::CoreML::Specification::WeightParams* LSTMWeightParams::release_forgetgatebiasvector() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.forgetGateBiasVector)
  
  ::CoreML::Specification::WeightParams* temp = forgetgatebiasvector_;
  forgetgatebiasvector_ = NULL;
  return temp;
}
void LSTMWeightParams::set_allocated_forgetgatebiasvector(::CoreML::Specification::WeightParams* forgetgatebiasvector) {
  delete forgetgatebiasvector_;
  forgetgatebiasvector_ = forgetgatebiasvector;
  if (forgetgatebiasvector) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.forgetGateBiasVector)
}

// .CoreML.Specification.WeightParams blockInputBiasVector = 42;
bool LSTMWeightParams::has_blockinputbiasvector() const {
  return this != internal_default_instance() && blockinputbiasvector_ != NULL;
}
void LSTMWeightParams::clear_blockinputbiasvector() {
  if (GetArenaNoVirtual() == NULL && blockinputbiasvector_ != NULL) delete blockinputbiasvector_;
  blockinputbiasvector_ = NULL;
}
const ::CoreML::Specification::WeightParams& LSTMWeightParams::blockinputbiasvector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.blockInputBiasVector)
  return blockinputbiasvector_ != NULL ? *blockinputbiasvector_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_blockinputbiasvector() {
  
  if (blockinputbiasvector_ == NULL) {
    blockinputbiasvector_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.blockInputBiasVector)
  return blockinputbiasvector_;
}
::CoreML::Specification::WeightParams* LSTMWeightParams::release_blockinputbiasvector() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.blockInputBiasVector)
  
  ::CoreML::Specification::WeightParams* temp = blockinputbiasvector_;
  blockinputbiasvector_ = NULL;
  return temp;
}
void LSTMWeightParams::set_allocated_blockinputbiasvector(::CoreML::Specification::WeightParams* blockinputbiasvector) {
  delete blockinputbiasvector_;
  blockinputbiasvector_ = blockinputbiasvector;
  if (blockinputbiasvector) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.blockInputBiasVector)
}

// .CoreML.Specification.WeightParams outputGateBiasVector = 43;
bool LSTMWeightParams::has_outputgatebiasvector() const {
  return this != internal_default_instance() && outputgatebiasvector_ != NULL;
}
void LSTMWeightParams::clear_outputgatebiasvector() {
  if (GetArenaNoVirtual() == NULL && outputgatebiasvector_ != NULL) delete outputgatebiasvector_;
  outputgatebiasvector_ = NULL;
}
const ::CoreML::Specification::WeightParams& LSTMWeightParams::outputgatebiasvector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.outputGateBiasVector)
  return outputgatebiasvector_ != NULL ? *outputgatebiasvector_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_outputgatebiasvector() {
  
  if (outputgatebiasvector_ == NULL) {
    outputgatebiasvector_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.outputGateBiasVector)
  return outputgatebiasvector_;
}
::CoreML::Specification::WeightParams* LSTMWeightParams::release_outputgatebiasvector() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.outputGateBiasVector)
  
  ::CoreML::Specification::WeightParams* temp = outputgatebiasvector_;
  outputgatebiasvector_ = NULL;
  return temp;
}
void LSTMWeightParams::set_allocated_outputgatebiasvector(::CoreML::Specification::WeightParams* outputgatebiasvector) {
  delete outputgatebiasvector_;
  outputgatebiasvector_ = outputgatebiasvector;
  if (outputgatebiasvector) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.outputGateBiasVector)
}

// .CoreML.Specification.WeightParams inputGatePeepholeVector = 60;
bool LSTMWeightParams::has_inputgatepeepholevector() const {
  return this != internal_default_instance() && inputgatepeepholevector_ != NULL;
}
void LSTMWeightParams::clear_inputgatepeepholevector() {
  if (GetArenaNoVirtual() == NULL && inputgatepeepholevector_ != NULL) delete inputgatepeepholevector_;
  inputgatepeepholevector_ = NULL;
}
const ::CoreML::Specification::WeightParams& LSTMWeightParams::inputgatepeepholevector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.inputGatePeepholeVector)
  return inputgatepeepholevector_ != NULL ? *inputgatepeepholevector_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_inputgatepeepholevector() {
  
  if (inputgatepeepholevector_ == NULL) {
    inputgatepeepholevector_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.inputGatePeepholeVector)
  return inputgatepeepholevector_;
}
::CoreML::Specification::WeightParams* LSTMWeightParams::release_inputgatepeepholevector() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.inputGatePeepholeVector)
  
  ::CoreML::Specification::WeightParams* temp = inputgatepeepholevector_;
  inputgatepeepholevector_ = NULL;
  return temp;
}
void LSTMWeightParams::set_allocated_inputgatepeepholevector(::CoreML::Specification::WeightParams* inputgatepeepholevector) {
  delete inputgatepeepholevector_;
  inputgatepeepholevector_ = inputgatepeepholevector;
  if (inputgatepeepholevector) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.inputGatePeepholeVector)
}

// .CoreML.Specification.WeightParams forgetGatePeepholeVector = 61;
bool LSTMWeightParams::has_forgetgatepeepholevector() const {
  return this != internal_default_instance() && forgetgatepeepholevector_ != NULL;
}
void LSTMWeightParams::clear_forgetgatepeepholevector() {
  if (GetArenaNoVirtual() == NULL && forgetgatepeepholevector_ != NULL) delete forgetgatepeepholevector_;
  forgetgatepeepholevector_ = NULL;
}
const ::CoreML::Specification::WeightParams& LSTMWeightParams::forgetgatepeepholevector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.forgetGatePeepholeVector)
  return forgetgatepeepholevector_ != NULL ? *forgetgatepeepholevector_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_forgetgatepeepholevector() {
  
  if (forgetgatepeepholevector_ == NULL) {
    forgetgatepeepholevector_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.forgetGatePeepholeVector)
  return forgetgatepeepholevector_;
}
::CoreML::Specification::WeightParams* LSTMWeightParams::release_forgetgatepeepholevector() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.forgetGatePeepholeVector)
  
  ::CoreML::Specification::WeightParams* temp = forgetgatepeepholevector_;
  forgetgatepeepholevector_ = NULL;
  return temp;
}
void LSTMWeightParams::set_allocated_forgetgatepeepholevector(::CoreML::Specification::WeightParams* forgetgatepeepholevector) {
  delete forgetgatepeepholevector_;
  forgetgatepeepholevector_ = forgetgatepeepholevector;
  if (forgetgatepeepholevector) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.forgetGatePeepholeVector)
}

// .CoreML.Specification.WeightParams outputGatePeepholeVector = 62;
bool LSTMWeightParams::has_outputgatepeepholevector() const {
  return this != internal_default_instance() && outputgatepeepholevector_ != NULL;
}
void LSTMWeightParams::clear_outputgatepeepholevector() {
  if (GetArenaNoVirtual() == NULL && outputgatepeepholevector_ != NULL) delete outputgatepeepholevector_;
  outputgatepeepholevector_ = NULL;
}
const ::CoreML::Specification::WeightParams& LSTMWeightParams::outputgatepeepholevector() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LSTMWeightParams.outputGatePeepholeVector)
  return outputgatepeepholevector_ != NULL ? *outputgatepeepholevector_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* LSTMWeightParams::mutable_outputgatepeepholevector() {
  
  if (outputgatepeepholevector_ == NULL) {
    outputgatepeepholevector_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LSTMWeightParams.outputGatePeepholeVector)
  return outputgatepeepholevector_;
}
::CoreML::Specification::WeightParams* LSTMWeightParams::release_outputgatepeepholevector() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LSTMWeightParams.outputGatePeepholeVector)
  
  ::CoreML::Specification::WeightParams* temp = outputgatepeepholevector_;
  outputgatepeepholevector_ = NULL;
  return temp;
}
void LSTMWeightParams::set_allocated_outputgatepeepholevector(::CoreML::Specification::WeightParams* outputgatepeepholevector) {
  delete outputgatepeepholevector_;
  outputgatepeepholevector_ = outputgatepeepholevector;
  if (outputgatepeepholevector) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LSTMWeightParams.outputGatePeepholeVector)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int UniDirectionalLSTMLayerParams::kInputVectorSizeFieldNumber;
const int UniDirectionalLSTMLayerParams::kOutputVectorSizeFieldNumber;
const int UniDirectionalLSTMLayerParams::kActivationsFieldNumber;
const int UniDirectionalLSTMLayerParams::kParamsFieldNumber;
const int UniDirectionalLSTMLayerParams::kWeightParamsFieldNumber;
const int UniDirectionalLSTMLayerParams::kReverseInputFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

UniDirectionalLSTMLayerParams::UniDirectionalLSTMLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.UniDirectionalLSTMLayerParams)
}
UniDirectionalLSTMLayerParams::UniDirectionalLSTMLayerParams(const UniDirectionalLSTMLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      activations_(from.activations_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_params()) {
    params_ = new ::CoreML::Specification::LSTMParams(*from.params_);
  } else {
    params_ = NULL;
  }
  if (from.has_weightparams()) {
    weightparams_ = new ::CoreML::Specification::LSTMWeightParams(*from.weightparams_);
  } else {
    weightparams_ = NULL;
  }
  ::memcpy(&inputvectorsize_, &from.inputvectorsize_,
    reinterpret_cast<char*>(&reverseinput_) -
    reinterpret_cast<char*>(&inputvectorsize_) + sizeof(reverseinput_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.UniDirectionalLSTMLayerParams)
}

void UniDirectionalLSTMLayerParams::SharedCtor() {
  ::memset(&params_, 0, reinterpret_cast<char*>(&reverseinput_) -
    reinterpret_cast<char*>(&params_) + sizeof(reverseinput_));
  _cached_size_ = 0;
}

UniDirectionalLSTMLayerParams::~UniDirectionalLSTMLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.UniDirectionalLSTMLayerParams)
  SharedDtor();
}

void UniDirectionalLSTMLayerParams::SharedDtor() {
  if (this != internal_default_instance()) {
    delete params_;
  }
  if (this != internal_default_instance()) {
    delete weightparams_;
  }
}

void UniDirectionalLSTMLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const UniDirectionalLSTMLayerParams& UniDirectionalLSTMLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

UniDirectionalLSTMLayerParams* UniDirectionalLSTMLayerParams::New(::google::protobuf::Arena* arena) const {
  UniDirectionalLSTMLayerParams* n = new UniDirectionalLSTMLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void UniDirectionalLSTMLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.UniDirectionalLSTMLayerParams)
  activations_.Clear();
  if (GetArenaNoVirtual() == NULL && params_ != NULL) {
    delete params_;
  }
  params_ = NULL;
  if (GetArenaNoVirtual() == NULL && weightparams_ != NULL) {
    delete weightparams_;
  }
  weightparams_ = NULL;
  ::memset(&inputvectorsize_, 0, reinterpret_cast<char*>(&reverseinput_) -
    reinterpret_cast<char*>(&inputvectorsize_) + sizeof(reverseinput_));
}

bool UniDirectionalLSTMLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.UniDirectionalLSTMLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(16383u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // uint64 inputVectorSize = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &inputvectorsize_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // uint64 outputVectorSize = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(16u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &outputvectorsize_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated .CoreML.Specification.ActivationParams activations = 10;
      case 10: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(82u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
                input, add_activations()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.LSTMParams params = 15;
      case 15: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(122u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_params()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.LSTMWeightParams weightParams = 20;
      case 20: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(162u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_weightparams()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool reverseInput = 100;
      case 100: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(800u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &reverseinput_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.UniDirectionalLSTMLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.UniDirectionalLSTMLayerParams)
  return false;
#undef DO_
}

void UniDirectionalLSTMLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.UniDirectionalLSTMLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // uint64 inputVectorSize = 1;
  if (this->inputvectorsize() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(1, this->inputvectorsize(), output);
  }

  // uint64 outputVectorSize = 2;
  if (this->outputvectorsize() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(2, this->outputvectorsize(), output);
  }

  // repeated .CoreML.Specification.ActivationParams activations = 10;
  for (unsigned int i = 0, n = this->activations_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      10, this->activations(i), output);
  }

  // .CoreML.Specification.LSTMParams params = 15;
  if (this->has_params()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      15, *this->params_, output);
  }

  // .CoreML.Specification.LSTMWeightParams weightParams = 20;
  if (this->has_weightparams()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      20, *this->weightparams_, output);
  }

  // bool reverseInput = 100;
  if (this->reverseinput() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(100, this->reverseinput(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.UniDirectionalLSTMLayerParams)
}

size_t UniDirectionalLSTMLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.UniDirectionalLSTMLayerParams)
  size_t total_size = 0;

  // repeated .CoreML.Specification.ActivationParams activations = 10;
  {
    unsigned int count = this->activations_size();
    total_size += 1UL * count;
    for (unsigned int i = 0; i < count; i++) {
      total_size +=
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          this->activations(i));
    }
  }

  // .CoreML.Specification.LSTMParams params = 15;
  if (this->has_params()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->params_);
  }

  // .CoreML.Specification.LSTMWeightParams weightParams = 20;
  if (this->has_weightparams()) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->weightparams_);
  }

  // uint64 inputVectorSize = 1;
  if (this->inputvectorsize() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->inputvectorsize());
  }

  // uint64 outputVectorSize = 2;
  if (this->outputvectorsize() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->outputvectorsize());
  }

  // bool reverseInput = 100;
  if (this->reverseinput() != 0) {
    total_size += 2 + 1;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void UniDirectionalLSTMLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const UniDirectionalLSTMLayerParams*>(&from));
}

void UniDirectionalLSTMLayerParams::MergeFrom(const UniDirectionalLSTMLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.UniDirectionalLSTMLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  activations_.MergeFrom(from.activations_);
  if (from.has_params()) {
    mutable_params()->::CoreML::Specification::LSTMParams::MergeFrom(from.params());
  }
  if (from.has_weightparams()) {
    mutable_weightparams()->::CoreML::Specification::LSTMWeightParams::MergeFrom(from.weightparams());
  }
  if (from.inputvectorsize() != 0) {
    set_inputvectorsize(from.inputvectorsize());
  }
  if (from.outputvectorsize() != 0) {
    set_outputvectorsize(from.outputvectorsize());
  }
  if (from.reverseinput() != 0) {
    set_reverseinput(from.reverseinput());
  }
}

void UniDirectionalLSTMLayerParams::CopyFrom(const UniDirectionalLSTMLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.UniDirectionalLSTMLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool UniDirectionalLSTMLayerParams::IsInitialized() const {
  return true;
}

void UniDirectionalLSTMLayerParams::Swap(UniDirectionalLSTMLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void UniDirectionalLSTMLayerParams::InternalSwap(UniDirectionalLSTMLayerParams* other) {
  activations_.InternalSwap(&other->activations_);
  std::swap(params_, other->params_);
  std::swap(weightparams_, other->weightparams_);
  std::swap(inputvectorsize_, other->inputvectorsize_);
  std::swap(outputvectorsize_, other->outputvectorsize_);
  std::swap(reverseinput_, other->reverseinput_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string UniDirectionalLSTMLayerParams::GetTypeName() const {
  return "CoreML.Specification.UniDirectionalLSTMLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// UniDirectionalLSTMLayerParams

// uint64 inputVectorSize = 1;
void UniDirectionalLSTMLayerParams::clear_inputvectorsize() {
  inputvectorsize_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 UniDirectionalLSTMLayerParams::inputvectorsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UniDirectionalLSTMLayerParams.inputVectorSize)
  return inputvectorsize_;
}
void UniDirectionalLSTMLayerParams::set_inputvectorsize(::google::protobuf::uint64 value) {
  
  inputvectorsize_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.UniDirectionalLSTMLayerParams.inputVectorSize)
}

// uint64 outputVectorSize = 2;
void UniDirectionalLSTMLayerParams::clear_outputvectorsize() {
  outputvectorsize_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 UniDirectionalLSTMLayerParams::outputvectorsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UniDirectionalLSTMLayerParams.outputVectorSize)
  return outputvectorsize_;
}
void UniDirectionalLSTMLayerParams::set_outputvectorsize(::google::protobuf::uint64 value) {
  
  outputvectorsize_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.UniDirectionalLSTMLayerParams.outputVectorSize)
}

// repeated .CoreML.Specification.ActivationParams activations = 10;
int UniDirectionalLSTMLayerParams::activations_size() const {
  return activations_.size();
}
void UniDirectionalLSTMLayerParams::clear_activations() {
  activations_.Clear();
}
const ::CoreML::Specification::ActivationParams& UniDirectionalLSTMLayerParams::activations(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UniDirectionalLSTMLayerParams.activations)
  return activations_.Get(index);
}
::CoreML::Specification::ActivationParams* UniDirectionalLSTMLayerParams::mutable_activations(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.UniDirectionalLSTMLayerParams.activations)
  return activations_.Mutable(index);
}
::CoreML::Specification::ActivationParams* UniDirectionalLSTMLayerParams::add_activations() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.UniDirectionalLSTMLayerParams.activations)
  return activations_.Add();
}
::google::protobuf::RepeatedPtrField< ::CoreML::Specification::ActivationParams >*
UniDirectionalLSTMLayerParams::mutable_activations() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.UniDirectionalLSTMLayerParams.activations)
  return &activations_;
}
const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::ActivationParams >&
UniDirectionalLSTMLayerParams::activations() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.UniDirectionalLSTMLayerParams.activations)
  return activations_;
}

// .CoreML.Specification.LSTMParams params = 15;
bool UniDirectionalLSTMLayerParams::has_params() const {
  return this != internal_default_instance() && params_ != NULL;
}
void UniDirectionalLSTMLayerParams::clear_params() {
  if (GetArenaNoVirtual() == NULL && params_ != NULL) delete params_;
  params_ = NULL;
}
const ::CoreML::Specification::LSTMParams& UniDirectionalLSTMLayerParams::params() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UniDirectionalLSTMLayerParams.params)
  return params_ != NULL ? *params_
                         : *::CoreML::Specification::LSTMParams::internal_default_instance();
}
::CoreML::Specification::LSTMParams* UniDirectionalLSTMLayerParams::mutable_params() {
  
  if (params_ == NULL) {
    params_ = new ::CoreML::Specification::LSTMParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.UniDirectionalLSTMLayerParams.params)
  return params_;
}
::CoreML::Specification::LSTMParams* UniDirectionalLSTMLayerParams::release_params() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.UniDirectionalLSTMLayerParams.params)
  
  ::CoreML::Specification::LSTMParams* temp = params_;
  params_ = NULL;
  return temp;
}
void UniDirectionalLSTMLayerParams::set_allocated_params(::CoreML::Specification::LSTMParams* params) {
  delete params_;
  params_ = params;
  if (params) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.UniDirectionalLSTMLayerParams.params)
}

// .CoreML.Specification.LSTMWeightParams weightParams = 20;
bool UniDirectionalLSTMLayerParams::has_weightparams() const {
  return this != internal_default_instance() && weightparams_ != NULL;
}
void UniDirectionalLSTMLayerParams::clear_weightparams() {
  if (GetArenaNoVirtual() == NULL && weightparams_ != NULL) delete weightparams_;
  weightparams_ = NULL;
}
const ::CoreML::Specification::LSTMWeightParams& UniDirectionalLSTMLayerParams::weightparams() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UniDirectionalLSTMLayerParams.weightParams)
  return weightparams_ != NULL ? *weightparams_
                         : *::CoreML::Specification::LSTMWeightParams::internal_default_instance();
}
::CoreML::Specification::LSTMWeightParams* UniDirectionalLSTMLayerParams::mutable_weightparams() {
  
  if (weightparams_ == NULL) {
    weightparams_ = new ::CoreML::Specification::LSTMWeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.UniDirectionalLSTMLayerParams.weightParams)
  return weightparams_;
}
::CoreML::Specification::LSTMWeightParams* UniDirectionalLSTMLayerParams::release_weightparams() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.UniDirectionalLSTMLayerParams.weightParams)
  
  ::CoreML::Specification::LSTMWeightParams* temp = weightparams_;
  weightparams_ = NULL;
  return temp;
}
void UniDirectionalLSTMLayerParams::set_allocated_weightparams(::CoreML::Specification::LSTMWeightParams* weightparams) {
  delete weightparams_;
  weightparams_ = weightparams;
  if (weightparams) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.UniDirectionalLSTMLayerParams.weightParams)
}

// bool reverseInput = 100;
void UniDirectionalLSTMLayerParams::clear_reverseinput() {
  reverseinput_ = false;
}
bool UniDirectionalLSTMLayerParams::reverseinput() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.UniDirectionalLSTMLayerParams.reverseInput)
  return reverseinput_;
}
void UniDirectionalLSTMLayerParams::set_reverseinput(bool value) {
  
  reverseinput_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.UniDirectionalLSTMLayerParams.reverseInput)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int BiDirectionalLSTMLayerParams::kInputVectorSizeFieldNumber;
const int BiDirectionalLSTMLayerParams::kOutputVectorSizeFieldNumber;
const int BiDirectionalLSTMLayerParams::kActivationsForwardLSTMFieldNumber;
const int BiDirectionalLSTMLayerParams::kActivationsBackwardLSTMFieldNumber;
const int BiDirectionalLSTMLayerParams::kParamsFieldNumber;
const int BiDirectionalLSTMLayerParams::kWeightParamsFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

BiDirectionalLSTMLayerParams::BiDirectionalLSTMLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.BiDirectionalLSTMLayerParams)
}
BiDirectionalLSTMLayerParams::BiDirectionalLSTMLayerParams(const BiDirectionalLSTMLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      activationsforwardlstm_(from.activationsforwardlstm_),
      activationsbackwardlstm_(from.activationsbackwardlstm_),
      weightparams_(from.weightparams_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_params()) {
    params_ = new ::CoreML::Specification::LSTMParams(*from.params_);
  } else {
    params_ = NULL;
  }
  ::memcpy(&inputvectorsize_, &from.inputvectorsize_,
    reinterpret_cast<char*>(&outputvectorsize_) -
    reinterpret_cast<char*>(&inputvectorsize_) + sizeof(outputvectorsize_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.BiDirectionalLSTMLayerParams)
}

void BiDirectionalLSTMLayerParams::SharedCtor() {
  ::memset(&params_, 0, reinterpret_cast<char*>(&outputvectorsize_) -
    reinterpret_cast<char*>(&params_) + sizeof(outputvectorsize_));
  _cached_size_ = 0;
}

BiDirectionalLSTMLayerParams::~BiDirectionalLSTMLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.BiDirectionalLSTMLayerParams)
  SharedDtor();
}

void BiDirectionalLSTMLayerParams::SharedDtor() {
  if (this != internal_default_instance()) {
    delete params_;
  }
}

void BiDirectionalLSTMLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const BiDirectionalLSTMLayerParams& BiDirectionalLSTMLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

BiDirectionalLSTMLayerParams* BiDirectionalLSTMLayerParams::New(::google::protobuf::Arena* arena) const {
  BiDirectionalLSTMLayerParams* n = new BiDirectionalLSTMLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void BiDirectionalLSTMLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.BiDirectionalLSTMLayerParams)
  activationsforwardlstm_.Clear();
  activationsbackwardlstm_.Clear();
  weightparams_.Clear();
  if (GetArenaNoVirtual() == NULL && params_ != NULL) {
    delete params_;
  }
  params_ = NULL;
  ::memset(&inputvectorsize_, 0, reinterpret_cast<char*>(&outputvectorsize_) -
    reinterpret_cast<char*>(&inputvectorsize_) + sizeof(outputvectorsize_));
}

bool BiDirectionalLSTMLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.BiDirectionalLSTMLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(16383u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // uint64 inputVectorSize = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &inputvectorsize_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // uint64 outputVectorSize = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(16u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &outputvectorsize_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated .CoreML.Specification.ActivationParams activationsForwardLSTM = 10;
      case 10: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(82u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
                input, add_activationsforwardlstm()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated .CoreML.Specification.ActivationParams activationsBackwardLSTM = 11;
      case 11: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(90u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
                input, add_activationsbackwardlstm()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.LSTMParams params = 15;
      case 15: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(122u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_params()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated .CoreML.Specification.LSTMWeightParams weightParams = 20;
      case 20: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(162u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
                input, add_weightparams()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.BiDirectionalLSTMLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.BiDirectionalLSTMLayerParams)
  return false;
#undef DO_
}

void BiDirectionalLSTMLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.BiDirectionalLSTMLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // uint64 inputVectorSize = 1;
  if (this->inputvectorsize() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(1, this->inputvectorsize(), output);
  }

  // uint64 outputVectorSize = 2;
  if (this->outputvectorsize() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(2, this->outputvectorsize(), output);
  }

  // repeated .CoreML.Specification.ActivationParams activationsForwardLSTM = 10;
  for (unsigned int i = 0, n = this->activationsforwardlstm_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      10, this->activationsforwardlstm(i), output);
  }

  // repeated .CoreML.Specification.ActivationParams activationsBackwardLSTM = 11;
  for (unsigned int i = 0, n = this->activationsbackwardlstm_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      11, this->activationsbackwardlstm(i), output);
  }

  // .CoreML.Specification.LSTMParams params = 15;
  if (this->has_params()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      15, *this->params_, output);
  }

  // repeated .CoreML.Specification.LSTMWeightParams weightParams = 20;
  for (unsigned int i = 0, n = this->weightparams_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      20, this->weightparams(i), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.BiDirectionalLSTMLayerParams)
}

size_t BiDirectionalLSTMLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.BiDirectionalLSTMLayerParams)
  size_t total_size = 0;

  // repeated .CoreML.Specification.ActivationParams activationsForwardLSTM = 10;
  {
    unsigned int count = this->activationsforwardlstm_size();
    total_size += 1UL * count;
    for (unsigned int i = 0; i < count; i++) {
      total_size +=
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          this->activationsforwardlstm(i));
    }
  }

  // repeated .CoreML.Specification.ActivationParams activationsBackwardLSTM = 11;
  {
    unsigned int count = this->activationsbackwardlstm_size();
    total_size += 1UL * count;
    for (unsigned int i = 0; i < count; i++) {
      total_size +=
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          this->activationsbackwardlstm(i));
    }
  }

  // repeated .CoreML.Specification.LSTMWeightParams weightParams = 20;
  {
    unsigned int count = this->weightparams_size();
    total_size += 2UL * count;
    for (unsigned int i = 0; i < count; i++) {
      total_size +=
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          this->weightparams(i));
    }
  }

  // .CoreML.Specification.LSTMParams params = 15;
  if (this->has_params()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->params_);
  }

  // uint64 inputVectorSize = 1;
  if (this->inputvectorsize() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->inputvectorsize());
  }

  // uint64 outputVectorSize = 2;
  if (this->outputvectorsize() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->outputvectorsize());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void BiDirectionalLSTMLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const BiDirectionalLSTMLayerParams*>(&from));
}

void BiDirectionalLSTMLayerParams::MergeFrom(const BiDirectionalLSTMLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.BiDirectionalLSTMLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  activationsforwardlstm_.MergeFrom(from.activationsforwardlstm_);
  activationsbackwardlstm_.MergeFrom(from.activationsbackwardlstm_);
  weightparams_.MergeFrom(from.weightparams_);
  if (from.has_params()) {
    mutable_params()->::CoreML::Specification::LSTMParams::MergeFrom(from.params());
  }
  if (from.inputvectorsize() != 0) {
    set_inputvectorsize(from.inputvectorsize());
  }
  if (from.outputvectorsize() != 0) {
    set_outputvectorsize(from.outputvectorsize());
  }
}

void BiDirectionalLSTMLayerParams::CopyFrom(const BiDirectionalLSTMLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.BiDirectionalLSTMLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool BiDirectionalLSTMLayerParams::IsInitialized() const {
  return true;
}

void BiDirectionalLSTMLayerParams::Swap(BiDirectionalLSTMLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void BiDirectionalLSTMLayerParams::InternalSwap(BiDirectionalLSTMLayerParams* other) {
  activationsforwardlstm_.InternalSwap(&other->activationsforwardlstm_);
  activationsbackwardlstm_.InternalSwap(&other->activationsbackwardlstm_);
  weightparams_.InternalSwap(&other->weightparams_);
  std::swap(params_, other->params_);
  std::swap(inputvectorsize_, other->inputvectorsize_);
  std::swap(outputvectorsize_, other->outputvectorsize_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string BiDirectionalLSTMLayerParams::GetTypeName() const {
  return "CoreML.Specification.BiDirectionalLSTMLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// BiDirectionalLSTMLayerParams

// uint64 inputVectorSize = 1;
void BiDirectionalLSTMLayerParams::clear_inputvectorsize() {
  inputvectorsize_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 BiDirectionalLSTMLayerParams::inputvectorsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BiDirectionalLSTMLayerParams.inputVectorSize)
  return inputvectorsize_;
}
void BiDirectionalLSTMLayerParams::set_inputvectorsize(::google::protobuf::uint64 value) {
  
  inputvectorsize_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.BiDirectionalLSTMLayerParams.inputVectorSize)
}

// uint64 outputVectorSize = 2;
void BiDirectionalLSTMLayerParams::clear_outputvectorsize() {
  outputvectorsize_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 BiDirectionalLSTMLayerParams::outputvectorsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BiDirectionalLSTMLayerParams.outputVectorSize)
  return outputvectorsize_;
}
void BiDirectionalLSTMLayerParams::set_outputvectorsize(::google::protobuf::uint64 value) {
  
  outputvectorsize_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.BiDirectionalLSTMLayerParams.outputVectorSize)
}

// repeated .CoreML.Specification.ActivationParams activationsForwardLSTM = 10;
int BiDirectionalLSTMLayerParams::activationsforwardlstm_size() const {
  return activationsforwardlstm_.size();
}
void BiDirectionalLSTMLayerParams::clear_activationsforwardlstm() {
  activationsforwardlstm_.Clear();
}
const ::CoreML::Specification::ActivationParams& BiDirectionalLSTMLayerParams::activationsforwardlstm(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BiDirectionalLSTMLayerParams.activationsForwardLSTM)
  return activationsforwardlstm_.Get(index);
}
::CoreML::Specification::ActivationParams* BiDirectionalLSTMLayerParams::mutable_activationsforwardlstm(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BiDirectionalLSTMLayerParams.activationsForwardLSTM)
  return activationsforwardlstm_.Mutable(index);
}
::CoreML::Specification::ActivationParams* BiDirectionalLSTMLayerParams::add_activationsforwardlstm() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.BiDirectionalLSTMLayerParams.activationsForwardLSTM)
  return activationsforwardlstm_.Add();
}
::google::protobuf::RepeatedPtrField< ::CoreML::Specification::ActivationParams >*
BiDirectionalLSTMLayerParams::mutable_activationsforwardlstm() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.BiDirectionalLSTMLayerParams.activationsForwardLSTM)
  return &activationsforwardlstm_;
}
const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::ActivationParams >&
BiDirectionalLSTMLayerParams::activationsforwardlstm() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.BiDirectionalLSTMLayerParams.activationsForwardLSTM)
  return activationsforwardlstm_;
}

// repeated .CoreML.Specification.ActivationParams activationsBackwardLSTM = 11;
int BiDirectionalLSTMLayerParams::activationsbackwardlstm_size() const {
  return activationsbackwardlstm_.size();
}
void BiDirectionalLSTMLayerParams::clear_activationsbackwardlstm() {
  activationsbackwardlstm_.Clear();
}
const ::CoreML::Specification::ActivationParams& BiDirectionalLSTMLayerParams::activationsbackwardlstm(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BiDirectionalLSTMLayerParams.activationsBackwardLSTM)
  return activationsbackwardlstm_.Get(index);
}
::CoreML::Specification::ActivationParams* BiDirectionalLSTMLayerParams::mutable_activationsbackwardlstm(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BiDirectionalLSTMLayerParams.activationsBackwardLSTM)
  return activationsbackwardlstm_.Mutable(index);
}
::CoreML::Specification::ActivationParams* BiDirectionalLSTMLayerParams::add_activationsbackwardlstm() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.BiDirectionalLSTMLayerParams.activationsBackwardLSTM)
  return activationsbackwardlstm_.Add();
}
::google::protobuf::RepeatedPtrField< ::CoreML::Specification::ActivationParams >*
BiDirectionalLSTMLayerParams::mutable_activationsbackwardlstm() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.BiDirectionalLSTMLayerParams.activationsBackwardLSTM)
  return &activationsbackwardlstm_;
}
const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::ActivationParams >&
BiDirectionalLSTMLayerParams::activationsbackwardlstm() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.BiDirectionalLSTMLayerParams.activationsBackwardLSTM)
  return activationsbackwardlstm_;
}

// .CoreML.Specification.LSTMParams params = 15;
bool BiDirectionalLSTMLayerParams::has_params() const {
  return this != internal_default_instance() && params_ != NULL;
}
void BiDirectionalLSTMLayerParams::clear_params() {
  if (GetArenaNoVirtual() == NULL && params_ != NULL) delete params_;
  params_ = NULL;
}
const ::CoreML::Specification::LSTMParams& BiDirectionalLSTMLayerParams::params() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BiDirectionalLSTMLayerParams.params)
  return params_ != NULL ? *params_
                         : *::CoreML::Specification::LSTMParams::internal_default_instance();
}
::CoreML::Specification::LSTMParams* BiDirectionalLSTMLayerParams::mutable_params() {
  
  if (params_ == NULL) {
    params_ = new ::CoreML::Specification::LSTMParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BiDirectionalLSTMLayerParams.params)
  return params_;
}
::CoreML::Specification::LSTMParams* BiDirectionalLSTMLayerParams::release_params() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.BiDirectionalLSTMLayerParams.params)
  
  ::CoreML::Specification::LSTMParams* temp = params_;
  params_ = NULL;
  return temp;
}
void BiDirectionalLSTMLayerParams::set_allocated_params(::CoreML::Specification::LSTMParams* params) {
  delete params_;
  params_ = params;
  if (params) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.BiDirectionalLSTMLayerParams.params)
}

// repeated .CoreML.Specification.LSTMWeightParams weightParams = 20;
int BiDirectionalLSTMLayerParams::weightparams_size() const {
  return weightparams_.size();
}
void BiDirectionalLSTMLayerParams::clear_weightparams() {
  weightparams_.Clear();
}
const ::CoreML::Specification::LSTMWeightParams& BiDirectionalLSTMLayerParams::weightparams(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BiDirectionalLSTMLayerParams.weightParams)
  return weightparams_.Get(index);
}
::CoreML::Specification::LSTMWeightParams* BiDirectionalLSTMLayerParams::mutable_weightparams(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BiDirectionalLSTMLayerParams.weightParams)
  return weightparams_.Mutable(index);
}
::CoreML::Specification::LSTMWeightParams* BiDirectionalLSTMLayerParams::add_weightparams() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.BiDirectionalLSTMLayerParams.weightParams)
  return weightparams_.Add();
}
::google::protobuf::RepeatedPtrField< ::CoreML::Specification::LSTMWeightParams >*
BiDirectionalLSTMLayerParams::mutable_weightparams() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.BiDirectionalLSTMLayerParams.weightParams)
  return &weightparams_;
}
const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::LSTMWeightParams >&
BiDirectionalLSTMLayerParams::weightparams() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.BiDirectionalLSTMLayerParams.weightParams)
  return weightparams_;
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int CustomLayerParams_CustomLayerParamValue::kDoubleValueFieldNumber;
const int CustomLayerParams_CustomLayerParamValue::kStringValueFieldNumber;
const int CustomLayerParams_CustomLayerParamValue::kIntValueFieldNumber;
const int CustomLayerParams_CustomLayerParamValue::kLongValueFieldNumber;
const int CustomLayerParams_CustomLayerParamValue::kBoolValueFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

CustomLayerParams_CustomLayerParamValue::CustomLayerParams_CustomLayerParamValue()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.CustomLayerParams.CustomLayerParamValue)
}
CustomLayerParams_CustomLayerParamValue::CustomLayerParams_CustomLayerParamValue(const CustomLayerParams_CustomLayerParamValue& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  clear_has_value();
  switch (from.value_case()) {
    case kDoubleValue: {
      set_doublevalue(from.doublevalue());
      break;
    }
    case kStringValue: {
      set_stringvalue(from.stringvalue());
      break;
    }
    case kIntValue: {
      set_intvalue(from.intvalue());
      break;
    }
    case kLongValue: {
      set_longvalue(from.longvalue());
      break;
    }
    case kBoolValue: {
      set_boolvalue(from.boolvalue());
      break;
    }
    case VALUE_NOT_SET: {
      break;
    }
  }
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.CustomLayerParams.CustomLayerParamValue)
}

void CustomLayerParams_CustomLayerParamValue::SharedCtor() {
  clear_has_value();
  _cached_size_ = 0;
}

CustomLayerParams_CustomLayerParamValue::~CustomLayerParams_CustomLayerParamValue() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.CustomLayerParams.CustomLayerParamValue)
  SharedDtor();
}

void CustomLayerParams_CustomLayerParamValue::SharedDtor() {
  if (has_value()) {
    clear_value();
  }
}

void CustomLayerParams_CustomLayerParamValue::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const CustomLayerParams_CustomLayerParamValue& CustomLayerParams_CustomLayerParamValue::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

CustomLayerParams_CustomLayerParamValue* CustomLayerParams_CustomLayerParamValue::New(::google::protobuf::Arena* arena) const {
  CustomLayerParams_CustomLayerParamValue* n = new CustomLayerParams_CustomLayerParamValue;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void CustomLayerParams_CustomLayerParamValue::clear_value() {
// @@protoc_insertion_point(one_of_clear_start:CoreML.Specification.CustomLayerParams.CustomLayerParamValue)
  switch (value_case()) {
    case kDoubleValue: {
      // No need to clear
      break;
    }
    case kStringValue: {
      value_.stringvalue_.DestroyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
      break;
    }
    case kIntValue: {
      // No need to clear
      break;
    }
    case kLongValue: {
      // No need to clear
      break;
    }
    case kBoolValue: {
      // No need to clear
      break;
    }
    case VALUE_NOT_SET: {
      break;
    }
  }
  _oneof_case_[0] = VALUE_NOT_SET;
}


void CustomLayerParams_CustomLayerParamValue::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.CustomLayerParams.CustomLayerParamValue)
  clear_value();
}

bool CustomLayerParams_CustomLayerParamValue::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.CustomLayerParams.CustomLayerParamValue)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(16383u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // double doubleValue = 10;
      case 10: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(81u)) {
          clear_value();
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   double, ::google::protobuf::internal::WireFormatLite::TYPE_DOUBLE>(
                 input, &value_.doublevalue_)));
          set_has_doublevalue();
        } else {
          goto handle_unusual;
        }
        break;
      }

      // string stringValue = 20;
      case 20: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(162u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadString(
                input, this->mutable_stringvalue()));
          DO_(::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
            this->stringvalue().data(), this->stringvalue().length(),
            ::google::protobuf::internal::WireFormatLite::PARSE,
            "CoreML.Specification.CustomLayerParams.CustomLayerParamValue.stringValue"));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // int32 intValue = 30;
      case 30: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(240u)) {
          clear_value();
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::int32, ::google::protobuf::internal::WireFormatLite::TYPE_INT32>(
                 input, &value_.intvalue_)));
          set_has_intvalue();
        } else {
          goto handle_unusual;
        }
        break;
      }

      // int64 longValue = 40;
      case 40: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(320u)) {
          clear_value();
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 input, &value_.longvalue_)));
          set_has_longvalue();
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool boolValue = 50;
      case 50: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(400u)) {
          clear_value();
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &value_.boolvalue_)));
          set_has_boolvalue();
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.CustomLayerParams.CustomLayerParamValue)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.CustomLayerParams.CustomLayerParamValue)
  return false;
#undef DO_
}

void CustomLayerParams_CustomLayerParamValue::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.CustomLayerParams.CustomLayerParamValue)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // double doubleValue = 10;
  if (has_doublevalue()) {
    ::google::protobuf::internal::WireFormatLite::WriteDouble(10, this->doublevalue(), output);
  }

  // string stringValue = 20;
  if (has_stringvalue()) {
    ::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
      this->stringvalue().data(), this->stringvalue().length(),
      ::google::protobuf::internal::WireFormatLite::SERIALIZE,
      "CoreML.Specification.CustomLayerParams.CustomLayerParamValue.stringValue");
    ::google::protobuf::internal::WireFormatLite::WriteStringMaybeAliased(
      20, this->stringvalue(), output);
  }

  // int32 intValue = 30;
  if (has_intvalue()) {
    ::google::protobuf::internal::WireFormatLite::WriteInt32(30, this->intvalue(), output);
  }

  // int64 longValue = 40;
  if (has_longvalue()) {
    ::google::protobuf::internal::WireFormatLite::WriteInt64(40, this->longvalue(), output);
  }

  // bool boolValue = 50;
  if (has_boolvalue()) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(50, this->boolvalue(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.CustomLayerParams.CustomLayerParamValue)
}

size_t CustomLayerParams_CustomLayerParamValue::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.CustomLayerParams.CustomLayerParamValue)
  size_t total_size = 0;

  switch (value_case()) {
    // double doubleValue = 10;
    case kDoubleValue: {
      total_size += 1 + 8;
      break;
    }
    // string stringValue = 20;
    case kStringValue: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::StringSize(
          this->stringvalue());
      break;
    }
    // int32 intValue = 30;
    case kIntValue: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(
          this->intvalue());
      break;
    }
    // int64 longValue = 40;
    case kLongValue: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::Int64Size(
          this->longvalue());
      break;
    }
    // bool boolValue = 50;
    case kBoolValue: {
      total_size += 2 + 1;
      break;
    }
    case VALUE_NOT_SET: {
      break;
    }
  }
  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void CustomLayerParams_CustomLayerParamValue::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const CustomLayerParams_CustomLayerParamValue*>(&from));
}

void CustomLayerParams_CustomLayerParamValue::MergeFrom(const CustomLayerParams_CustomLayerParamValue& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.CustomLayerParams.CustomLayerParamValue)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  switch (from.value_case()) {
    case kDoubleValue: {
      set_doublevalue(from.doublevalue());
      break;
    }
    case kStringValue: {
      set_stringvalue(from.stringvalue());
      break;
    }
    case kIntValue: {
      set_intvalue(from.intvalue());
      break;
    }
    case kLongValue: {
      set_longvalue(from.longvalue());
      break;
    }
    case kBoolValue: {
      set_boolvalue(from.boolvalue());
      break;
    }
    case VALUE_NOT_SET: {
      break;
    }
  }
}

void CustomLayerParams_CustomLayerParamValue::CopyFrom(const CustomLayerParams_CustomLayerParamValue& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.CustomLayerParams.CustomLayerParamValue)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool CustomLayerParams_CustomLayerParamValue::IsInitialized() const {
  return true;
}

void CustomLayerParams_CustomLayerParamValue::Swap(CustomLayerParams_CustomLayerParamValue* other) {
  if (other == this) return;
  InternalSwap(other);
}
void CustomLayerParams_CustomLayerParamValue::InternalSwap(CustomLayerParams_CustomLayerParamValue* other) {
  std::swap(value_, other->value_);
  std::swap(_oneof_case_[0], other->_oneof_case_[0]);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string CustomLayerParams_CustomLayerParamValue::GetTypeName() const {
  return "CoreML.Specification.CustomLayerParams.CustomLayerParamValue";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// CustomLayerParams_CustomLayerParamValue

// double doubleValue = 10;
bool CustomLayerParams_CustomLayerParamValue::has_doublevalue() const {
  return value_case() == kDoubleValue;
}
void CustomLayerParams_CustomLayerParamValue::set_has_doublevalue() {
  _oneof_case_[0] = kDoubleValue;
}
void CustomLayerParams_CustomLayerParamValue::clear_doublevalue() {
  if (has_doublevalue()) {
    value_.doublevalue_ = 0;
    clear_has_value();
  }
}
double CustomLayerParams_CustomLayerParamValue::doublevalue() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.doubleValue)
  if (has_doublevalue()) {
    return value_.doublevalue_;
  }
  return 0;
}
void CustomLayerParams_CustomLayerParamValue::set_doublevalue(double value) {
  if (!has_doublevalue()) {
    clear_value();
    set_has_doublevalue();
  }
  value_.doublevalue_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.doubleValue)
}

// string stringValue = 20;
bool CustomLayerParams_CustomLayerParamValue::has_stringvalue() const {
  return value_case() == kStringValue;
}
void CustomLayerParams_CustomLayerParamValue::set_has_stringvalue() {
  _oneof_case_[0] = kStringValue;
}
void CustomLayerParams_CustomLayerParamValue::clear_stringvalue() {
  if (has_stringvalue()) {
    value_.stringvalue_.DestroyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
    clear_has_value();
  }
}
const ::std::string& CustomLayerParams_CustomLayerParamValue::stringvalue() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.stringValue)
  if (has_stringvalue()) {
    return value_.stringvalue_.GetNoArena();
  }
  return *&::google::protobuf::internal::GetEmptyStringAlreadyInited();
}
void CustomLayerParams_CustomLayerParamValue::set_stringvalue(const ::std::string& value) {
  // @@protoc_insertion_point(field_set:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.stringValue)
  if (!has_stringvalue()) {
    clear_value();
    set_has_stringvalue();
    value_.stringvalue_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  }
  value_.stringvalue_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.stringValue)
}
#if LANG_CXX11
void CustomLayerParams_CustomLayerParamValue::set_stringvalue(::std::string&& value) {
  // @@protoc_insertion_point(field_set:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.stringValue)
  if (!has_stringvalue()) {
    clear_value();
    set_has_stringvalue();
    value_.stringvalue_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  }
  value_.stringvalue_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.stringValue)
}
#endif
void CustomLayerParams_CustomLayerParamValue::set_stringvalue(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  if (!has_stringvalue()) {
    clear_value();
    set_has_stringvalue();
    value_.stringvalue_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  }
  value_.stringvalue_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(value));
  // @@protoc_insertion_point(field_set_char:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.stringValue)
}
void CustomLayerParams_CustomLayerParamValue::set_stringvalue(const char* value, size_t size) {
  if (!has_stringvalue()) {
    clear_value();
    set_has_stringvalue();
    value_.stringvalue_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  }
  value_.stringvalue_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(
      reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.stringValue)
}
::std::string* CustomLayerParams_CustomLayerParamValue::mutable_stringvalue() {
  if (!has_stringvalue()) {
    clear_value();
    set_has_stringvalue();
    value_.stringvalue_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.stringValue)
  return value_.stringvalue_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
::std::string* CustomLayerParams_CustomLayerParamValue::release_stringvalue() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.stringValue)
  if (has_stringvalue()) {
    clear_has_value();
    return value_.stringvalue_.ReleaseNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  } else {
    return NULL;
  }
}
void CustomLayerParams_CustomLayerParamValue::set_allocated_stringvalue(::std::string* stringvalue) {
  if (!has_stringvalue()) {
    value_.stringvalue_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  }
  clear_value();
  if (stringvalue != NULL) {
    set_has_stringvalue();
    value_.stringvalue_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
        stringvalue);
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.stringValue)
}

// int32 intValue = 30;
bool CustomLayerParams_CustomLayerParamValue::has_intvalue() const {
  return value_case() == kIntValue;
}
void CustomLayerParams_CustomLayerParamValue::set_has_intvalue() {
  _oneof_case_[0] = kIntValue;
}
void CustomLayerParams_CustomLayerParamValue::clear_intvalue() {
  if (has_intvalue()) {
    value_.intvalue_ = 0;
    clear_has_value();
  }
}
::google::protobuf::int32 CustomLayerParams_CustomLayerParamValue::intvalue() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.intValue)
  if (has_intvalue()) {
    return value_.intvalue_;
  }
  return 0;
}
void CustomLayerParams_CustomLayerParamValue::set_intvalue(::google::protobuf::int32 value) {
  if (!has_intvalue()) {
    clear_value();
    set_has_intvalue();
  }
  value_.intvalue_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.intValue)
}

// int64 longValue = 40;
bool CustomLayerParams_CustomLayerParamValue::has_longvalue() const {
  return value_case() == kLongValue;
}
void CustomLayerParams_CustomLayerParamValue::set_has_longvalue() {
  _oneof_case_[0] = kLongValue;
}
void CustomLayerParams_CustomLayerParamValue::clear_longvalue() {
  if (has_longvalue()) {
    value_.longvalue_ = GOOGLE_LONGLONG(0);
    clear_has_value();
  }
}
::google::protobuf::int64 CustomLayerParams_CustomLayerParamValue::longvalue() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.longValue)
  if (has_longvalue()) {
    return value_.longvalue_;
  }
  return GOOGLE_LONGLONG(0);
}
void CustomLayerParams_CustomLayerParamValue::set_longvalue(::google::protobuf::int64 value) {
  if (!has_longvalue()) {
    clear_value();
    set_has_longvalue();
  }
  value_.longvalue_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.longValue)
}

// bool boolValue = 50;
bool CustomLayerParams_CustomLayerParamValue::has_boolvalue() const {
  return value_case() == kBoolValue;
}
void CustomLayerParams_CustomLayerParamValue::set_has_boolvalue() {
  _oneof_case_[0] = kBoolValue;
}
void CustomLayerParams_CustomLayerParamValue::clear_boolvalue() {
  if (has_boolvalue()) {
    value_.boolvalue_ = false;
    clear_has_value();
  }
}
bool CustomLayerParams_CustomLayerParamValue::boolvalue() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.boolValue)
  if (has_boolvalue()) {
    return value_.boolvalue_;
  }
  return false;
}
void CustomLayerParams_CustomLayerParamValue::set_boolvalue(bool value) {
  if (!has_boolvalue()) {
    clear_value();
    set_has_boolvalue();
  }
  value_.boolvalue_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.CustomLayerParams.CustomLayerParamValue.boolValue)
}

bool CustomLayerParams_CustomLayerParamValue::has_value() const {
  return value_case() != VALUE_NOT_SET;
}
void CustomLayerParams_CustomLayerParamValue::clear_has_value() {
  _oneof_case_[0] = VALUE_NOT_SET;
}
CustomLayerParams_CustomLayerParamValue::ValueCase CustomLayerParams_CustomLayerParamValue::value_case() const {
  return CustomLayerParams_CustomLayerParamValue::ValueCase(_oneof_case_[0]);
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if PROTOBUF_INLINE_NOT_IN_HEADERS
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int CustomLayerParams::kClassNameFieldNumber;
const int CustomLayerParams::kWeightsFieldNumber;
const int CustomLayerParams::kParametersFieldNumber;
const int CustomLayerParams::kDescriptionFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

CustomLayerParams::CustomLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.CustomLayerParams)
}
CustomLayerParams::CustomLayerParams(const CustomLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      weights_(from.weights_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  parameters_.MergeFrom(from.parameters_);
  classname_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (from.classname().size() > 0) {
    classname_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.classname_);
  }
  description_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (from.description().size() > 0) {
    description_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.description_);
  }
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.CustomLayerParams)
}

void CustomLayerParams::SharedCtor() {
  classname_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  description_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  _cached_size_ = 0;
}

CustomLayerParams::~CustomLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.CustomLayerParams)
  SharedDtor();
}

void CustomLayerParams::SharedDtor() {
  classname_.DestroyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  description_.DestroyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}

void CustomLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const CustomLayerParams& CustomLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

CustomLayerParams* CustomLayerParams::New(::google::protobuf::Arena* arena) const {
  CustomLayerParams* n = new CustomLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void CustomLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.CustomLayerParams)
  weights_.Clear();
  parameters_.Clear();
  classname_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  description_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}

bool CustomLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.CustomLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(16383u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // string className = 10;
      case 10: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(82u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadString(
                input, this->mutable_classname()));
          DO_(::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
            this->classname().data(), this->classname().length(),
            ::google::protobuf::internal::WireFormatLite::PARSE,
            "CoreML.Specification.CustomLayerParams.className"));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated .CoreML.Specification.WeightParams weights = 20;
      case 20: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(162u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
                input, add_weights()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // map<string, .CoreML.Specification.CustomLayerParams.CustomLayerParamValue> parameters = 30;
      case 30: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(242u)) {
          CustomLayerParams_ParametersEntry::Parser< ::google::protobuf::internal::MapFieldLite<
              CustomLayerParams_ParametersEntry,
              ::std::string, ::CoreML::Specification::CustomLayerParams_CustomLayerParamValue,
              ::google::protobuf::internal::WireFormatLite::TYPE_STRING,
              ::google::protobuf::internal::WireFormatLite::TYPE_MESSAGE,
              0 >,
            ::google::protobuf::Map< ::std::string, ::CoreML::Specification::CustomLayerParams_CustomLayerParamValue > > parser(&parameters_);
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
              input, &parser));
          DO_(::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
            parser.key().data(), parser.key().length(),
            ::google::protobuf::internal::WireFormatLite::PARSE,
            "CoreML.Specification.CustomLayerParams.ParametersEntry.key"));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // string description = 40;
      case 40: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(322u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadString(
                input, this->mutable_description()));
          DO_(::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
            this->description().data(), this->description().length(),
            ::google::protobuf::internal::WireFormatLite::PARSE,
            "CoreML.Specification.CustomLayerParams.description"));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.CustomLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.CustomLayerParams)
  return false;
#undef DO_
}

void CustomLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.CustomLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // string className = 10;
  if (this->classname().size() > 0) {
    ::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
      this->classname().data(), this->classname().length(),
      ::google::protobuf::internal::WireFormatLite::SERIALIZE,
      "CoreML.Specification.CustomLayerParams.className");
    ::google::protobuf::internal::WireFormatLite::WriteStringMaybeAliased(
      10, this->classname(), output);
  }

  // repeated .CoreML.Specification.WeightParams weights = 20;
  for (unsigned int i = 0, n = this->weights_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      20, this->weights(i), output);
  }

  // map<string, .CoreML.Specification.CustomLayerParams.CustomLayerParamValue> parameters = 30;
  if (!this->parameters().empty()) {
    typedef ::google::protobuf::Map< ::std::string, ::CoreML::Specification::CustomLayerParams_CustomLayerParamValue >::const_pointer
        ConstPtr;
    typedef ConstPtr SortItem;
    typedef ::google::protobuf::internal::CompareByDerefFirst<SortItem> Less;
    struct Utf8Check {
      static void Check(ConstPtr p) {
        ::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
          p->first.data(), p->first.length(),
          ::google::protobuf::internal::WireFormatLite::SERIALIZE,
          "CoreML.Specification.CustomLayerParams.ParametersEntry.key");
      }
    };

    if (output->IsSerializationDeterministic() &&
        this->parameters().size() > 1) {
      ::google::protobuf::scoped_array<SortItem> items(
          new SortItem[this->parameters().size()]);
      typedef ::google::protobuf::Map< ::std::string, ::CoreML::Specification::CustomLayerParams_CustomLayerParamValue >::size_type size_type;
      size_type n = 0;
      for (::google::protobuf::Map< ::std::string, ::CoreML::Specification::CustomLayerParams_CustomLayerParamValue >::const_iterator
          it = this->parameters().begin();
          it != this->parameters().end(); ++it, ++n) {
        items[n] = SortItem(&*it);
      }
      ::std::sort(&items[0], &items[n], Less());
      ::google::protobuf::scoped_ptr<CustomLayerParams_ParametersEntry> entry;
      for (size_type i = 0; i < n; i++) {
        entry.reset(parameters_.NewEntryWrapper(
            items[i]->first, items[i]->second));
        ::google::protobuf::internal::WireFormatLite::WriteMessage(
            30, *entry, output);
        Utf8Check::Check(items[i]);
      }
    } else {
      ::google::protobuf::scoped_ptr<CustomLayerParams_ParametersEntry> entry;
      for (::google::protobuf::Map< ::std::string, ::CoreML::Specification::CustomLayerParams_CustomLayerParamValue >::const_iterator
          it = this->parameters().begin();
          it != this->parameters().end(); ++it) {
        entry.reset(parameters_.NewEntryWrapper(
            it->first, it->second));
        ::google::protobuf::internal::WireFormatLite::WriteMessage(
            30, *entry, output);
        Utf8Check::Check(&*it);
      }
    }
  }

  // string description = 40;
  if (this->description().size() > 0) {
    ::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
      this->description().data(), this->description().length(),
      ::google::protobuf::internal::WireFormatLite::SERIALIZE,
      "CoreML.Specification.CustomLayerParams.description");
    ::google::protobuf::internal::WireFormatLite::WriteStringMaybeAliased(
      40, this->description(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.CustomLayerParams)
}

size_t CustomLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.CustomLayerParams)
  size_t total_size = 0;

  // repeated .CoreML.Specification.WeightParams weights = 20;
  {
    unsigned int count = this->weights_size();
    total_size += 2UL * count;
    for (unsigned int i = 0; i < count; i++) {
      total_size +=
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          this->weights(i));
    }
  }

  // map<string, .CoreML.Specification.CustomLayerParams.CustomLayerParamValue> parameters = 30;
  total_size += 2 *
      ::google::protobuf::internal::FromIntSize(this->parameters_size());
  {
    ::google::protobuf::scoped_ptr<CustomLayerParams_ParametersEntry> entry;
    for (::google::protobuf::Map< ::std::string, ::CoreML::Specification::CustomLayerParams_CustomLayerParamValue >::const_iterator
        it = this->parameters().begin();
        it != this->parameters().end(); ++it) {
      entry.reset(parameters_.NewEntryWrapper(it->first, it->second));
      total_size += ::google::protobuf::internal::WireFormatLite::
          MessageSizeNoVirtual(*entry);
    }
  }

  // string className = 10;
  if (this->classname().size() > 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::StringSize(
        this->classname());
  }

  // string description = 40;
  if (this->description().size() > 0) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::StringSize(
        this->description());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void CustomLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const CustomLayerParams*>(&from));
}

void CustomLayerParams::MergeFrom(const CustomLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.CustomLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  weights_.MergeFrom(from.weights_);
  parameters_.MergeFrom(from.parameters_);
  if (from.classname().size() > 0) {

    classname_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.classname_);
  }
  if (from.description().size() > 0) {

    description_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.description_);
  }
}

void CustomLayerParams::CopyFrom(const CustomLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.CustomLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool CustomLayerParams::IsInitialized() const {
  return true;
}

void CustomLayerParams::Swap(CustomLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void CustomLayerParams::InternalSwap(CustomLayerParams* other) {
  weights_.InternalSwap(&other->weights_);
  parameters_.Swap(&other->parameters_);
  classname_.Swap(&other->classname_);
  description_.Swap(&other->description_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string CustomLayerParams::GetTypeName() const {
  return "CoreML.Specification.CustomLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// CustomLayerParams

// string className = 10;
void CustomLayerParams::clear_classname() {
  classname_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
const ::std::string& CustomLayerParams::classname() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CustomLayerParams.className)
  return classname_.GetNoArena();
}
void CustomLayerParams::set_classname(const ::std::string& value) {
  
  classname_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.CustomLayerParams.className)
}
#if LANG_CXX11
void CustomLayerParams::set_classname(::std::string&& value) {
  
  classname_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:CoreML.Specification.CustomLayerParams.className)
}
#endif
void CustomLayerParams::set_classname(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  
  classname_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:CoreML.Specification.CustomLayerParams.className)
}
void CustomLayerParams::set_classname(const char* value, size_t size) {
  
  classname_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:CoreML.Specification.CustomLayerParams.className)
}
::std::string* CustomLayerParams::mutable_classname() {
  
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.CustomLayerParams.className)
  return classname_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
::std::string* CustomLayerParams::release_classname() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.CustomLayerParams.className)
  
  return classname_.ReleaseNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
void CustomLayerParams::set_allocated_classname(::std::string* classname) {
  if (classname != NULL) {
    
  } else {
    
  }
  classname_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), classname);
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.CustomLayerParams.className)
}

// repeated .CoreML.Specification.WeightParams weights = 20;
int CustomLayerParams::weights_size() const {
  return weights_.size();
}
void CustomLayerParams::clear_weights() {
  weights_.Clear();
}
const ::CoreML::Specification::WeightParams& CustomLayerParams::weights(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CustomLayerParams.weights)
  return weights_.Get(index);
}
::CoreML::Specification::WeightParams* CustomLayerParams::mutable_weights(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.CustomLayerParams.weights)
  return weights_.Mutable(index);
}
::CoreML::Specification::WeightParams* CustomLayerParams::add_weights() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.CustomLayerParams.weights)
  return weights_.Add();
}
::google::protobuf::RepeatedPtrField< ::CoreML::Specification::WeightParams >*
CustomLayerParams::mutable_weights() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.CustomLayerParams.weights)
  return &weights_;
}
const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::WeightParams >&
CustomLayerParams::weights() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.CustomLayerParams.weights)
  return weights_;
}

// map<string, .CoreML.Specification.CustomLayerParams.CustomLayerParamValue> parameters = 30;
int CustomLayerParams::parameters_size() const {
  return parameters_.size();
}
void CustomLayerParams::clear_parameters() {
  parameters_.Clear();
}
 const ::google::protobuf::Map< ::std::string, ::CoreML::Specification::CustomLayerParams_CustomLayerParamValue >&
CustomLayerParams::parameters() const {
  // @@protoc_insertion_point(field_map:CoreML.Specification.CustomLayerParams.parameters)
  return parameters_.GetMap();
}
 ::google::protobuf::Map< ::std::string, ::CoreML::Specification::CustomLayerParams_CustomLayerParamValue >*
CustomLayerParams::mutable_parameters() {
  // @@protoc_insertion_point(field_mutable_map:CoreML.Specification.CustomLayerParams.parameters)
  return parameters_.MutableMap();
}

// string description = 40;
void CustomLayerParams::clear_description() {
  description_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
const ::std::string& CustomLayerParams::description() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CustomLayerParams.description)
  return description_.GetNoArena();
}
void CustomLayerParams::set_description(const ::std::string& value) {
  
  description_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.CustomLayerParams.description)
}
#if LANG_CXX11
void CustomLayerParams::set_description(::std::string&& value) {
  
  description_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:CoreML.Specification.CustomLayerParams.description)
}
#endif
void CustomLayerParams::set_description(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  
  description_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:CoreML.Specification.CustomLayerParams.description)
}
void CustomLayerParams::set_description(const char* value, size_t size) {
  
  description_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:CoreML.Specification.CustomLayerParams.description)
}
::std::string* CustomLayerParams::mutable_description() {
  
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.CustomLayerParams.description)
  return description_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
::std::string* CustomLayerParams::release_description() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.CustomLayerParams.description)
  
  return description_.ReleaseNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
void CustomLayerParams::set_allocated_description(::std::string* description) {
  if (description != NULL) {
    
  } else {
    
  }
  description_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), description);
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.CustomLayerParams.description)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int TransposeNDLayerParams::kAxesFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

TransposeNDLayerParams::TransposeNDLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.TransposeNDLayerParams)
}
TransposeNDLayerParams::TransposeNDLayerParams(const TransposeNDLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      axes_(from.axes_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.TransposeNDLayerParams)
}

void TransposeNDLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

TransposeNDLayerParams::~TransposeNDLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.TransposeNDLayerParams)
  SharedDtor();
}

void TransposeNDLayerParams::SharedDtor() {
}

void TransposeNDLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const TransposeNDLayerParams& TransposeNDLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

TransposeNDLayerParams* TransposeNDLayerParams::New(::google::protobuf::Arena* arena) const {
  TransposeNDLayerParams* n = new TransposeNDLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void TransposeNDLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.TransposeNDLayerParams)
  axes_.Clear();
}

bool TransposeNDLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.TransposeNDLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated uint64 axes = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, this->mutable_axes())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(8u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 1, 10u, input, this->mutable_axes())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.TransposeNDLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.TransposeNDLayerParams)
  return false;
#undef DO_
}

void TransposeNDLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.TransposeNDLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated uint64 axes = 1;
  if (this->axes_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(1, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_axes_cached_byte_size_);
  }
  for (int i = 0, n = this->axes_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64NoTag(
      this->axes(i), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.TransposeNDLayerParams)
}

size_t TransposeNDLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.TransposeNDLayerParams)
  size_t total_size = 0;

  // repeated uint64 axes = 1;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      UInt64Size(this->axes_);
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _axes_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void TransposeNDLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const TransposeNDLayerParams*>(&from));
}

void TransposeNDLayerParams::MergeFrom(const TransposeNDLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.TransposeNDLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  axes_.MergeFrom(from.axes_);
}

void TransposeNDLayerParams::CopyFrom(const TransposeNDLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.TransposeNDLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool TransposeNDLayerParams::IsInitialized() const {
  return true;
}

void TransposeNDLayerParams::Swap(TransposeNDLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void TransposeNDLayerParams::InternalSwap(TransposeNDLayerParams* other) {
  axes_.InternalSwap(&other->axes_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string TransposeNDLayerParams::GetTypeName() const {
  return "CoreML.Specification.TransposeNDLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// TransposeNDLayerParams

// repeated uint64 axes = 1;
int TransposeNDLayerParams::axes_size() const {
  return axes_.size();
}
void TransposeNDLayerParams::clear_axes() {
  axes_.Clear();
}
::google::protobuf::uint64 TransposeNDLayerParams::axes(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.TransposeNDLayerParams.axes)
  return axes_.Get(index);
}
void TransposeNDLayerParams::set_axes(int index, ::google::protobuf::uint64 value) {
  axes_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.TransposeNDLayerParams.axes)
}
void TransposeNDLayerParams::add_axes(::google::protobuf::uint64 value) {
  axes_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.TransposeNDLayerParams.axes)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
TransposeNDLayerParams::axes() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.TransposeNDLayerParams.axes)
  return axes_;
}
::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
TransposeNDLayerParams::mutable_axes() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.TransposeNDLayerParams.axes)
  return &axes_;
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int BatchedMatMulParams::kTransposeAFieldNumber;
const int BatchedMatMulParams::kTransposeBFieldNumber;
const int BatchedMatMulParams::kWeightMatrixFirstDimensionFieldNumber;
const int BatchedMatMulParams::kWeightMatrixSecondDimensionFieldNumber;
const int BatchedMatMulParams::kHasBiasFieldNumber;
const int BatchedMatMulParams::kWeightsFieldNumber;
const int BatchedMatMulParams::kBiasFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

BatchedMatMulParams::BatchedMatMulParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.BatchedMatMulParams)
}
BatchedMatMulParams::BatchedMatMulParams(const BatchedMatMulParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_weights()) {
    weights_ = new ::CoreML::Specification::WeightParams(*from.weights_);
  } else {
    weights_ = NULL;
  }
  if (from.has_bias()) {
    bias_ = new ::CoreML::Specification::WeightParams(*from.bias_);
  } else {
    bias_ = NULL;
  }
  ::memcpy(&weightmatrixfirstdimension_, &from.weightmatrixfirstdimension_,
    reinterpret_cast<char*>(&hasbias_) -
    reinterpret_cast<char*>(&weightmatrixfirstdimension_) + sizeof(hasbias_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.BatchedMatMulParams)
}

void BatchedMatMulParams::SharedCtor() {
  ::memset(&weights_, 0, reinterpret_cast<char*>(&hasbias_) -
    reinterpret_cast<char*>(&weights_) + sizeof(hasbias_));
  _cached_size_ = 0;
}

BatchedMatMulParams::~BatchedMatMulParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.BatchedMatMulParams)
  SharedDtor();
}

void BatchedMatMulParams::SharedDtor() {
  if (this != internal_default_instance()) {
    delete weights_;
  }
  if (this != internal_default_instance()) {
    delete bias_;
  }
}

void BatchedMatMulParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const BatchedMatMulParams& BatchedMatMulParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

BatchedMatMulParams* BatchedMatMulParams::New(::google::protobuf::Arena* arena) const {
  BatchedMatMulParams* n = new BatchedMatMulParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void BatchedMatMulParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.BatchedMatMulParams)
  if (GetArenaNoVirtual() == NULL && weights_ != NULL) {
    delete weights_;
  }
  weights_ = NULL;
  if (GetArenaNoVirtual() == NULL && bias_ != NULL) {
    delete bias_;
  }
  bias_ = NULL;
  ::memset(&weightmatrixfirstdimension_, 0, reinterpret_cast<char*>(&hasbias_) -
    reinterpret_cast<char*>(&weightmatrixfirstdimension_) + sizeof(hasbias_));
}

bool BatchedMatMulParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.BatchedMatMulParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // bool transposeA = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &transposea_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool transposeB = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(16u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &transposeb_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // uint64 weightMatrixFirstDimension = 5;
      case 5: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(40u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &weightmatrixfirstdimension_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // uint64 weightMatrixSecondDimension = 6;
      case 6: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(48u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &weightmatrixseconddimension_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool hasBias = 7;
      case 7: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(56u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &hasbias_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams weights = 8;
      case 8: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(66u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_weights()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams bias = 9;
      case 9: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(74u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_bias()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.BatchedMatMulParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.BatchedMatMulParams)
  return false;
#undef DO_
}

void BatchedMatMulParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.BatchedMatMulParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // bool transposeA = 1;
  if (this->transposea() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(1, this->transposea(), output);
  }

  // bool transposeB = 2;
  if (this->transposeb() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(2, this->transposeb(), output);
  }

  // uint64 weightMatrixFirstDimension = 5;
  if (this->weightmatrixfirstdimension() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(5, this->weightmatrixfirstdimension(), output);
  }

  // uint64 weightMatrixSecondDimension = 6;
  if (this->weightmatrixseconddimension() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(6, this->weightmatrixseconddimension(), output);
  }

  // bool hasBias = 7;
  if (this->hasbias() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(7, this->hasbias(), output);
  }

  // .CoreML.Specification.WeightParams weights = 8;
  if (this->has_weights()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      8, *this->weights_, output);
  }

  // .CoreML.Specification.WeightParams bias = 9;
  if (this->has_bias()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      9, *this->bias_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.BatchedMatMulParams)
}

size_t BatchedMatMulParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.BatchedMatMulParams)
  size_t total_size = 0;

  // .CoreML.Specification.WeightParams weights = 8;
  if (this->has_weights()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->weights_);
  }

  // .CoreML.Specification.WeightParams bias = 9;
  if (this->has_bias()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->bias_);
  }

  // uint64 weightMatrixFirstDimension = 5;
  if (this->weightmatrixfirstdimension() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->weightmatrixfirstdimension());
  }

  // uint64 weightMatrixSecondDimension = 6;
  if (this->weightmatrixseconddimension() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->weightmatrixseconddimension());
  }

  // bool transposeA = 1;
  if (this->transposea() != 0) {
    total_size += 1 + 1;
  }

  // bool transposeB = 2;
  if (this->transposeb() != 0) {
    total_size += 1 + 1;
  }

  // bool hasBias = 7;
  if (this->hasbias() != 0) {
    total_size += 1 + 1;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void BatchedMatMulParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const BatchedMatMulParams*>(&from));
}

void BatchedMatMulParams::MergeFrom(const BatchedMatMulParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.BatchedMatMulParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.has_weights()) {
    mutable_weights()->::CoreML::Specification::WeightParams::MergeFrom(from.weights());
  }
  if (from.has_bias()) {
    mutable_bias()->::CoreML::Specification::WeightParams::MergeFrom(from.bias());
  }
  if (from.weightmatrixfirstdimension() != 0) {
    set_weightmatrixfirstdimension(from.weightmatrixfirstdimension());
  }
  if (from.weightmatrixseconddimension() != 0) {
    set_weightmatrixseconddimension(from.weightmatrixseconddimension());
  }
  if (from.transposea() != 0) {
    set_transposea(from.transposea());
  }
  if (from.transposeb() != 0) {
    set_transposeb(from.transposeb());
  }
  if (from.hasbias() != 0) {
    set_hasbias(from.hasbias());
  }
}

void BatchedMatMulParams::CopyFrom(const BatchedMatMulParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.BatchedMatMulParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool BatchedMatMulParams::IsInitialized() const {
  return true;
}

void BatchedMatMulParams::Swap(BatchedMatMulParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void BatchedMatMulParams::InternalSwap(BatchedMatMulParams* other) {
  std::swap(weights_, other->weights_);
  std::swap(bias_, other->bias_);
  std::swap(weightmatrixfirstdimension_, other->weightmatrixfirstdimension_);
  std::swap(weightmatrixseconddimension_, other->weightmatrixseconddimension_);
  std::swap(transposea_, other->transposea_);
  std::swap(transposeb_, other->transposeb_);
  std::swap(hasbias_, other->hasbias_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string BatchedMatMulParams::GetTypeName() const {
  return "CoreML.Specification.BatchedMatMulParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// BatchedMatMulParams

// bool transposeA = 1;
void BatchedMatMulParams::clear_transposea() {
  transposea_ = false;
}
bool BatchedMatMulParams::transposea() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchedMatMulParams.transposeA)
  return transposea_;
}
void BatchedMatMulParams::set_transposea(bool value) {
  
  transposea_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.BatchedMatMulParams.transposeA)
}

// bool transposeB = 2;
void BatchedMatMulParams::clear_transposeb() {
  transposeb_ = false;
}
bool BatchedMatMulParams::transposeb() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchedMatMulParams.transposeB)
  return transposeb_;
}
void BatchedMatMulParams::set_transposeb(bool value) {
  
  transposeb_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.BatchedMatMulParams.transposeB)
}

// uint64 weightMatrixFirstDimension = 5;
void BatchedMatMulParams::clear_weightmatrixfirstdimension() {
  weightmatrixfirstdimension_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 BatchedMatMulParams::weightmatrixfirstdimension() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchedMatMulParams.weightMatrixFirstDimension)
  return weightmatrixfirstdimension_;
}
void BatchedMatMulParams::set_weightmatrixfirstdimension(::google::protobuf::uint64 value) {
  
  weightmatrixfirstdimension_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.BatchedMatMulParams.weightMatrixFirstDimension)
}

// uint64 weightMatrixSecondDimension = 6;
void BatchedMatMulParams::clear_weightmatrixseconddimension() {
  weightmatrixseconddimension_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 BatchedMatMulParams::weightmatrixseconddimension() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchedMatMulParams.weightMatrixSecondDimension)
  return weightmatrixseconddimension_;
}
void BatchedMatMulParams::set_weightmatrixseconddimension(::google::protobuf::uint64 value) {
  
  weightmatrixseconddimension_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.BatchedMatMulParams.weightMatrixSecondDimension)
}

// bool hasBias = 7;
void BatchedMatMulParams::clear_hasbias() {
  hasbias_ = false;
}
bool BatchedMatMulParams::hasbias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchedMatMulParams.hasBias)
  return hasbias_;
}
void BatchedMatMulParams::set_hasbias(bool value) {
  
  hasbias_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.BatchedMatMulParams.hasBias)
}

// .CoreML.Specification.WeightParams weights = 8;
bool BatchedMatMulParams::has_weights() const {
  return this != internal_default_instance() && weights_ != NULL;
}
void BatchedMatMulParams::clear_weights() {
  if (GetArenaNoVirtual() == NULL && weights_ != NULL) delete weights_;
  weights_ = NULL;
}
const ::CoreML::Specification::WeightParams& BatchedMatMulParams::weights() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchedMatMulParams.weights)
  return weights_ != NULL ? *weights_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* BatchedMatMulParams::mutable_weights() {
  
  if (weights_ == NULL) {
    weights_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BatchedMatMulParams.weights)
  return weights_;
}
::CoreML::Specification::WeightParams* BatchedMatMulParams::release_weights() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.BatchedMatMulParams.weights)
  
  ::CoreML::Specification::WeightParams* temp = weights_;
  weights_ = NULL;
  return temp;
}
void BatchedMatMulParams::set_allocated_weights(::CoreML::Specification::WeightParams* weights) {
  delete weights_;
  weights_ = weights;
  if (weights) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.BatchedMatMulParams.weights)
}

// .CoreML.Specification.WeightParams bias = 9;
bool BatchedMatMulParams::has_bias() const {
  return this != internal_default_instance() && bias_ != NULL;
}
void BatchedMatMulParams::clear_bias() {
  if (GetArenaNoVirtual() == NULL && bias_ != NULL) delete bias_;
  bias_ = NULL;
}
const ::CoreML::Specification::WeightParams& BatchedMatMulParams::bias() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BatchedMatMulParams.bias)
  return bias_ != NULL ? *bias_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* BatchedMatMulParams::mutable_bias() {
  
  if (bias_ == NULL) {
    bias_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.BatchedMatMulParams.bias)
  return bias_;
}
::CoreML::Specification::WeightParams* BatchedMatMulParams::release_bias() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.BatchedMatMulParams.bias)
  
  ::CoreML::Specification::WeightParams* temp = bias_;
  bias_ = NULL;
  return temp;
}
void BatchedMatMulParams::set_allocated_bias(::CoreML::Specification::WeightParams* bias) {
  delete bias_;
  bias_ = bias;
  if (bias) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.BatchedMatMulParams.bias)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ConcatNDLayerParams::kAxisFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ConcatNDLayerParams::ConcatNDLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ConcatNDLayerParams)
}
ConcatNDLayerParams::ConcatNDLayerParams(const ConcatNDLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  axis_ = from.axis_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ConcatNDLayerParams)
}

void ConcatNDLayerParams::SharedCtor() {
  axis_ = GOOGLE_LONGLONG(0);
  _cached_size_ = 0;
}

ConcatNDLayerParams::~ConcatNDLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ConcatNDLayerParams)
  SharedDtor();
}

void ConcatNDLayerParams::SharedDtor() {
}

void ConcatNDLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ConcatNDLayerParams& ConcatNDLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ConcatNDLayerParams* ConcatNDLayerParams::New(::google::protobuf::Arena* arena) const {
  ConcatNDLayerParams* n = new ConcatNDLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ConcatNDLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ConcatNDLayerParams)
  axis_ = GOOGLE_LONGLONG(0);
}

bool ConcatNDLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ConcatNDLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // int64 axis = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 input, &axis_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ConcatNDLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ConcatNDLayerParams)
  return false;
#undef DO_
}

void ConcatNDLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ConcatNDLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // int64 axis = 1;
  if (this->axis() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteInt64(1, this->axis(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ConcatNDLayerParams)
}

size_t ConcatNDLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ConcatNDLayerParams)
  size_t total_size = 0;

  // int64 axis = 1;
  if (this->axis() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::Int64Size(
        this->axis());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ConcatNDLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ConcatNDLayerParams*>(&from));
}

void ConcatNDLayerParams::MergeFrom(const ConcatNDLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ConcatNDLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.axis() != 0) {
    set_axis(from.axis());
  }
}

void ConcatNDLayerParams::CopyFrom(const ConcatNDLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ConcatNDLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ConcatNDLayerParams::IsInitialized() const {
  return true;
}

void ConcatNDLayerParams::Swap(ConcatNDLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ConcatNDLayerParams::InternalSwap(ConcatNDLayerParams* other) {
  std::swap(axis_, other->axis_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ConcatNDLayerParams::GetTypeName() const {
  return "CoreML.Specification.ConcatNDLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ConcatNDLayerParams

// int64 axis = 1;
void ConcatNDLayerParams::clear_axis() {
  axis_ = GOOGLE_LONGLONG(0);
}
::google::protobuf::int64 ConcatNDLayerParams::axis() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ConcatNDLayerParams.axis)
  return axis_;
}
void ConcatNDLayerParams::set_axis(::google::protobuf::int64 value) {
  
  axis_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ConcatNDLayerParams.axis)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int SoftmaxNDLayerParams::kAxisFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

SoftmaxNDLayerParams::SoftmaxNDLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.SoftmaxNDLayerParams)
}
SoftmaxNDLayerParams::SoftmaxNDLayerParams(const SoftmaxNDLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  axis_ = from.axis_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.SoftmaxNDLayerParams)
}

void SoftmaxNDLayerParams::SharedCtor() {
  axis_ = GOOGLE_LONGLONG(0);
  _cached_size_ = 0;
}

SoftmaxNDLayerParams::~SoftmaxNDLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.SoftmaxNDLayerParams)
  SharedDtor();
}

void SoftmaxNDLayerParams::SharedDtor() {
}

void SoftmaxNDLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const SoftmaxNDLayerParams& SoftmaxNDLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

SoftmaxNDLayerParams* SoftmaxNDLayerParams::New(::google::protobuf::Arena* arena) const {
  SoftmaxNDLayerParams* n = new SoftmaxNDLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void SoftmaxNDLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.SoftmaxNDLayerParams)
  axis_ = GOOGLE_LONGLONG(0);
}

bool SoftmaxNDLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.SoftmaxNDLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // int64 axis = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 input, &axis_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.SoftmaxNDLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.SoftmaxNDLayerParams)
  return false;
#undef DO_
}

void SoftmaxNDLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.SoftmaxNDLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // int64 axis = 1;
  if (this->axis() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteInt64(1, this->axis(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.SoftmaxNDLayerParams)
}

size_t SoftmaxNDLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.SoftmaxNDLayerParams)
  size_t total_size = 0;

  // int64 axis = 1;
  if (this->axis() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::Int64Size(
        this->axis());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void SoftmaxNDLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const SoftmaxNDLayerParams*>(&from));
}

void SoftmaxNDLayerParams::MergeFrom(const SoftmaxNDLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.SoftmaxNDLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.axis() != 0) {
    set_axis(from.axis());
  }
}

void SoftmaxNDLayerParams::CopyFrom(const SoftmaxNDLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.SoftmaxNDLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool SoftmaxNDLayerParams::IsInitialized() const {
  return true;
}

void SoftmaxNDLayerParams::Swap(SoftmaxNDLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void SoftmaxNDLayerParams::InternalSwap(SoftmaxNDLayerParams* other) {
  std::swap(axis_, other->axis_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string SoftmaxNDLayerParams::GetTypeName() const {
  return "CoreML.Specification.SoftmaxNDLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// SoftmaxNDLayerParams

// int64 axis = 1;
void SoftmaxNDLayerParams::clear_axis() {
  axis_ = GOOGLE_LONGLONG(0);
}
::google::protobuf::int64 SoftmaxNDLayerParams::axis() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SoftmaxNDLayerParams.axis)
  return axis_;
}
void SoftmaxNDLayerParams::set_axis(::google::protobuf::int64 value) {
  
  axis_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SoftmaxNDLayerParams.axis)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int LoadConstantNDLayerParams::kShapeFieldNumber;
const int LoadConstantNDLayerParams::kDataFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

LoadConstantNDLayerParams::LoadConstantNDLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.LoadConstantNDLayerParams)
}
LoadConstantNDLayerParams::LoadConstantNDLayerParams(const LoadConstantNDLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      shape_(from.shape_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_data()) {
    data_ = new ::CoreML::Specification::WeightParams(*from.data_);
  } else {
    data_ = NULL;
  }
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.LoadConstantNDLayerParams)
}

void LoadConstantNDLayerParams::SharedCtor() {
  data_ = NULL;
  _cached_size_ = 0;
}

LoadConstantNDLayerParams::~LoadConstantNDLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.LoadConstantNDLayerParams)
  SharedDtor();
}

void LoadConstantNDLayerParams::SharedDtor() {
  if (this != internal_default_instance()) {
    delete data_;
  }
}

void LoadConstantNDLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const LoadConstantNDLayerParams& LoadConstantNDLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

LoadConstantNDLayerParams* LoadConstantNDLayerParams::New(::google::protobuf::Arena* arena) const {
  LoadConstantNDLayerParams* n = new LoadConstantNDLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void LoadConstantNDLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.LoadConstantNDLayerParams)
  shape_.Clear();
  if (GetArenaNoVirtual() == NULL && data_ != NULL) {
    delete data_;
  }
  data_ = NULL;
}

bool LoadConstantNDLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.LoadConstantNDLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated uint64 shape = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, this->mutable_shape())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(8u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 1, 10u, input, this->mutable_shape())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.WeightParams data = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(18u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_data()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.LoadConstantNDLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.LoadConstantNDLayerParams)
  return false;
#undef DO_
}

void LoadConstantNDLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.LoadConstantNDLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated uint64 shape = 1;
  if (this->shape_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(1, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_shape_cached_byte_size_);
  }
  for (int i = 0, n = this->shape_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64NoTag(
      this->shape(i), output);
  }

  // .CoreML.Specification.WeightParams data = 2;
  if (this->has_data()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      2, *this->data_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.LoadConstantNDLayerParams)
}

size_t LoadConstantNDLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.LoadConstantNDLayerParams)
  size_t total_size = 0;

  // repeated uint64 shape = 1;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      UInt64Size(this->shape_);
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _shape_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // .CoreML.Specification.WeightParams data = 2;
  if (this->has_data()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->data_);
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void LoadConstantNDLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const LoadConstantNDLayerParams*>(&from));
}

void LoadConstantNDLayerParams::MergeFrom(const LoadConstantNDLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.LoadConstantNDLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  shape_.MergeFrom(from.shape_);
  if (from.has_data()) {
    mutable_data()->::CoreML::Specification::WeightParams::MergeFrom(from.data());
  }
}

void LoadConstantNDLayerParams::CopyFrom(const LoadConstantNDLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.LoadConstantNDLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool LoadConstantNDLayerParams::IsInitialized() const {
  return true;
}

void LoadConstantNDLayerParams::Swap(LoadConstantNDLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void LoadConstantNDLayerParams::InternalSwap(LoadConstantNDLayerParams* other) {
  shape_.InternalSwap(&other->shape_);
  std::swap(data_, other->data_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string LoadConstantNDLayerParams::GetTypeName() const {
  return "CoreML.Specification.LoadConstantNDLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// LoadConstantNDLayerParams

// repeated uint64 shape = 1;
int LoadConstantNDLayerParams::shape_size() const {
  return shape_.size();
}
void LoadConstantNDLayerParams::clear_shape() {
  shape_.Clear();
}
::google::protobuf::uint64 LoadConstantNDLayerParams::shape(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LoadConstantNDLayerParams.shape)
  return shape_.Get(index);
}
void LoadConstantNDLayerParams::set_shape(int index, ::google::protobuf::uint64 value) {
  shape_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.LoadConstantNDLayerParams.shape)
}
void LoadConstantNDLayerParams::add_shape(::google::protobuf::uint64 value) {
  shape_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.LoadConstantNDLayerParams.shape)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
LoadConstantNDLayerParams::shape() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.LoadConstantNDLayerParams.shape)
  return shape_;
}
::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
LoadConstantNDLayerParams::mutable_shape() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.LoadConstantNDLayerParams.shape)
  return &shape_;
}

// .CoreML.Specification.WeightParams data = 2;
bool LoadConstantNDLayerParams::has_data() const {
  return this != internal_default_instance() && data_ != NULL;
}
void LoadConstantNDLayerParams::clear_data() {
  if (GetArenaNoVirtual() == NULL && data_ != NULL) delete data_;
  data_ = NULL;
}
const ::CoreML::Specification::WeightParams& LoadConstantNDLayerParams::data() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LoadConstantNDLayerParams.data)
  return data_ != NULL ? *data_
                         : *::CoreML::Specification::WeightParams::internal_default_instance();
}
::CoreML::Specification::WeightParams* LoadConstantNDLayerParams::mutable_data() {
  
  if (data_ == NULL) {
    data_ = new ::CoreML::Specification::WeightParams;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LoadConstantNDLayerParams.data)
  return data_;
}
::CoreML::Specification::WeightParams* LoadConstantNDLayerParams::release_data() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LoadConstantNDLayerParams.data)
  
  ::CoreML::Specification::WeightParams* temp = data_;
  data_ = NULL;
  return temp;
}
void LoadConstantNDLayerParams::set_allocated_data(::CoreML::Specification::WeightParams* data) {
  delete data_;
  data_ = data;
  if (data) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LoadConstantNDLayerParams.data)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int FillLikeLayerParams::kValueFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

FillLikeLayerParams::FillLikeLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.FillLikeLayerParams)
}
FillLikeLayerParams::FillLikeLayerParams(const FillLikeLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  value_ = from.value_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.FillLikeLayerParams)
}

void FillLikeLayerParams::SharedCtor() {
  value_ = 0;
  _cached_size_ = 0;
}

FillLikeLayerParams::~FillLikeLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.FillLikeLayerParams)
  SharedDtor();
}

void FillLikeLayerParams::SharedDtor() {
}

void FillLikeLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const FillLikeLayerParams& FillLikeLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

FillLikeLayerParams* FillLikeLayerParams::New(::google::protobuf::Arena* arena) const {
  FillLikeLayerParams* n = new FillLikeLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void FillLikeLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.FillLikeLayerParams)
  value_ = 0;
}

bool FillLikeLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.FillLikeLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // float value = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(13u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &value_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.FillLikeLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.FillLikeLayerParams)
  return false;
#undef DO_
}

void FillLikeLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.FillLikeLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // float value = 1;
  if (this->value() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(1, this->value(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.FillLikeLayerParams)
}

size_t FillLikeLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.FillLikeLayerParams)
  size_t total_size = 0;

  // float value = 1;
  if (this->value() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void FillLikeLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const FillLikeLayerParams*>(&from));
}

void FillLikeLayerParams::MergeFrom(const FillLikeLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.FillLikeLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.value() != 0) {
    set_value(from.value());
  }
}

void FillLikeLayerParams::CopyFrom(const FillLikeLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.FillLikeLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool FillLikeLayerParams::IsInitialized() const {
  return true;
}

void FillLikeLayerParams::Swap(FillLikeLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void FillLikeLayerParams::InternalSwap(FillLikeLayerParams* other) {
  std::swap(value_, other->value_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string FillLikeLayerParams::GetTypeName() const {
  return "CoreML.Specification.FillLikeLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// FillLikeLayerParams

// float value = 1;
void FillLikeLayerParams::clear_value() {
  value_ = 0;
}
float FillLikeLayerParams::value() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.FillLikeLayerParams.value)
  return value_;
}
void FillLikeLayerParams::set_value(float value) {
  
  value_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.FillLikeLayerParams.value)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int FillStaticLayerParams::kValueFieldNumber;
const int FillStaticLayerParams::kTargetShapeFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

FillStaticLayerParams::FillStaticLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.FillStaticLayerParams)
}
FillStaticLayerParams::FillStaticLayerParams(const FillStaticLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      targetshape_(from.targetshape_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  value_ = from.value_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.FillStaticLayerParams)
}

void FillStaticLayerParams::SharedCtor() {
  value_ = 0;
  _cached_size_ = 0;
}

FillStaticLayerParams::~FillStaticLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.FillStaticLayerParams)
  SharedDtor();
}

void FillStaticLayerParams::SharedDtor() {
}

void FillStaticLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const FillStaticLayerParams& FillStaticLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

FillStaticLayerParams* FillStaticLayerParams::New(::google::protobuf::Arena* arena) const {
  FillStaticLayerParams* n = new FillStaticLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void FillStaticLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.FillStaticLayerParams)
  targetshape_.Clear();
  value_ = 0;
}

bool FillStaticLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.FillStaticLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // float value = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(13u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &value_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated uint64 targetShape = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(18u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, this->mutable_targetshape())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(16u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 1, 18u, input, this->mutable_targetshape())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.FillStaticLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.FillStaticLayerParams)
  return false;
#undef DO_
}

void FillStaticLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.FillStaticLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // float value = 1;
  if (this->value() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(1, this->value(), output);
  }

  // repeated uint64 targetShape = 2;
  if (this->targetshape_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(2, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_targetshape_cached_byte_size_);
  }
  for (int i = 0, n = this->targetshape_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64NoTag(
      this->targetshape(i), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.FillStaticLayerParams)
}

size_t FillStaticLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.FillStaticLayerParams)
  size_t total_size = 0;

  // repeated uint64 targetShape = 2;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      UInt64Size(this->targetshape_);
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _targetshape_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // float value = 1;
  if (this->value() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void FillStaticLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const FillStaticLayerParams*>(&from));
}

void FillStaticLayerParams::MergeFrom(const FillStaticLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.FillStaticLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  targetshape_.MergeFrom(from.targetshape_);
  if (from.value() != 0) {
    set_value(from.value());
  }
}

void FillStaticLayerParams::CopyFrom(const FillStaticLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.FillStaticLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool FillStaticLayerParams::IsInitialized() const {
  return true;
}

void FillStaticLayerParams::Swap(FillStaticLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void FillStaticLayerParams::InternalSwap(FillStaticLayerParams* other) {
  targetshape_.InternalSwap(&other->targetshape_);
  std::swap(value_, other->value_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string FillStaticLayerParams::GetTypeName() const {
  return "CoreML.Specification.FillStaticLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// FillStaticLayerParams

// float value = 1;
void FillStaticLayerParams::clear_value() {
  value_ = 0;
}
float FillStaticLayerParams::value() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.FillStaticLayerParams.value)
  return value_;
}
void FillStaticLayerParams::set_value(float value) {
  
  value_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.FillStaticLayerParams.value)
}

// repeated uint64 targetShape = 2;
int FillStaticLayerParams::targetshape_size() const {
  return targetshape_.size();
}
void FillStaticLayerParams::clear_targetshape() {
  targetshape_.Clear();
}
::google::protobuf::uint64 FillStaticLayerParams::targetshape(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.FillStaticLayerParams.targetShape)
  return targetshape_.Get(index);
}
void FillStaticLayerParams::set_targetshape(int index, ::google::protobuf::uint64 value) {
  targetshape_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.FillStaticLayerParams.targetShape)
}
void FillStaticLayerParams::add_targetshape(::google::protobuf::uint64 value) {
  targetshape_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.FillStaticLayerParams.targetShape)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
FillStaticLayerParams::targetshape() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.FillStaticLayerParams.targetShape)
  return targetshape_;
}
::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
FillStaticLayerParams::mutable_targetshape() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.FillStaticLayerParams.targetShape)
  return &targetshape_;
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int FillDynamicLayerParams::kValueFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

FillDynamicLayerParams::FillDynamicLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.FillDynamicLayerParams)
}
FillDynamicLayerParams::FillDynamicLayerParams(const FillDynamicLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  value_ = from.value_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.FillDynamicLayerParams)
}

void FillDynamicLayerParams::SharedCtor() {
  value_ = 0;
  _cached_size_ = 0;
}

FillDynamicLayerParams::~FillDynamicLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.FillDynamicLayerParams)
  SharedDtor();
}

void FillDynamicLayerParams::SharedDtor() {
}

void FillDynamicLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const FillDynamicLayerParams& FillDynamicLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

FillDynamicLayerParams* FillDynamicLayerParams::New(::google::protobuf::Arena* arena) const {
  FillDynamicLayerParams* n = new FillDynamicLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void FillDynamicLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.FillDynamicLayerParams)
  value_ = 0;
}

bool FillDynamicLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.FillDynamicLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // float value = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(13u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &value_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.FillDynamicLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.FillDynamicLayerParams)
  return false;
#undef DO_
}

void FillDynamicLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.FillDynamicLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // float value = 1;
  if (this->value() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(1, this->value(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.FillDynamicLayerParams)
}

size_t FillDynamicLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.FillDynamicLayerParams)
  size_t total_size = 0;

  // float value = 1;
  if (this->value() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void FillDynamicLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const FillDynamicLayerParams*>(&from));
}

void FillDynamicLayerParams::MergeFrom(const FillDynamicLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.FillDynamicLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.value() != 0) {
    set_value(from.value());
  }
}

void FillDynamicLayerParams::CopyFrom(const FillDynamicLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.FillDynamicLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool FillDynamicLayerParams::IsInitialized() const {
  return true;
}

void FillDynamicLayerParams::Swap(FillDynamicLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void FillDynamicLayerParams::InternalSwap(FillDynamicLayerParams* other) {
  std::swap(value_, other->value_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string FillDynamicLayerParams::GetTypeName() const {
  return "CoreML.Specification.FillDynamicLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// FillDynamicLayerParams

// float value = 1;
void FillDynamicLayerParams::clear_value() {
  value_ = 0;
}
float FillDynamicLayerParams::value() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.FillDynamicLayerParams.value)
  return value_;
}
void FillDynamicLayerParams::set_value(float value) {
  
  value_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.FillDynamicLayerParams.value)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

WhereLayerParams::WhereLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.WhereLayerParams)
}
WhereLayerParams::WhereLayerParams(const WhereLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.WhereLayerParams)
}

void WhereLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

WhereLayerParams::~WhereLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.WhereLayerParams)
  SharedDtor();
}

void WhereLayerParams::SharedDtor() {
}

void WhereLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const WhereLayerParams& WhereLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

WhereLayerParams* WhereLayerParams::New(::google::protobuf::Arena* arena) const {
  WhereLayerParams* n = new WhereLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void WhereLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.WhereLayerParams)
}

bool WhereLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.WhereLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.WhereLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.WhereLayerParams)
  return false;
#undef DO_
}

void WhereLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.WhereLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.WhereLayerParams)
}

size_t WhereLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.WhereLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void WhereLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const WhereLayerParams*>(&from));
}

void WhereLayerParams::MergeFrom(const WhereLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.WhereLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void WhereLayerParams::CopyFrom(const WhereLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.WhereLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool WhereLayerParams::IsInitialized() const {
  return true;
}

void WhereLayerParams::Swap(WhereLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void WhereLayerParams::InternalSwap(WhereLayerParams* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string WhereLayerParams::GetTypeName() const {
  return "CoreML.Specification.WhereLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// WhereLayerParams

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

SineLayerParams::SineLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.SineLayerParams)
}
SineLayerParams::SineLayerParams(const SineLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.SineLayerParams)
}

void SineLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

SineLayerParams::~SineLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.SineLayerParams)
  SharedDtor();
}

void SineLayerParams::SharedDtor() {
}

void SineLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const SineLayerParams& SineLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

SineLayerParams* SineLayerParams::New(::google::protobuf::Arena* arena) const {
  SineLayerParams* n = new SineLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void SineLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.SineLayerParams)
}

bool SineLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.SineLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.SineLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.SineLayerParams)
  return false;
#undef DO_
}

void SineLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.SineLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.SineLayerParams)
}

size_t SineLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.SineLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void SineLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const SineLayerParams*>(&from));
}

void SineLayerParams::MergeFrom(const SineLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.SineLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void SineLayerParams::CopyFrom(const SineLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.SineLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool SineLayerParams::IsInitialized() const {
  return true;
}

void SineLayerParams::Swap(SineLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void SineLayerParams::InternalSwap(SineLayerParams* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string SineLayerParams::GetTypeName() const {
  return "CoreML.Specification.SineLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// SineLayerParams

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

CosineLayerParams::CosineLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.CosineLayerParams)
}
CosineLayerParams::CosineLayerParams(const CosineLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.CosineLayerParams)
}

void CosineLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

CosineLayerParams::~CosineLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.CosineLayerParams)
  SharedDtor();
}

void CosineLayerParams::SharedDtor() {
}

void CosineLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const CosineLayerParams& CosineLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

CosineLayerParams* CosineLayerParams::New(::google::protobuf::Arena* arena) const {
  CosineLayerParams* n = new CosineLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void CosineLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.CosineLayerParams)
}

bool CosineLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.CosineLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.CosineLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.CosineLayerParams)
  return false;
#undef DO_
}

void CosineLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.CosineLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.CosineLayerParams)
}

size_t CosineLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.CosineLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void CosineLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const CosineLayerParams*>(&from));
}

void CosineLayerParams::MergeFrom(const CosineLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.CosineLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void CosineLayerParams::CopyFrom(const CosineLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.CosineLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool CosineLayerParams::IsInitialized() const {
  return true;
}

void CosineLayerParams::Swap(CosineLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void CosineLayerParams::InternalSwap(CosineLayerParams* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string CosineLayerParams::GetTypeName() const {
  return "CoreML.Specification.CosineLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// CosineLayerParams

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

PowBroadcastableLayerParams::PowBroadcastableLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.PowBroadcastableLayerParams)
}
PowBroadcastableLayerParams::PowBroadcastableLayerParams(const PowBroadcastableLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.PowBroadcastableLayerParams)
}

void PowBroadcastableLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

PowBroadcastableLayerParams::~PowBroadcastableLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.PowBroadcastableLayerParams)
  SharedDtor();
}

void PowBroadcastableLayerParams::SharedDtor() {
}

void PowBroadcastableLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const PowBroadcastableLayerParams& PowBroadcastableLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

PowBroadcastableLayerParams* PowBroadcastableLayerParams::New(::google::protobuf::Arena* arena) const {
  PowBroadcastableLayerParams* n = new PowBroadcastableLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void PowBroadcastableLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.PowBroadcastableLayerParams)
}

bool PowBroadcastableLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.PowBroadcastableLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.PowBroadcastableLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.PowBroadcastableLayerParams)
  return false;
#undef DO_
}

void PowBroadcastableLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.PowBroadcastableLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.PowBroadcastableLayerParams)
}

size_t PowBroadcastableLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.PowBroadcastableLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void PowBroadcastableLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const PowBroadcastableLayerParams*>(&from));
}

void PowBroadcastableLayerParams::MergeFrom(const PowBroadcastableLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.PowBroadcastableLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void PowBroadcastableLayerParams::CopyFrom(const PowBroadcastableLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.PowBroadcastableLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool PowBroadcastableLayerParams::IsInitialized() const {
  return true;
}

void PowBroadcastableLayerParams::Swap(PowBroadcastableLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void PowBroadcastableLayerParams::InternalSwap(PowBroadcastableLayerParams* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string PowBroadcastableLayerParams::GetTypeName() const {
  return "CoreML.Specification.PowBroadcastableLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// PowBroadcastableLayerParams

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

Exp2LayerParams::Exp2LayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.Exp2LayerParams)
}
Exp2LayerParams::Exp2LayerParams(const Exp2LayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.Exp2LayerParams)
}

void Exp2LayerParams::SharedCtor() {
  _cached_size_ = 0;
}

Exp2LayerParams::~Exp2LayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.Exp2LayerParams)
  SharedDtor();
}

void Exp2LayerParams::SharedDtor() {
}

void Exp2LayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const Exp2LayerParams& Exp2LayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

Exp2LayerParams* Exp2LayerParams::New(::google::protobuf::Arena* arena) const {
  Exp2LayerParams* n = new Exp2LayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void Exp2LayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.Exp2LayerParams)
}

bool Exp2LayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.Exp2LayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.Exp2LayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.Exp2LayerParams)
  return false;
#undef DO_
}

void Exp2LayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.Exp2LayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.Exp2LayerParams)
}

size_t Exp2LayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.Exp2LayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void Exp2LayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const Exp2LayerParams*>(&from));
}

void Exp2LayerParams::MergeFrom(const Exp2LayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.Exp2LayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void Exp2LayerParams::CopyFrom(const Exp2LayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.Exp2LayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool Exp2LayerParams::IsInitialized() const {
  return true;
}

void Exp2LayerParams::Swap(Exp2LayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void Exp2LayerParams::InternalSwap(Exp2LayerParams* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string Exp2LayerParams::GetTypeName() const {
  return "CoreML.Specification.Exp2LayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// Exp2LayerParams

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

UpperTriangularLayerParams::UpperTriangularLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.UpperTriangularLayerParams)
}
UpperTriangularLayerParams::UpperTriangularLayerParams(const UpperTriangularLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.UpperTriangularLayerParams)
}

void UpperTriangularLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

UpperTriangularLayerParams::~UpperTriangularLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.UpperTriangularLayerParams)
  SharedDtor();
}

void UpperTriangularLayerParams::SharedDtor() {
}

void UpperTriangularLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const UpperTriangularLayerParams& UpperTriangularLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

UpperTriangularLayerParams* UpperTriangularLayerParams::New(::google::protobuf::Arena* arena) const {
  UpperTriangularLayerParams* n = new UpperTriangularLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void UpperTriangularLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.UpperTriangularLayerParams)
}

bool UpperTriangularLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.UpperTriangularLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.UpperTriangularLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.UpperTriangularLayerParams)
  return false;
#undef DO_
}

void UpperTriangularLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.UpperTriangularLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.UpperTriangularLayerParams)
}

size_t UpperTriangularLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.UpperTriangularLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void UpperTriangularLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const UpperTriangularLayerParams*>(&from));
}

void UpperTriangularLayerParams::MergeFrom(const UpperTriangularLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.UpperTriangularLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void UpperTriangularLayerParams::CopyFrom(const UpperTriangularLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.UpperTriangularLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool UpperTriangularLayerParams::IsInitialized() const {
  return true;
}

void UpperTriangularLayerParams::Swap(UpperTriangularLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void UpperTriangularLayerParams::InternalSwap(UpperTriangularLayerParams* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string UpperTriangularLayerParams::GetTypeName() const {
  return "CoreML.Specification.UpperTriangularLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// UpperTriangularLayerParams

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

LowerTriangularLayerParams::LowerTriangularLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.LowerTriangularLayerParams)
}
LowerTriangularLayerParams::LowerTriangularLayerParams(const LowerTriangularLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.LowerTriangularLayerParams)
}

void LowerTriangularLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

LowerTriangularLayerParams::~LowerTriangularLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.LowerTriangularLayerParams)
  SharedDtor();
}

void LowerTriangularLayerParams::SharedDtor() {
}

void LowerTriangularLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const LowerTriangularLayerParams& LowerTriangularLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

LowerTriangularLayerParams* LowerTriangularLayerParams::New(::google::protobuf::Arena* arena) const {
  LowerTriangularLayerParams* n = new LowerTriangularLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void LowerTriangularLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.LowerTriangularLayerParams)
}

bool LowerTriangularLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.LowerTriangularLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.LowerTriangularLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.LowerTriangularLayerParams)
  return false;
#undef DO_
}

void LowerTriangularLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.LowerTriangularLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.LowerTriangularLayerParams)
}

size_t LowerTriangularLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.LowerTriangularLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void LowerTriangularLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const LowerTriangularLayerParams*>(&from));
}

void LowerTriangularLayerParams::MergeFrom(const LowerTriangularLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.LowerTriangularLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void LowerTriangularLayerParams::CopyFrom(const LowerTriangularLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.LowerTriangularLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool LowerTriangularLayerParams::IsInitialized() const {
  return true;
}

void LowerTriangularLayerParams::Swap(LowerTriangularLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void LowerTriangularLayerParams::InternalSwap(LowerTriangularLayerParams* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string LowerTriangularLayerParams::GetTypeName() const {
  return "CoreML.Specification.LowerTriangularLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// LowerTriangularLayerParams

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

BroadcastToLikeLayerParams::BroadcastToLikeLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.BroadcastToLikeLayerParams)
}
BroadcastToLikeLayerParams::BroadcastToLikeLayerParams(const BroadcastToLikeLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.BroadcastToLikeLayerParams)
}

void BroadcastToLikeLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

BroadcastToLikeLayerParams::~BroadcastToLikeLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.BroadcastToLikeLayerParams)
  SharedDtor();
}

void BroadcastToLikeLayerParams::SharedDtor() {
}

void BroadcastToLikeLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const BroadcastToLikeLayerParams& BroadcastToLikeLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

BroadcastToLikeLayerParams* BroadcastToLikeLayerParams::New(::google::protobuf::Arena* arena) const {
  BroadcastToLikeLayerParams* n = new BroadcastToLikeLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void BroadcastToLikeLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.BroadcastToLikeLayerParams)
}

bool BroadcastToLikeLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.BroadcastToLikeLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.BroadcastToLikeLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.BroadcastToLikeLayerParams)
  return false;
#undef DO_
}

void BroadcastToLikeLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.BroadcastToLikeLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.BroadcastToLikeLayerParams)
}

size_t BroadcastToLikeLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.BroadcastToLikeLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void BroadcastToLikeLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const BroadcastToLikeLayerParams*>(&from));
}

void BroadcastToLikeLayerParams::MergeFrom(const BroadcastToLikeLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.BroadcastToLikeLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void BroadcastToLikeLayerParams::CopyFrom(const BroadcastToLikeLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.BroadcastToLikeLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool BroadcastToLikeLayerParams::IsInitialized() const {
  return true;
}

void BroadcastToLikeLayerParams::Swap(BroadcastToLikeLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void BroadcastToLikeLayerParams::InternalSwap(BroadcastToLikeLayerParams* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string BroadcastToLikeLayerParams::GetTypeName() const {
  return "CoreML.Specification.BroadcastToLikeLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// BroadcastToLikeLayerParams

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int BroadcastToStaticLayerParams::kTargetShapeFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

BroadcastToStaticLayerParams::BroadcastToStaticLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.BroadcastToStaticLayerParams)
}
BroadcastToStaticLayerParams::BroadcastToStaticLayerParams(const BroadcastToStaticLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      targetshape_(from.targetshape_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.BroadcastToStaticLayerParams)
}

void BroadcastToStaticLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

BroadcastToStaticLayerParams::~BroadcastToStaticLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.BroadcastToStaticLayerParams)
  SharedDtor();
}

void BroadcastToStaticLayerParams::SharedDtor() {
}

void BroadcastToStaticLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const BroadcastToStaticLayerParams& BroadcastToStaticLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

BroadcastToStaticLayerParams* BroadcastToStaticLayerParams::New(::google::protobuf::Arena* arena) const {
  BroadcastToStaticLayerParams* n = new BroadcastToStaticLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void BroadcastToStaticLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.BroadcastToStaticLayerParams)
  targetshape_.Clear();
}

bool BroadcastToStaticLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.BroadcastToStaticLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated uint64 targetShape = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, this->mutable_targetshape())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(8u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 1, 10u, input, this->mutable_targetshape())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.BroadcastToStaticLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.BroadcastToStaticLayerParams)
  return false;
#undef DO_
}

void BroadcastToStaticLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.BroadcastToStaticLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated uint64 targetShape = 1;
  if (this->targetshape_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(1, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_targetshape_cached_byte_size_);
  }
  for (int i = 0, n = this->targetshape_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64NoTag(
      this->targetshape(i), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.BroadcastToStaticLayerParams)
}

size_t BroadcastToStaticLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.BroadcastToStaticLayerParams)
  size_t total_size = 0;

  // repeated uint64 targetShape = 1;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      UInt64Size(this->targetshape_);
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _targetshape_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void BroadcastToStaticLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const BroadcastToStaticLayerParams*>(&from));
}

void BroadcastToStaticLayerParams::MergeFrom(const BroadcastToStaticLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.BroadcastToStaticLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  targetshape_.MergeFrom(from.targetshape_);
}

void BroadcastToStaticLayerParams::CopyFrom(const BroadcastToStaticLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.BroadcastToStaticLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool BroadcastToStaticLayerParams::IsInitialized() const {
  return true;
}

void BroadcastToStaticLayerParams::Swap(BroadcastToStaticLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void BroadcastToStaticLayerParams::InternalSwap(BroadcastToStaticLayerParams* other) {
  targetshape_.InternalSwap(&other->targetshape_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string BroadcastToStaticLayerParams::GetTypeName() const {
  return "CoreML.Specification.BroadcastToStaticLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// BroadcastToStaticLayerParams

// repeated uint64 targetShape = 1;
int BroadcastToStaticLayerParams::targetshape_size() const {
  return targetshape_.size();
}
void BroadcastToStaticLayerParams::clear_targetshape() {
  targetshape_.Clear();
}
::google::protobuf::uint64 BroadcastToStaticLayerParams::targetshape(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.BroadcastToStaticLayerParams.targetShape)
  return targetshape_.Get(index);
}
void BroadcastToStaticLayerParams::set_targetshape(int index, ::google::protobuf::uint64 value) {
  targetshape_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.BroadcastToStaticLayerParams.targetShape)
}
void BroadcastToStaticLayerParams::add_targetshape(::google::protobuf::uint64 value) {
  targetshape_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.BroadcastToStaticLayerParams.targetShape)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
BroadcastToStaticLayerParams::targetshape() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.BroadcastToStaticLayerParams.targetShape)
  return targetshape_;
}
::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
BroadcastToStaticLayerParams::mutable_targetshape() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.BroadcastToStaticLayerParams.targetShape)
  return &targetshape_;
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

BroadcastToDynamicLayerParams::BroadcastToDynamicLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.BroadcastToDynamicLayerParams)
}
BroadcastToDynamicLayerParams::BroadcastToDynamicLayerParams(const BroadcastToDynamicLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.BroadcastToDynamicLayerParams)
}

void BroadcastToDynamicLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

BroadcastToDynamicLayerParams::~BroadcastToDynamicLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.BroadcastToDynamicLayerParams)
  SharedDtor();
}

void BroadcastToDynamicLayerParams::SharedDtor() {
}

void BroadcastToDynamicLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const BroadcastToDynamicLayerParams& BroadcastToDynamicLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

BroadcastToDynamicLayerParams* BroadcastToDynamicLayerParams::New(::google::protobuf::Arena* arena) const {
  BroadcastToDynamicLayerParams* n = new BroadcastToDynamicLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void BroadcastToDynamicLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.BroadcastToDynamicLayerParams)
}

bool BroadcastToDynamicLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.BroadcastToDynamicLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.BroadcastToDynamicLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.BroadcastToDynamicLayerParams)
  return false;
#undef DO_
}

void BroadcastToDynamicLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.BroadcastToDynamicLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.BroadcastToDynamicLayerParams)
}

size_t BroadcastToDynamicLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.BroadcastToDynamicLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void BroadcastToDynamicLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const BroadcastToDynamicLayerParams*>(&from));
}

void BroadcastToDynamicLayerParams::MergeFrom(const BroadcastToDynamicLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.BroadcastToDynamicLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void BroadcastToDynamicLayerParams::CopyFrom(const BroadcastToDynamicLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.BroadcastToDynamicLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool BroadcastToDynamicLayerParams::IsInitialized() const {
  return true;
}

void BroadcastToDynamicLayerParams::Swap(BroadcastToDynamicLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void BroadcastToDynamicLayerParams::InternalSwap(BroadcastToDynamicLayerParams* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string BroadcastToDynamicLayerParams::GetTypeName() const {
  return "CoreML.Specification.BroadcastToDynamicLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// BroadcastToDynamicLayerParams

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

AddBroadcastableLayerParams::AddBroadcastableLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.AddBroadcastableLayerParams)
}
AddBroadcastableLayerParams::AddBroadcastableLayerParams(const AddBroadcastableLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.AddBroadcastableLayerParams)
}

void AddBroadcastableLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

AddBroadcastableLayerParams::~AddBroadcastableLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.AddBroadcastableLayerParams)
  SharedDtor();
}

void AddBroadcastableLayerParams::SharedDtor() {
}

void AddBroadcastableLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const AddBroadcastableLayerParams& AddBroadcastableLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

AddBroadcastableLayerParams* AddBroadcastableLayerParams::New(::google::protobuf::Arena* arena) const {
  AddBroadcastableLayerParams* n = new AddBroadcastableLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void AddBroadcastableLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.AddBroadcastableLayerParams)
}

bool AddBroadcastableLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.AddBroadcastableLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.AddBroadcastableLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.AddBroadcastableLayerParams)
  return false;
#undef DO_
}

void AddBroadcastableLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.AddBroadcastableLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.AddBroadcastableLayerParams)
}

size_t AddBroadcastableLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.AddBroadcastableLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void AddBroadcastableLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const AddBroadcastableLayerParams*>(&from));
}

void AddBroadcastableLayerParams::MergeFrom(const AddBroadcastableLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.AddBroadcastableLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void AddBroadcastableLayerParams::CopyFrom(const AddBroadcastableLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.AddBroadcastableLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool AddBroadcastableLayerParams::IsInitialized() const {
  return true;
}

void AddBroadcastableLayerParams::Swap(AddBroadcastableLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void AddBroadcastableLayerParams::InternalSwap(AddBroadcastableLayerParams* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string AddBroadcastableLayerParams::GetTypeName() const {
  return "CoreML.Specification.AddBroadcastableLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// AddBroadcastableLayerParams

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

SubtractBroadcastableLayerParams::SubtractBroadcastableLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.SubtractBroadcastableLayerParams)
}
SubtractBroadcastableLayerParams::SubtractBroadcastableLayerParams(const SubtractBroadcastableLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.SubtractBroadcastableLayerParams)
}

void SubtractBroadcastableLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

SubtractBroadcastableLayerParams::~SubtractBroadcastableLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.SubtractBroadcastableLayerParams)
  SharedDtor();
}

void SubtractBroadcastableLayerParams::SharedDtor() {
}

void SubtractBroadcastableLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const SubtractBroadcastableLayerParams& SubtractBroadcastableLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

SubtractBroadcastableLayerParams* SubtractBroadcastableLayerParams::New(::google::protobuf::Arena* arena) const {
  SubtractBroadcastableLayerParams* n = new SubtractBroadcastableLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void SubtractBroadcastableLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.SubtractBroadcastableLayerParams)
}

bool SubtractBroadcastableLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.SubtractBroadcastableLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.SubtractBroadcastableLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.SubtractBroadcastableLayerParams)
  return false;
#undef DO_
}

void SubtractBroadcastableLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.SubtractBroadcastableLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.SubtractBroadcastableLayerParams)
}

size_t SubtractBroadcastableLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.SubtractBroadcastableLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void SubtractBroadcastableLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const SubtractBroadcastableLayerParams*>(&from));
}

void SubtractBroadcastableLayerParams::MergeFrom(const SubtractBroadcastableLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.SubtractBroadcastableLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void SubtractBroadcastableLayerParams::CopyFrom(const SubtractBroadcastableLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.SubtractBroadcastableLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool SubtractBroadcastableLayerParams::IsInitialized() const {
  return true;
}

void SubtractBroadcastableLayerParams::Swap(SubtractBroadcastableLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void SubtractBroadcastableLayerParams::InternalSwap(SubtractBroadcastableLayerParams* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string SubtractBroadcastableLayerParams::GetTypeName() const {
  return "CoreML.Specification.SubtractBroadcastableLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// SubtractBroadcastableLayerParams

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

MultiplyBroadcastableLayerParams::MultiplyBroadcastableLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.MultiplyBroadcastableLayerParams)
}
MultiplyBroadcastableLayerParams::MultiplyBroadcastableLayerParams(const MultiplyBroadcastableLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.MultiplyBroadcastableLayerParams)
}

void MultiplyBroadcastableLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

MultiplyBroadcastableLayerParams::~MultiplyBroadcastableLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.MultiplyBroadcastableLayerParams)
  SharedDtor();
}

void MultiplyBroadcastableLayerParams::SharedDtor() {
}

void MultiplyBroadcastableLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const MultiplyBroadcastableLayerParams& MultiplyBroadcastableLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

MultiplyBroadcastableLayerParams* MultiplyBroadcastableLayerParams::New(::google::protobuf::Arena* arena) const {
  MultiplyBroadcastableLayerParams* n = new MultiplyBroadcastableLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void MultiplyBroadcastableLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.MultiplyBroadcastableLayerParams)
}

bool MultiplyBroadcastableLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.MultiplyBroadcastableLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.MultiplyBroadcastableLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.MultiplyBroadcastableLayerParams)
  return false;
#undef DO_
}

void MultiplyBroadcastableLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.MultiplyBroadcastableLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.MultiplyBroadcastableLayerParams)
}

size_t MultiplyBroadcastableLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.MultiplyBroadcastableLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void MultiplyBroadcastableLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const MultiplyBroadcastableLayerParams*>(&from));
}

void MultiplyBroadcastableLayerParams::MergeFrom(const MultiplyBroadcastableLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.MultiplyBroadcastableLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void MultiplyBroadcastableLayerParams::CopyFrom(const MultiplyBroadcastableLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.MultiplyBroadcastableLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool MultiplyBroadcastableLayerParams::IsInitialized() const {
  return true;
}

void MultiplyBroadcastableLayerParams::Swap(MultiplyBroadcastableLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void MultiplyBroadcastableLayerParams::InternalSwap(MultiplyBroadcastableLayerParams* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string MultiplyBroadcastableLayerParams::GetTypeName() const {
  return "CoreML.Specification.MultiplyBroadcastableLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// MultiplyBroadcastableLayerParams

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

DivideBroadcastableLayerParams::DivideBroadcastableLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.DivideBroadcastableLayerParams)
}
DivideBroadcastableLayerParams::DivideBroadcastableLayerParams(const DivideBroadcastableLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.DivideBroadcastableLayerParams)
}

void DivideBroadcastableLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

DivideBroadcastableLayerParams::~DivideBroadcastableLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.DivideBroadcastableLayerParams)
  SharedDtor();
}

void DivideBroadcastableLayerParams::SharedDtor() {
}

void DivideBroadcastableLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const DivideBroadcastableLayerParams& DivideBroadcastableLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

DivideBroadcastableLayerParams* DivideBroadcastableLayerParams::New(::google::protobuf::Arena* arena) const {
  DivideBroadcastableLayerParams* n = new DivideBroadcastableLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void DivideBroadcastableLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.DivideBroadcastableLayerParams)
}

bool DivideBroadcastableLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.DivideBroadcastableLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.DivideBroadcastableLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.DivideBroadcastableLayerParams)
  return false;
#undef DO_
}

void DivideBroadcastableLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.DivideBroadcastableLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.DivideBroadcastableLayerParams)
}

size_t DivideBroadcastableLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.DivideBroadcastableLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void DivideBroadcastableLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const DivideBroadcastableLayerParams*>(&from));
}

void DivideBroadcastableLayerParams::MergeFrom(const DivideBroadcastableLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.DivideBroadcastableLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void DivideBroadcastableLayerParams::CopyFrom(const DivideBroadcastableLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.DivideBroadcastableLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool DivideBroadcastableLayerParams::IsInitialized() const {
  return true;
}

void DivideBroadcastableLayerParams::Swap(DivideBroadcastableLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void DivideBroadcastableLayerParams::InternalSwap(DivideBroadcastableLayerParams* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string DivideBroadcastableLayerParams::GetTypeName() const {
  return "CoreML.Specification.DivideBroadcastableLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// DivideBroadcastableLayerParams

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int GatherLayerParams::kAxisFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

GatherLayerParams::GatherLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.GatherLayerParams)
}
GatherLayerParams::GatherLayerParams(const GatherLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  axis_ = from.axis_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.GatherLayerParams)
}

void GatherLayerParams::SharedCtor() {
  axis_ = GOOGLE_LONGLONG(0);
  _cached_size_ = 0;
}

GatherLayerParams::~GatherLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.GatherLayerParams)
  SharedDtor();
}

void GatherLayerParams::SharedDtor() {
}

void GatherLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const GatherLayerParams& GatherLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

GatherLayerParams* GatherLayerParams::New(::google::protobuf::Arena* arena) const {
  GatherLayerParams* n = new GatherLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void GatherLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.GatherLayerParams)
  axis_ = GOOGLE_LONGLONG(0);
}

bool GatherLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.GatherLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // int64 axis = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 input, &axis_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.GatherLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.GatherLayerParams)
  return false;
#undef DO_
}

void GatherLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.GatherLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // int64 axis = 1;
  if (this->axis() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteInt64(1, this->axis(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.GatherLayerParams)
}

size_t GatherLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.GatherLayerParams)
  size_t total_size = 0;

  // int64 axis = 1;
  if (this->axis() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::Int64Size(
        this->axis());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void GatherLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const GatherLayerParams*>(&from));
}

void GatherLayerParams::MergeFrom(const GatherLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.GatherLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.axis() != 0) {
    set_axis(from.axis());
  }
}

void GatherLayerParams::CopyFrom(const GatherLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.GatherLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool GatherLayerParams::IsInitialized() const {
  return true;
}

void GatherLayerParams::Swap(GatherLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void GatherLayerParams::InternalSwap(GatherLayerParams* other) {
  std::swap(axis_, other->axis_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string GatherLayerParams::GetTypeName() const {
  return "CoreML.Specification.GatherLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// GatherLayerParams

// int64 axis = 1;
void GatherLayerParams::clear_axis() {
  axis_ = GOOGLE_LONGLONG(0);
}
::google::protobuf::int64 GatherLayerParams::axis() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.GatherLayerParams.axis)
  return axis_;
}
void GatherLayerParams::set_axis(::google::protobuf::int64 value) {
  
  axis_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.GatherLayerParams.axis)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ScatterLayerParams::ScatterLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ScatterLayerParams)
}
ScatterLayerParams::ScatterLayerParams(const ScatterLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ScatterLayerParams)
}

void ScatterLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

ScatterLayerParams::~ScatterLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ScatterLayerParams)
  SharedDtor();
}

void ScatterLayerParams::SharedDtor() {
}

void ScatterLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ScatterLayerParams& ScatterLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ScatterLayerParams* ScatterLayerParams::New(::google::protobuf::Arena* arena) const {
  ScatterLayerParams* n = new ScatterLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ScatterLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ScatterLayerParams)
}

bool ScatterLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ScatterLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ScatterLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ScatterLayerParams)
  return false;
#undef DO_
}

void ScatterLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ScatterLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ScatterLayerParams)
}

size_t ScatterLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ScatterLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ScatterLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ScatterLayerParams*>(&from));
}

void ScatterLayerParams::MergeFrom(const ScatterLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ScatterLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void ScatterLayerParams::CopyFrom(const ScatterLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ScatterLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ScatterLayerParams::IsInitialized() const {
  return true;
}

void ScatterLayerParams::Swap(ScatterLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ScatterLayerParams::InternalSwap(ScatterLayerParams* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ScatterLayerParams::GetTypeName() const {
  return "CoreML.Specification.ScatterLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ScatterLayerParams

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int StackNDLayerParams::kAxisFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

StackNDLayerParams::StackNDLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.StackNDLayerParams)
}
StackNDLayerParams::StackNDLayerParams(const StackNDLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  axis_ = from.axis_;
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.StackNDLayerParams)
}

void StackNDLayerParams::SharedCtor() {
  axis_ = GOOGLE_LONGLONG(0);
  _cached_size_ = 0;
}

StackNDLayerParams::~StackNDLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.StackNDLayerParams)
  SharedDtor();
}

void StackNDLayerParams::SharedDtor() {
}

void StackNDLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const StackNDLayerParams& StackNDLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

StackNDLayerParams* StackNDLayerParams::New(::google::protobuf::Arena* arena) const {
  StackNDLayerParams* n = new StackNDLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void StackNDLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.StackNDLayerParams)
  axis_ = GOOGLE_LONGLONG(0);
}

bool StackNDLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.StackNDLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // int64 axis = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 input, &axis_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.StackNDLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.StackNDLayerParams)
  return false;
#undef DO_
}

void StackNDLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.StackNDLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // int64 axis = 1;
  if (this->axis() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteInt64(1, this->axis(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.StackNDLayerParams)
}

size_t StackNDLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.StackNDLayerParams)
  size_t total_size = 0;

  // int64 axis = 1;
  if (this->axis() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::Int64Size(
        this->axis());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void StackNDLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const StackNDLayerParams*>(&from));
}

void StackNDLayerParams::MergeFrom(const StackNDLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.StackNDLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.axis() != 0) {
    set_axis(from.axis());
  }
}

void StackNDLayerParams::CopyFrom(const StackNDLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.StackNDLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool StackNDLayerParams::IsInitialized() const {
  return true;
}

void StackNDLayerParams::Swap(StackNDLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void StackNDLayerParams::InternalSwap(StackNDLayerParams* other) {
  std::swap(axis_, other->axis_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string StackNDLayerParams::GetTypeName() const {
  return "CoreML.Specification.StackNDLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// StackNDLayerParams

// int64 axis = 1;
void StackNDLayerParams::clear_axis() {
  axis_ = GOOGLE_LONGLONG(0);
}
::google::protobuf::int64 StackNDLayerParams::axis() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.StackNDLayerParams.axis)
  return axis_;
}
void StackNDLayerParams::set_axis(::google::protobuf::int64 value) {
  
  axis_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.StackNDLayerParams.axis)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int RankPreservingReshapeLayerParams::kTargetShapeFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

RankPreservingReshapeLayerParams::RankPreservingReshapeLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.RankPreservingReshapeLayerParams)
}
RankPreservingReshapeLayerParams::RankPreservingReshapeLayerParams(const RankPreservingReshapeLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      targetshape_(from.targetshape_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.RankPreservingReshapeLayerParams)
}

void RankPreservingReshapeLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

RankPreservingReshapeLayerParams::~RankPreservingReshapeLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.RankPreservingReshapeLayerParams)
  SharedDtor();
}

void RankPreservingReshapeLayerParams::SharedDtor() {
}

void RankPreservingReshapeLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const RankPreservingReshapeLayerParams& RankPreservingReshapeLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

RankPreservingReshapeLayerParams* RankPreservingReshapeLayerParams::New(::google::protobuf::Arena* arena) const {
  RankPreservingReshapeLayerParams* n = new RankPreservingReshapeLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void RankPreservingReshapeLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.RankPreservingReshapeLayerParams)
  targetshape_.Clear();
}

bool RankPreservingReshapeLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.RankPreservingReshapeLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated int64 targetShape = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 input, this->mutable_targetshape())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(8u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 1, 10u, input, this->mutable_targetshape())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.RankPreservingReshapeLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.RankPreservingReshapeLayerParams)
  return false;
#undef DO_
}

void RankPreservingReshapeLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.RankPreservingReshapeLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated int64 targetShape = 1;
  if (this->targetshape_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(1, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_targetshape_cached_byte_size_);
  }
  for (int i = 0, n = this->targetshape_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteInt64NoTag(
      this->targetshape(i), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.RankPreservingReshapeLayerParams)
}

size_t RankPreservingReshapeLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.RankPreservingReshapeLayerParams)
  size_t total_size = 0;

  // repeated int64 targetShape = 1;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      Int64Size(this->targetshape_);
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _targetshape_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void RankPreservingReshapeLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const RankPreservingReshapeLayerParams*>(&from));
}

void RankPreservingReshapeLayerParams::MergeFrom(const RankPreservingReshapeLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.RankPreservingReshapeLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  targetshape_.MergeFrom(from.targetshape_);
}

void RankPreservingReshapeLayerParams::CopyFrom(const RankPreservingReshapeLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.RankPreservingReshapeLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool RankPreservingReshapeLayerParams::IsInitialized() const {
  return true;
}

void RankPreservingReshapeLayerParams::Swap(RankPreservingReshapeLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void RankPreservingReshapeLayerParams::InternalSwap(RankPreservingReshapeLayerParams* other) {
  targetshape_.InternalSwap(&other->targetshape_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string RankPreservingReshapeLayerParams::GetTypeName() const {
  return "CoreML.Specification.RankPreservingReshapeLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// RankPreservingReshapeLayerParams

// repeated int64 targetShape = 1;
int RankPreservingReshapeLayerParams::targetshape_size() const {
  return targetshape_.size();
}
void RankPreservingReshapeLayerParams::clear_targetshape() {
  targetshape_.Clear();
}
::google::protobuf::int64 RankPreservingReshapeLayerParams::targetshape(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RankPreservingReshapeLayerParams.targetShape)
  return targetshape_.Get(index);
}
void RankPreservingReshapeLayerParams::set_targetshape(int index, ::google::protobuf::int64 value) {
  targetshape_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.RankPreservingReshapeLayerParams.targetShape)
}
void RankPreservingReshapeLayerParams::add_targetshape(::google::protobuf::int64 value) {
  targetshape_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.RankPreservingReshapeLayerParams.targetShape)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
RankPreservingReshapeLayerParams::targetshape() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.RankPreservingReshapeLayerParams.targetShape)
  return targetshape_;
}
::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
RankPreservingReshapeLayerParams::mutable_targetshape() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.RankPreservingReshapeLayerParams.targetShape)
  return &targetshape_;
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ExpandDimsLayerParams::kAxesFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ExpandDimsLayerParams::ExpandDimsLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ExpandDimsLayerParams)
}
ExpandDimsLayerParams::ExpandDimsLayerParams(const ExpandDimsLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      axes_(from.axes_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ExpandDimsLayerParams)
}

void ExpandDimsLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

ExpandDimsLayerParams::~ExpandDimsLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ExpandDimsLayerParams)
  SharedDtor();
}

void ExpandDimsLayerParams::SharedDtor() {
}

void ExpandDimsLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ExpandDimsLayerParams& ExpandDimsLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ExpandDimsLayerParams* ExpandDimsLayerParams::New(::google::protobuf::Arena* arena) const {
  ExpandDimsLayerParams* n = new ExpandDimsLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ExpandDimsLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ExpandDimsLayerParams)
  axes_.Clear();
}

bool ExpandDimsLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ExpandDimsLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated int64 axes = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 input, this->mutable_axes())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(8u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 1, 10u, input, this->mutable_axes())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ExpandDimsLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ExpandDimsLayerParams)
  return false;
#undef DO_
}

void ExpandDimsLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ExpandDimsLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated int64 axes = 1;
  if (this->axes_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(1, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_axes_cached_byte_size_);
  }
  for (int i = 0, n = this->axes_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteInt64NoTag(
      this->axes(i), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ExpandDimsLayerParams)
}

size_t ExpandDimsLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ExpandDimsLayerParams)
  size_t total_size = 0;

  // repeated int64 axes = 1;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      Int64Size(this->axes_);
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _axes_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ExpandDimsLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ExpandDimsLayerParams*>(&from));
}

void ExpandDimsLayerParams::MergeFrom(const ExpandDimsLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ExpandDimsLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  axes_.MergeFrom(from.axes_);
}

void ExpandDimsLayerParams::CopyFrom(const ExpandDimsLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ExpandDimsLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ExpandDimsLayerParams::IsInitialized() const {
  return true;
}

void ExpandDimsLayerParams::Swap(ExpandDimsLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ExpandDimsLayerParams::InternalSwap(ExpandDimsLayerParams* other) {
  axes_.InternalSwap(&other->axes_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ExpandDimsLayerParams::GetTypeName() const {
  return "CoreML.Specification.ExpandDimsLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ExpandDimsLayerParams

// repeated int64 axes = 1;
int ExpandDimsLayerParams::axes_size() const {
  return axes_.size();
}
void ExpandDimsLayerParams::clear_axes() {
  axes_.Clear();
}
::google::protobuf::int64 ExpandDimsLayerParams::axes(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ExpandDimsLayerParams.axes)
  return axes_.Get(index);
}
void ExpandDimsLayerParams::set_axes(int index, ::google::protobuf::int64 value) {
  axes_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.ExpandDimsLayerParams.axes)
}
void ExpandDimsLayerParams::add_axes(::google::protobuf::int64 value) {
  axes_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.ExpandDimsLayerParams.axes)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
ExpandDimsLayerParams::axes() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.ExpandDimsLayerParams.axes)
  return axes_;
}
::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
ExpandDimsLayerParams::mutable_axes() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.ExpandDimsLayerParams.axes)
  return &axes_;
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int SqueezeLayerParams::kAxesFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

SqueezeLayerParams::SqueezeLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.SqueezeLayerParams)
}
SqueezeLayerParams::SqueezeLayerParams(const SqueezeLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      axes_(from.axes_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.SqueezeLayerParams)
}

void SqueezeLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

SqueezeLayerParams::~SqueezeLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.SqueezeLayerParams)
  SharedDtor();
}

void SqueezeLayerParams::SharedDtor() {
}

void SqueezeLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const SqueezeLayerParams& SqueezeLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

SqueezeLayerParams* SqueezeLayerParams::New(::google::protobuf::Arena* arena) const {
  SqueezeLayerParams* n = new SqueezeLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void SqueezeLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.SqueezeLayerParams)
  axes_.Clear();
}

bool SqueezeLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.SqueezeLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated int64 axes = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 input, this->mutable_axes())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(8u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 1, 10u, input, this->mutable_axes())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.SqueezeLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.SqueezeLayerParams)
  return false;
#undef DO_
}

void SqueezeLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.SqueezeLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated int64 axes = 1;
  if (this->axes_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(1, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_axes_cached_byte_size_);
  }
  for (int i = 0, n = this->axes_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteInt64NoTag(
      this->axes(i), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.SqueezeLayerParams)
}

size_t SqueezeLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.SqueezeLayerParams)
  size_t total_size = 0;

  // repeated int64 axes = 1;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      Int64Size(this->axes_);
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _axes_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void SqueezeLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const SqueezeLayerParams*>(&from));
}

void SqueezeLayerParams::MergeFrom(const SqueezeLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.SqueezeLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  axes_.MergeFrom(from.axes_);
}

void SqueezeLayerParams::CopyFrom(const SqueezeLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.SqueezeLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool SqueezeLayerParams::IsInitialized() const {
  return true;
}

void SqueezeLayerParams::Swap(SqueezeLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void SqueezeLayerParams::InternalSwap(SqueezeLayerParams* other) {
  axes_.InternalSwap(&other->axes_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string SqueezeLayerParams::GetTypeName() const {
  return "CoreML.Specification.SqueezeLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// SqueezeLayerParams

// repeated int64 axes = 1;
int SqueezeLayerParams::axes_size() const {
  return axes_.size();
}
void SqueezeLayerParams::clear_axes() {
  axes_.Clear();
}
::google::protobuf::int64 SqueezeLayerParams::axes(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SqueezeLayerParams.axes)
  return axes_.Get(index);
}
void SqueezeLayerParams::set_axes(int index, ::google::protobuf::int64 value) {
  axes_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.SqueezeLayerParams.axes)
}
void SqueezeLayerParams::add_axes(::google::protobuf::int64 value) {
  axes_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.SqueezeLayerParams.axes)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
SqueezeLayerParams::axes() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.SqueezeLayerParams.axes)
  return axes_;
}
::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
SqueezeLayerParams::mutable_axes() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.SqueezeLayerParams.axes)
  return &axes_;
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int SplitNDLayerParams::kAxisFieldNumber;
const int SplitNDLayerParams::kNumSplitsFieldNumber;
const int SplitNDLayerParams::kSplitSizesFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

SplitNDLayerParams::SplitNDLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.SplitNDLayerParams)
}
SplitNDLayerParams::SplitNDLayerParams(const SplitNDLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      splitsizes_(from.splitsizes_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::memcpy(&axis_, &from.axis_,
    reinterpret_cast<char*>(&numsplits_) -
    reinterpret_cast<char*>(&axis_) + sizeof(numsplits_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.SplitNDLayerParams)
}

void SplitNDLayerParams::SharedCtor() {
  ::memset(&axis_, 0, reinterpret_cast<char*>(&numsplits_) -
    reinterpret_cast<char*>(&axis_) + sizeof(numsplits_));
  _cached_size_ = 0;
}

SplitNDLayerParams::~SplitNDLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.SplitNDLayerParams)
  SharedDtor();
}

void SplitNDLayerParams::SharedDtor() {
}

void SplitNDLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const SplitNDLayerParams& SplitNDLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

SplitNDLayerParams* SplitNDLayerParams::New(::google::protobuf::Arena* arena) const {
  SplitNDLayerParams* n = new SplitNDLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void SplitNDLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.SplitNDLayerParams)
  splitsizes_.Clear();
  ::memset(&axis_, 0, reinterpret_cast<char*>(&numsplits_) -
    reinterpret_cast<char*>(&axis_) + sizeof(numsplits_));
}

bool SplitNDLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.SplitNDLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // int64 axis = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 input, &axis_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // uint64 numSplits = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(16u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &numsplits_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated uint64 splitSizes = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(26u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, this->mutable_splitsizes())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(24u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 1, 26u, input, this->mutable_splitsizes())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.SplitNDLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.SplitNDLayerParams)
  return false;
#undef DO_
}

void SplitNDLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.SplitNDLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // int64 axis = 1;
  if (this->axis() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteInt64(1, this->axis(), output);
  }

  // uint64 numSplits = 2;
  if (this->numsplits() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(2, this->numsplits(), output);
  }

  // repeated uint64 splitSizes = 3;
  if (this->splitsizes_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(3, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_splitsizes_cached_byte_size_);
  }
  for (int i = 0, n = this->splitsizes_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64NoTag(
      this->splitsizes(i), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.SplitNDLayerParams)
}

size_t SplitNDLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.SplitNDLayerParams)
  size_t total_size = 0;

  // repeated uint64 splitSizes = 3;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      UInt64Size(this->splitsizes_);
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _splitsizes_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // int64 axis = 1;
  if (this->axis() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::Int64Size(
        this->axis());
  }

  // uint64 numSplits = 2;
  if (this->numsplits() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->numsplits());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void SplitNDLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const SplitNDLayerParams*>(&from));
}

void SplitNDLayerParams::MergeFrom(const SplitNDLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.SplitNDLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  splitsizes_.MergeFrom(from.splitsizes_);
  if (from.axis() != 0) {
    set_axis(from.axis());
  }
  if (from.numsplits() != 0) {
    set_numsplits(from.numsplits());
  }
}

void SplitNDLayerParams::CopyFrom(const SplitNDLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.SplitNDLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool SplitNDLayerParams::IsInitialized() const {
  return true;
}

void SplitNDLayerParams::Swap(SplitNDLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void SplitNDLayerParams::InternalSwap(SplitNDLayerParams* other) {
  splitsizes_.InternalSwap(&other->splitsizes_);
  std::swap(axis_, other->axis_);
  std::swap(numsplits_, other->numsplits_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string SplitNDLayerParams::GetTypeName() const {
  return "CoreML.Specification.SplitNDLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// SplitNDLayerParams

// int64 axis = 1;
void SplitNDLayerParams::clear_axis() {
  axis_ = GOOGLE_LONGLONG(0);
}
::google::protobuf::int64 SplitNDLayerParams::axis() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SplitNDLayerParams.axis)
  return axis_;
}
void SplitNDLayerParams::set_axis(::google::protobuf::int64 value) {
  
  axis_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SplitNDLayerParams.axis)
}

// uint64 numSplits = 2;
void SplitNDLayerParams::clear_numsplits() {
  numsplits_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 SplitNDLayerParams::numsplits() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SplitNDLayerParams.numSplits)
  return numsplits_;
}
void SplitNDLayerParams::set_numsplits(::google::protobuf::uint64 value) {
  
  numsplits_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SplitNDLayerParams.numSplits)
}

// repeated uint64 splitSizes = 3;
int SplitNDLayerParams::splitsizes_size() const {
  return splitsizes_.size();
}
void SplitNDLayerParams::clear_splitsizes() {
  splitsizes_.Clear();
}
::google::protobuf::uint64 SplitNDLayerParams::splitsizes(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SplitNDLayerParams.splitSizes)
  return splitsizes_.Get(index);
}
void SplitNDLayerParams::set_splitsizes(int index, ::google::protobuf::uint64 value) {
  splitsizes_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.SplitNDLayerParams.splitSizes)
}
void SplitNDLayerParams::add_splitsizes(::google::protobuf::uint64 value) {
  splitsizes_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.SplitNDLayerParams.splitSizes)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
SplitNDLayerParams::splitsizes() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.SplitNDLayerParams.splitSizes)
  return splitsizes_;
}
::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
SplitNDLayerParams::mutable_splitsizes() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.SplitNDLayerParams.splitSizes)
  return &splitsizes_;
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

CeilLayerParams::CeilLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.CeilLayerParams)
}
CeilLayerParams::CeilLayerParams(const CeilLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.CeilLayerParams)
}

void CeilLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

CeilLayerParams::~CeilLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.CeilLayerParams)
  SharedDtor();
}

void CeilLayerParams::SharedDtor() {
}

void CeilLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const CeilLayerParams& CeilLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

CeilLayerParams* CeilLayerParams::New(::google::protobuf::Arena* arena) const {
  CeilLayerParams* n = new CeilLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void CeilLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.CeilLayerParams)
}

bool CeilLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.CeilLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.CeilLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.CeilLayerParams)
  return false;
#undef DO_
}

void CeilLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.CeilLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.CeilLayerParams)
}

size_t CeilLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.CeilLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void CeilLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const CeilLayerParams*>(&from));
}

void CeilLayerParams::MergeFrom(const CeilLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.CeilLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void CeilLayerParams::CopyFrom(const CeilLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.CeilLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool CeilLayerParams::IsInitialized() const {
  return true;
}

void CeilLayerParams::Swap(CeilLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void CeilLayerParams::InternalSwap(CeilLayerParams* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string CeilLayerParams::GetTypeName() const {
  return "CoreML.Specification.CeilLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// CeilLayerParams

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

FloorLayerParams::FloorLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.FloorLayerParams)
}
FloorLayerParams::FloorLayerParams(const FloorLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.FloorLayerParams)
}

void FloorLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

FloorLayerParams::~FloorLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.FloorLayerParams)
  SharedDtor();
}

void FloorLayerParams::SharedDtor() {
}

void FloorLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const FloorLayerParams& FloorLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

FloorLayerParams* FloorLayerParams::New(::google::protobuf::Arena* arena) const {
  FloorLayerParams* n = new FloorLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void FloorLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.FloorLayerParams)
}

bool FloorLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.FloorLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.FloorLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.FloorLayerParams)
  return false;
#undef DO_
}

void FloorLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.FloorLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.FloorLayerParams)
}

size_t FloorLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.FloorLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void FloorLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const FloorLayerParams*>(&from));
}

void FloorLayerParams::MergeFrom(const FloorLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.FloorLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void FloorLayerParams::CopyFrom(const FloorLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.FloorLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool FloorLayerParams::IsInitialized() const {
  return true;
}

void FloorLayerParams::Swap(FloorLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void FloorLayerParams::InternalSwap(FloorLayerParams* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string FloorLayerParams::GetTypeName() const {
  return "CoreML.Specification.FloorLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// FloorLayerParams

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ClipLayerParams::kMinValFieldNumber;
const int ClipLayerParams::kMaxValFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ClipLayerParams::ClipLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ClipLayerParams)
}
ClipLayerParams::ClipLayerParams(const ClipLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::memcpy(&minval_, &from.minval_,
    reinterpret_cast<char*>(&maxval_) -
    reinterpret_cast<char*>(&minval_) + sizeof(maxval_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ClipLayerParams)
}

void ClipLayerParams::SharedCtor() {
  ::memset(&minval_, 0, reinterpret_cast<char*>(&maxval_) -
    reinterpret_cast<char*>(&minval_) + sizeof(maxval_));
  _cached_size_ = 0;
}

ClipLayerParams::~ClipLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ClipLayerParams)
  SharedDtor();
}

void ClipLayerParams::SharedDtor() {
}

void ClipLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ClipLayerParams& ClipLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ClipLayerParams* ClipLayerParams::New(::google::protobuf::Arena* arena) const {
  ClipLayerParams* n = new ClipLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ClipLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ClipLayerParams)
  ::memset(&minval_, 0, reinterpret_cast<char*>(&maxval_) -
    reinterpret_cast<char*>(&minval_) + sizeof(maxval_));
}

bool ClipLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ClipLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // float minVal = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(13u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &minval_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // float maxVal = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(21u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &maxval_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ClipLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ClipLayerParams)
  return false;
#undef DO_
}

void ClipLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ClipLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // float minVal = 1;
  if (this->minval() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(1, this->minval(), output);
  }

  // float maxVal = 2;
  if (this->maxval() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(2, this->maxval(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ClipLayerParams)
}

size_t ClipLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ClipLayerParams)
  size_t total_size = 0;

  // float minVal = 1;
  if (this->minval() != 0) {
    total_size += 1 + 4;
  }

  // float maxVal = 2;
  if (this->maxval() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ClipLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ClipLayerParams*>(&from));
}

void ClipLayerParams::MergeFrom(const ClipLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ClipLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.minval() != 0) {
    set_minval(from.minval());
  }
  if (from.maxval() != 0) {
    set_maxval(from.maxval());
  }
}

void ClipLayerParams::CopyFrom(const ClipLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ClipLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ClipLayerParams::IsInitialized() const {
  return true;
}

void ClipLayerParams::Swap(ClipLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ClipLayerParams::InternalSwap(ClipLayerParams* other) {
  std::swap(minval_, other->minval_);
  std::swap(maxval_, other->maxval_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ClipLayerParams::GetTypeName() const {
  return "CoreML.Specification.ClipLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ClipLayerParams

// float minVal = 1;
void ClipLayerParams::clear_minval() {
  minval_ = 0;
}
float ClipLayerParams::minval() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ClipLayerParams.minVal)
  return minval_;
}
void ClipLayerParams::set_minval(float value) {
  
  minval_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ClipLayerParams.minVal)
}

// float maxVal = 2;
void ClipLayerParams::clear_maxval() {
  maxval_ = 0;
}
float ClipLayerParams::maxval() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.ClipLayerParams.maxVal)
  return maxval_;
}
void ClipLayerParams::set_maxval(float value) {
  
  maxval_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.ClipLayerParams.maxVal)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int SliceNDLayerParams::kBeginIdsFieldNumber;
const int SliceNDLayerParams::kBeginMasksFieldNumber;
const int SliceNDLayerParams::kEndIdsFieldNumber;
const int SliceNDLayerParams::kEndMasksFieldNumber;
const int SliceNDLayerParams::kStridesFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

SliceNDLayerParams::SliceNDLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.SliceNDLayerParams)
}
SliceNDLayerParams::SliceNDLayerParams(const SliceNDLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      beginids_(from.beginids_),
      beginmasks_(from.beginmasks_),
      endids_(from.endids_),
      endmasks_(from.endmasks_),
      strides_(from.strides_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.SliceNDLayerParams)
}

void SliceNDLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

SliceNDLayerParams::~SliceNDLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.SliceNDLayerParams)
  SharedDtor();
}

void SliceNDLayerParams::SharedDtor() {
}

void SliceNDLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const SliceNDLayerParams& SliceNDLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

SliceNDLayerParams* SliceNDLayerParams::New(::google::protobuf::Arena* arena) const {
  SliceNDLayerParams* n = new SliceNDLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void SliceNDLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.SliceNDLayerParams)
  beginids_.Clear();
  beginmasks_.Clear();
  endids_.Clear();
  endmasks_.Clear();
  strides_.Clear();
}

bool SliceNDLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.SliceNDLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated int64 beginIds = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 input, this->mutable_beginids())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(8u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 1, 10u, input, this->mutable_beginids())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated bool beginMasks = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(18u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, this->mutable_beginmasks())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(16u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 1, 18u, input, this->mutable_beginmasks())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated int64 endIds = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(26u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 input, this->mutable_endids())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(24u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 1, 26u, input, this->mutable_endids())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated bool endMasks = 4;
      case 4: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(34u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, this->mutable_endmasks())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(32u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 1, 34u, input, this->mutable_endmasks())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated int64 strides = 5;
      case 5: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(42u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 input, this->mutable_strides())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(40u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 1, 42u, input, this->mutable_strides())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.SliceNDLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.SliceNDLayerParams)
  return false;
#undef DO_
}

void SliceNDLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.SliceNDLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated int64 beginIds = 1;
  if (this->beginids_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(1, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_beginids_cached_byte_size_);
  }
  for (int i = 0, n = this->beginids_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteInt64NoTag(
      this->beginids(i), output);
  }

  // repeated bool beginMasks = 2;
  if (this->beginmasks_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(2, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_beginmasks_cached_byte_size_);
    ::google::protobuf::internal::WireFormatLite::WriteBoolArray(
      this->beginmasks().data(), this->beginmasks_size(), output);
  }

  // repeated int64 endIds = 3;
  if (this->endids_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(3, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_endids_cached_byte_size_);
  }
  for (int i = 0, n = this->endids_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteInt64NoTag(
      this->endids(i), output);
  }

  // repeated bool endMasks = 4;
  if (this->endmasks_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(4, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_endmasks_cached_byte_size_);
    ::google::protobuf::internal::WireFormatLite::WriteBoolArray(
      this->endmasks().data(), this->endmasks_size(), output);
  }

  // repeated int64 strides = 5;
  if (this->strides_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(5, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_strides_cached_byte_size_);
  }
  for (int i = 0, n = this->strides_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteInt64NoTag(
      this->strides(i), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.SliceNDLayerParams)
}

size_t SliceNDLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.SliceNDLayerParams)
  size_t total_size = 0;

  // repeated int64 beginIds = 1;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      Int64Size(this->beginids_);
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _beginids_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // repeated bool beginMasks = 2;
  {
    unsigned int count = this->beginmasks_size();
    size_t data_size = 1UL * count;
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _beginmasks_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // repeated int64 endIds = 3;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      Int64Size(this->endids_);
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _endids_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // repeated bool endMasks = 4;
  {
    unsigned int count = this->endmasks_size();
    size_t data_size = 1UL * count;
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _endmasks_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // repeated int64 strides = 5;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      Int64Size(this->strides_);
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _strides_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void SliceNDLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const SliceNDLayerParams*>(&from));
}

void SliceNDLayerParams::MergeFrom(const SliceNDLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.SliceNDLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  beginids_.MergeFrom(from.beginids_);
  beginmasks_.MergeFrom(from.beginmasks_);
  endids_.MergeFrom(from.endids_);
  endmasks_.MergeFrom(from.endmasks_);
  strides_.MergeFrom(from.strides_);
}

void SliceNDLayerParams::CopyFrom(const SliceNDLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.SliceNDLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool SliceNDLayerParams::IsInitialized() const {
  return true;
}

void SliceNDLayerParams::Swap(SliceNDLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void SliceNDLayerParams::InternalSwap(SliceNDLayerParams* other) {
  beginids_.InternalSwap(&other->beginids_);
  beginmasks_.InternalSwap(&other->beginmasks_);
  endids_.InternalSwap(&other->endids_);
  endmasks_.InternalSwap(&other->endmasks_);
  strides_.InternalSwap(&other->strides_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string SliceNDLayerParams::GetTypeName() const {
  return "CoreML.Specification.SliceNDLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// SliceNDLayerParams

// repeated int64 beginIds = 1;
int SliceNDLayerParams::beginids_size() const {
  return beginids_.size();
}
void SliceNDLayerParams::clear_beginids() {
  beginids_.Clear();
}
::google::protobuf::int64 SliceNDLayerParams::beginids(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SliceNDLayerParams.beginIds)
  return beginids_.Get(index);
}
void SliceNDLayerParams::set_beginids(int index, ::google::protobuf::int64 value) {
  beginids_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.SliceNDLayerParams.beginIds)
}
void SliceNDLayerParams::add_beginids(::google::protobuf::int64 value) {
  beginids_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.SliceNDLayerParams.beginIds)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
SliceNDLayerParams::beginids() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.SliceNDLayerParams.beginIds)
  return beginids_;
}
::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
SliceNDLayerParams::mutable_beginids() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.SliceNDLayerParams.beginIds)
  return &beginids_;
}

// repeated bool beginMasks = 2;
int SliceNDLayerParams::beginmasks_size() const {
  return beginmasks_.size();
}
void SliceNDLayerParams::clear_beginmasks() {
  beginmasks_.Clear();
}
bool SliceNDLayerParams::beginmasks(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SliceNDLayerParams.beginMasks)
  return beginmasks_.Get(index);
}
void SliceNDLayerParams::set_beginmasks(int index, bool value) {
  beginmasks_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.SliceNDLayerParams.beginMasks)
}
void SliceNDLayerParams::add_beginmasks(bool value) {
  beginmasks_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.SliceNDLayerParams.beginMasks)
}
const ::google::protobuf::RepeatedField< bool >&
SliceNDLayerParams::beginmasks() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.SliceNDLayerParams.beginMasks)
  return beginmasks_;
}
::google::protobuf::RepeatedField< bool >*
SliceNDLayerParams::mutable_beginmasks() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.SliceNDLayerParams.beginMasks)
  return &beginmasks_;
}

// repeated int64 endIds = 3;
int SliceNDLayerParams::endids_size() const {
  return endids_.size();
}
void SliceNDLayerParams::clear_endids() {
  endids_.Clear();
}
::google::protobuf::int64 SliceNDLayerParams::endids(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SliceNDLayerParams.endIds)
  return endids_.Get(index);
}
void SliceNDLayerParams::set_endids(int index, ::google::protobuf::int64 value) {
  endids_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.SliceNDLayerParams.endIds)
}
void SliceNDLayerParams::add_endids(::google::protobuf::int64 value) {
  endids_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.SliceNDLayerParams.endIds)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
SliceNDLayerParams::endids() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.SliceNDLayerParams.endIds)
  return endids_;
}
::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
SliceNDLayerParams::mutable_endids() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.SliceNDLayerParams.endIds)
  return &endids_;
}

// repeated bool endMasks = 4;
int SliceNDLayerParams::endmasks_size() const {
  return endmasks_.size();
}
void SliceNDLayerParams::clear_endmasks() {
  endmasks_.Clear();
}
bool SliceNDLayerParams::endmasks(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SliceNDLayerParams.endMasks)
  return endmasks_.Get(index);
}
void SliceNDLayerParams::set_endmasks(int index, bool value) {
  endmasks_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.SliceNDLayerParams.endMasks)
}
void SliceNDLayerParams::add_endmasks(bool value) {
  endmasks_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.SliceNDLayerParams.endMasks)
}
const ::google::protobuf::RepeatedField< bool >&
SliceNDLayerParams::endmasks() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.SliceNDLayerParams.endMasks)
  return endmasks_;
}
::google::protobuf::RepeatedField< bool >*
SliceNDLayerParams::mutable_endmasks() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.SliceNDLayerParams.endMasks)
  return &endmasks_;
}

// repeated int64 strides = 5;
int SliceNDLayerParams::strides_size() const {
  return strides_.size();
}
void SliceNDLayerParams::clear_strides() {
  strides_.Clear();
}
::google::protobuf::int64 SliceNDLayerParams::strides(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SliceNDLayerParams.strides)
  return strides_.Get(index);
}
void SliceNDLayerParams::set_strides(int index, ::google::protobuf::int64 value) {
  strides_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.SliceNDLayerParams.strides)
}
void SliceNDLayerParams::add_strides(::google::protobuf::int64 value) {
  strides_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.SliceNDLayerParams.strides)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::int64 >&
SliceNDLayerParams::strides() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.SliceNDLayerParams.strides)
  return strides_;
}
::google::protobuf::RepeatedField< ::google::protobuf::int64 >*
SliceNDLayerParams::mutable_strides() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.SliceNDLayerParams.strides)
  return &strides_;
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int TileLayerParams::kRepsFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

TileLayerParams::TileLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.TileLayerParams)
}
TileLayerParams::TileLayerParams(const TileLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      reps_(from.reps_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.TileLayerParams)
}

void TileLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

TileLayerParams::~TileLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.TileLayerParams)
  SharedDtor();
}

void TileLayerParams::SharedDtor() {
}

void TileLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const TileLayerParams& TileLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

TileLayerParams* TileLayerParams::New(::google::protobuf::Arena* arena) const {
  TileLayerParams* n = new TileLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void TileLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.TileLayerParams)
  reps_.Clear();
}

bool TileLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.TileLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated uint64 reps = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, this->mutable_reps())));
        } else if (static_cast< ::google::protobuf::uint8>(tag) ==
                   static_cast< ::google::protobuf::uint8>(8u)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 1, 10u, input, this->mutable_reps())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.TileLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.TileLayerParams)
  return false;
#undef DO_
}

void TileLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.TileLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated uint64 reps = 1;
  if (this->reps_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(1, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(_reps_cached_byte_size_);
  }
  for (int i = 0, n = this->reps_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64NoTag(
      this->reps(i), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.TileLayerParams)
}

size_t TileLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.TileLayerParams)
  size_t total_size = 0;

  // repeated uint64 reps = 1;
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      UInt64Size(this->reps_);
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(data_size);
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _reps_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void TileLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const TileLayerParams*>(&from));
}

void TileLayerParams::MergeFrom(const TileLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.TileLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  reps_.MergeFrom(from.reps_);
}

void TileLayerParams::CopyFrom(const TileLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.TileLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool TileLayerParams::IsInitialized() const {
  return true;
}

void TileLayerParams::Swap(TileLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void TileLayerParams::InternalSwap(TileLayerParams* other) {
  reps_.InternalSwap(&other->reps_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string TileLayerParams::GetTypeName() const {
  return "CoreML.Specification.TileLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// TileLayerParams

// repeated uint64 reps = 1;
int TileLayerParams::reps_size() const {
  return reps_.size();
}
void TileLayerParams::clear_reps() {
  reps_.Clear();
}
::google::protobuf::uint64 TileLayerParams::reps(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.TileLayerParams.reps)
  return reps_.Get(index);
}
void TileLayerParams::set_reps(int index, ::google::protobuf::uint64 value) {
  reps_.Set(index, value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.TileLayerParams.reps)
}
void TileLayerParams::add_reps(::google::protobuf::uint64 value) {
  reps_.Add(value);
  // @@protoc_insertion_point(field_add:CoreML.Specification.TileLayerParams.reps)
}
const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
TileLayerParams::reps() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.TileLayerParams.reps)
  return reps_;
}
::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
TileLayerParams::mutable_reps() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.TileLayerParams.reps)
  return &reps_;
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

GetShapeLayerParams::GetShapeLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.GetShapeLayerParams)
}
GetShapeLayerParams::GetShapeLayerParams(const GetShapeLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.GetShapeLayerParams)
}

void GetShapeLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

GetShapeLayerParams::~GetShapeLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.GetShapeLayerParams)
  SharedDtor();
}

void GetShapeLayerParams::SharedDtor() {
}

void GetShapeLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const GetShapeLayerParams& GetShapeLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

GetShapeLayerParams* GetShapeLayerParams::New(::google::protobuf::Arena* arena) const {
  GetShapeLayerParams* n = new GetShapeLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void GetShapeLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.GetShapeLayerParams)
}

bool GetShapeLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.GetShapeLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.GetShapeLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.GetShapeLayerParams)
  return false;
#undef DO_
}

void GetShapeLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.GetShapeLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.GetShapeLayerParams)
}

size_t GetShapeLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.GetShapeLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void GetShapeLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const GetShapeLayerParams*>(&from));
}

void GetShapeLayerParams::MergeFrom(const GetShapeLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.GetShapeLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void GetShapeLayerParams::CopyFrom(const GetShapeLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.GetShapeLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool GetShapeLayerParams::IsInitialized() const {
  return true;
}

void GetShapeLayerParams::Swap(GetShapeLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void GetShapeLayerParams::InternalSwap(GetShapeLayerParams* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string GetShapeLayerParams::GetTypeName() const {
  return "CoreML.Specification.GetShapeLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// GetShapeLayerParams

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ErfActivationLayerParams::ErfActivationLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.ErfActivationLayerParams)
}
ErfActivationLayerParams::ErfActivationLayerParams(const ErfActivationLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.ErfActivationLayerParams)
}

void ErfActivationLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

ErfActivationLayerParams::~ErfActivationLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.ErfActivationLayerParams)
  SharedDtor();
}

void ErfActivationLayerParams::SharedDtor() {
}

void ErfActivationLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ErfActivationLayerParams& ErfActivationLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

ErfActivationLayerParams* ErfActivationLayerParams::New(::google::protobuf::Arena* arena) const {
  ErfActivationLayerParams* n = new ErfActivationLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void ErfActivationLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.ErfActivationLayerParams)
}

bool ErfActivationLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.ErfActivationLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.ErfActivationLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.ErfActivationLayerParams)
  return false;
#undef DO_
}

void ErfActivationLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.ErfActivationLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.ErfActivationLayerParams)
}

size_t ErfActivationLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.ErfActivationLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ErfActivationLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const ErfActivationLayerParams*>(&from));
}

void ErfActivationLayerParams::MergeFrom(const ErfActivationLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.ErfActivationLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void ErfActivationLayerParams::CopyFrom(const ErfActivationLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.ErfActivationLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ErfActivationLayerParams::IsInitialized() const {
  return true;
}

void ErfActivationLayerParams::Swap(ErfActivationLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ErfActivationLayerParams::InternalSwap(ErfActivationLayerParams* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string ErfActivationLayerParams::GetTypeName() const {
  return "CoreML.Specification.ErfActivationLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ErfActivationLayerParams

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

GeluActivationLayerParams::GeluActivationLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.GeluActivationLayerParams)
}
GeluActivationLayerParams::GeluActivationLayerParams(const GeluActivationLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.GeluActivationLayerParams)
}

void GeluActivationLayerParams::SharedCtor() {
  _cached_size_ = 0;
}

GeluActivationLayerParams::~GeluActivationLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.GeluActivationLayerParams)
  SharedDtor();
}

void GeluActivationLayerParams::SharedDtor() {
}

void GeluActivationLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const GeluActivationLayerParams& GeluActivationLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

GeluActivationLayerParams* GeluActivationLayerParams::New(::google::protobuf::Arena* arena) const {
  GeluActivationLayerParams* n = new GeluActivationLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void GeluActivationLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.GeluActivationLayerParams)
}

bool GeluActivationLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.GeluActivationLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0 ||
        ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
        ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.GeluActivationLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.GeluActivationLayerParams)
  return false;
#undef DO_
}

void GeluActivationLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.GeluActivationLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.GeluActivationLayerParams)
}

size_t GeluActivationLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.GeluActivationLayerParams)
  size_t total_size = 0;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void GeluActivationLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const GeluActivationLayerParams*>(&from));
}

void GeluActivationLayerParams::MergeFrom(const GeluActivationLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.GeluActivationLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void GeluActivationLayerParams::CopyFrom(const GeluActivationLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.GeluActivationLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool GeluActivationLayerParams::IsInitialized() const {
  return true;
}

void GeluActivationLayerParams::Swap(GeluActivationLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void GeluActivationLayerParams::InternalSwap(GeluActivationLayerParams* other) {
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string GeluActivationLayerParams::GetTypeName() const {
  return "CoreML.Specification.GeluActivationLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// GeluActivationLayerParams

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int RangeLayerParams::kEndValueFieldNumber;
const int RangeLayerParams::kStartValueFieldNumber;
const int RangeLayerParams::kStepSizeValueFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

RangeLayerParams::RangeLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.RangeLayerParams)
}
RangeLayerParams::RangeLayerParams(const RangeLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::memcpy(&endvalue_, &from.endvalue_,
    reinterpret_cast<char*>(&stepsizevalue_) -
    reinterpret_cast<char*>(&endvalue_) + sizeof(stepsizevalue_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.RangeLayerParams)
}

void RangeLayerParams::SharedCtor() {
  ::memset(&endvalue_, 0, reinterpret_cast<char*>(&stepsizevalue_) -
    reinterpret_cast<char*>(&endvalue_) + sizeof(stepsizevalue_));
  _cached_size_ = 0;
}

RangeLayerParams::~RangeLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.RangeLayerParams)
  SharedDtor();
}

void RangeLayerParams::SharedDtor() {
}

void RangeLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const RangeLayerParams& RangeLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

RangeLayerParams* RangeLayerParams::New(::google::protobuf::Arena* arena) const {
  RangeLayerParams* n = new RangeLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void RangeLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.RangeLayerParams)
  ::memset(&endvalue_, 0, reinterpret_cast<char*>(&stepsizevalue_) -
    reinterpret_cast<char*>(&endvalue_) + sizeof(stepsizevalue_));
}

bool RangeLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.RangeLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // float endValue = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(13u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &endvalue_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // float startValue = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(21u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &startvalue_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // float stepSizeValue = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(29u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &stepsizevalue_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.RangeLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.RangeLayerParams)
  return false;
#undef DO_
}

void RangeLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.RangeLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // float endValue = 1;
  if (this->endvalue() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(1, this->endvalue(), output);
  }

  // float startValue = 2;
  if (this->startvalue() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(2, this->startvalue(), output);
  }

  // float stepSizeValue = 3;
  if (this->stepsizevalue() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(3, this->stepsizevalue(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.RangeLayerParams)
}

size_t RangeLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.RangeLayerParams)
  size_t total_size = 0;

  // float endValue = 1;
  if (this->endvalue() != 0) {
    total_size += 1 + 4;
  }

  // float startValue = 2;
  if (this->startvalue() != 0) {
    total_size += 1 + 4;
  }

  // float stepSizeValue = 3;
  if (this->stepsizevalue() != 0) {
    total_size += 1 + 4;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void RangeLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const RangeLayerParams*>(&from));
}

void RangeLayerParams::MergeFrom(const RangeLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.RangeLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.endvalue() != 0) {
    set_endvalue(from.endvalue());
  }
  if (from.startvalue() != 0) {
    set_startvalue(from.startvalue());
  }
  if (from.stepsizevalue() != 0) {
    set_stepsizevalue(from.stepsizevalue());
  }
}

void RangeLayerParams::CopyFrom(const RangeLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.RangeLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool RangeLayerParams::IsInitialized() const {
  return true;
}

void RangeLayerParams::Swap(RangeLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void RangeLayerParams::InternalSwap(RangeLayerParams* other) {
  std::swap(endvalue_, other->endvalue_);
  std::swap(startvalue_, other->startvalue_);
  std::swap(stepsizevalue_, other->stepsizevalue_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string RangeLayerParams::GetTypeName() const {
  return "CoreML.Specification.RangeLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// RangeLayerParams

// float endValue = 1;
void RangeLayerParams::clear_endvalue() {
  endvalue_ = 0;
}
float RangeLayerParams::endvalue() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RangeLayerParams.endValue)
  return endvalue_;
}
void RangeLayerParams::set_endvalue(float value) {
  
  endvalue_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.RangeLayerParams.endValue)
}

// float startValue = 2;
void RangeLayerParams::clear_startvalue() {
  startvalue_ = 0;
}
float RangeLayerParams::startvalue() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RangeLayerParams.startValue)
  return startvalue_;
}
void RangeLayerParams::set_startvalue(float value) {
  
  startvalue_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.RangeLayerParams.startValue)
}

// float stepSizeValue = 3;
void RangeLayerParams::clear_stepsizevalue() {
  stepsizevalue_ = 0;
}
float RangeLayerParams::stepsizevalue() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.RangeLayerParams.stepSizeValue)
  return stepsizevalue_;
}
void RangeLayerParams::set_stepsizevalue(float value) {
  
  stepsizevalue_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.RangeLayerParams.stepSizeValue)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int SlidingWindowsLayerParams::kAxisFieldNumber;
const int SlidingWindowsLayerParams::kWindowSizeFieldNumber;
const int SlidingWindowsLayerParams::kStepFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

SlidingWindowsLayerParams::SlidingWindowsLayerParams()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.SlidingWindowsLayerParams)
}
SlidingWindowsLayerParams::SlidingWindowsLayerParams(const SlidingWindowsLayerParams& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::memcpy(&axis_, &from.axis_,
    reinterpret_cast<char*>(&step_) -
    reinterpret_cast<char*>(&axis_) + sizeof(step_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.SlidingWindowsLayerParams)
}

void SlidingWindowsLayerParams::SharedCtor() {
  ::memset(&axis_, 0, reinterpret_cast<char*>(&step_) -
    reinterpret_cast<char*>(&axis_) + sizeof(step_));
  _cached_size_ = 0;
}

SlidingWindowsLayerParams::~SlidingWindowsLayerParams() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.SlidingWindowsLayerParams)
  SharedDtor();
}

void SlidingWindowsLayerParams::SharedDtor() {
}

void SlidingWindowsLayerParams::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const SlidingWindowsLayerParams& SlidingWindowsLayerParams::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

SlidingWindowsLayerParams* SlidingWindowsLayerParams::New(::google::protobuf::Arena* arena) const {
  SlidingWindowsLayerParams* n = new SlidingWindowsLayerParams;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void SlidingWindowsLayerParams::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.SlidingWindowsLayerParams)
  ::memset(&axis_, 0, reinterpret_cast<char*>(&step_) -
    reinterpret_cast<char*>(&axis_) + sizeof(step_));
}

bool SlidingWindowsLayerParams::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.SlidingWindowsLayerParams)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // int64 axis = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 input, &axis_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // uint64 windowSize = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(16u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &windowsize_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // uint64 step = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(24u)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &step_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.SlidingWindowsLayerParams)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.SlidingWindowsLayerParams)
  return false;
#undef DO_
}

void SlidingWindowsLayerParams::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.SlidingWindowsLayerParams)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // int64 axis = 1;
  if (this->axis() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteInt64(1, this->axis(), output);
  }

  // uint64 windowSize = 2;
  if (this->windowsize() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(2, this->windowsize(), output);
  }

  // uint64 step = 3;
  if (this->step() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(3, this->step(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.SlidingWindowsLayerParams)
}

size_t SlidingWindowsLayerParams::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.SlidingWindowsLayerParams)
  size_t total_size = 0;

  // int64 axis = 1;
  if (this->axis() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::Int64Size(
        this->axis());
  }

  // uint64 windowSize = 2;
  if (this->windowsize() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->windowsize());
  }

  // uint64 step = 3;
  if (this->step() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->step());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void SlidingWindowsLayerParams::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const SlidingWindowsLayerParams*>(&from));
}

void SlidingWindowsLayerParams::MergeFrom(const SlidingWindowsLayerParams& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.SlidingWindowsLayerParams)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.axis() != 0) {
    set_axis(from.axis());
  }
  if (from.windowsize() != 0) {
    set_windowsize(from.windowsize());
  }
  if (from.step() != 0) {
    set_step(from.step());
  }
}

void SlidingWindowsLayerParams::CopyFrom(const SlidingWindowsLayerParams& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.SlidingWindowsLayerParams)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool SlidingWindowsLayerParams::IsInitialized() const {
  return true;
}

void SlidingWindowsLayerParams::Swap(SlidingWindowsLayerParams* other) {
  if (other == this) return;
  InternalSwap(other);
}
void SlidingWindowsLayerParams::InternalSwap(SlidingWindowsLayerParams* other) {
  std::swap(axis_, other->axis_);
  std::swap(windowsize_, other->windowsize_);
  std::swap(step_, other->step_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string SlidingWindowsLayerParams::GetTypeName() const {
  return "CoreML.Specification.SlidingWindowsLayerParams";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// SlidingWindowsLayerParams

// int64 axis = 1;
void SlidingWindowsLayerParams::clear_axis() {
  axis_ = GOOGLE_LONGLONG(0);
}
::google::protobuf::int64 SlidingWindowsLayerParams::axis() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SlidingWindowsLayerParams.axis)
  return axis_;
}
void SlidingWindowsLayerParams::set_axis(::google::protobuf::int64 value) {
  
  axis_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SlidingWindowsLayerParams.axis)
}

// uint64 windowSize = 2;
void SlidingWindowsLayerParams::clear_windowsize() {
  windowsize_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 SlidingWindowsLayerParams::windowsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SlidingWindowsLayerParams.windowSize)
  return windowsize_;
}
void SlidingWindowsLayerParams::set_windowsize(::google::protobuf::uint64 value) {
  
  windowsize_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SlidingWindowsLayerParams.windowSize)
}

// uint64 step = 3;
void SlidingWindowsLayerParams::clear_step() {
  step_ = GOOGLE_ULONGLONG(0);
}
::google::protobuf::uint64 SlidingWindowsLayerParams::step() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SlidingWindowsLayerParams.step)
  return step_;
}
void SlidingWindowsLayerParams::set_step(::google::protobuf::uint64 value) {
  
  step_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.SlidingWindowsLayerParams.step)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int NeuralNetworkClassifier::kLayersFieldNumber;
const int NeuralNetworkClassifier::kPreprocessingFieldNumber;
const int NeuralNetworkClassifier::kArrayInputShapeMappingFieldNumber;
const int NeuralNetworkClassifier::kImageInputShapeMappingFieldNumber;
const int NeuralNetworkClassifier::kUpdateParamsFieldNumber;
const int NeuralNetworkClassifier::kStringClassLabelsFieldNumber;
const int NeuralNetworkClassifier::kInt64ClassLabelsFieldNumber;
const int NeuralNetworkClassifier::kLabelProbabilityLayerNameFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

NeuralNetworkClassifier::NeuralNetworkClassifier()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.NeuralNetworkClassifier)
}
NeuralNetworkClassifier::NeuralNetworkClassifier(const NeuralNetworkClassifier& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      layers_(from.layers_),
      preprocessing_(from.preprocessing_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  labelprobabilitylayername_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (from.labelprobabilitylayername().size() > 0) {
    labelprobabilitylayername_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.labelprobabilitylayername_);
  }
  if (from.has_updateparams()) {
    updateparams_ = new ::CoreML::Specification::NetworkUpdateParameters(*from.updateparams_);
  } else {
    updateparams_ = NULL;
  }
  ::memcpy(&arrayinputshapemapping_, &from.arrayinputshapemapping_,
    reinterpret_cast<char*>(&imageinputshapemapping_) -
    reinterpret_cast<char*>(&arrayinputshapemapping_) + sizeof(imageinputshapemapping_));
  clear_has_ClassLabels();
  switch (from.ClassLabels_case()) {
    case kStringClassLabels: {
      mutable_stringclasslabels()->::CoreML::Specification::StringVector::MergeFrom(from.stringclasslabels());
      break;
    }
    case kInt64ClassLabels: {
      mutable_int64classlabels()->::CoreML::Specification::Int64Vector::MergeFrom(from.int64classlabels());
      break;
    }
    case CLASSLABELS_NOT_SET: {
      break;
    }
  }
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.NeuralNetworkClassifier)
}

void NeuralNetworkClassifier::SharedCtor() {
  labelprobabilitylayername_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  ::memset(&updateparams_, 0, reinterpret_cast<char*>(&imageinputshapemapping_) -
    reinterpret_cast<char*>(&updateparams_) + sizeof(imageinputshapemapping_));
  clear_has_ClassLabels();
  _cached_size_ = 0;
}

NeuralNetworkClassifier::~NeuralNetworkClassifier() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.NeuralNetworkClassifier)
  SharedDtor();
}

void NeuralNetworkClassifier::SharedDtor() {
  labelprobabilitylayername_.DestroyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (this != internal_default_instance()) {
    delete updateparams_;
  }
  if (has_ClassLabels()) {
    clear_ClassLabels();
  }
}

void NeuralNetworkClassifier::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const NeuralNetworkClassifier& NeuralNetworkClassifier::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

NeuralNetworkClassifier* NeuralNetworkClassifier::New(::google::protobuf::Arena* arena) const {
  NeuralNetworkClassifier* n = new NeuralNetworkClassifier;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void NeuralNetworkClassifier::clear_ClassLabels() {
// @@protoc_insertion_point(one_of_clear_start:CoreML.Specification.NeuralNetworkClassifier)
  switch (ClassLabels_case()) {
    case kStringClassLabels: {
      delete ClassLabels_.stringclasslabels_;
      break;
    }
    case kInt64ClassLabels: {
      delete ClassLabels_.int64classlabels_;
      break;
    }
    case CLASSLABELS_NOT_SET: {
      break;
    }
  }
  _oneof_case_[0] = CLASSLABELS_NOT_SET;
}


void NeuralNetworkClassifier::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.NeuralNetworkClassifier)
  layers_.Clear();
  preprocessing_.Clear();
  labelprobabilitylayername_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (GetArenaNoVirtual() == NULL && updateparams_ != NULL) {
    delete updateparams_;
  }
  updateparams_ = NULL;
  ::memset(&arrayinputshapemapping_, 0, reinterpret_cast<char*>(&imageinputshapemapping_) -
    reinterpret_cast<char*>(&arrayinputshapemapping_) + sizeof(imageinputshapemapping_));
  clear_ClassLabels();
}

bool NeuralNetworkClassifier::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.NeuralNetworkClassifier)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(16383u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated .CoreML.Specification.NeuralNetworkLayer layers = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
                input, add_layers()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated .CoreML.Specification.NeuralNetworkPreprocessing preprocessing = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(18u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
                input, add_preprocessing()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.NeuralNetworkMultiArrayShapeMapping arrayInputShapeMapping = 5;
      case 5: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(40u)) {
          int value;
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   int, ::google::protobuf::internal::WireFormatLite::TYPE_ENUM>(
                 input, &value)));
          set_arrayinputshapemapping(static_cast< ::CoreML::Specification::NeuralNetworkMultiArrayShapeMapping >(value));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.NeuralNetworkImageShapeMapping imageInputShapeMapping = 6;
      case 6: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(48u)) {
          int value;
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   int, ::google::protobuf::internal::WireFormatLite::TYPE_ENUM>(
                 input, &value)));
          set_imageinputshapemapping(static_cast< ::CoreML::Specification::NeuralNetworkImageShapeMapping >(value));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.NetworkUpdateParameters updateParams = 10;
      case 10: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(82u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_updateparams()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.StringVector stringClassLabels = 100;
      case 100: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(802u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_stringclasslabels()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.Int64Vector int64ClassLabels = 101;
      case 101: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(810u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_int64classlabels()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // string labelProbabilityLayerName = 200;
      case 200: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(1602u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadString(
                input, this->mutable_labelprobabilitylayername()));
          DO_(::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
            this->labelprobabilitylayername().data(), this->labelprobabilitylayername().length(),
            ::google::protobuf::internal::WireFormatLite::PARSE,
            "CoreML.Specification.NeuralNetworkClassifier.labelProbabilityLayerName"));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.NeuralNetworkClassifier)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.NeuralNetworkClassifier)
  return false;
#undef DO_
}

void NeuralNetworkClassifier::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.NeuralNetworkClassifier)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated .CoreML.Specification.NeuralNetworkLayer layers = 1;
  for (unsigned int i = 0, n = this->layers_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1, this->layers(i), output);
  }

  // repeated .CoreML.Specification.NeuralNetworkPreprocessing preprocessing = 2;
  for (unsigned int i = 0, n = this->preprocessing_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      2, this->preprocessing(i), output);
  }

  // .CoreML.Specification.NeuralNetworkMultiArrayShapeMapping arrayInputShapeMapping = 5;
  if (this->arrayinputshapemapping() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteEnum(
      5, this->arrayinputshapemapping(), output);
  }

  // .CoreML.Specification.NeuralNetworkImageShapeMapping imageInputShapeMapping = 6;
  if (this->imageinputshapemapping() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteEnum(
      6, this->imageinputshapemapping(), output);
  }

  // .CoreML.Specification.NetworkUpdateParameters updateParams = 10;
  if (this->has_updateparams()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      10, *this->updateparams_, output);
  }

  // .CoreML.Specification.StringVector stringClassLabels = 100;
  if (has_stringclasslabels()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      100, *ClassLabels_.stringclasslabels_, output);
  }

  // .CoreML.Specification.Int64Vector int64ClassLabels = 101;
  if (has_int64classlabels()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      101, *ClassLabels_.int64classlabels_, output);
  }

  // string labelProbabilityLayerName = 200;
  if (this->labelprobabilitylayername().size() > 0) {
    ::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
      this->labelprobabilitylayername().data(), this->labelprobabilitylayername().length(),
      ::google::protobuf::internal::WireFormatLite::SERIALIZE,
      "CoreML.Specification.NeuralNetworkClassifier.labelProbabilityLayerName");
    ::google::protobuf::internal::WireFormatLite::WriteStringMaybeAliased(
      200, this->labelprobabilitylayername(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.NeuralNetworkClassifier)
}

size_t NeuralNetworkClassifier::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.NeuralNetworkClassifier)
  size_t total_size = 0;

  // repeated .CoreML.Specification.NeuralNetworkLayer layers = 1;
  {
    unsigned int count = this->layers_size();
    total_size += 1UL * count;
    for (unsigned int i = 0; i < count; i++) {
      total_size +=
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          this->layers(i));
    }
  }

  // repeated .CoreML.Specification.NeuralNetworkPreprocessing preprocessing = 2;
  {
    unsigned int count = this->preprocessing_size();
    total_size += 1UL * count;
    for (unsigned int i = 0; i < count; i++) {
      total_size +=
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          this->preprocessing(i));
    }
  }

  // string labelProbabilityLayerName = 200;
  if (this->labelprobabilitylayername().size() > 0) {
    total_size += 2 +
      ::google::protobuf::internal::WireFormatLite::StringSize(
        this->labelprobabilitylayername());
  }

  // .CoreML.Specification.NetworkUpdateParameters updateParams = 10;
  if (this->has_updateparams()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->updateparams_);
  }

  // .CoreML.Specification.NeuralNetworkMultiArrayShapeMapping arrayInputShapeMapping = 5;
  if (this->arrayinputshapemapping() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::EnumSize(this->arrayinputshapemapping());
  }

  // .CoreML.Specification.NeuralNetworkImageShapeMapping imageInputShapeMapping = 6;
  if (this->imageinputshapemapping() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::EnumSize(this->imageinputshapemapping());
  }

  switch (ClassLabels_case()) {
    // .CoreML.Specification.StringVector stringClassLabels = 100;
    case kStringClassLabels: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *ClassLabels_.stringclasslabels_);
      break;
    }
    // .CoreML.Specification.Int64Vector int64ClassLabels = 101;
    case kInt64ClassLabels: {
      total_size += 2 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *ClassLabels_.int64classlabels_);
      break;
    }
    case CLASSLABELS_NOT_SET: {
      break;
    }
  }
  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void NeuralNetworkClassifier::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const NeuralNetworkClassifier*>(&from));
}

void NeuralNetworkClassifier::MergeFrom(const NeuralNetworkClassifier& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.NeuralNetworkClassifier)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  layers_.MergeFrom(from.layers_);
  preprocessing_.MergeFrom(from.preprocessing_);
  if (from.labelprobabilitylayername().size() > 0) {

    labelprobabilitylayername_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.labelprobabilitylayername_);
  }
  if (from.has_updateparams()) {
    mutable_updateparams()->::CoreML::Specification::NetworkUpdateParameters::MergeFrom(from.updateparams());
  }
  if (from.arrayinputshapemapping() != 0) {
    set_arrayinputshapemapping(from.arrayinputshapemapping());
  }
  if (from.imageinputshapemapping() != 0) {
    set_imageinputshapemapping(from.imageinputshapemapping());
  }
  switch (from.ClassLabels_case()) {
    case kStringClassLabels: {
      mutable_stringclasslabels()->::CoreML::Specification::StringVector::MergeFrom(from.stringclasslabels());
      break;
    }
    case kInt64ClassLabels: {
      mutable_int64classlabels()->::CoreML::Specification::Int64Vector::MergeFrom(from.int64classlabels());
      break;
    }
    case CLASSLABELS_NOT_SET: {
      break;
    }
  }
}

void NeuralNetworkClassifier::CopyFrom(const NeuralNetworkClassifier& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.NeuralNetworkClassifier)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool NeuralNetworkClassifier::IsInitialized() const {
  return true;
}

void NeuralNetworkClassifier::Swap(NeuralNetworkClassifier* other) {
  if (other == this) return;
  InternalSwap(other);
}
void NeuralNetworkClassifier::InternalSwap(NeuralNetworkClassifier* other) {
  layers_.InternalSwap(&other->layers_);
  preprocessing_.InternalSwap(&other->preprocessing_);
  labelprobabilitylayername_.Swap(&other->labelprobabilitylayername_);
  std::swap(updateparams_, other->updateparams_);
  std::swap(arrayinputshapemapping_, other->arrayinputshapemapping_);
  std::swap(imageinputshapemapping_, other->imageinputshapemapping_);
  std::swap(ClassLabels_, other->ClassLabels_);
  std::swap(_oneof_case_[0], other->_oneof_case_[0]);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string NeuralNetworkClassifier::GetTypeName() const {
  return "CoreML.Specification.NeuralNetworkClassifier";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// NeuralNetworkClassifier

// repeated .CoreML.Specification.NeuralNetworkLayer layers = 1;
int NeuralNetworkClassifier::layers_size() const {
  return layers_.size();
}
void NeuralNetworkClassifier::clear_layers() {
  layers_.Clear();
}
const ::CoreML::Specification::NeuralNetworkLayer& NeuralNetworkClassifier::layers(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkClassifier.layers)
  return layers_.Get(index);
}
::CoreML::Specification::NeuralNetworkLayer* NeuralNetworkClassifier::mutable_layers(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkClassifier.layers)
  return layers_.Mutable(index);
}
::CoreML::Specification::NeuralNetworkLayer* NeuralNetworkClassifier::add_layers() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.NeuralNetworkClassifier.layers)
  return layers_.Add();
}
::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkLayer >*
NeuralNetworkClassifier::mutable_layers() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.NeuralNetworkClassifier.layers)
  return &layers_;
}
const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkLayer >&
NeuralNetworkClassifier::layers() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.NeuralNetworkClassifier.layers)
  return layers_;
}

// repeated .CoreML.Specification.NeuralNetworkPreprocessing preprocessing = 2;
int NeuralNetworkClassifier::preprocessing_size() const {
  return preprocessing_.size();
}
void NeuralNetworkClassifier::clear_preprocessing() {
  preprocessing_.Clear();
}
const ::CoreML::Specification::NeuralNetworkPreprocessing& NeuralNetworkClassifier::preprocessing(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkClassifier.preprocessing)
  return preprocessing_.Get(index);
}
::CoreML::Specification::NeuralNetworkPreprocessing* NeuralNetworkClassifier::mutable_preprocessing(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkClassifier.preprocessing)
  return preprocessing_.Mutable(index);
}
::CoreML::Specification::NeuralNetworkPreprocessing* NeuralNetworkClassifier::add_preprocessing() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.NeuralNetworkClassifier.preprocessing)
  return preprocessing_.Add();
}
::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkPreprocessing >*
NeuralNetworkClassifier::mutable_preprocessing() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.NeuralNetworkClassifier.preprocessing)
  return &preprocessing_;
}
const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkPreprocessing >&
NeuralNetworkClassifier::preprocessing() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.NeuralNetworkClassifier.preprocessing)
  return preprocessing_;
}

// .CoreML.Specification.NeuralNetworkMultiArrayShapeMapping arrayInputShapeMapping = 5;
void NeuralNetworkClassifier::clear_arrayinputshapemapping() {
  arrayinputshapemapping_ = 0;
}
::CoreML::Specification::NeuralNetworkMultiArrayShapeMapping NeuralNetworkClassifier::arrayinputshapemapping() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkClassifier.arrayInputShapeMapping)
  return static_cast< ::CoreML::Specification::NeuralNetworkMultiArrayShapeMapping >(arrayinputshapemapping_);
}
void NeuralNetworkClassifier::set_arrayinputshapemapping(::CoreML::Specification::NeuralNetworkMultiArrayShapeMapping value) {
  
  arrayinputshapemapping_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkClassifier.arrayInputShapeMapping)
}

// .CoreML.Specification.NeuralNetworkImageShapeMapping imageInputShapeMapping = 6;
void NeuralNetworkClassifier::clear_imageinputshapemapping() {
  imageinputshapemapping_ = 0;
}
::CoreML::Specification::NeuralNetworkImageShapeMapping NeuralNetworkClassifier::imageinputshapemapping() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkClassifier.imageInputShapeMapping)
  return static_cast< ::CoreML::Specification::NeuralNetworkImageShapeMapping >(imageinputshapemapping_);
}
void NeuralNetworkClassifier::set_imageinputshapemapping(::CoreML::Specification::NeuralNetworkImageShapeMapping value) {
  
  imageinputshapemapping_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkClassifier.imageInputShapeMapping)
}

// .CoreML.Specification.NetworkUpdateParameters updateParams = 10;
bool NeuralNetworkClassifier::has_updateparams() const {
  return this != internal_default_instance() && updateparams_ != NULL;
}
void NeuralNetworkClassifier::clear_updateparams() {
  if (GetArenaNoVirtual() == NULL && updateparams_ != NULL) delete updateparams_;
  updateparams_ = NULL;
}
const ::CoreML::Specification::NetworkUpdateParameters& NeuralNetworkClassifier::updateparams() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkClassifier.updateParams)
  return updateparams_ != NULL ? *updateparams_
                         : *::CoreML::Specification::NetworkUpdateParameters::internal_default_instance();
}
::CoreML::Specification::NetworkUpdateParameters* NeuralNetworkClassifier::mutable_updateparams() {
  
  if (updateparams_ == NULL) {
    updateparams_ = new ::CoreML::Specification::NetworkUpdateParameters;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkClassifier.updateParams)
  return updateparams_;
}
::CoreML::Specification::NetworkUpdateParameters* NeuralNetworkClassifier::release_updateparams() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkClassifier.updateParams)
  
  ::CoreML::Specification::NetworkUpdateParameters* temp = updateparams_;
  updateparams_ = NULL;
  return temp;
}
void NeuralNetworkClassifier::set_allocated_updateparams(::CoreML::Specification::NetworkUpdateParameters* updateparams) {
  delete updateparams_;
  updateparams_ = updateparams;
  if (updateparams) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkClassifier.updateParams)
}

// .CoreML.Specification.StringVector stringClassLabels = 100;
bool NeuralNetworkClassifier::has_stringclasslabels() const {
  return ClassLabels_case() == kStringClassLabels;
}
void NeuralNetworkClassifier::set_has_stringclasslabels() {
  _oneof_case_[0] = kStringClassLabels;
}
void NeuralNetworkClassifier::clear_stringclasslabels() {
  if (has_stringclasslabels()) {
    delete ClassLabels_.stringclasslabels_;
    clear_has_ClassLabels();
  }
}
 const ::CoreML::Specification::StringVector& NeuralNetworkClassifier::stringclasslabels() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkClassifier.stringClassLabels)
  return has_stringclasslabels()
      ? *ClassLabels_.stringclasslabels_
      : ::CoreML::Specification::StringVector::default_instance();
}
::CoreML::Specification::StringVector* NeuralNetworkClassifier::mutable_stringclasslabels() {
  if (!has_stringclasslabels()) {
    clear_ClassLabels();
    set_has_stringclasslabels();
    ClassLabels_.stringclasslabels_ = new ::CoreML::Specification::StringVector;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkClassifier.stringClassLabels)
  return ClassLabels_.stringclasslabels_;
}
::CoreML::Specification::StringVector* NeuralNetworkClassifier::release_stringclasslabels() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkClassifier.stringClassLabels)
  if (has_stringclasslabels()) {
    clear_has_ClassLabels();
    ::CoreML::Specification::StringVector* temp = ClassLabels_.stringclasslabels_;
    ClassLabels_.stringclasslabels_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkClassifier::set_allocated_stringclasslabels(::CoreML::Specification::StringVector* stringclasslabels) {
  clear_ClassLabels();
  if (stringclasslabels) {
    set_has_stringclasslabels();
    ClassLabels_.stringclasslabels_ = stringclasslabels;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkClassifier.stringClassLabels)
}

// .CoreML.Specification.Int64Vector int64ClassLabels = 101;
bool NeuralNetworkClassifier::has_int64classlabels() const {
  return ClassLabels_case() == kInt64ClassLabels;
}
void NeuralNetworkClassifier::set_has_int64classlabels() {
  _oneof_case_[0] = kInt64ClassLabels;
}
void NeuralNetworkClassifier::clear_int64classlabels() {
  if (has_int64classlabels()) {
    delete ClassLabels_.int64classlabels_;
    clear_has_ClassLabels();
  }
}
 const ::CoreML::Specification::Int64Vector& NeuralNetworkClassifier::int64classlabels() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkClassifier.int64ClassLabels)
  return has_int64classlabels()
      ? *ClassLabels_.int64classlabels_
      : ::CoreML::Specification::Int64Vector::default_instance();
}
::CoreML::Specification::Int64Vector* NeuralNetworkClassifier::mutable_int64classlabels() {
  if (!has_int64classlabels()) {
    clear_ClassLabels();
    set_has_int64classlabels();
    ClassLabels_.int64classlabels_ = new ::CoreML::Specification::Int64Vector;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkClassifier.int64ClassLabels)
  return ClassLabels_.int64classlabels_;
}
::CoreML::Specification::Int64Vector* NeuralNetworkClassifier::release_int64classlabels() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkClassifier.int64ClassLabels)
  if (has_int64classlabels()) {
    clear_has_ClassLabels();
    ::CoreML::Specification::Int64Vector* temp = ClassLabels_.int64classlabels_;
    ClassLabels_.int64classlabels_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void NeuralNetworkClassifier::set_allocated_int64classlabels(::CoreML::Specification::Int64Vector* int64classlabels) {
  clear_ClassLabels();
  if (int64classlabels) {
    set_has_int64classlabels();
    ClassLabels_.int64classlabels_ = int64classlabels;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkClassifier.int64ClassLabels)
}

// string labelProbabilityLayerName = 200;
void NeuralNetworkClassifier::clear_labelprobabilitylayername() {
  labelprobabilitylayername_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
const ::std::string& NeuralNetworkClassifier::labelprobabilitylayername() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkClassifier.labelProbabilityLayerName)
  return labelprobabilitylayername_.GetNoArena();
}
void NeuralNetworkClassifier::set_labelprobabilitylayername(const ::std::string& value) {
  
  labelprobabilitylayername_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkClassifier.labelProbabilityLayerName)
}
#if LANG_CXX11
void NeuralNetworkClassifier::set_labelprobabilitylayername(::std::string&& value) {
  
  labelprobabilitylayername_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:CoreML.Specification.NeuralNetworkClassifier.labelProbabilityLayerName)
}
#endif
void NeuralNetworkClassifier::set_labelprobabilitylayername(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  
  labelprobabilitylayername_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:CoreML.Specification.NeuralNetworkClassifier.labelProbabilityLayerName)
}
void NeuralNetworkClassifier::set_labelprobabilitylayername(const char* value, size_t size) {
  
  labelprobabilitylayername_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:CoreML.Specification.NeuralNetworkClassifier.labelProbabilityLayerName)
}
::std::string* NeuralNetworkClassifier::mutable_labelprobabilitylayername() {
  
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkClassifier.labelProbabilityLayerName)
  return labelprobabilitylayername_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
::std::string* NeuralNetworkClassifier::release_labelprobabilitylayername() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkClassifier.labelProbabilityLayerName)
  
  return labelprobabilitylayername_.ReleaseNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
void NeuralNetworkClassifier::set_allocated_labelprobabilitylayername(::std::string* labelprobabilitylayername) {
  if (labelprobabilitylayername != NULL) {
    
  } else {
    
  }
  labelprobabilitylayername_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), labelprobabilitylayername);
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkClassifier.labelProbabilityLayerName)
}

bool NeuralNetworkClassifier::has_ClassLabels() const {
  return ClassLabels_case() != CLASSLABELS_NOT_SET;
}
void NeuralNetworkClassifier::clear_has_ClassLabels() {
  _oneof_case_[0] = CLASSLABELS_NOT_SET;
}
NeuralNetworkClassifier::ClassLabelsCase NeuralNetworkClassifier::ClassLabels_case() const {
  return NeuralNetworkClassifier::ClassLabelsCase(_oneof_case_[0]);
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int NeuralNetworkRegressor::kLayersFieldNumber;
const int NeuralNetworkRegressor::kPreprocessingFieldNumber;
const int NeuralNetworkRegressor::kArrayInputShapeMappingFieldNumber;
const int NeuralNetworkRegressor::kImageInputShapeMappingFieldNumber;
const int NeuralNetworkRegressor::kUpdateParamsFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

NeuralNetworkRegressor::NeuralNetworkRegressor()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.NeuralNetworkRegressor)
}
NeuralNetworkRegressor::NeuralNetworkRegressor(const NeuralNetworkRegressor& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      layers_(from.layers_),
      preprocessing_(from.preprocessing_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_updateparams()) {
    updateparams_ = new ::CoreML::Specification::NetworkUpdateParameters(*from.updateparams_);
  } else {
    updateparams_ = NULL;
  }
  ::memcpy(&arrayinputshapemapping_, &from.arrayinputshapemapping_,
    reinterpret_cast<char*>(&imageinputshapemapping_) -
    reinterpret_cast<char*>(&arrayinputshapemapping_) + sizeof(imageinputshapemapping_));
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.NeuralNetworkRegressor)
}

void NeuralNetworkRegressor::SharedCtor() {
  ::memset(&updateparams_, 0, reinterpret_cast<char*>(&imageinputshapemapping_) -
    reinterpret_cast<char*>(&updateparams_) + sizeof(imageinputshapemapping_));
  _cached_size_ = 0;
}

NeuralNetworkRegressor::~NeuralNetworkRegressor() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.NeuralNetworkRegressor)
  SharedDtor();
}

void NeuralNetworkRegressor::SharedDtor() {
  if (this != internal_default_instance()) {
    delete updateparams_;
  }
}

void NeuralNetworkRegressor::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const NeuralNetworkRegressor& NeuralNetworkRegressor::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

NeuralNetworkRegressor* NeuralNetworkRegressor::New(::google::protobuf::Arena* arena) const {
  NeuralNetworkRegressor* n = new NeuralNetworkRegressor;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void NeuralNetworkRegressor::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.NeuralNetworkRegressor)
  layers_.Clear();
  preprocessing_.Clear();
  if (GetArenaNoVirtual() == NULL && updateparams_ != NULL) {
    delete updateparams_;
  }
  updateparams_ = NULL;
  ::memset(&arrayinputshapemapping_, 0, reinterpret_cast<char*>(&imageinputshapemapping_) -
    reinterpret_cast<char*>(&arrayinputshapemapping_) + sizeof(imageinputshapemapping_));
}

bool NeuralNetworkRegressor::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.NeuralNetworkRegressor)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated .CoreML.Specification.NeuralNetworkLayer layers = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
                input, add_layers()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated .CoreML.Specification.NeuralNetworkPreprocessing preprocessing = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(18u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
                input, add_preprocessing()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.NeuralNetworkMultiArrayShapeMapping arrayInputShapeMapping = 5;
      case 5: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(40u)) {
          int value;
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   int, ::google::protobuf::internal::WireFormatLite::TYPE_ENUM>(
                 input, &value)));
          set_arrayinputshapemapping(static_cast< ::CoreML::Specification::NeuralNetworkMultiArrayShapeMapping >(value));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.NeuralNetworkImageShapeMapping imageInputShapeMapping = 6;
      case 6: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(48u)) {
          int value;
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   int, ::google::protobuf::internal::WireFormatLite::TYPE_ENUM>(
                 input, &value)));
          set_imageinputshapemapping(static_cast< ::CoreML::Specification::NeuralNetworkImageShapeMapping >(value));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.NetworkUpdateParameters updateParams = 10;
      case 10: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(82u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_updateparams()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.NeuralNetworkRegressor)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.NeuralNetworkRegressor)
  return false;
#undef DO_
}

void NeuralNetworkRegressor::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.NeuralNetworkRegressor)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated .CoreML.Specification.NeuralNetworkLayer layers = 1;
  for (unsigned int i = 0, n = this->layers_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1, this->layers(i), output);
  }

  // repeated .CoreML.Specification.NeuralNetworkPreprocessing preprocessing = 2;
  for (unsigned int i = 0, n = this->preprocessing_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      2, this->preprocessing(i), output);
  }

  // .CoreML.Specification.NeuralNetworkMultiArrayShapeMapping arrayInputShapeMapping = 5;
  if (this->arrayinputshapemapping() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteEnum(
      5, this->arrayinputshapemapping(), output);
  }

  // .CoreML.Specification.NeuralNetworkImageShapeMapping imageInputShapeMapping = 6;
  if (this->imageinputshapemapping() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteEnum(
      6, this->imageinputshapemapping(), output);
  }

  // .CoreML.Specification.NetworkUpdateParameters updateParams = 10;
  if (this->has_updateparams()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      10, *this->updateparams_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.NeuralNetworkRegressor)
}

size_t NeuralNetworkRegressor::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.NeuralNetworkRegressor)
  size_t total_size = 0;

  // repeated .CoreML.Specification.NeuralNetworkLayer layers = 1;
  {
    unsigned int count = this->layers_size();
    total_size += 1UL * count;
    for (unsigned int i = 0; i < count; i++) {
      total_size +=
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          this->layers(i));
    }
  }

  // repeated .CoreML.Specification.NeuralNetworkPreprocessing preprocessing = 2;
  {
    unsigned int count = this->preprocessing_size();
    total_size += 1UL * count;
    for (unsigned int i = 0; i < count; i++) {
      total_size +=
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          this->preprocessing(i));
    }
  }

  // .CoreML.Specification.NetworkUpdateParameters updateParams = 10;
  if (this->has_updateparams()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->updateparams_);
  }

  // .CoreML.Specification.NeuralNetworkMultiArrayShapeMapping arrayInputShapeMapping = 5;
  if (this->arrayinputshapemapping() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::EnumSize(this->arrayinputshapemapping());
  }

  // .CoreML.Specification.NeuralNetworkImageShapeMapping imageInputShapeMapping = 6;
  if (this->imageinputshapemapping() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::EnumSize(this->imageinputshapemapping());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void NeuralNetworkRegressor::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const NeuralNetworkRegressor*>(&from));
}

void NeuralNetworkRegressor::MergeFrom(const NeuralNetworkRegressor& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.NeuralNetworkRegressor)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  layers_.MergeFrom(from.layers_);
  preprocessing_.MergeFrom(from.preprocessing_);
  if (from.has_updateparams()) {
    mutable_updateparams()->::CoreML::Specification::NetworkUpdateParameters::MergeFrom(from.updateparams());
  }
  if (from.arrayinputshapemapping() != 0) {
    set_arrayinputshapemapping(from.arrayinputshapemapping());
  }
  if (from.imageinputshapemapping() != 0) {
    set_imageinputshapemapping(from.imageinputshapemapping());
  }
}

void NeuralNetworkRegressor::CopyFrom(const NeuralNetworkRegressor& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.NeuralNetworkRegressor)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool NeuralNetworkRegressor::IsInitialized() const {
  return true;
}

void NeuralNetworkRegressor::Swap(NeuralNetworkRegressor* other) {
  if (other == this) return;
  InternalSwap(other);
}
void NeuralNetworkRegressor::InternalSwap(NeuralNetworkRegressor* other) {
  layers_.InternalSwap(&other->layers_);
  preprocessing_.InternalSwap(&other->preprocessing_);
  std::swap(updateparams_, other->updateparams_);
  std::swap(arrayinputshapemapping_, other->arrayinputshapemapping_);
  std::swap(imageinputshapemapping_, other->imageinputshapemapping_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string NeuralNetworkRegressor::GetTypeName() const {
  return "CoreML.Specification.NeuralNetworkRegressor";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// NeuralNetworkRegressor

// repeated .CoreML.Specification.NeuralNetworkLayer layers = 1;
int NeuralNetworkRegressor::layers_size() const {
  return layers_.size();
}
void NeuralNetworkRegressor::clear_layers() {
  layers_.Clear();
}
const ::CoreML::Specification::NeuralNetworkLayer& NeuralNetworkRegressor::layers(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkRegressor.layers)
  return layers_.Get(index);
}
::CoreML::Specification::NeuralNetworkLayer* NeuralNetworkRegressor::mutable_layers(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkRegressor.layers)
  return layers_.Mutable(index);
}
::CoreML::Specification::NeuralNetworkLayer* NeuralNetworkRegressor::add_layers() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.NeuralNetworkRegressor.layers)
  return layers_.Add();
}
::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkLayer >*
NeuralNetworkRegressor::mutable_layers() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.NeuralNetworkRegressor.layers)
  return &layers_;
}
const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkLayer >&
NeuralNetworkRegressor::layers() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.NeuralNetworkRegressor.layers)
  return layers_;
}

// repeated .CoreML.Specification.NeuralNetworkPreprocessing preprocessing = 2;
int NeuralNetworkRegressor::preprocessing_size() const {
  return preprocessing_.size();
}
void NeuralNetworkRegressor::clear_preprocessing() {
  preprocessing_.Clear();
}
const ::CoreML::Specification::NeuralNetworkPreprocessing& NeuralNetworkRegressor::preprocessing(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkRegressor.preprocessing)
  return preprocessing_.Get(index);
}
::CoreML::Specification::NeuralNetworkPreprocessing* NeuralNetworkRegressor::mutable_preprocessing(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkRegressor.preprocessing)
  return preprocessing_.Mutable(index);
}
::CoreML::Specification::NeuralNetworkPreprocessing* NeuralNetworkRegressor::add_preprocessing() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.NeuralNetworkRegressor.preprocessing)
  return preprocessing_.Add();
}
::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkPreprocessing >*
NeuralNetworkRegressor::mutable_preprocessing() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.NeuralNetworkRegressor.preprocessing)
  return &preprocessing_;
}
const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::NeuralNetworkPreprocessing >&
NeuralNetworkRegressor::preprocessing() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.NeuralNetworkRegressor.preprocessing)
  return preprocessing_;
}

// .CoreML.Specification.NeuralNetworkMultiArrayShapeMapping arrayInputShapeMapping = 5;
void NeuralNetworkRegressor::clear_arrayinputshapemapping() {
  arrayinputshapemapping_ = 0;
}
::CoreML::Specification::NeuralNetworkMultiArrayShapeMapping NeuralNetworkRegressor::arrayinputshapemapping() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkRegressor.arrayInputShapeMapping)
  return static_cast< ::CoreML::Specification::NeuralNetworkMultiArrayShapeMapping >(arrayinputshapemapping_);
}
void NeuralNetworkRegressor::set_arrayinputshapemapping(::CoreML::Specification::NeuralNetworkMultiArrayShapeMapping value) {
  
  arrayinputshapemapping_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkRegressor.arrayInputShapeMapping)
}

// .CoreML.Specification.NeuralNetworkImageShapeMapping imageInputShapeMapping = 6;
void NeuralNetworkRegressor::clear_imageinputshapemapping() {
  imageinputshapemapping_ = 0;
}
::CoreML::Specification::NeuralNetworkImageShapeMapping NeuralNetworkRegressor::imageinputshapemapping() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkRegressor.imageInputShapeMapping)
  return static_cast< ::CoreML::Specification::NeuralNetworkImageShapeMapping >(imageinputshapemapping_);
}
void NeuralNetworkRegressor::set_imageinputshapemapping(::CoreML::Specification::NeuralNetworkImageShapeMapping value) {
  
  imageinputshapemapping_ = value;
  // @@protoc_insertion_point(field_set:CoreML.Specification.NeuralNetworkRegressor.imageInputShapeMapping)
}

// .CoreML.Specification.NetworkUpdateParameters updateParams = 10;
bool NeuralNetworkRegressor::has_updateparams() const {
  return this != internal_default_instance() && updateparams_ != NULL;
}
void NeuralNetworkRegressor::clear_updateparams() {
  if (GetArenaNoVirtual() == NULL && updateparams_ != NULL) delete updateparams_;
  updateparams_ = NULL;
}
const ::CoreML::Specification::NetworkUpdateParameters& NeuralNetworkRegressor::updateparams() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NeuralNetworkRegressor.updateParams)
  return updateparams_ != NULL ? *updateparams_
                         : *::CoreML::Specification::NetworkUpdateParameters::internal_default_instance();
}
::CoreML::Specification::NetworkUpdateParameters* NeuralNetworkRegressor::mutable_updateparams() {
  
  if (updateparams_ == NULL) {
    updateparams_ = new ::CoreML::Specification::NetworkUpdateParameters;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NeuralNetworkRegressor.updateParams)
  return updateparams_;
}
::CoreML::Specification::NetworkUpdateParameters* NeuralNetworkRegressor::release_updateparams() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NeuralNetworkRegressor.updateParams)
  
  ::CoreML::Specification::NetworkUpdateParameters* temp = updateparams_;
  updateparams_ = NULL;
  return temp;
}
void NeuralNetworkRegressor::set_allocated_updateparams(::CoreML::Specification::NetworkUpdateParameters* updateparams) {
  delete updateparams_;
  updateparams_ = updateparams;
  if (updateparams) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NeuralNetworkRegressor.updateParams)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int NetworkUpdateParameters::kLossLayersFieldNumber;
const int NetworkUpdateParameters::kOptimizerFieldNumber;
const int NetworkUpdateParameters::kEpochsFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

NetworkUpdateParameters::NetworkUpdateParameters()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.NetworkUpdateParameters)
}
NetworkUpdateParameters::NetworkUpdateParameters(const NetworkUpdateParameters& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      losslayers_(from.losslayers_),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_optimizer()) {
    optimizer_ = new ::CoreML::Specification::Optimizer(*from.optimizer_);
  } else {
    optimizer_ = NULL;
  }
  if (from.has_epochs()) {
    epochs_ = new ::CoreML::Specification::Int64Parameter(*from.epochs_);
  } else {
    epochs_ = NULL;
  }
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.NetworkUpdateParameters)
}

void NetworkUpdateParameters::SharedCtor() {
  ::memset(&optimizer_, 0, reinterpret_cast<char*>(&epochs_) -
    reinterpret_cast<char*>(&optimizer_) + sizeof(epochs_));
  _cached_size_ = 0;
}

NetworkUpdateParameters::~NetworkUpdateParameters() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.NetworkUpdateParameters)
  SharedDtor();
}

void NetworkUpdateParameters::SharedDtor() {
  if (this != internal_default_instance()) {
    delete optimizer_;
  }
  if (this != internal_default_instance()) {
    delete epochs_;
  }
}

void NetworkUpdateParameters::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const NetworkUpdateParameters& NetworkUpdateParameters::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

NetworkUpdateParameters* NetworkUpdateParameters::New(::google::protobuf::Arena* arena) const {
  NetworkUpdateParameters* n = new NetworkUpdateParameters;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void NetworkUpdateParameters::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.NetworkUpdateParameters)
  losslayers_.Clear();
  if (GetArenaNoVirtual() == NULL && optimizer_ != NULL) {
    delete optimizer_;
  }
  optimizer_ = NULL;
  if (GetArenaNoVirtual() == NULL && epochs_ != NULL) {
    delete epochs_;
  }
  epochs_ = NULL;
}

bool NetworkUpdateParameters::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.NetworkUpdateParameters)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated .CoreML.Specification.LossLayer lossLayers = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
                input, add_losslayers()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.Optimizer optimizer = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(18u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_optimizer()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.Int64Parameter epochs = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(26u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_epochs()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.NetworkUpdateParameters)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.NetworkUpdateParameters)
  return false;
#undef DO_
}

void NetworkUpdateParameters::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.NetworkUpdateParameters)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated .CoreML.Specification.LossLayer lossLayers = 1;
  for (unsigned int i = 0, n = this->losslayers_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1, this->losslayers(i), output);
  }

  // .CoreML.Specification.Optimizer optimizer = 2;
  if (this->has_optimizer()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      2, *this->optimizer_, output);
  }

  // .CoreML.Specification.Int64Parameter epochs = 3;
  if (this->has_epochs()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      3, *this->epochs_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.NetworkUpdateParameters)
}

size_t NetworkUpdateParameters::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.NetworkUpdateParameters)
  size_t total_size = 0;

  // repeated .CoreML.Specification.LossLayer lossLayers = 1;
  {
    unsigned int count = this->losslayers_size();
    total_size += 1UL * count;
    for (unsigned int i = 0; i < count; i++) {
      total_size +=
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          this->losslayers(i));
    }
  }

  // .CoreML.Specification.Optimizer optimizer = 2;
  if (this->has_optimizer()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->optimizer_);
  }

  // .CoreML.Specification.Int64Parameter epochs = 3;
  if (this->has_epochs()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->epochs_);
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void NetworkUpdateParameters::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const NetworkUpdateParameters*>(&from));
}

void NetworkUpdateParameters::MergeFrom(const NetworkUpdateParameters& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.NetworkUpdateParameters)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  losslayers_.MergeFrom(from.losslayers_);
  if (from.has_optimizer()) {
    mutable_optimizer()->::CoreML::Specification::Optimizer::MergeFrom(from.optimizer());
  }
  if (from.has_epochs()) {
    mutable_epochs()->::CoreML::Specification::Int64Parameter::MergeFrom(from.epochs());
  }
}

void NetworkUpdateParameters::CopyFrom(const NetworkUpdateParameters& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.NetworkUpdateParameters)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool NetworkUpdateParameters::IsInitialized() const {
  return true;
}

void NetworkUpdateParameters::Swap(NetworkUpdateParameters* other) {
  if (other == this) return;
  InternalSwap(other);
}
void NetworkUpdateParameters::InternalSwap(NetworkUpdateParameters* other) {
  losslayers_.InternalSwap(&other->losslayers_);
  std::swap(optimizer_, other->optimizer_);
  std::swap(epochs_, other->epochs_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string NetworkUpdateParameters::GetTypeName() const {
  return "CoreML.Specification.NetworkUpdateParameters";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// NetworkUpdateParameters

// repeated .CoreML.Specification.LossLayer lossLayers = 1;
int NetworkUpdateParameters::losslayers_size() const {
  return losslayers_.size();
}
void NetworkUpdateParameters::clear_losslayers() {
  losslayers_.Clear();
}
const ::CoreML::Specification::LossLayer& NetworkUpdateParameters::losslayers(int index) const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NetworkUpdateParameters.lossLayers)
  return losslayers_.Get(index);
}
::CoreML::Specification::LossLayer* NetworkUpdateParameters::mutable_losslayers(int index) {
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NetworkUpdateParameters.lossLayers)
  return losslayers_.Mutable(index);
}
::CoreML::Specification::LossLayer* NetworkUpdateParameters::add_losslayers() {
  // @@protoc_insertion_point(field_add:CoreML.Specification.NetworkUpdateParameters.lossLayers)
  return losslayers_.Add();
}
::google::protobuf::RepeatedPtrField< ::CoreML::Specification::LossLayer >*
NetworkUpdateParameters::mutable_losslayers() {
  // @@protoc_insertion_point(field_mutable_list:CoreML.Specification.NetworkUpdateParameters.lossLayers)
  return &losslayers_;
}
const ::google::protobuf::RepeatedPtrField< ::CoreML::Specification::LossLayer >&
NetworkUpdateParameters::losslayers() const {
  // @@protoc_insertion_point(field_list:CoreML.Specification.NetworkUpdateParameters.lossLayers)
  return losslayers_;
}

// .CoreML.Specification.Optimizer optimizer = 2;
bool NetworkUpdateParameters::has_optimizer() const {
  return this != internal_default_instance() && optimizer_ != NULL;
}
void NetworkUpdateParameters::clear_optimizer() {
  if (GetArenaNoVirtual() == NULL && optimizer_ != NULL) delete optimizer_;
  optimizer_ = NULL;
}
const ::CoreML::Specification::Optimizer& NetworkUpdateParameters::optimizer() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NetworkUpdateParameters.optimizer)
  return optimizer_ != NULL ? *optimizer_
                         : *::CoreML::Specification::Optimizer::internal_default_instance();
}
::CoreML::Specification::Optimizer* NetworkUpdateParameters::mutable_optimizer() {
  
  if (optimizer_ == NULL) {
    optimizer_ = new ::CoreML::Specification::Optimizer;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NetworkUpdateParameters.optimizer)
  return optimizer_;
}
::CoreML::Specification::Optimizer* NetworkUpdateParameters::release_optimizer() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NetworkUpdateParameters.optimizer)
  
  ::CoreML::Specification::Optimizer* temp = optimizer_;
  optimizer_ = NULL;
  return temp;
}
void NetworkUpdateParameters::set_allocated_optimizer(::CoreML::Specification::Optimizer* optimizer) {
  delete optimizer_;
  optimizer_ = optimizer;
  if (optimizer) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NetworkUpdateParameters.optimizer)
}

// .CoreML.Specification.Int64Parameter epochs = 3;
bool NetworkUpdateParameters::has_epochs() const {
  return this != internal_default_instance() && epochs_ != NULL;
}
void NetworkUpdateParameters::clear_epochs() {
  if (GetArenaNoVirtual() == NULL && epochs_ != NULL) delete epochs_;
  epochs_ = NULL;
}
const ::CoreML::Specification::Int64Parameter& NetworkUpdateParameters::epochs() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.NetworkUpdateParameters.epochs)
  return epochs_ != NULL ? *epochs_
                         : *::CoreML::Specification::Int64Parameter::internal_default_instance();
}
::CoreML::Specification::Int64Parameter* NetworkUpdateParameters::mutable_epochs() {
  
  if (epochs_ == NULL) {
    epochs_ = new ::CoreML::Specification::Int64Parameter;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.NetworkUpdateParameters.epochs)
  return epochs_;
}
::CoreML::Specification::Int64Parameter* NetworkUpdateParameters::release_epochs() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.NetworkUpdateParameters.epochs)
  
  ::CoreML::Specification::Int64Parameter* temp = epochs_;
  epochs_ = NULL;
  return temp;
}
void NetworkUpdateParameters::set_allocated_epochs(::CoreML::Specification::Int64Parameter* epochs) {
  delete epochs_;
  epochs_ = epochs;
  if (epochs) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.NetworkUpdateParameters.epochs)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int LossLayer::kNameFieldNumber;
const int LossLayer::kCrossEntropyLossLayerFieldNumber;
const int LossLayer::kMseLossLayerFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

LossLayer::LossLayer()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.LossLayer)
}
LossLayer::LossLayer(const LossLayer& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  name_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (from.name().size() > 0) {
    name_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.name_);
  }
  clear_has_LossLayerType();
  switch (from.LossLayerType_case()) {
    case kCrossEntropyLossLayer: {
      mutable_crossentropylosslayer()->::CoreML::Specification::CrossEntropyLossLayer::MergeFrom(from.crossentropylosslayer());
      break;
    }
    case kMseLossLayer: {
      mutable_mselosslayer()->::CoreML::Specification::MSELossLayer::MergeFrom(from.mselosslayer());
      break;
    }
    case LOSSLAYERTYPE_NOT_SET: {
      break;
    }
  }
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.LossLayer)
}

void LossLayer::SharedCtor() {
  name_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  clear_has_LossLayerType();
  _cached_size_ = 0;
}

LossLayer::~LossLayer() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.LossLayer)
  SharedDtor();
}

void LossLayer::SharedDtor() {
  name_.DestroyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (has_LossLayerType()) {
    clear_LossLayerType();
  }
}

void LossLayer::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const LossLayer& LossLayer::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

LossLayer* LossLayer::New(::google::protobuf::Arena* arena) const {
  LossLayer* n = new LossLayer;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void LossLayer::clear_LossLayerType() {
// @@protoc_insertion_point(one_of_clear_start:CoreML.Specification.LossLayer)
  switch (LossLayerType_case()) {
    case kCrossEntropyLossLayer: {
      delete LossLayerType_.crossentropylosslayer_;
      break;
    }
    case kMseLossLayer: {
      delete LossLayerType_.mselosslayer_;
      break;
    }
    case LOSSLAYERTYPE_NOT_SET: {
      break;
    }
  }
  _oneof_case_[0] = LOSSLAYERTYPE_NOT_SET;
}


void LossLayer::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.LossLayer)
  name_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  clear_LossLayerType();
}

bool LossLayer::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.LossLayer)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // string name = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadString(
                input, this->mutable_name()));
          DO_(::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
            this->name().data(), this->name().length(),
            ::google::protobuf::internal::WireFormatLite::PARSE,
            "CoreML.Specification.LossLayer.name"));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.CrossEntropyLossLayer crossEntropyLossLayer = 10;
      case 10: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(82u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_crossentropylosslayer()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.MSELossLayer mseLossLayer = 11;
      case 11: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(90u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_mselosslayer()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.LossLayer)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.LossLayer)
  return false;
#undef DO_
}

void LossLayer::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.LossLayer)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // string name = 1;
  if (this->name().size() > 0) {
    ::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
      this->name().data(), this->name().length(),
      ::google::protobuf::internal::WireFormatLite::SERIALIZE,
      "CoreML.Specification.LossLayer.name");
    ::google::protobuf::internal::WireFormatLite::WriteStringMaybeAliased(
      1, this->name(), output);
  }

  // .CoreML.Specification.CrossEntropyLossLayer crossEntropyLossLayer = 10;
  if (has_crossentropylosslayer()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      10, *LossLayerType_.crossentropylosslayer_, output);
  }

  // .CoreML.Specification.MSELossLayer mseLossLayer = 11;
  if (has_mselosslayer()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      11, *LossLayerType_.mselosslayer_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.LossLayer)
}

size_t LossLayer::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.LossLayer)
  size_t total_size = 0;

  // string name = 1;
  if (this->name().size() > 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::StringSize(
        this->name());
  }

  switch (LossLayerType_case()) {
    // .CoreML.Specification.CrossEntropyLossLayer crossEntropyLossLayer = 10;
    case kCrossEntropyLossLayer: {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *LossLayerType_.crossentropylosslayer_);
      break;
    }
    // .CoreML.Specification.MSELossLayer mseLossLayer = 11;
    case kMseLossLayer: {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *LossLayerType_.mselosslayer_);
      break;
    }
    case LOSSLAYERTYPE_NOT_SET: {
      break;
    }
  }
  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void LossLayer::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const LossLayer*>(&from));
}

void LossLayer::MergeFrom(const LossLayer& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.LossLayer)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.name().size() > 0) {

    name_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.name_);
  }
  switch (from.LossLayerType_case()) {
    case kCrossEntropyLossLayer: {
      mutable_crossentropylosslayer()->::CoreML::Specification::CrossEntropyLossLayer::MergeFrom(from.crossentropylosslayer());
      break;
    }
    case kMseLossLayer: {
      mutable_mselosslayer()->::CoreML::Specification::MSELossLayer::MergeFrom(from.mselosslayer());
      break;
    }
    case LOSSLAYERTYPE_NOT_SET: {
      break;
    }
  }
}

void LossLayer::CopyFrom(const LossLayer& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.LossLayer)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool LossLayer::IsInitialized() const {
  return true;
}

void LossLayer::Swap(LossLayer* other) {
  if (other == this) return;
  InternalSwap(other);
}
void LossLayer::InternalSwap(LossLayer* other) {
  name_.Swap(&other->name_);
  std::swap(LossLayerType_, other->LossLayerType_);
  std::swap(_oneof_case_[0], other->_oneof_case_[0]);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string LossLayer::GetTypeName() const {
  return "CoreML.Specification.LossLayer";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// LossLayer

// string name = 1;
void LossLayer::clear_name() {
  name_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
const ::std::string& LossLayer::name() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LossLayer.name)
  return name_.GetNoArena();
}
void LossLayer::set_name(const ::std::string& value) {
  
  name_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.LossLayer.name)
}
#if LANG_CXX11
void LossLayer::set_name(::std::string&& value) {
  
  name_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:CoreML.Specification.LossLayer.name)
}
#endif
void LossLayer::set_name(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  
  name_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:CoreML.Specification.LossLayer.name)
}
void LossLayer::set_name(const char* value, size_t size) {
  
  name_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:CoreML.Specification.LossLayer.name)
}
::std::string* LossLayer::mutable_name() {
  
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LossLayer.name)
  return name_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
::std::string* LossLayer::release_name() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LossLayer.name)
  
  return name_.ReleaseNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
void LossLayer::set_allocated_name(::std::string* name) {
  if (name != NULL) {
    
  } else {
    
  }
  name_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), name);
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LossLayer.name)
}

// .CoreML.Specification.CrossEntropyLossLayer crossEntropyLossLayer = 10;
bool LossLayer::has_crossentropylosslayer() const {
  return LossLayerType_case() == kCrossEntropyLossLayer;
}
void LossLayer::set_has_crossentropylosslayer() {
  _oneof_case_[0] = kCrossEntropyLossLayer;
}
void LossLayer::clear_crossentropylosslayer() {
  if (has_crossentropylosslayer()) {
    delete LossLayerType_.crossentropylosslayer_;
    clear_has_LossLayerType();
  }
}
 const ::CoreML::Specification::CrossEntropyLossLayer& LossLayer::crossentropylosslayer() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LossLayer.crossEntropyLossLayer)
  return has_crossentropylosslayer()
      ? *LossLayerType_.crossentropylosslayer_
      : ::CoreML::Specification::CrossEntropyLossLayer::default_instance();
}
::CoreML::Specification::CrossEntropyLossLayer* LossLayer::mutable_crossentropylosslayer() {
  if (!has_crossentropylosslayer()) {
    clear_LossLayerType();
    set_has_crossentropylosslayer();
    LossLayerType_.crossentropylosslayer_ = new ::CoreML::Specification::CrossEntropyLossLayer;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LossLayer.crossEntropyLossLayer)
  return LossLayerType_.crossentropylosslayer_;
}
::CoreML::Specification::CrossEntropyLossLayer* LossLayer::release_crossentropylosslayer() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LossLayer.crossEntropyLossLayer)
  if (has_crossentropylosslayer()) {
    clear_has_LossLayerType();
    ::CoreML::Specification::CrossEntropyLossLayer* temp = LossLayerType_.crossentropylosslayer_;
    LossLayerType_.crossentropylosslayer_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void LossLayer::set_allocated_crossentropylosslayer(::CoreML::Specification::CrossEntropyLossLayer* crossentropylosslayer) {
  clear_LossLayerType();
  if (crossentropylosslayer) {
    set_has_crossentropylosslayer();
    LossLayerType_.crossentropylosslayer_ = crossentropylosslayer;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LossLayer.crossEntropyLossLayer)
}

// .CoreML.Specification.MSELossLayer mseLossLayer = 11;
bool LossLayer::has_mselosslayer() const {
  return LossLayerType_case() == kMseLossLayer;
}
void LossLayer::set_has_mselosslayer() {
  _oneof_case_[0] = kMseLossLayer;
}
void LossLayer::clear_mselosslayer() {
  if (has_mselosslayer()) {
    delete LossLayerType_.mselosslayer_;
    clear_has_LossLayerType();
  }
}
 const ::CoreML::Specification::MSELossLayer& LossLayer::mselosslayer() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.LossLayer.mseLossLayer)
  return has_mselosslayer()
      ? *LossLayerType_.mselosslayer_
      : ::CoreML::Specification::MSELossLayer::default_instance();
}
::CoreML::Specification::MSELossLayer* LossLayer::mutable_mselosslayer() {
  if (!has_mselosslayer()) {
    clear_LossLayerType();
    set_has_mselosslayer();
    LossLayerType_.mselosslayer_ = new ::CoreML::Specification::MSELossLayer;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.LossLayer.mseLossLayer)
  return LossLayerType_.mselosslayer_;
}
::CoreML::Specification::MSELossLayer* LossLayer::release_mselosslayer() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.LossLayer.mseLossLayer)
  if (has_mselosslayer()) {
    clear_has_LossLayerType();
    ::CoreML::Specification::MSELossLayer* temp = LossLayerType_.mselosslayer_;
    LossLayerType_.mselosslayer_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void LossLayer::set_allocated_mselosslayer(::CoreML::Specification::MSELossLayer* mselosslayer) {
  clear_LossLayerType();
  if (mselosslayer) {
    set_has_mselosslayer();
    LossLayerType_.mselosslayer_ = mselosslayer;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.LossLayer.mseLossLayer)
}

bool LossLayer::has_LossLayerType() const {
  return LossLayerType_case() != LOSSLAYERTYPE_NOT_SET;
}
void LossLayer::clear_has_LossLayerType() {
  _oneof_case_[0] = LOSSLAYERTYPE_NOT_SET;
}
LossLayer::LossLayerTypeCase LossLayer::LossLayerType_case() const {
  return LossLayer::LossLayerTypeCase(_oneof_case_[0]);
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int CrossEntropyLossLayer::kInputFieldNumber;
const int CrossEntropyLossLayer::kTargetFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

CrossEntropyLossLayer::CrossEntropyLossLayer()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.CrossEntropyLossLayer)
}
CrossEntropyLossLayer::CrossEntropyLossLayer(const CrossEntropyLossLayer& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  input_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (from.input().size() > 0) {
    input_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.input_);
  }
  target_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (from.target().size() > 0) {
    target_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.target_);
  }
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.CrossEntropyLossLayer)
}

void CrossEntropyLossLayer::SharedCtor() {
  input_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  target_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  _cached_size_ = 0;
}

CrossEntropyLossLayer::~CrossEntropyLossLayer() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.CrossEntropyLossLayer)
  SharedDtor();
}

void CrossEntropyLossLayer::SharedDtor() {
  input_.DestroyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  target_.DestroyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}

void CrossEntropyLossLayer::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const CrossEntropyLossLayer& CrossEntropyLossLayer::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

CrossEntropyLossLayer* CrossEntropyLossLayer::New(::google::protobuf::Arena* arena) const {
  CrossEntropyLossLayer* n = new CrossEntropyLossLayer;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void CrossEntropyLossLayer::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.CrossEntropyLossLayer)
  input_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  target_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}

bool CrossEntropyLossLayer::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.CrossEntropyLossLayer)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // string input = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadString(
                input, this->mutable_input()));
          DO_(::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
            this->input().data(), this->input().length(),
            ::google::protobuf::internal::WireFormatLite::PARSE,
            "CoreML.Specification.CrossEntropyLossLayer.input"));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // string target = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(18u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadString(
                input, this->mutable_target()));
          DO_(::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
            this->target().data(), this->target().length(),
            ::google::protobuf::internal::WireFormatLite::PARSE,
            "CoreML.Specification.CrossEntropyLossLayer.target"));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.CrossEntropyLossLayer)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.CrossEntropyLossLayer)
  return false;
#undef DO_
}

void CrossEntropyLossLayer::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.CrossEntropyLossLayer)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // string input = 1;
  if (this->input().size() > 0) {
    ::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
      this->input().data(), this->input().length(),
      ::google::protobuf::internal::WireFormatLite::SERIALIZE,
      "CoreML.Specification.CrossEntropyLossLayer.input");
    ::google::protobuf::internal::WireFormatLite::WriteStringMaybeAliased(
      1, this->input(), output);
  }

  // string target = 2;
  if (this->target().size() > 0) {
    ::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
      this->target().data(), this->target().length(),
      ::google::protobuf::internal::WireFormatLite::SERIALIZE,
      "CoreML.Specification.CrossEntropyLossLayer.target");
    ::google::protobuf::internal::WireFormatLite::WriteStringMaybeAliased(
      2, this->target(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.CrossEntropyLossLayer)
}

size_t CrossEntropyLossLayer::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.CrossEntropyLossLayer)
  size_t total_size = 0;

  // string input = 1;
  if (this->input().size() > 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::StringSize(
        this->input());
  }

  // string target = 2;
  if (this->target().size() > 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::StringSize(
        this->target());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void CrossEntropyLossLayer::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const CrossEntropyLossLayer*>(&from));
}

void CrossEntropyLossLayer::MergeFrom(const CrossEntropyLossLayer& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.CrossEntropyLossLayer)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.input().size() > 0) {

    input_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.input_);
  }
  if (from.target().size() > 0) {

    target_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.target_);
  }
}

void CrossEntropyLossLayer::CopyFrom(const CrossEntropyLossLayer& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.CrossEntropyLossLayer)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool CrossEntropyLossLayer::IsInitialized() const {
  return true;
}

void CrossEntropyLossLayer::Swap(CrossEntropyLossLayer* other) {
  if (other == this) return;
  InternalSwap(other);
}
void CrossEntropyLossLayer::InternalSwap(CrossEntropyLossLayer* other) {
  input_.Swap(&other->input_);
  target_.Swap(&other->target_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string CrossEntropyLossLayer::GetTypeName() const {
  return "CoreML.Specification.CrossEntropyLossLayer";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// CrossEntropyLossLayer

// string input = 1;
void CrossEntropyLossLayer::clear_input() {
  input_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
const ::std::string& CrossEntropyLossLayer::input() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CrossEntropyLossLayer.input)
  return input_.GetNoArena();
}
void CrossEntropyLossLayer::set_input(const ::std::string& value) {
  
  input_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.CrossEntropyLossLayer.input)
}
#if LANG_CXX11
void CrossEntropyLossLayer::set_input(::std::string&& value) {
  
  input_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:CoreML.Specification.CrossEntropyLossLayer.input)
}
#endif
void CrossEntropyLossLayer::set_input(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  
  input_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:CoreML.Specification.CrossEntropyLossLayer.input)
}
void CrossEntropyLossLayer::set_input(const char* value, size_t size) {
  
  input_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:CoreML.Specification.CrossEntropyLossLayer.input)
}
::std::string* CrossEntropyLossLayer::mutable_input() {
  
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.CrossEntropyLossLayer.input)
  return input_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
::std::string* CrossEntropyLossLayer::release_input() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.CrossEntropyLossLayer.input)
  
  return input_.ReleaseNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
void CrossEntropyLossLayer::set_allocated_input(::std::string* input) {
  if (input != NULL) {
    
  } else {
    
  }
  input_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), input);
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.CrossEntropyLossLayer.input)
}

// string target = 2;
void CrossEntropyLossLayer::clear_target() {
  target_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
const ::std::string& CrossEntropyLossLayer::target() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.CrossEntropyLossLayer.target)
  return target_.GetNoArena();
}
void CrossEntropyLossLayer::set_target(const ::std::string& value) {
  
  target_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.CrossEntropyLossLayer.target)
}
#if LANG_CXX11
void CrossEntropyLossLayer::set_target(::std::string&& value) {
  
  target_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:CoreML.Specification.CrossEntropyLossLayer.target)
}
#endif
void CrossEntropyLossLayer::set_target(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  
  target_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:CoreML.Specification.CrossEntropyLossLayer.target)
}
void CrossEntropyLossLayer::set_target(const char* value, size_t size) {
  
  target_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:CoreML.Specification.CrossEntropyLossLayer.target)
}
::std::string* CrossEntropyLossLayer::mutable_target() {
  
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.CrossEntropyLossLayer.target)
  return target_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
::std::string* CrossEntropyLossLayer::release_target() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.CrossEntropyLossLayer.target)
  
  return target_.ReleaseNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
void CrossEntropyLossLayer::set_allocated_target(::std::string* target) {
  if (target != NULL) {
    
  } else {
    
  }
  target_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), target);
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.CrossEntropyLossLayer.target)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int MSELossLayer::kInputFieldNumber;
const int MSELossLayer::kTargetFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

MSELossLayer::MSELossLayer()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.MSELossLayer)
}
MSELossLayer::MSELossLayer(const MSELossLayer& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  input_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (from.input().size() > 0) {
    input_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.input_);
  }
  target_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (from.target().size() > 0) {
    target_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.target_);
  }
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.MSELossLayer)
}

void MSELossLayer::SharedCtor() {
  input_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  target_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  _cached_size_ = 0;
}

MSELossLayer::~MSELossLayer() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.MSELossLayer)
  SharedDtor();
}

void MSELossLayer::SharedDtor() {
  input_.DestroyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  target_.DestroyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}

void MSELossLayer::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const MSELossLayer& MSELossLayer::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

MSELossLayer* MSELossLayer::New(::google::protobuf::Arena* arena) const {
  MSELossLayer* n = new MSELossLayer;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void MSELossLayer::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.MSELossLayer)
  input_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  target_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}

bool MSELossLayer::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.MSELossLayer)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // string input = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadString(
                input, this->mutable_input()));
          DO_(::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
            this->input().data(), this->input().length(),
            ::google::protobuf::internal::WireFormatLite::PARSE,
            "CoreML.Specification.MSELossLayer.input"));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // string target = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(18u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadString(
                input, this->mutable_target()));
          DO_(::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
            this->target().data(), this->target().length(),
            ::google::protobuf::internal::WireFormatLite::PARSE,
            "CoreML.Specification.MSELossLayer.target"));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.MSELossLayer)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.MSELossLayer)
  return false;
#undef DO_
}

void MSELossLayer::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.MSELossLayer)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // string input = 1;
  if (this->input().size() > 0) {
    ::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
      this->input().data(), this->input().length(),
      ::google::protobuf::internal::WireFormatLite::SERIALIZE,
      "CoreML.Specification.MSELossLayer.input");
    ::google::protobuf::internal::WireFormatLite::WriteStringMaybeAliased(
      1, this->input(), output);
  }

  // string target = 2;
  if (this->target().size() > 0) {
    ::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
      this->target().data(), this->target().length(),
      ::google::protobuf::internal::WireFormatLite::SERIALIZE,
      "CoreML.Specification.MSELossLayer.target");
    ::google::protobuf::internal::WireFormatLite::WriteStringMaybeAliased(
      2, this->target(), output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.MSELossLayer)
}

size_t MSELossLayer::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.MSELossLayer)
  size_t total_size = 0;

  // string input = 1;
  if (this->input().size() > 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::StringSize(
        this->input());
  }

  // string target = 2;
  if (this->target().size() > 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::StringSize(
        this->target());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void MSELossLayer::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const MSELossLayer*>(&from));
}

void MSELossLayer::MergeFrom(const MSELossLayer& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.MSELossLayer)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.input().size() > 0) {

    input_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.input_);
  }
  if (from.target().size() > 0) {

    target_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.target_);
  }
}

void MSELossLayer::CopyFrom(const MSELossLayer& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.MSELossLayer)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool MSELossLayer::IsInitialized() const {
  return true;
}

void MSELossLayer::Swap(MSELossLayer* other) {
  if (other == this) return;
  InternalSwap(other);
}
void MSELossLayer::InternalSwap(MSELossLayer* other) {
  input_.Swap(&other->input_);
  target_.Swap(&other->target_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string MSELossLayer::GetTypeName() const {
  return "CoreML.Specification.MSELossLayer";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// MSELossLayer

// string input = 1;
void MSELossLayer::clear_input() {
  input_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
const ::std::string& MSELossLayer::input() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.MSELossLayer.input)
  return input_.GetNoArena();
}
void MSELossLayer::set_input(const ::std::string& value) {
  
  input_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.MSELossLayer.input)
}
#if LANG_CXX11
void MSELossLayer::set_input(::std::string&& value) {
  
  input_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:CoreML.Specification.MSELossLayer.input)
}
#endif
void MSELossLayer::set_input(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  
  input_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:CoreML.Specification.MSELossLayer.input)
}
void MSELossLayer::set_input(const char* value, size_t size) {
  
  input_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:CoreML.Specification.MSELossLayer.input)
}
::std::string* MSELossLayer::mutable_input() {
  
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.MSELossLayer.input)
  return input_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
::std::string* MSELossLayer::release_input() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.MSELossLayer.input)
  
  return input_.ReleaseNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
void MSELossLayer::set_allocated_input(::std::string* input) {
  if (input != NULL) {
    
  } else {
    
  }
  input_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), input);
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.MSELossLayer.input)
}

// string target = 2;
void MSELossLayer::clear_target() {
  target_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
const ::std::string& MSELossLayer::target() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.MSELossLayer.target)
  return target_.GetNoArena();
}
void MSELossLayer::set_target(const ::std::string& value) {
  
  target_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:CoreML.Specification.MSELossLayer.target)
}
#if LANG_CXX11
void MSELossLayer::set_target(::std::string&& value) {
  
  target_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:CoreML.Specification.MSELossLayer.target)
}
#endif
void MSELossLayer::set_target(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  
  target_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:CoreML.Specification.MSELossLayer.target)
}
void MSELossLayer::set_target(const char* value, size_t size) {
  
  target_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:CoreML.Specification.MSELossLayer.target)
}
::std::string* MSELossLayer::mutable_target() {
  
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.MSELossLayer.target)
  return target_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
::std::string* MSELossLayer::release_target() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.MSELossLayer.target)
  
  return target_.ReleaseNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
void MSELossLayer::set_allocated_target(::std::string* target) {
  if (target != NULL) {
    
  } else {
    
  }
  target_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), target);
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.MSELossLayer.target)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int Optimizer::kSgdOptimizerFieldNumber;
const int Optimizer::kAdamOptimizerFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

Optimizer::Optimizer()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.Optimizer)
}
Optimizer::Optimizer(const Optimizer& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  clear_has_OptimizerType();
  switch (from.OptimizerType_case()) {
    case kSgdOptimizer: {
      mutable_sgdoptimizer()->::CoreML::Specification::SGDOptimizer::MergeFrom(from.sgdoptimizer());
      break;
    }
    case kAdamOptimizer: {
      mutable_adamoptimizer()->::CoreML::Specification::AdamOptimizer::MergeFrom(from.adamoptimizer());
      break;
    }
    case OPTIMIZERTYPE_NOT_SET: {
      break;
    }
  }
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.Optimizer)
}

void Optimizer::SharedCtor() {
  clear_has_OptimizerType();
  _cached_size_ = 0;
}

Optimizer::~Optimizer() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.Optimizer)
  SharedDtor();
}

void Optimizer::SharedDtor() {
  if (has_OptimizerType()) {
    clear_OptimizerType();
  }
}

void Optimizer::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const Optimizer& Optimizer::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

Optimizer* Optimizer::New(::google::protobuf::Arena* arena) const {
  Optimizer* n = new Optimizer;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void Optimizer::clear_OptimizerType() {
// @@protoc_insertion_point(one_of_clear_start:CoreML.Specification.Optimizer)
  switch (OptimizerType_case()) {
    case kSgdOptimizer: {
      delete OptimizerType_.sgdoptimizer_;
      break;
    }
    case kAdamOptimizer: {
      delete OptimizerType_.adamoptimizer_;
      break;
    }
    case OPTIMIZERTYPE_NOT_SET: {
      break;
    }
  }
  _oneof_case_[0] = OPTIMIZERTYPE_NOT_SET;
}


void Optimizer::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.Optimizer)
  clear_OptimizerType();
}

bool Optimizer::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.Optimizer)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // .CoreML.Specification.SGDOptimizer sgdOptimizer = 10;
      case 10: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(82u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_sgdoptimizer()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.AdamOptimizer adamOptimizer = 11;
      case 11: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(90u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_adamoptimizer()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.Optimizer)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.Optimizer)
  return false;
#undef DO_
}

void Optimizer::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.Optimizer)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // .CoreML.Specification.SGDOptimizer sgdOptimizer = 10;
  if (has_sgdoptimizer()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      10, *OptimizerType_.sgdoptimizer_, output);
  }

  // .CoreML.Specification.AdamOptimizer adamOptimizer = 11;
  if (has_adamoptimizer()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      11, *OptimizerType_.adamoptimizer_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.Optimizer)
}

size_t Optimizer::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.Optimizer)
  size_t total_size = 0;

  switch (OptimizerType_case()) {
    // .CoreML.Specification.SGDOptimizer sgdOptimizer = 10;
    case kSgdOptimizer: {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *OptimizerType_.sgdoptimizer_);
      break;
    }
    // .CoreML.Specification.AdamOptimizer adamOptimizer = 11;
    case kAdamOptimizer: {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *OptimizerType_.adamoptimizer_);
      break;
    }
    case OPTIMIZERTYPE_NOT_SET: {
      break;
    }
  }
  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void Optimizer::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const Optimizer*>(&from));
}

void Optimizer::MergeFrom(const Optimizer& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.Optimizer)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  switch (from.OptimizerType_case()) {
    case kSgdOptimizer: {
      mutable_sgdoptimizer()->::CoreML::Specification::SGDOptimizer::MergeFrom(from.sgdoptimizer());
      break;
    }
    case kAdamOptimizer: {
      mutable_adamoptimizer()->::CoreML::Specification::AdamOptimizer::MergeFrom(from.adamoptimizer());
      break;
    }
    case OPTIMIZERTYPE_NOT_SET: {
      break;
    }
  }
}

void Optimizer::CopyFrom(const Optimizer& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.Optimizer)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool Optimizer::IsInitialized() const {
  return true;
}

void Optimizer::Swap(Optimizer* other) {
  if (other == this) return;
  InternalSwap(other);
}
void Optimizer::InternalSwap(Optimizer* other) {
  std::swap(OptimizerType_, other->OptimizerType_);
  std::swap(_oneof_case_[0], other->_oneof_case_[0]);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string Optimizer::GetTypeName() const {
  return "CoreML.Specification.Optimizer";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// Optimizer

// .CoreML.Specification.SGDOptimizer sgdOptimizer = 10;
bool Optimizer::has_sgdoptimizer() const {
  return OptimizerType_case() == kSgdOptimizer;
}
void Optimizer::set_has_sgdoptimizer() {
  _oneof_case_[0] = kSgdOptimizer;
}
void Optimizer::clear_sgdoptimizer() {
  if (has_sgdoptimizer()) {
    delete OptimizerType_.sgdoptimizer_;
    clear_has_OptimizerType();
  }
}
 const ::CoreML::Specification::SGDOptimizer& Optimizer::sgdoptimizer() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.Optimizer.sgdOptimizer)
  return has_sgdoptimizer()
      ? *OptimizerType_.sgdoptimizer_
      : ::CoreML::Specification::SGDOptimizer::default_instance();
}
::CoreML::Specification::SGDOptimizer* Optimizer::mutable_sgdoptimizer() {
  if (!has_sgdoptimizer()) {
    clear_OptimizerType();
    set_has_sgdoptimizer();
    OptimizerType_.sgdoptimizer_ = new ::CoreML::Specification::SGDOptimizer;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.Optimizer.sgdOptimizer)
  return OptimizerType_.sgdoptimizer_;
}
::CoreML::Specification::SGDOptimizer* Optimizer::release_sgdoptimizer() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.Optimizer.sgdOptimizer)
  if (has_sgdoptimizer()) {
    clear_has_OptimizerType();
    ::CoreML::Specification::SGDOptimizer* temp = OptimizerType_.sgdoptimizer_;
    OptimizerType_.sgdoptimizer_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void Optimizer::set_allocated_sgdoptimizer(::CoreML::Specification::SGDOptimizer* sgdoptimizer) {
  clear_OptimizerType();
  if (sgdoptimizer) {
    set_has_sgdoptimizer();
    OptimizerType_.sgdoptimizer_ = sgdoptimizer;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.Optimizer.sgdOptimizer)
}

// .CoreML.Specification.AdamOptimizer adamOptimizer = 11;
bool Optimizer::has_adamoptimizer() const {
  return OptimizerType_case() == kAdamOptimizer;
}
void Optimizer::set_has_adamoptimizer() {
  _oneof_case_[0] = kAdamOptimizer;
}
void Optimizer::clear_adamoptimizer() {
  if (has_adamoptimizer()) {
    delete OptimizerType_.adamoptimizer_;
    clear_has_OptimizerType();
  }
}
 const ::CoreML::Specification::AdamOptimizer& Optimizer::adamoptimizer() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.Optimizer.adamOptimizer)
  return has_adamoptimizer()
      ? *OptimizerType_.adamoptimizer_
      : ::CoreML::Specification::AdamOptimizer::default_instance();
}
::CoreML::Specification::AdamOptimizer* Optimizer::mutable_adamoptimizer() {
  if (!has_adamoptimizer()) {
    clear_OptimizerType();
    set_has_adamoptimizer();
    OptimizerType_.adamoptimizer_ = new ::CoreML::Specification::AdamOptimizer;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.Optimizer.adamOptimizer)
  return OptimizerType_.adamoptimizer_;
}
::CoreML::Specification::AdamOptimizer* Optimizer::release_adamoptimizer() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.Optimizer.adamOptimizer)
  if (has_adamoptimizer()) {
    clear_has_OptimizerType();
    ::CoreML::Specification::AdamOptimizer* temp = OptimizerType_.adamoptimizer_;
    OptimizerType_.adamoptimizer_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
void Optimizer::set_allocated_adamoptimizer(::CoreML::Specification::AdamOptimizer* adamoptimizer) {
  clear_OptimizerType();
  if (adamoptimizer) {
    set_has_adamoptimizer();
    OptimizerType_.adamoptimizer_ = adamoptimizer;
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.Optimizer.adamOptimizer)
}

bool Optimizer::has_OptimizerType() const {
  return OptimizerType_case() != OPTIMIZERTYPE_NOT_SET;
}
void Optimizer::clear_has_OptimizerType() {
  _oneof_case_[0] = OPTIMIZERTYPE_NOT_SET;
}
Optimizer::OptimizerTypeCase Optimizer::OptimizerType_case() const {
  return Optimizer::OptimizerTypeCase(_oneof_case_[0]);
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int SGDOptimizer::kLearningRateFieldNumber;
const int SGDOptimizer::kMiniBatchSizeFieldNumber;
const int SGDOptimizer::kMomentumFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

SGDOptimizer::SGDOptimizer()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.SGDOptimizer)
}
SGDOptimizer::SGDOptimizer(const SGDOptimizer& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_learningrate()) {
    learningrate_ = new ::CoreML::Specification::DoubleParameter(*from.learningrate_);
  } else {
    learningrate_ = NULL;
  }
  if (from.has_minibatchsize()) {
    minibatchsize_ = new ::CoreML::Specification::Int64Parameter(*from.minibatchsize_);
  } else {
    minibatchsize_ = NULL;
  }
  if (from.has_momentum()) {
    momentum_ = new ::CoreML::Specification::DoubleParameter(*from.momentum_);
  } else {
    momentum_ = NULL;
  }
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.SGDOptimizer)
}

void SGDOptimizer::SharedCtor() {
  ::memset(&learningrate_, 0, reinterpret_cast<char*>(&momentum_) -
    reinterpret_cast<char*>(&learningrate_) + sizeof(momentum_));
  _cached_size_ = 0;
}

SGDOptimizer::~SGDOptimizer() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.SGDOptimizer)
  SharedDtor();
}

void SGDOptimizer::SharedDtor() {
  if (this != internal_default_instance()) {
    delete learningrate_;
  }
  if (this != internal_default_instance()) {
    delete minibatchsize_;
  }
  if (this != internal_default_instance()) {
    delete momentum_;
  }
}

void SGDOptimizer::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const SGDOptimizer& SGDOptimizer::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

SGDOptimizer* SGDOptimizer::New(::google::protobuf::Arena* arena) const {
  SGDOptimizer* n = new SGDOptimizer;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void SGDOptimizer::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.SGDOptimizer)
  if (GetArenaNoVirtual() == NULL && learningrate_ != NULL) {
    delete learningrate_;
  }
  learningrate_ = NULL;
  if (GetArenaNoVirtual() == NULL && minibatchsize_ != NULL) {
    delete minibatchsize_;
  }
  minibatchsize_ = NULL;
  if (GetArenaNoVirtual() == NULL && momentum_ != NULL) {
    delete momentum_;
  }
  momentum_ = NULL;
}

bool SGDOptimizer::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.SGDOptimizer)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // .CoreML.Specification.DoubleParameter learningRate = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_learningrate()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.Int64Parameter miniBatchSize = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(18u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_minibatchsize()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.DoubleParameter momentum = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(26u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_momentum()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.SGDOptimizer)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.SGDOptimizer)
  return false;
#undef DO_
}

void SGDOptimizer::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.SGDOptimizer)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // .CoreML.Specification.DoubleParameter learningRate = 1;
  if (this->has_learningrate()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1, *this->learningrate_, output);
  }

  // .CoreML.Specification.Int64Parameter miniBatchSize = 2;
  if (this->has_minibatchsize()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      2, *this->minibatchsize_, output);
  }

  // .CoreML.Specification.DoubleParameter momentum = 3;
  if (this->has_momentum()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      3, *this->momentum_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.SGDOptimizer)
}

size_t SGDOptimizer::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.SGDOptimizer)
  size_t total_size = 0;

  // .CoreML.Specification.DoubleParameter learningRate = 1;
  if (this->has_learningrate()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->learningrate_);
  }

  // .CoreML.Specification.Int64Parameter miniBatchSize = 2;
  if (this->has_minibatchsize()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->minibatchsize_);
  }

  // .CoreML.Specification.DoubleParameter momentum = 3;
  if (this->has_momentum()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->momentum_);
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void SGDOptimizer::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const SGDOptimizer*>(&from));
}

void SGDOptimizer::MergeFrom(const SGDOptimizer& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.SGDOptimizer)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.has_learningrate()) {
    mutable_learningrate()->::CoreML::Specification::DoubleParameter::MergeFrom(from.learningrate());
  }
  if (from.has_minibatchsize()) {
    mutable_minibatchsize()->::CoreML::Specification::Int64Parameter::MergeFrom(from.minibatchsize());
  }
  if (from.has_momentum()) {
    mutable_momentum()->::CoreML::Specification::DoubleParameter::MergeFrom(from.momentum());
  }
}

void SGDOptimizer::CopyFrom(const SGDOptimizer& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.SGDOptimizer)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool SGDOptimizer::IsInitialized() const {
  return true;
}

void SGDOptimizer::Swap(SGDOptimizer* other) {
  if (other == this) return;
  InternalSwap(other);
}
void SGDOptimizer::InternalSwap(SGDOptimizer* other) {
  std::swap(learningrate_, other->learningrate_);
  std::swap(minibatchsize_, other->minibatchsize_);
  std::swap(momentum_, other->momentum_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string SGDOptimizer::GetTypeName() const {
  return "CoreML.Specification.SGDOptimizer";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// SGDOptimizer

// .CoreML.Specification.DoubleParameter learningRate = 1;
bool SGDOptimizer::has_learningrate() const {
  return this != internal_default_instance() && learningrate_ != NULL;
}
void SGDOptimizer::clear_learningrate() {
  if (GetArenaNoVirtual() == NULL && learningrate_ != NULL) delete learningrate_;
  learningrate_ = NULL;
}
const ::CoreML::Specification::DoubleParameter& SGDOptimizer::learningrate() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SGDOptimizer.learningRate)
  return learningrate_ != NULL ? *learningrate_
                         : *::CoreML::Specification::DoubleParameter::internal_default_instance();
}
::CoreML::Specification::DoubleParameter* SGDOptimizer::mutable_learningrate() {
  
  if (learningrate_ == NULL) {
    learningrate_ = new ::CoreML::Specification::DoubleParameter;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.SGDOptimizer.learningRate)
  return learningrate_;
}
::CoreML::Specification::DoubleParameter* SGDOptimizer::release_learningrate() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.SGDOptimizer.learningRate)
  
  ::CoreML::Specification::DoubleParameter* temp = learningrate_;
  learningrate_ = NULL;
  return temp;
}
void SGDOptimizer::set_allocated_learningrate(::CoreML::Specification::DoubleParameter* learningrate) {
  delete learningrate_;
  learningrate_ = learningrate;
  if (learningrate) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.SGDOptimizer.learningRate)
}

// .CoreML.Specification.Int64Parameter miniBatchSize = 2;
bool SGDOptimizer::has_minibatchsize() const {
  return this != internal_default_instance() && minibatchsize_ != NULL;
}
void SGDOptimizer::clear_minibatchsize() {
  if (GetArenaNoVirtual() == NULL && minibatchsize_ != NULL) delete minibatchsize_;
  minibatchsize_ = NULL;
}
const ::CoreML::Specification::Int64Parameter& SGDOptimizer::minibatchsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SGDOptimizer.miniBatchSize)
  return minibatchsize_ != NULL ? *minibatchsize_
                         : *::CoreML::Specification::Int64Parameter::internal_default_instance();
}
::CoreML::Specification::Int64Parameter* SGDOptimizer::mutable_minibatchsize() {
  
  if (minibatchsize_ == NULL) {
    minibatchsize_ = new ::CoreML::Specification::Int64Parameter;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.SGDOptimizer.miniBatchSize)
  return minibatchsize_;
}
::CoreML::Specification::Int64Parameter* SGDOptimizer::release_minibatchsize() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.SGDOptimizer.miniBatchSize)
  
  ::CoreML::Specification::Int64Parameter* temp = minibatchsize_;
  minibatchsize_ = NULL;
  return temp;
}
void SGDOptimizer::set_allocated_minibatchsize(::CoreML::Specification::Int64Parameter* minibatchsize) {
  delete minibatchsize_;
  minibatchsize_ = minibatchsize;
  if (minibatchsize) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.SGDOptimizer.miniBatchSize)
}

// .CoreML.Specification.DoubleParameter momentum = 3;
bool SGDOptimizer::has_momentum() const {
  return this != internal_default_instance() && momentum_ != NULL;
}
void SGDOptimizer::clear_momentum() {
  if (GetArenaNoVirtual() == NULL && momentum_ != NULL) delete momentum_;
  momentum_ = NULL;
}
const ::CoreML::Specification::DoubleParameter& SGDOptimizer::momentum() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.SGDOptimizer.momentum)
  return momentum_ != NULL ? *momentum_
                         : *::CoreML::Specification::DoubleParameter::internal_default_instance();
}
::CoreML::Specification::DoubleParameter* SGDOptimizer::mutable_momentum() {
  
  if (momentum_ == NULL) {
    momentum_ = new ::CoreML::Specification::DoubleParameter;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.SGDOptimizer.momentum)
  return momentum_;
}
::CoreML::Specification::DoubleParameter* SGDOptimizer::release_momentum() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.SGDOptimizer.momentum)
  
  ::CoreML::Specification::DoubleParameter* temp = momentum_;
  momentum_ = NULL;
  return temp;
}
void SGDOptimizer::set_allocated_momentum(::CoreML::Specification::DoubleParameter* momentum) {
  delete momentum_;
  momentum_ = momentum;
  if (momentum) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.SGDOptimizer.momentum)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int AdamOptimizer::kLearningRateFieldNumber;
const int AdamOptimizer::kMiniBatchSizeFieldNumber;
const int AdamOptimizer::kBeta1FieldNumber;
const int AdamOptimizer::kBeta2FieldNumber;
const int AdamOptimizer::kEpsFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

AdamOptimizer::AdamOptimizer()
  : ::google::protobuf::MessageLite(), _internal_metadata_(NULL) {
  if (GOOGLE_PREDICT_TRUE(this != internal_default_instance())) {
    protobuf_NeuralNetwork_2eproto::InitDefaults();
  }
  SharedCtor();
  // @@protoc_insertion_point(constructor:CoreML.Specification.AdamOptimizer)
}
AdamOptimizer::AdamOptimizer(const AdamOptimizer& from)
  : ::google::protobuf::MessageLite(),
      _internal_metadata_(NULL),
      _cached_size_(0) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_learningrate()) {
    learningrate_ = new ::CoreML::Specification::DoubleParameter(*from.learningrate_);
  } else {
    learningrate_ = NULL;
  }
  if (from.has_minibatchsize()) {
    minibatchsize_ = new ::CoreML::Specification::Int64Parameter(*from.minibatchsize_);
  } else {
    minibatchsize_ = NULL;
  }
  if (from.has_beta1()) {
    beta1_ = new ::CoreML::Specification::DoubleParameter(*from.beta1_);
  } else {
    beta1_ = NULL;
  }
  if (from.has_beta2()) {
    beta2_ = new ::CoreML::Specification::DoubleParameter(*from.beta2_);
  } else {
    beta2_ = NULL;
  }
  if (from.has_eps()) {
    eps_ = new ::CoreML::Specification::DoubleParameter(*from.eps_);
  } else {
    eps_ = NULL;
  }
  // @@protoc_insertion_point(copy_constructor:CoreML.Specification.AdamOptimizer)
}

void AdamOptimizer::SharedCtor() {
  ::memset(&learningrate_, 0, reinterpret_cast<char*>(&eps_) -
    reinterpret_cast<char*>(&learningrate_) + sizeof(eps_));
  _cached_size_ = 0;
}

AdamOptimizer::~AdamOptimizer() {
  // @@protoc_insertion_point(destructor:CoreML.Specification.AdamOptimizer)
  SharedDtor();
}

void AdamOptimizer::SharedDtor() {
  if (this != internal_default_instance()) {
    delete learningrate_;
  }
  if (this != internal_default_instance()) {
    delete minibatchsize_;
  }
  if (this != internal_default_instance()) {
    delete beta1_;
  }
  if (this != internal_default_instance()) {
    delete beta2_;
  }
  if (this != internal_default_instance()) {
    delete eps_;
  }
}

void AdamOptimizer::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const AdamOptimizer& AdamOptimizer::default_instance() {
  protobuf_NeuralNetwork_2eproto::InitDefaults();
  return *internal_default_instance();
}

AdamOptimizer* AdamOptimizer::New(::google::protobuf::Arena* arena) const {
  AdamOptimizer* n = new AdamOptimizer;
  if (arena != NULL) {
    arena->Own(n);
  }
  return n;
}

void AdamOptimizer::Clear() {
// @@protoc_insertion_point(message_clear_start:CoreML.Specification.AdamOptimizer)
  if (GetArenaNoVirtual() == NULL && learningrate_ != NULL) {
    delete learningrate_;
  }
  learningrate_ = NULL;
  if (GetArenaNoVirtual() == NULL && minibatchsize_ != NULL) {
    delete minibatchsize_;
  }
  minibatchsize_ = NULL;
  if (GetArenaNoVirtual() == NULL && beta1_ != NULL) {
    delete beta1_;
  }
  beta1_ = NULL;
  if (GetArenaNoVirtual() == NULL && beta2_ != NULL) {
    delete beta2_;
  }
  beta2_ = NULL;
  if (GetArenaNoVirtual() == NULL && eps_ != NULL) {
    delete eps_;
  }
  eps_ = NULL;
}

bool AdamOptimizer::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:CoreML.Specification.AdamOptimizer)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // .CoreML.Specification.DoubleParameter learningRate = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_learningrate()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.Int64Parameter miniBatchSize = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(18u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_minibatchsize()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.DoubleParameter beta1 = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(26u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_beta1()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.DoubleParameter beta2 = 4;
      case 4: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(34u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_beta2()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .CoreML.Specification.DoubleParameter eps = 5;
      case 5: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(42u)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_eps()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:CoreML.Specification.AdamOptimizer)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:CoreML.Specification.AdamOptimizer)
  return false;
#undef DO_
}

void AdamOptimizer::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:CoreML.Specification.AdamOptimizer)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // .CoreML.Specification.DoubleParameter learningRate = 1;
  if (this->has_learningrate()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      1, *this->learningrate_, output);
  }

  // .CoreML.Specification.Int64Parameter miniBatchSize = 2;
  if (this->has_minibatchsize()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      2, *this->minibatchsize_, output);
  }

  // .CoreML.Specification.DoubleParameter beta1 = 3;
  if (this->has_beta1()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      3, *this->beta1_, output);
  }

  // .CoreML.Specification.DoubleParameter beta2 = 4;
  if (this->has_beta2()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      4, *this->beta2_, output);
  }

  // .CoreML.Specification.DoubleParameter eps = 5;
  if (this->has_eps()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessage(
      5, *this->eps_, output);
  }

  // @@protoc_insertion_point(serialize_end:CoreML.Specification.AdamOptimizer)
}

size_t AdamOptimizer::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:CoreML.Specification.AdamOptimizer)
  size_t total_size = 0;

  // .CoreML.Specification.DoubleParameter learningRate = 1;
  if (this->has_learningrate()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->learningrate_);
  }

  // .CoreML.Specification.Int64Parameter miniBatchSize = 2;
  if (this->has_minibatchsize()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->minibatchsize_);
  }

  // .CoreML.Specification.DoubleParameter beta1 = 3;
  if (this->has_beta1()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->beta1_);
  }

  // .CoreML.Specification.DoubleParameter beta2 = 4;
  if (this->has_beta2()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->beta2_);
  }

  // .CoreML.Specification.DoubleParameter eps = 5;
  if (this->has_eps()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->eps_);
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = cached_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void AdamOptimizer::CheckTypeAndMergeFrom(
    const ::google::protobuf::MessageLite& from) {
  MergeFrom(*::google::protobuf::down_cast<const AdamOptimizer*>(&from));
}

void AdamOptimizer::MergeFrom(const AdamOptimizer& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:CoreML.Specification.AdamOptimizer)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.has_learningrate()) {
    mutable_learningrate()->::CoreML::Specification::DoubleParameter::MergeFrom(from.learningrate());
  }
  if (from.has_minibatchsize()) {
    mutable_minibatchsize()->::CoreML::Specification::Int64Parameter::MergeFrom(from.minibatchsize());
  }
  if (from.has_beta1()) {
    mutable_beta1()->::CoreML::Specification::DoubleParameter::MergeFrom(from.beta1());
  }
  if (from.has_beta2()) {
    mutable_beta2()->::CoreML::Specification::DoubleParameter::MergeFrom(from.beta2());
  }
  if (from.has_eps()) {
    mutable_eps()->::CoreML::Specification::DoubleParameter::MergeFrom(from.eps());
  }
}

void AdamOptimizer::CopyFrom(const AdamOptimizer& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:CoreML.Specification.AdamOptimizer)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool AdamOptimizer::IsInitialized() const {
  return true;
}

void AdamOptimizer::Swap(AdamOptimizer* other) {
  if (other == this) return;
  InternalSwap(other);
}
void AdamOptimizer::InternalSwap(AdamOptimizer* other) {
  std::swap(learningrate_, other->learningrate_);
  std::swap(minibatchsize_, other->minibatchsize_);
  std::swap(beta1_, other->beta1_);
  std::swap(beta2_, other->beta2_);
  std::swap(eps_, other->eps_);
  std::swap(_cached_size_, other->_cached_size_);
}

::std::string AdamOptimizer::GetTypeName() const {
  return "CoreML.Specification.AdamOptimizer";
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// AdamOptimizer

// .CoreML.Specification.DoubleParameter learningRate = 1;
bool AdamOptimizer::has_learningrate() const {
  return this != internal_default_instance() && learningrate_ != NULL;
}
void AdamOptimizer::clear_learningrate() {
  if (GetArenaNoVirtual() == NULL && learningrate_ != NULL) delete learningrate_;
  learningrate_ = NULL;
}
const ::CoreML::Specification::DoubleParameter& AdamOptimizer::learningrate() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.AdamOptimizer.learningRate)
  return learningrate_ != NULL ? *learningrate_
                         : *::CoreML::Specification::DoubleParameter::internal_default_instance();
}
::CoreML::Specification::DoubleParameter* AdamOptimizer::mutable_learningrate() {
  
  if (learningrate_ == NULL) {
    learningrate_ = new ::CoreML::Specification::DoubleParameter;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.AdamOptimizer.learningRate)
  return learningrate_;
}
::CoreML::Specification::DoubleParameter* AdamOptimizer::release_learningrate() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.AdamOptimizer.learningRate)
  
  ::CoreML::Specification::DoubleParameter* temp = learningrate_;
  learningrate_ = NULL;
  return temp;
}
void AdamOptimizer::set_allocated_learningrate(::CoreML::Specification::DoubleParameter* learningrate) {
  delete learningrate_;
  learningrate_ = learningrate;
  if (learningrate) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.AdamOptimizer.learningRate)
}

// .CoreML.Specification.Int64Parameter miniBatchSize = 2;
bool AdamOptimizer::has_minibatchsize() const {
  return this != internal_default_instance() && minibatchsize_ != NULL;
}
void AdamOptimizer::clear_minibatchsize() {
  if (GetArenaNoVirtual() == NULL && minibatchsize_ != NULL) delete minibatchsize_;
  minibatchsize_ = NULL;
}
const ::CoreML::Specification::Int64Parameter& AdamOptimizer::minibatchsize() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.AdamOptimizer.miniBatchSize)
  return minibatchsize_ != NULL ? *minibatchsize_
                         : *::CoreML::Specification::Int64Parameter::internal_default_instance();
}
::CoreML::Specification::Int64Parameter* AdamOptimizer::mutable_minibatchsize() {
  
  if (minibatchsize_ == NULL) {
    minibatchsize_ = new ::CoreML::Specification::Int64Parameter;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.AdamOptimizer.miniBatchSize)
  return minibatchsize_;
}
::CoreML::Specification::Int64Parameter* AdamOptimizer::release_minibatchsize() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.AdamOptimizer.miniBatchSize)
  
  ::CoreML::Specification::Int64Parameter* temp = minibatchsize_;
  minibatchsize_ = NULL;
  return temp;
}
void AdamOptimizer::set_allocated_minibatchsize(::CoreML::Specification::Int64Parameter* minibatchsize) {
  delete minibatchsize_;
  minibatchsize_ = minibatchsize;
  if (minibatchsize) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.AdamOptimizer.miniBatchSize)
}

// .CoreML.Specification.DoubleParameter beta1 = 3;
bool AdamOptimizer::has_beta1() const {
  return this != internal_default_instance() && beta1_ != NULL;
}
void AdamOptimizer::clear_beta1() {
  if (GetArenaNoVirtual() == NULL && beta1_ != NULL) delete beta1_;
  beta1_ = NULL;
}
const ::CoreML::Specification::DoubleParameter& AdamOptimizer::beta1() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.AdamOptimizer.beta1)
  return beta1_ != NULL ? *beta1_
                         : *::CoreML::Specification::DoubleParameter::internal_default_instance();
}
::CoreML::Specification::DoubleParameter* AdamOptimizer::mutable_beta1() {
  
  if (beta1_ == NULL) {
    beta1_ = new ::CoreML::Specification::DoubleParameter;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.AdamOptimizer.beta1)
  return beta1_;
}
::CoreML::Specification::DoubleParameter* AdamOptimizer::release_beta1() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.AdamOptimizer.beta1)
  
  ::CoreML::Specification::DoubleParameter* temp = beta1_;
  beta1_ = NULL;
  return temp;
}
void AdamOptimizer::set_allocated_beta1(::CoreML::Specification::DoubleParameter* beta1) {
  delete beta1_;
  beta1_ = beta1;
  if (beta1) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.AdamOptimizer.beta1)
}

// .CoreML.Specification.DoubleParameter beta2 = 4;
bool AdamOptimizer::has_beta2() const {
  return this != internal_default_instance() && beta2_ != NULL;
}
void AdamOptimizer::clear_beta2() {
  if (GetArenaNoVirtual() == NULL && beta2_ != NULL) delete beta2_;
  beta2_ = NULL;
}
const ::CoreML::Specification::DoubleParameter& AdamOptimizer::beta2() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.AdamOptimizer.beta2)
  return beta2_ != NULL ? *beta2_
                         : *::CoreML::Specification::DoubleParameter::internal_default_instance();
}
::CoreML::Specification::DoubleParameter* AdamOptimizer::mutable_beta2() {
  
  if (beta2_ == NULL) {
    beta2_ = new ::CoreML::Specification::DoubleParameter;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.AdamOptimizer.beta2)
  return beta2_;
}
::CoreML::Specification::DoubleParameter* AdamOptimizer::release_beta2() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.AdamOptimizer.beta2)
  
  ::CoreML::Specification::DoubleParameter* temp = beta2_;
  beta2_ = NULL;
  return temp;
}
void AdamOptimizer::set_allocated_beta2(::CoreML::Specification::DoubleParameter* beta2) {
  delete beta2_;
  beta2_ = beta2;
  if (beta2) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.AdamOptimizer.beta2)
}

// .CoreML.Specification.DoubleParameter eps = 5;
bool AdamOptimizer::has_eps() const {
  return this != internal_default_instance() && eps_ != NULL;
}
void AdamOptimizer::clear_eps() {
  if (GetArenaNoVirtual() == NULL && eps_ != NULL) delete eps_;
  eps_ = NULL;
}
const ::CoreML::Specification::DoubleParameter& AdamOptimizer::eps() const {
  // @@protoc_insertion_point(field_get:CoreML.Specification.AdamOptimizer.eps)
  return eps_ != NULL ? *eps_
                         : *::CoreML::Specification::DoubleParameter::internal_default_instance();
}
::CoreML::Specification::DoubleParameter* AdamOptimizer::mutable_eps() {
  
  if (eps_ == NULL) {
    eps_ = new ::CoreML::Specification::DoubleParameter;
  }
  // @@protoc_insertion_point(field_mutable:CoreML.Specification.AdamOptimizer.eps)
  return eps_;
}
::CoreML::Specification::DoubleParameter* AdamOptimizer::release_eps() {
  // @@protoc_insertion_point(field_release:CoreML.Specification.AdamOptimizer.eps)
  
  ::CoreML::Specification::DoubleParameter* temp = eps_;
  eps_ = NULL;
  return temp;
}
void AdamOptimizer::set_allocated_eps(::CoreML::Specification::DoubleParameter* eps) {
  delete eps_;
  eps_ = eps;
  if (eps) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:CoreML.Specification.AdamOptimizer.eps)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// @@protoc_insertion_point(namespace_scope)

}  // namespace Specification
}  // namespace CoreML

// @@protoc_insertion_point(global_scope)
